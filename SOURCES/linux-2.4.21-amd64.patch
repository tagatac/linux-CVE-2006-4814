diff -urNp linux-130/arch/x86_64/Makefile linux-180/arch/x86_64/Makefile
--- linux-130/arch/x86_64/Makefile
+++ linux-180/arch/x86_64/Makefile
@@ -66,11 +66,6 @@ SUBDIRS += arch/x86_64/ia32
 CORE_FILES += arch/x86_64/ia32/ia32.o
 endif
 
-ifdef CONFIG_HOSTFS
-SUBDIRS += arch/x86_64/hostfs
-core-$(CONFIG_HOSTFS) += arch/x86_64/hostfs/hostfs.o
-endif
-
 CORE_FILES += $(core-y)
 
 arch/x86_64/tools: dummy
diff -urNp linux-130/arch/x86_64/boot/setup.S linux-180/arch/x86_64/boot/setup.S
--- linux-130/arch/x86_64/boot/setup.S
+++ linux-180/arch/x86_64/boot/setup.S
@@ -346,6 +346,11 @@ long_mode_panic:
 	
 sse_ok:
 	popw	%ds
+
+# tell BIOS we want to go to long mode
+	movl  $0xec00,%eax	# declare target operating mode
+	movl  $2,%ebx		# long mode
+	int $0x15			
 	
 # Get memory size (extended mem, kB)
 
diff -urNp linux-130/arch/x86_64/defconfig linux-180/arch/x86_64/defconfig
--- linux-130/arch/x86_64/defconfig
+++ linux-180/arch/x86_64/defconfig
@@ -201,6 +201,7 @@ CONFIG_BLK_DEV_IDECD=y
 # CONFIG_BLK_DEV_IDEFLOPPY is not set
 # CONFIG_BLK_DEV_IDESCSI is not set
 # CONFIG_IDE_TASK_IOCTL is not set
+# CONFIG_IDE_TASKFILE_IO is not set
 
 #
 # IDE chipset support/bugfixes
diff -urNp linux-130/arch/x86_64/ia32/ia32_binfmt.c linux-180/arch/x86_64/ia32/ia32_binfmt.c
--- linux-130/arch/x86_64/ia32/ia32_binfmt.c
+++ linux-180/arch/x86_64/ia32/ia32_binfmt.c
@@ -264,9 +264,10 @@ int ia32_setup_arg_pages(struct linux_bi
 	{
 		mpnt->vm_mm = current->mm;
 		mpnt->vm_start = PAGE_MASK & (unsigned long) bprm->p;
-		mpnt->vm_end = IA32_STACK_TOP;
-		mpnt->vm_page_prot = PAGE_COPY_EXEC;
-		mpnt->vm_flags = VM_STACK_FLAGS;
+		mpnt->vm_end = IA32_STACK_TOP;		
+		mpnt->vm_flags = vm_stack_flags32; 
+		mpnt->vm_page_prot = (mpnt->vm_flags & VM_EXEC) ? 
+			PAGE_COPY_EXEC : PAGE_COPY;
 		mpnt->vm_ops = NULL;
 		mpnt->vm_pgoff = 0;
 		mpnt->vm_file = NULL;
diff -urNp linux-130/arch/x86_64/ia32/ia32entry.S linux-180/arch/x86_64/ia32/ia32entry.S
--- linux-130/arch/x86_64/ia32/ia32entry.S
+++ linux-180/arch/x86_64/ia32/ia32entry.S
@@ -52,6 +52,7 @@ ENTRY(ia32_cstar_target)
 ENTRY(ia32_syscall)
 	swapgs	
 	sti
+	mov %eax,%eax
 	pushq %rax
 	cld
 	SAVE_ARGS
@@ -355,18 +356,18 @@ ia32_sys_call_table:
 	.quad sys_ni_syscall    /* security */
 	.quad sys_gettid	
 	.quad sys_readahead	/* 225 */ 
-	.quad quiet_ni_syscall  /* xattr syscalls 226-237 */
-	.quad quiet_ni_syscall
-	.quad quiet_ni_syscall
-	.quad quiet_ni_syscall
-	.quad quiet_ni_syscall  /* 230 */
-	.quad quiet_ni_syscall  
-	.quad quiet_ni_syscall  
-	.quad quiet_ni_syscall  
-	.quad quiet_ni_syscall  
-	.quad quiet_ni_syscall  /* 235 */
-	.quad quiet_ni_syscall  
-	.quad quiet_ni_syscall  /* fremovexattr - 237 */
+	.quad sys_setxattr
+	.quad sys_lsetxattr
+	.quad sys_fsetxattr
+	.quad sys_getxattr
+	.quad sys_lgetxattr	/* 230 */
+	.quad sys_fgetxattr
+	.quad sys_listxattr
+	.quad sys_llistxattr
+	.quad sys_flistxattr
+	.quad sys_removexattr	/* 235 */
+	.quad sys_lremovexattr
+	.quad sys_fremovexattr
 	.quad sys_tkill
 	.quad sys_sendfile64
 	.quad quiet_ni_syscall  /* futex */
diff -urNp linux-130/arch/x86_64/ia32/sys_ia32.c linux-180/arch/x86_64/ia32/sys_ia32.c
--- linux-130/arch/x86_64/ia32/sys_ia32.c
+++ linux-180/arch/x86_64/ia32/sys_ia32.c
@@ -16,7 +16,7 @@
  *
  * This file assumes that there is a hole at the end of user address space.
  *
- * $Id: sys_ia32.c,v 1.54 2003/03/24 09:28:26 ak Exp $
+ * $Id: sys_ia32.c,v 1.56 2003/04/10 10:45:37 ak Exp $
  */
 
 #include <linux/config.h>
@@ -967,21 +967,22 @@ get_iovec32(struct iovec32 *iov32, struc
 	u32 buf, len;
 	struct iovec *ivp, *iov;
 	unsigned long totlen; 
+	u32 ccount = *count;
 
 	/* Get the "struct iovec" from user memory */
 
 	*errp = 0; 
-	if (!*count)
+	if (!ccount)
 		return 0;
 	*errp = -EINVAL;
-	if (*count > UIO_MAXIOV)
+	if (ccount > UIO_MAXIOV)
 		return(struct iovec *)0;
 	*errp = -EFAULT;
-	if(verify_area(VERIFY_READ, iov32, sizeof(struct iovec32)**count))
+	if(verify_area(VERIFY_READ, iov32, sizeof(struct iovec32)*ccount))
 		return(struct iovec *)0;
-	if (*count > UIO_FASTIOV) {
+	if (ccount > UIO_FASTIOV) {
 		*errp = -ENOMEM; 
-		iov = kmalloc(*count*sizeof(struct iovec), GFP_KERNEL);
+		iov = kmalloc(ccount*sizeof(struct iovec), GFP_KERNEL);
 		if (!iov)
 			return((struct iovec *)0);
 	} else
@@ -989,7 +990,7 @@ get_iovec32(struct iovec32 *iov32, struc
 
 	ivp = iov;
 	totlen = 0;
-	for (i = 0; i < *count; i++) {
+	for (i = 0; i < ccount; i++) {
 		*errp = __get_user(len, &iov32->iov_len) |
 		  	__get_user(buf, &iov32->iov_base);	
 		if (*errp)
@@ -1062,8 +1063,8 @@ sys32_writev(int fd, struct iovec32 *vec
 #define RESOURCE32(x) ((x > RLIM_INFINITY32) ? RLIM_INFINITY32 : x)
 
 struct rlimit32 {
-	int	rlim_cur;
-	int	rlim_max;
+	unsigned	rlim_cur;
+	unsigned	rlim_max;
 };
 
 extern asmlinkage long sys_getrlimit(unsigned int resource, struct rlimit *rlim);
@@ -1080,6 +1081,10 @@ sys32_getrlimit(unsigned int resource, s
 	ret = sys_getrlimit(resource, &r);
 	set_fs(old_fs);
 	if (!ret) {
+		if (r.rlim_cur >= 0xffffffff) 
+			r.rlim_cur = RLIM_INFINITY32;
+		if (r.rlim_max >= 0xffffffff) 
+			r.rlim_max = RLIM_INFINITY32;
 		if (verify_area(VERIFY_WRITE, rlim, sizeof(struct rlimit32)) ||
 		    __put_user(RESOURCE32(r.rlim_cur), &rlim->rlim_cur) ||
 		    __put_user(RESOURCE32(r.rlim_max), &rlim->rlim_max))
@@ -1099,9 +1104,13 @@ sys32_old_getrlimit(unsigned int resourc
 	
 	old_fs = get_fs();
 	set_fs(KERNEL_DS);
-	ret = sys_old_getrlimit(resource, &r);
+	ret = sys_getrlimit(resource, &r);
 	set_fs(old_fs);
 	if (!ret) {
+		if (r.rlim_cur >= 0x7fffffff) 
+			r.rlim_cur = RLIM_INFINITY32;
+		if (r.rlim_max >= 0x7fffffff) 
+			r.rlim_max = RLIM_INFINITY32;	
 		if (verify_area(VERIFY_WRITE, rlim, sizeof(struct rlimit32)) ||
 		    __put_user(r.rlim_cur, &rlim->rlim_cur) ||
 		    __put_user(r.rlim_max, &rlim->rlim_max))
@@ -1494,7 +1503,11 @@ struct sysinfo32 {
         u32 totalswap;
         u32 freeswap;
         unsigned short procs;
-        char _f[22];
+	unsigned short pad; 
+        u32 totalhigh;
+        u32 freehigh;
+        u32 mem_unit;
+        char _f[20-2*sizeof(u32)-sizeof(int)];
 };
 
 extern asmlinkage long sys_sysinfo(struct sysinfo *info);
@@ -1520,7 +1533,10 @@ sys32_sysinfo(struct sysinfo32 *info)
 	    __put_user (s.bufferram, &info->bufferram) ||
 	    __put_user (s.totalswap, &info->totalswap) ||
 	    __put_user (s.freeswap, &info->freeswap) ||
-	    __put_user (s.procs, &info->procs))
+	    __put_user (s.procs, &info->procs) ||
+	    __put_user (s.totalhigh, &info->totalhigh) || 
+	    __put_user (s.freehigh, &info->freehigh) ||
+	    __put_user (s.mem_unit, &info->mem_unit))
 		return -EFAULT;
 	return 0;
 }
@@ -2550,7 +2566,7 @@ struct exec_domain ia32_exec_domain = { 
 
 static int __init ia32_init (void)
 {
-	printk("IA32 emulation $Id: sys_ia32.c,v 1.54 2003/03/24 09:28:26 ak Exp $\n");  
+	printk("IA32 emulation $Id: sys_ia32.c,v 1.56 2003/04/10 10:45:37 ak Exp $\n");  
 	ia32_exec_domain.signal_map = default_exec_domain.signal_map;
 	ia32_exec_domain.signal_invmap = default_exec_domain.signal_invmap;
 	register_exec_domain(&ia32_exec_domain);
diff -urNp linux-130/arch/x86_64/kernel/bluesmoke.c linux-180/arch/x86_64/kernel/bluesmoke.c
--- linux-130/arch/x86_64/kernel/bluesmoke.c
+++ linux-180/arch/x86_64/kernel/bluesmoke.c
@@ -25,9 +25,6 @@ static unsigned long mce_cpus; 
 static int banks;
 static unsigned long ignored_banks, disabled_banks;
 
-/* Machine Check on everything dubious. This is a good setting
-   for device driver testing. */
-#define K8_DRIVER_DEBUG ((1<<13)-1)
 /* Report RAM errors and Hyper Transport Problems, but ignore Device
    aborts and GART errors. */
 #define K8_NORMAL_OP    0xff
@@ -125,9 +122,71 @@ static struct pci_dev *find_k8_nb(void)
 	return NULL;
 }
 
-static void check_k8_nb(void)
+static char *transaction[] = { 
+	"instruction", "data", "generic", "reserved"
+}; 
+static char *cachelevel[] = { 
+	"level 0", "level 1", "level 2", "level generic"
+};
+static char *memtrans[] = { 
+	"generic error", "generic read", "generic write", "data read",
+	"data write", "instruction fetch", "prefetch", "snoop",
+	"?", "?", "?", "?", "?", "?", "?"
+};
+static char *partproc[] = { 
+	"local node origin", "local node response", 
+	"local node observed", "generic" 
+};
+static char *timeout[] = { 
+	"request didn't time out",
+	"request timed out"
+};
+static char *memoryio[] = { 
+	"memory access", "res.", "i/o access", "generic"
+}; 
+static char *extendederr[] = { 
+	"ecc error", 
+	"crc error",
+	"sync error",
+	"mst abort",
+	"tgt abort",
+	"gart error",
+	"rmw error",
+	"wdog error",
+	"chipkill ecc error", 
+	"<9>","<10>","<11>","<12>",
+	"<13>","<14>","<15>"
+}; 
+static char *highbits[32] = { 
+	[31] = "previous error lost", 
+	[30] = "error overflow",
+	[29] = "error uncorrected",
+	[28] = "error enable",
+	[27] = "misc error valid",
+	[26] = "error address valid", 
+	[25] = "processor context corrupt", 
+	[24] = "res24",
+	[23] = "res23",
+	/* 22-15 ecc syndrome bits */
+	[14] = "corrected ecc error",
+	[13] = "uncorrected ecc error",
+	[12] = "res12",
+	[11] = "res11",
+	[10] = "res10",
+	[9] = "res9",
+	[8] = "dram scrub error", 
+	[7] = "res7",
+	/* 6-4 ht link number of error */ 
+	[3] = "res3",
+	[2] = "res2",
+	[1] = "err cpu0",
+	[0] = "err cpu1",
+};
+
+static void check_k8_nb(int header)
 {
 	struct pci_dev *nb;
+	unsigned long mc4_ctl;
 	nb = find_k8_nb(); 
 	if (nb == NULL)
 		return;
@@ -137,22 +196,68 @@ static void check_k8_nb(void)
 	pci_read_config_dword(nb, 0x4c, &statushigh);
 	if (!(statushigh & (1<<31)))
 		return;
-	printk(KERN_ERR "Northbridge status %08x%08x\n",
-	       statushigh,statuslow); 
-	if (statuslow & 0x10) 
-		printk(KERN_ERR "GART error %d\n", statuslow & 0xf); 
-	if (statushigh & (1<<31))
-		printk(KERN_ERR "Lost an northbridge error\n"); 
-	if (statushigh & (1<<25))
-		printk(KERN_EMERG "NB status: unrecoverable\n"); 
+
+	/* Check to see if this is a GART TLB error.  If so,
+	 * check to see if we want to bother with it...
+	 *
+	 * statuslow[15:0] == 0x001x indicates GART TLB error
+	 * mc4_ctl[10] enables GART TLB error reporting.
+	 */
+	rdmsrl(MSR_IA32_MC0_CTL+0x10, mc4_ctl);
+	if (((statuslow & 0xfff0) == 0x10) && !(mc4_ctl & (1<<10))) {
+
+		/* Clear the error */
+		statushigh &= ~(1<<31);
+		pci_write_config_dword(nb, 0x4c, statushigh);
+		return;
+	}
+
+	if (header) 
+		printk(KERN_ERR "CPU %d: Silent Northbridge MCE\n", smp_processor_id());
+
+	printk(KERN_ERR "Northbridge status %08x:%08x\n",
+	       statushigh,statuslow);
+
+	printk(KERN_ERR "    Error %s\n", extendederr[(statuslow >> 16) & 0xf]); 
+
+	unsigned short errcode = statuslow & 0xffff;	
+	switch ((statuslow >> 16) & 0xF) { 
+	case 5: 					
+		printk(KERN_ERR "    GART TLB error %s %s\n", 
+		       transaction[(errcode >> 2) & 3], 
+		       cachelevel[errcode & 3]);
+		break;
+	case 8:
+		printk(KERN_ERR "    ECC error syndrome %x\n", 
+		       (((statuslow >> 24) & 0xff)  << 8) | ((statushigh >> 15) & 0xff));
+		/*FALL THROUGH*/
+	default:
+		printk(KERN_ERR "    bus error %s, %s\n    %s\n    %s, %s\n",
+		       partproc[(errcode >> 9) & 0x3],
+		       timeout[(errcode >> 8) & 1],
+		       memtrans[(errcode >> 4) & 0xf],
+		       memoryio[(errcode >> 2) & 0x3], 
+		       cachelevel[(errcode & 0x3)]); 
+		/* should only print when it was a HyperTransport related error. */
+		printk(KERN_ERR "    link number %x\n", (statushigh >> 4) & 3);
+		break;
+	} 
+
+
+	int i;
+	for (i = 0; i < 32; i++) { 
+		if (i == 26 || i == 28) 
+			continue;
+		if (highbits[i] && (statushigh & (1<<i)))
+			printk(KERN_ERR "    %s\n", highbits[i]); 
+	}
+
 	if (statushigh & (1<<26)) { 
 		u32 addrhigh, addrlow; 
 		pci_read_config_dword(nb, 0x54, &addrhigh); 
 		pci_read_config_dword(nb, 0x50, &addrlow); 
-		printk(KERN_ERR "NB error address %08x%08x\n", addrhigh,addrlow); 
+		printk(KERN_ERR "    NB error address %08x%08x\n", addrhigh,addrlow); 
 	}
-	if (statushigh & (1<<29))
-		printk(KERN_EMERG "Error uncorrected\n"); 
 	statushigh &= ~(1<<31); 
 	pci_write_config_dword(nb, 0x4c, statushigh); 		
 }
@@ -164,9 +269,11 @@ static void k8_machine_check(struct pt_r
 	rdmsrl(MSR_IA32_MCG_STATUS, status); 
 	if ((status & (1<<2)) == 0) { 
 		if (!regs) 
-			check_k8_nb();
+			check_k8_nb(1);
 		return; 
 	}
+	printk(KERN_EMERG "CPU %d: Machine Check Exception: %016Lx\n", smp_processor_id(), status);
+
 	if (status & 1)
 		printk(KERN_EMERG "MCG_STATUS: unrecoverable\n"); 
 
@@ -184,7 +291,7 @@ static void k8_machine_check(struct pt_r
 	if (nbstatus & (1UL<57))
 		printk(KERN_EMERG "Unrecoverable condition\n"); 
 		
-	check_k8_nb();
+	check_k8_nb(0);
 
 	if (nbstatus & (1UL<<58)) { 
 		u64 adr;
@@ -195,9 +302,6 @@ static void k8_machine_check(struct pt_r
 	wrmsrl(MSR_IA32_MC0_STATUS+4*4, 0); 
 	wrmsrl(MSR_IA32_MCG_STATUS, 0);
        
-	if (regs && (status & (1<<1)))
-		printk(KERN_EMERG "MCE at EIP %lx ESP %lx\n", regs->rip, regs->rsp); 
-
  others:
 	generic_machine_check(regs, error_code); 
 } 
@@ -223,7 +327,7 @@ static void mcheck_timer_other(void *dat
 } 
 
 static void mcheck_timer_dist(void *data)
-{ 
+{ 	
 	smp_call_function(mcheck_timer_other,0,0,0);
 	k8_machine_check(NULL, 0); 
 	mcheck_timer.expires = jiffies + mcheck_interval;
@@ -245,7 +349,6 @@ static void __init k8_mcheck_init(struct
 {
 	u64 cap;
 	int i;
-	struct pci_dev *nb; 
 
 	if (!test_bit(X86_FEATURE_MCE, &c->x86_capability) || 
 	    !test_bit(X86_FEATURE_MCA, &c->x86_capability))
@@ -256,21 +359,14 @@ static void __init k8_mcheck_init(struct
 	machine_check_vector = k8_machine_check; 
 	for (i = 0; i < banks; i++) { 
 		u64 val = ((1UL<<i) & disabled_banks) ? 0 : ~0UL; 
+		if (val && i == 4)
+			val = k8_nb_flags;
 		wrmsrl(MSR_IA32_MC0_CTL+4*i, val);
 		wrmsrl(MSR_IA32_MC0_STATUS+4*i,0); 
 	}
-
-	nb = find_k8_nb(); 
-	if (nb != NULL) {
-		u32 reg, reg2;
-		pci_read_config_dword(nb, 0x40, &reg); 
-		pci_write_config_dword(nb, 0x40, k8_nb_flags);
-		pci_read_config_dword(nb, 0x44, &reg2);
-		pci_write_config_dword(nb, 0x44, reg2); 
-		printk(KERN_INFO "Machine Check for K8 Northbridge %d enabled (%x,%x)\n",
-		       nb->devfn, reg, reg2);
-		ignored_banks |= (1UL<<4); 
-	} 
+	
+	if (cap & (1<<8))
+		wrmsrl(MSR_IA32_MCG_CTL, 0xffffffffffffffffULL);
 
 	set_in_cr4(X86_CR4_MCE);	   	
 
@@ -368,7 +464,6 @@ static int __init mcheck_disable(char *s
    mce=nok8 disable k8 specific features
    mce=disable<NUMBER> disable bank NUMBER
    mce=enable<NUMBER> enable bank number
-   mce=device	Enable device driver test reporting in NB
    mce=NUMBER mcheck timer interval number seconds. 
    Can be also comma separated in a single mce= */
 static int __init mcheck_enable(char *str)
@@ -385,11 +480,9 @@ static int __init mcheck_enable(char *st
 			disabled_banks |= ~(1<<simple_strtol(p+7,NULL,0));
 		else if (!strcmp(p,"nok8"))
 			nok8 = 1;
-		else if (!strcmp(p,"device"))
-			k8_nb_flags = K8_DRIVER_DEBUG;
 	}
 	return 0;
 }
 
 __setup("nomce", mcheck_disable);
-__setup("mce", mcheck_enable);
+__setup("mce=", mcheck_enable);
diff -urNp linux-130/arch/x86_64/kernel/entry.S linux-180/arch/x86_64/kernel/entry.S
--- linux-130/arch/x86_64/kernel/entry.S
+++ linux-180/arch/x86_64/kernel/entry.S
@@ -351,7 +351,7 @@ iret_label:	
 	.section .fixup,"ax"
 	/* force a signal here? this matches i386 behaviour */
 bad_iret:
-	movq $-9999,%rdi	/* better code? */
+	movq $9,%rdi		# SIGKILL
 	jmp do_exit			
 	.previous	
 
@@ -436,6 +436,49 @@ ENTRY(\x)
 	jmp error_entry
 	.endm
 
+	.macro errorentry_stack sym
+	pushq %rax		/* save rax in RDI slot */
+	leaq  \sym(%rip),%rax
+	testl $3,CS-RDI(%rsp)	/* test for user mode in saved CS */
+	je error_entry		/* branch if we were in kernel mode */
+
+	/* here if user mode - we need to switch to per-thread kernel stack */
+	pushq %rsi			/* save rsi on old stack first */
+	swapgs
+	GET_CURRENT(%rax)
+	movq  tsk_thread+thread_rsp0(%rax),%rax
+	movq  %rsp,%rsi			/* save old stack ptr (at RSI level) */
+#define FRAME_SIZE 168
+	leaq  RSI-FRAME_SIZE(%rax),%rsp	/* switch to new stk (at RSI level) */
+	pushq %rdx
+	movq  8(%rsi),%rdx		/* load rax */
+	pushq %rcx
+	pushq %rdx			/* store rax */
+	pushq %r8
+	pushq %r9
+	pushq %r10
+	pushq %r11
+	cld
+	SAVE_REST			/* new stack is now at frame bottom */
+	xorl  %r15d,%r15d
+	movq  RSI-RSI(%rsi),%rdx	/* copy regs from old to new stack */
+	movq  %rdx,RSI(%rsp)
+	movq  ORIG_RAX-RSI(%rsi),%rdx
+	movq  %rdx,ORIG_RAX(%rsp)
+	movq  RIP-RSI(%rsi),%rdx
+	movq  %rdx,RIP(%rsp)
+	movq  CS-RSI(%rsi),%rdx
+	movq  %rdx,CS(%rsp)
+	movq  EFLAGS-RSI(%rsi),%rdx
+	movq  %rdx,EFLAGS(%rsp)
+	movq  RSP-RSI(%rsi),%rdx
+	movq  %rdx,RSP(%rsp)
+	movq  SS-RSI(%rsi),%rdx
+	movq  %rdx,SS(%rsp)
+	leaq  \sym(%rip),%rax		/* reload target function address */
+	jmp error_action		/* jump into common code */
+	.endm
+
 /*
  * Exception entry point. This expects an error code/orig_rax on the stack
  * and the exception handler in %rax.	
@@ -682,7 +725,7 @@ ENTRY(reserved)
 	zeroentry do_reserved
 
 ENTRY(double_fault)
-	errorentry do_double_fault	
+	errorentry_stack do_double_fault
 
 ENTRY(invalid_TSS)
 	errorentry do_invalid_TSS
@@ -691,7 +734,7 @@ ENTRY(segment_not_present)
 	errorentry do_segment_not_present
 
 ENTRY(stack_segment)
-	errorentry do_stack_segment
+	errorentry_stack do_stack_segment
 
 ENTRY(general_protection)
 	errorentry do_general_protection
diff -urNp linux-130/arch/x86_64/kernel/head.S linux-180/arch/x86_64/kernel/head.S
--- linux-130/arch/x86_64/kernel/head.S
+++ linux-180/arch/x86_64/kernel/head.S
@@ -6,7 +6,7 @@
  *  Copyright (C) 2000 Karsten Keil <kkeil@suse.de>
  *  Copyright (C) 2001,2002 Andi Kleen <ak@suse.de>
  *
- *  $Id: head.S,v 1.55 2003/02/11 12:29:15 ak Exp $
+ *  $Id: head.S,v 1.56 2003/05/12 14:38:43 ak Exp $
  */
 
 
@@ -340,7 +340,7 @@ gdt32_end:	
 .align 64 /* cacheline aligned, keep this synchronized with asm/desc.h */
 ENTRY(gdt_table)
 	.quad	0x0000000000000000	/* This one is magic */
-	.quad	0x0000000000000000	/* unused */
+	.quad	0x00af9a000000ffff ^ (1<<21)	/* __KERNEL_COMPAT32_CS */	
 	.quad	0x00af9a000000ffff	/* __KERNEL_CS */
 	.quad	0x00cf92000000ffff	/* __KERNEL_DS */
 	.quad	0x00cffe000000ffff	/* __USER32_CS */
diff -urNp linux-130/arch/x86_64/kernel/head64.c linux-180/arch/x86_64/kernel/head64.c
--- linux-130/arch/x86_64/kernel/head64.c
+++ linux-180/arch/x86_64/kernel/head64.c
@@ -25,8 +25,6 @@ static void __init clear_bss(void)
 	printk("ok\n");
 }
 
-extern char x86_boot_params[2048];
-
 #define NEW_CL_POINTER		0x228	/* Relative to real mode data */
 #define OLD_CL_MAGIC_ADDR	0x90020
 #define OLD_CL_MAGIC            0xA33F
@@ -40,7 +38,7 @@ static void __init copy_bootdata(char *r
 	int new_data;
 	char * command_line;
 
-	memcpy(x86_boot_params, real_mode_data, 2048); 
+	memcpy(x86_boot_params, real_mode_data, sizeof(x86_boot_params));
 	new_data = *(int *) (x86_boot_params + NEW_CL_POINTER);
 	if (!new_data) {
 		if (OLD_CL_MAGIC != * (u16 *) OLD_CL_MAGIC_ADDR) {
diff -urNp linux-130/arch/x86_64/kernel/ioport.c linux-180/arch/x86_64/kernel/ioport.c
--- linux-130/arch/x86_64/kernel/ioport.c
+++ linux-180/arch/x86_64/kernel/ioport.c
@@ -19,35 +19,13 @@
 /* Set EXTENT bits starting at BASE in BITMAP to value TURN_ON. */
 static void set_bitmap(unsigned long *bitmap, short base, short extent, int new_value)
 {
-	unsigned long mask;
-	unsigned long *bitmap_base = bitmap + base / sizeof(long);
-	unsigned low_index = base & (BITS_PER_LONG - 1);
-	int length = low_index + extent;
-
-	if (low_index != 0) {
-		mask = (~0UL << low_index);
-		if (length < 64)
-			mask &= ~(~0UL << length);
-		if (new_value)
-			*bitmap_base++ |= mask;
-		else
-			*bitmap_base++ &= ~mask;
-		length -= 64;
-	}
+	int i;
 
-	mask = (new_value ? ~0UL : 0UL);
-	while (length >= 64) {
-		*bitmap_base++ = mask;
-		length -= 64;
-	}
-
-	if (length > 0) {
-		mask = ~(~0UL << length);
+	for (i = base; i<base+extent; i++)
 		if (new_value)
-			*bitmap_base++ |= mask;
+			__set_bit(i, bitmap);
 		else
-			*bitmap_base++ &= ~mask;
-	}
+			clear_bit(i, bitmap);
 }
 
 /*
diff -urNp linux-130/arch/x86_64/kernel/irq.c linux-180/arch/x86_64/kernel/irq.c
--- linux-130/arch/x86_64/kernel/irq.c
+++ linux-180/arch/x86_64/kernel/irq.c
@@ -60,7 +60,7 @@ static inline void stack_overflow_check(
 
 	if (regs->rsp >= curbase && regs->rsp <= curbase + THREAD_SIZE &&
 	    regs->rsp <  curbase + sizeof(struct task_struct) + 128 && 
-	    warned + 60*HZ >= jiffies) { 
+	    time_after_eq(jiffies, warned + 60 * HZ)) { 
 		printk("do_IRQ: %s near stack overflow (cur:%Lx,rsp:%lx)\n",
 		       current->comm, curbase, regs->rsp); 
 		show_stack(NULL);
diff -urNp linux-130/arch/x86_64/kernel/mtrr.c linux-180/arch/x86_64/kernel/mtrr.c
--- linux-130/arch/x86_64/kernel/mtrr.c
+++ linux-180/arch/x86_64/kernel/mtrr.c
@@ -70,6 +70,9 @@
 
 #define MTRR_VERSION "2.02 (20020716)"
 
+#define MTRR_BEG_BIT 12
+#define MTRR_END_BIT 7
+
 #undef Dprintk
 
 #define Dprintk(...) 
@@ -192,8 +195,9 @@ static u64 size_or_mask, size_and_mask;
 
 static void get_mtrr (unsigned int reg, u64 *base, u32 *size, mtrr_type * type)
 {
-	u32 mask_lo, mask_hi, base_lo, base_hi;
-	u64 newsize;
+	u32 count, tmp, mask_lo, mask_hi;
+	int i;
+	u32 base_lo, base_hi;
 
 	rdmsr (MSR_MTRRphysMask(reg), mask_lo, mask_hi);
 	if ((mask_lo & 0x800) == 0) {
@@ -206,10 +210,16 @@ static void get_mtrr (unsigned int reg, 
 
 	rdmsr (MSR_MTRRphysBase(reg), base_lo, base_hi);
 
-	/* Work out the shifted address mask. */
-	newsize = (u64) mask_hi << 32 | (mask_lo & ~0x800);
-	newsize = ~newsize+1;
-	*size = (u32) newsize >> PAGE_SHIFT;
+	count = 0;
+	tmp = mask_lo >> MTRR_BEG_BIT;
+	for (i=MTRR_BEG_BIT; i <= 31; i++, tmp = tmp >> 1)
+		count = (count << (~tmp & 1)) | (~tmp & 1);
+
+	tmp = mask_hi;
+	for (i=0; i <= MTRR_END_BIT; i++, tmp = tmp >> 1)
+		count = (count << (~tmp & 1)) | (~tmp & 1);
+
+	*size = (count+1);
 	*base = base_hi << (32 - PAGE_SHIFT) | base_lo >> PAGE_SHIFT;
 	*type = base_lo & 0xff;
 }
@@ -243,7 +253,7 @@ static void set_mtrr_up (unsigned int re
 		base64 = (base << PAGE_SHIFT) & size_and_mask;
 		wrmsr (MSR_MTRRphysBase(reg), base64 | type, base64 >> 32);
 
-		size64 = ~((size << PAGE_SHIFT) - 1);
+		size64 = ~(((u64)size << PAGE_SHIFT) - 1);
 		size64 = size64 & size_and_mask;
 		wrmsr (MSR_MTRRphysMask(reg), (u32) (size64 | 0x800), (u32) (size64 >> 32));
 	}
diff -urNp linux-130/arch/x86_64/kernel/pci-gart.c linux-180/arch/x86_64/kernel/pci-gart.c
--- linux-130/arch/x86_64/kernel/pci-gart.c
+++ linux-180/arch/x86_64/kernel/pci-gart.c
@@ -65,6 +65,12 @@ extern int fallback_aper_force;
 static spinlock_t iommu_bitmap_lock = SPIN_LOCK_UNLOCKED;
 static unsigned long *iommu_gart_bitmap; /* guarded by iommu_bitmap_lock */
 
+/* Entry for a "guard page" that GART entries get mapped to when
+ * they're otherwise not in use.  Used to deal with over-eager
+ * prefetching in some PCI chipsets.
+ */
+static u32 gart_unmapped_entry;
+
 #define GPTE_VALID    1
 #define GPTE_COHERENT 2
 #define GPTE_ENCODE(x) (((x) & 0xfffff000) | (((x) >> 32) << 4) | GPTE_VALID | GPTE_COHERENT)
@@ -204,7 +210,7 @@ void pci_free_consistent(struct pci_dev 
 	int i;
 
 	size = round_up(size, PAGE_SIZE); 
-	if (bus < iommu_bus_base || bus > iommu_bus_base + iommu_size) { 
+	if (bus < iommu_bus_base || bus >= iommu_bus_base + iommu_size) { 
 		free_pages((unsigned long)vaddr, get_order(size)); 		
 		return;
 	} 
@@ -213,7 +219,7 @@ void pci_free_consistent(struct pci_dev 
 	for (i = 0; i < size; i++) {
 		pte = iommu_gatt_base[iommu_page + i];
 		BUG_ON((pte & GPTE_VALID) == 0); 
-		iommu_gatt_base[iommu_page + i] = 0; 		
+		iommu_gatt_base[iommu_page + i] = gart_unmapped_entry; 		
 		free_page((unsigned long) __va(GPTE_DECODE(pte)));
 	} 
 	flush_gart(); 
@@ -345,12 +351,12 @@ void pci_unmap_single(struct pci_dev *hw
 	unsigned long iommu_page; 
 	int i, npages;
 	if (dma_addr < iommu_bus_base + EMERGENCY_PAGES*PAGE_SIZE || 
-	    dma_addr > iommu_bus_base + iommu_size)
+	    dma_addr >= iommu_bus_base + iommu_size)
 		return;
 	iommu_page = (dma_addr - iommu_bus_base)>>PAGE_SHIFT;	
 	npages = round_up(size + (dma_addr & ~PAGE_MASK), PAGE_SIZE) >> PAGE_SHIFT;
 	for (i = 0; i < npages; i++) { 
-		iommu_gatt_base[iommu_page + i] = 0; 
+		iommu_gatt_base[iommu_page + i] = gart_unmapped_entry;
 #ifdef CONFIG_IOMMU_LEAK
 		if (iommu_leak_tab)
 			iommu_leak_tab[iommu_page + i] = 0; 
@@ -472,6 +478,8 @@ void __init pci_iommu_init(void)
 	agp_kern_info info;
 	unsigned long aper_size;
 	unsigned long iommu_start;
+	unsigned long scratch;
+	long i;
 
 #ifndef CONFIG_AGP
 	no_agp = 1; 
@@ -534,6 +542,26 @@ void __init pci_iommu_init(void)
 	iommu_gatt_base = agp_gatt_table + (iommu_start>>PAGE_SHIFT);
 	bad_dma_address = iommu_bus_base;
 
+	/*
+	 * Unmap the IOMMU part of the GART. The alias of the page is always mapped
+	 * with cache enabled and there is no full cache coherency across the GART
+	 * remapping. The unmapping avoids automatic prefetches from the CPU
+	 * allocating cache lines in there. All CPU accesses are done via the
+	 * direct mapping to the backing memory. The GART address is only used by PCI
+	 * devices.
+	 */
+	clear_kernel_mapping((unsigned long)__va(iommu_bus_base), iommu_size);
+
+	/*
+	 * Map all other entries to the guard page
+	 */
+	scratch = get_zeroed_page(GFP_KERNEL);
+	if (!scratch)
+		panic("Cannot allocate iommu scratch page");
+	gart_unmapped_entry = GPTE_ENCODE(__pa(scratch));
+	for (i = EMERGENCY_PAGES; i < iommu_pages; i++)
+		iommu_gatt_base[i] = gart_unmapped_entry;
+
 	asm volatile("wbinvd" ::: "memory");
 } 
 
diff -urNp linux-130/arch/x86_64/kernel/pci-pc.c linux-180/arch/x86_64/kernel/pci-pc.c
--- linux-130/arch/x86_64/kernel/pci-pc.c
+++ linux-180/arch/x86_64/kernel/pci-pc.c
@@ -304,28 +304,40 @@ static void __devinit pcibios_fixup_ghos
 static void __devinit pcibios_fixup_peer_bridges(void)
 {
 	int n;
-	struct pci_bus bus;
-	struct pci_dev dev;
+	struct pci_bus *bus;
+	struct pci_dev *dev;
 	u16 l;
 
 	if (pcibios_last_bus <= 0 || pcibios_last_bus >= 0xff)
 		return;
+
+	bus = kmalloc(sizeof(*bus), GFP_ATOMIC);
+	if (!bus)
+		return;
+	dev = kmalloc(sizeof(*dev), GFP_ATOMIC);
+	if (!dev) {
+		kfree(bus);
+		return;
+	}
+
 	DBG("PCI: Peer bridge fixup\n");
 	for (n=0; n <= pcibios_last_bus; n++) {
 		if (pci_bus_exists(&pci_root_buses, n))
 			continue;
-		bus.number = n;
-		bus.ops = pci_root_ops;
-		dev.bus = &bus;
-		for(dev.devfn=0; dev.devfn<256; dev.devfn += 8)
-			if (!pci_read_config_word(&dev, PCI_VENDOR_ID, &l) &&
+		bus->number = n;
+		bus->ops = pci_root_ops;
+		dev->bus = bus;
+		for(dev->devfn=0; dev->devfn<256; dev->devfn += 8)
+			if (!pci_read_config_word(dev, PCI_VENDOR_ID, &l) &&
 			    l != 0x0000 && l != 0xffff) {
-				DBG("Found device at %02x:%02x [%04x]\n", n, dev.devfn, l);
+				DBG("Found device at %02x:%02x [%04x]\n", n, dev->devfn, l);
 				printk(KERN_INFO "PCI: Discovered peer bus %02x\n", n);
 				pci_scan_bus(n, pci_root_ops, NULL);
 				break;
 			}
 	}
+	kfree(dev);
+	kfree(bus);
 }
 
 static void __devinit pci_scan_mptable(void)
@@ -335,11 +347,11 @@ static void __devinit pci_scan_mptable(v
 	/* Handle ACPI here */
 	if (!smp_found_config) { 
 		printk(KERN_WARNING "PCI: Warning: no mptable. Scanning busses upto 0xff\n"); 
-		pcibios_last_bus = 0xff; 
+		pcibios_last_bus = 0xfe; 
 		return;
 	} 
 
-	pcibios_last_bus = 0xff;
+	pcibios_last_bus = 0xfe;
 
 	for (i = 0; i < MAX_MP_BUSSES; i++) {
 		int n = mp_bus_id_to_pci_bus[i]; 
diff -urNp linux-130/arch/x86_64/kernel/ptrace.c linux-180/arch/x86_64/kernel/ptrace.c
--- linux-130/arch/x86_64/kernel/ptrace.c
+++ linux-180/arch/x86_64/kernel/ptrace.c
@@ -114,13 +114,13 @@ static int putreg(struct task_struct *ch
 			child->thread.es = value & 0xffff;
 			return 0;
 		case offsetof(struct user_regs_struct,fs_base):
-			if (!((value >> 48) == 0 || (value >> 48) == 0xffff))
-				return -EIO; 
+			if (value >= TASK_SIZE)
+				return -EIO;
 			child->thread.fs = value;
 			return 0;
 		case offsetof(struct user_regs_struct,gs_base):
-			if (!((value >> 48) == 0 || (value >> 48) == 0xffff))
-				return -EIO; 
+			if (value >= TASK_SIZE)
+				return -EIO;
 			child->thread.gs = value;
 			return 0;
 		case offsetof(struct user_regs_struct, eflags):
@@ -138,8 +138,20 @@ static int putreg(struct task_struct *ch
 			if ((value & 3) != 3)
 				return -EIO;
 			value &= 0xffff;
-            break;
-	}      
+			break;
+		case offsetof(struct user_regs_struct, rip):
+			/*
+			 * If the %rip value is bogus (noncanonical), then
+			 * sysretq can take a #GP fault in kernel mode.  We
+			 * can't afford to let it, because the trap won't
+			 * switch stacks and so would try to run on the
+			 * user stack.  So just disallow directly setting
+			 * any value in danger of being noncanonical.
+			 */
+			if (value >= TASK_SIZE)
+				return -EIO;
+			break;
+	}
 	put_stack_long(child, regno - sizeof(struct pt_regs), value);
 	return 0;
 }
diff -urNp linux-130/arch/x86_64/kernel/setup64.c linux-180/arch/x86_64/kernel/setup64.c
--- linux-130/arch/x86_64/kernel/setup64.c
+++ linux-180/arch/x86_64/kernel/setup64.c
@@ -3,7 +3,7 @@
  * Copyright (C) 1995  Linus Torvalds
  * Copyright 2001, 2002 SuSE Labs / Andi Kleen.
  * See setup.c for older changelog.
- * $Id: setup64.c,v 1.19 2003/02/21 19:37:21 ak Exp $
+ * $Id: setup64.c,v 1.23 2003/05/16 14:22:27 ak Exp $
  */ 
 #include <linux/config.h>
 #include <linux/init.h>
@@ -18,6 +18,7 @@
 #include <asm/atomic.h>
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
+#include <asm/mman.h>
 
 char x86_boot_params[2048] __initdata = {0,};
 
@@ -31,24 +32,82 @@ extern void ia32_cstar_target(void); 
 struct desc_ptr gdt_descr = { 0 /* filled in */, (unsigned long) gdt_table }; 
 struct desc_ptr idt_descr = { 256 * 16, (unsigned long) idt_table }; 
 
+/* When you change the default make sure the no EFER path below sets the 
+   correct flags everywhere. */
 unsigned long __supported_pte_mask = ~0UL; 
-static int do_not_nx = 1; 
-
+static int do_not_nx __initdata = 0;  
+unsigned long vm_stack_flags = __VM_STACK_FLAGS; 
+unsigned long vm_stack_flags32 = __VM_STACK_FLAGS; 
+unsigned long vm_data_default_flags = __VM_DATA_DEFAULT_FLAGS; 
+unsigned long vm_data_default_flags32 = __VM_DATA_DEFAULT_FLAGS; 
+unsigned long vm_force_exec32 = PROT_EXEC; 
+ 
 char boot_cpu_stack[IRQSTACKSIZE] __cacheline_aligned;
 
+/* noexec=on|off
+
+on	Enable
+off	Disable
+noforce (default) Don't enable by default for heap/stack/data, 
+	but allow PROT_EXEC to be effective
+
+*/ 
+
 static int __init nonx_setup(char *str)
 {
-	if (!strncmp(str,"off",3)) { 
-		__supported_pte_mask &= ~_PAGE_NX; 
-		do_not_nx = 1; 
-	} else if (!strncmp(str, "on",3)) { 
+	if (!strncmp(str, "on",3)) { 
+  		__supported_pte_mask |= _PAGE_NX; 
 		do_not_nx = 0; 
-		__supported_pte_mask |= _PAGE_NX; 
-	} 
+		vm_data_default_flags &= ~VM_EXEC; 
+		vm_stack_flags &= ~VM_EXEC;  
+	} else if (!strncmp(str, "noforce",7) || !strncmp(str,"off",3)) { 
+		do_not_nx = (str[0] == 'o');
+		if (do_not_nx) 
+			__supported_pte_mask &= ~_PAGE_NX; 
+		vm_data_default_flags |= VM_EXEC; 
+		vm_stack_flags |= VM_EXEC;
+	}
 	return 1;
 } 
 
+/* noexec32=opt{,opt} 
+
+Control the no exec default for 32bit processes. Can be also overwritten
+per executable using ELF header flags (e.g. needed for the X server)
+Requires noexec=on or noexec=noforce to be effective.
+
+Valid options: 
+   all,on    Heap,stack,data is non executable. 	
+   off       (default) Heap,stack,data is executable
+   stack     Stack is non executable, heap/data is.
+   force     Don't imply PROT_EXEC for PROT_READ 
+   compat    (default) Imply PROT_EXEC for PROT_READ
+
+*/
+static int __init nonx32_setup(char *str)
+{
+	char *s;
+	while ((s = strsep(&str, ",")) != NULL) { 
+		if (!strcmp(s, "all") || !strcmp(s,"on")) {
+			vm_data_default_flags32 &= ~VM_EXEC; 
+			vm_stack_flags32 &= ~VM_EXEC;  
+		} else if (!strcmp(s, "off")) { 
+			vm_data_default_flags32 |= VM_EXEC; 
+			vm_stack_flags32 |= VM_EXEC;  
+		} else if (!strcmp(s, "stack")) { 
+			vm_data_default_flags32 |= VM_EXEC; 
+			vm_stack_flags32 &= ~VM_EXEC;  		
+		} else if (!strcmp(s, "force")) { 
+			vm_force_exec32 = 0; 
+		} else if (!strcmp(s, "compat")) { 
+			vm_force_exec32 = PROT_EXEC;
+		} 
+  	} 
+  	return 1;
+} 
+
 __setup("noexec=", nonx_setup); 
+__setup("noexec32=", nonx32_setup);
 
 void pda_init(int cpu)
 { 
@@ -144,16 +203,15 @@ void __init cpu_init (void)
 	wrmsrl(MSR_CSTAR, ia32_cstar_target); 
 #endif
 
-		rdmsrl(MSR_EFER, efer); 
-	if (!(efer & EFER_NX) || do_not_nx) { 
-			__supported_pte_mask &= ~_PAGE_NX; 
-		} 
+	rdmsrl(MSR_EFER, efer);
+	if (!(efer & EFER_NX) || do_not_nx)
+		__supported_pte_mask &= ~_PAGE_NX;
 
 	t->io_map_base = INVALID_IO_BITMAP_OFFSET;	
 	memset(t->io_bitmap, 0xff, sizeof(t->io_bitmap));
 
 	/* Flags to clear on syscall */
-	wrmsrl(MSR_SYSCALL_MASK, EF_TF|EF_DF|EF_IE); 
+	wrmsrl(MSR_SYSCALL_MASK, EF_TF|EF_DF|EF_IE|EF_NT);
 
 	wrmsrl(MSR_FS_BASE, 0);
 	wrmsrl(MSR_KERNEL_GS_BASE, 0);
diff -urNp linux-130/arch/x86_64/kernel/time.c linux-180/arch/x86_64/kernel/time.c
--- linux-130/arch/x86_64/kernel/time.c
+++ linux-180/arch/x86_64/kernel/time.c
@@ -63,6 +63,11 @@ static unsigned int do_gettimeoffset_hpe
 	return ((hpet_readl(HPET_COUNTER) - vxtime.last) * vxtime.quot) >> 32;
 }
 
+static unsigned int do_gettimeoffset_nop(void)
+{
+	return 0;
+}
+
 unsigned int (*do_gettimeoffset)(void) = do_gettimeoffset_tsc;
 
 /*
@@ -80,9 +85,9 @@ void do_gettimeofday(struct timeval *tv)
 		sequence = __vxtime_sequence[1];
 		rmb();
 
-	sec = xtime.tv_sec;
-	usec = xtime.tv_usec
-		+ (jiffies - wall_jiffies) * tick
+		sec = xtime.tv_sec;
+		usec = xtime.tv_usec
+			+ (jiffies - wall_jiffies) * tick
 			+ do_gettimeoffset();
 
 		rmb(); 
@@ -549,8 +554,14 @@ void __init time_init_smp(void)
 			do_gettimeoffset = do_gettimeoffset_tsc;
 		}		
 	} else {
+		if (notsc) {
+			timetype = "PIT";
+			vxtime.mode = VXTIME_STUPID;
+			do_gettimeoffset = do_gettimeoffset_nop;
+		} else {
 			timetype = "PIT/TSC";
 			vxtime.mode = VXTIME_TSC;
+		}
 	}
 	printk(KERN_INFO "time.c: Using %s based timekeeping.\n", timetype);
 }
diff -urNp linux-130/arch/x86_64/kernel/traps.c linux-180/arch/x86_64/kernel/traps.c
--- linux-130/arch/x86_64/kernel/traps.c
+++ linux-180/arch/x86_64/kernel/traps.c
@@ -153,9 +153,10 @@ unsigned long *in_exception_stack(int cp
 { 
 	int k;
 	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
-		unsigned long end = init_tss[cpu].ist[k] + EXCEPTION_STKSZ; 
+		unsigned long end = init_tss[cpu].ist[k];
+		unsigned long start = end - EXCEPTION_STKSZ;
 
-		if (stack >= init_tss[cpu].ist[k]  && stack <= end) 
+		if (stack >= start && stack < end)
 			return (unsigned long *)end;
 	}
 	return 0;
@@ -276,6 +277,14 @@ void show_stack(unsigned long * rsp)
 	show_trace((unsigned long *)rsp);
 }
 
+/*
+ * The architecture-independent backtrace generator
+ */
+void dump_stack(void)
+{
+	show_stack((unsigned long *)0);
+}
+
 void show_registers(struct pt_regs *regs)
 {
 	int i;
@@ -837,7 +846,7 @@ void __init trap_init(void)
 	set_intr_gate(19,&simd_coprocessor_error);
 
 #ifdef CONFIG_IA32_EMULATION
-	set_intr_gate(IA32_SYSCALL_VECTOR, ia32_syscall);
+	set_system_gate(IA32_SYSCALL_VECTOR, ia32_syscall);
 #endif
 
 	/*
diff -urNp linux-130/arch/x86_64/kernel/x8664_ksyms.c linux-180/arch/x86_64/kernel/x8664_ksyms.c
--- linux-130/arch/x86_64/kernel/x8664_ksyms.c
+++ linux-180/arch/x86_64/kernel/x8664_ksyms.c
@@ -65,6 +65,7 @@ EXPORT_SYMBOL_NOVERS(__up_wakeup);
 EXPORT_SYMBOL(csum_partial_copy_nocheck);
 /* Delay loops */
 EXPORT_SYMBOL(__udelay);
+EXPORT_SYMBOL(__ndelay);
 EXPORT_SYMBOL(__delay);
 EXPORT_SYMBOL(__const_udelay);
 
@@ -195,13 +196,9 @@ EXPORT_SYMBOL(copy_to_user);
 EXPORT_SYMBOL(copy_user_generic);
 
 /* Export kernel syscalls */
-EXPORT_SYMBOL(sys_wait4);
 EXPORT_SYMBOL(sys_exit);
-EXPORT_SYMBOL(sys_write);
-EXPORT_SYMBOL(sys_read);
 EXPORT_SYMBOL(sys_open);
 EXPORT_SYMBOL(sys_lseek);
-EXPORT_SYMBOL(sys_dup);
 EXPORT_SYMBOL(sys_delete_module);
 EXPORT_SYMBOL(sys_sync);
 EXPORT_SYMBOL(sys_pause);
diff -urNp linux-130/arch/x86_64/lib/csum-partial.c linux-180/arch/x86_64/lib/csum-partial.c
--- linux-130/arch/x86_64/lib/csum-partial.c
+++ linux-180/arch/x86_64/lib/csum-partial.c
@@ -141,6 +141,6 @@ unsigned csum_partial(const unsigned cha
  */
 unsigned short ip_compute_csum(unsigned char * buff, int len)
 {
-	return ~csum_partial(buff,len,0); 
+	return csum_fold(csum_partial(buff,len,0)); 
 }
 
diff -urNp linux-130/arch/x86_64/mm/fault.c linux-180/arch/x86_64/mm/fault.c
--- linux-130/arch/x86_64/mm/fault.c
+++ linux-180/arch/x86_64/mm/fault.c
@@ -106,7 +106,8 @@ int exception_trace = 1;
  *	bit 0 == 0 means no page found, 1 means protection fault
  *	bit 1 == 0 means read, 1 means write
  *	bit 2 == 0 means kernel, 1 means user-mode
- *      bit 3 == 1 means fault was an instruction fetch
+ *	bit 3 == 0 means PTE is okay, 1 means PTE had reserved bit(s) set
+ *	bit 4 == 0 means data access, 1 means instruction fetch
  */
 asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code)
 {
@@ -158,6 +159,23 @@ asmlinkage void do_page_fault(struct pt_
 	if (in_interrupt() || !mm)
 		goto no_context;
 
+	/*
+	 * When running a 32-bit-compatibility-mode application, it's
+	 * possible under unusual circumstances to incur an instruction
+	 * fetch fault for an address with upper bits set, despite the
+	 * saved EIP value being correct (with all upper bits clear).
+	 * If this occurs, it's sufficient to simply return to user-mode
+	 * at the saved EIP value.  If the target page is non-resident,
+	 * a 2nd fault would occur, but the faulting address would be
+	 * correct (allowing the 2nd fault to be processed normally).
+	 */
+	if (unlikely(regs->cs == __USER32_CS) &&
+	    (address >> 32) != 0UL &&
+	    (regs->rip >> 32) == 0UL &&
+	    (error_code & 0x1e) == 0x14 &&
+	    (tsk->thread.flags & THREAD_IA32))
+		return;
+
 again:
 	down_read(&mm->mmap_sem);
 
diff -urNp linux-130/arch/x86_64/mm/init.c linux-180/arch/x86_64/mm/init.c
--- linux-130/arch/x86_64/mm/init.c
+++ linux-180/arch/x86_64/mm/init.c
@@ -41,28 +41,6 @@ mmu_gather_t mmu_gathers[NR_CPUS];
 
 static unsigned long totalram_pages;
 
-int do_check_pgt_cache(int low, int high)
-{
-	int freed = 0;
-	if(read_pda(pgtable_cache_sz) > high) {
-		do {
-			if (read_pda(pgd_quick)) {
-				pgd_free_slow(pgd_alloc_one_fast());
-				freed++;
-			}
-			if (read_pda(pmd_quick)) {
-				pmd_free_slow(pmd_alloc_one_fast(NULL, 0));
-				freed++;
-			}
-			if (read_pda(pte_quick)) {
-				pte_free_slow(pte_alloc_one_fast(NULL, 0));
-				freed++;
-			}
-		} while(read_pda(pgtable_cache_sz) > low);
-	}
-	return freed;
-}
-
 #ifndef CONFIG_DISCONTIGMEM
 /*
  * NOTE: pagetable_init alloc all the fixmap pagetables contiguous on the
@@ -514,3 +492,32 @@ void free_bootmem_generic(unsigned long 
 	free_bootmem(phys, len);    
 #endif
 }
+
+/* Unmap a kernel mapping if it exists. This is useful to avoid prefetches
+   from the CPU leading to inconsistent cache lines. address and size
+   must be aligned to 2MB boundaries.
+   Does nothing when the mapping doesn't exist. */
+void __init clear_kernel_mapping(unsigned long address, unsigned long size)
+{
+	unsigned long end = address + size;
+
+	BUG_ON(address & ~LARGE_PAGE_MASK);
+	BUG_ON(size & ~LARGE_PAGE_MASK);
+
+	for (; address < end; address += LARGE_PAGE_SIZE) {
+		pgd_t *pgd = pgd_offset_k(address);
+		if (!pgd || pgd_none(*pgd))
+			continue;
+		pmd_t *pmd = pmd_offset(pgd, address);
+		if (!pmd || pmd_none(*pmd))
+			continue;
+		if (0 == (pmd_val(*pmd) & _PAGE_PSE)) {
+			/* Could handle this, but it should not happen currently. */
+			printk(KERN_ERR
+		"clear_kernel_mapping: mapping has been split. will leak memory\n");
+			pmd_ERROR(*pmd);
+		}
+		set_pmd(pmd, __pmd(0));
+	}
+	__flush_tlb_all();
+}
diff -urNp linux-130/arch/x86_64/tools/offset.c linux-180/arch/x86_64/tools/offset.c
--- linux-130/arch/x86_64/tools/offset.c
+++ linux-180/arch/x86_64/tools/offset.c
@@ -59,6 +59,7 @@ int main(void) 
 
 
 	outconst("#define thread_flags %0" , offsetof(struct thread_struct, flags));
+	outconst("#define thread_rsp0 %0" , offsetof(struct thread_struct, rsp0));
 	outconst("#define ASM_THREAD_IA32 %0", THREAD_IA32);
 
 	outconst("#define PER_CPU_GDT_SIZE %0", sizeof(struct per_cpu_gdt)); 
diff -urNp linux-130/include/asm-x86_64/bitops.h linux-180/include/asm-x86_64/bitops.h
--- linux-130/include/asm-x86_64/bitops.h
+++ linux-180/include/asm-x86_64/bitops.h
@@ -6,6 +6,7 @@
  */
 
 #include <linux/config.h>
+#include <linux/compiler.h>
 
 /*
  * These have to be done with inline assembly: that way the bit-setting
@@ -38,7 +39,7 @@ static __inline__ void set_bit(long nr, 
 	__asm__ __volatile__( LOCK_PREFIX
 		"btsq %1,%0"
 		:"=m" (ADDR)
-		:"dIr" (nr));
+		:"dIr" (nr) : "memory");
 }
 
 /**
@@ -52,10 +53,10 @@ static __inline__ void set_bit(long nr, 
  */
 static __inline__ void __set_bit(long nr, volatile void * addr)
 {
-	__asm__(
+	__asm__ volatile(
 		"btsq %1,%0"
 		:"=m" (ADDR)
-		:"dIr" (nr));
+		:"dIr" (nr) : "memory");
 }
 
 /**
@@ -73,8 +74,17 @@ static __inline__ void clear_bit(long nr
 	__asm__ __volatile__( LOCK_PREFIX
 		"btrq %1,%0"
 		:"=m" (ADDR)
-		:"dIr" (nr));
+		:"dIr" (nr) : "memory");
 }
+
+static __inline__ void __clear_bit(long nr, volatile void * addr)
+{
+	__asm__ __volatile__(
+		"btrq %1,%0"
+		:"=m" (ADDR)
+		:"dIr" (nr) : "memory");
+}
+
 #define smp_mb__before_clear_bit()	barrier()
 #define smp_mb__after_clear_bit()	barrier()
 
@@ -97,7 +107,7 @@ static __inline__ void __change_bit(long
 
 /**
  * change_bit - Toggle a bit in memory
- * @nr: Bit to clear
+ * @nr: Bit to change
  * @addr: Address to start counting from
  *
  * change_bit() is atomic and may not be reordered.
@@ -153,7 +163,7 @@ static __inline__ int __test_and_set_bit
 
 /**
  * test_and_clear_bit - Clear a bit and return its old value
- * @nr: Bit to set
+ * @nr: Bit to clear
  * @addr: Address to count from
  *
  * This operation is atomic and cannot be reordered.  
@@ -172,7 +182,7 @@ static __inline__ int test_and_clear_bit
 
 /**
  * __test_and_clear_bit - Clear a bit and return its old value
- * @nr: Bit to set
+ * @nr: Bit to clear
  * @addr: Address to count from
  *
  * This operation is non-atomic and can be reordered.  
@@ -204,7 +214,7 @@ static __inline__ int __test_and_change_
 
 /**
  * test_and_change_bit - Change a bit and return its new value
- * @nr: Bit to set
+ * @nr: Bit to change
  * @addr: Address to count from
  *
  * This operation is atomic and cannot be reordered.  
@@ -227,7 +237,7 @@ static __inline__ int test_and_change_bi
  * @nr: bit number to test
  * @addr: Address to start counting from
  */
-static int test_bit(int nr, const volatile void * addr);
+static int test_bit(long nr, const volatile void * addr);
 #endif
 
 static __inline__ int constant_test_bit(long nr, const volatile void * addr)
@@ -235,7 +245,7 @@ static __inline__ int constant_test_bit(
 	return ((1UL << (nr & 31)) & (((const volatile unsigned int *) addr)[nr >> 5])) != 0;
 }
 
-static __inline__ int variable_test_bit(long nr, volatile void * addr)
+static __inline__ int variable_test_bit(long nr, volatile const void * addr)
 {
 	long oldbit;
 
@@ -251,13 +261,15 @@ static __inline__ int variable_test_bit(
  constant_test_bit((nr),(addr)) : \
  variable_test_bit((nr),(addr)))
 
+#undef ADDR
+
 /**
  * find_first_zero_bit - find the first zero bit in a memory region
  * @addr: The address to start the search at
- * @size: The maximum bitnumber to search
+ * @size: The maximum size to search
  *
  * Returns the bit-number of the first zero bit, not the number of the byte
- * containing a bit. -1 when none found.
+ * containing a bit.
  */
 static __inline__ int find_first_zero_bit(void * addr, unsigned size)
 {
@@ -290,28 +302,88 @@ static __inline__ int find_first_zero_bi
  */
 static __inline__ int find_next_zero_bit (void * addr, int size, int offset)
 {
+	unsigned long * p = ((unsigned long *) addr) + (offset >> 6);
+	unsigned long set = 0;
+	unsigned long res, bit = offset&63;
+	
+	if (bit) {
+		/*
+		 * Look for zero in first word
+		 */
+		__asm__("bsfq %1,%0\n\t"
+			"cmoveq %2,%0"
+			: "=r" (set)
+			: "r" (~(*p >> bit)), "r"(64L));
+		if (set < (64 - bit))
+			return set + offset;
+		set = 64 - bit;
+		p++;
+	}
+	/*
+	 * No zero yet, search remaining full words for a zero
+	 */
+	res = find_first_zero_bit (p, size - 64 * (p - (unsigned long *) addr));
+	return (offset + set + res);
+}
+
+
+/**
+ * find_first_bit - find the first set bit in a memory region
+ * @addr: The address to start the search at
+ * @size: The maximum size to search
+ *
+ * Returns the bit-number of the first set bit, not the number of the byte
+ * containing a bit.
+ */
+static __inline__ int find_first_bit(void * addr, unsigned size)
+{
+	int d0, d1;
+	int res;
+
+	/* This looks at memory. Mark it volatile to tell gcc not to move it around */
+	__asm__ __volatile__(
+		"xorl %%eax,%%eax\n\t"
+		"repe; scasl\n\t"
+		"jz 1f\n\t"
+		"leaq -4(%%rdi),%%rdi\n\t"
+		"bsfq (%%rdi),%%rax\n"
+		"1:\tsubl %%ebx,%%edi\n\t"
+		"shll $3,%%edi\n\t"
+		"addl %%edi,%%eax"
+		:"=a" (res), "=&c" (d0), "=&D" (d1)
+		:"1" ((size + 31) >> 5), "2" (addr), "b" (addr));
+	return res;
+}
+
+
+/**
+ * find_next_bit - find the first set bit in a memory region
+ * @addr: The address to base the search on
+ * @offset: The bitnumber to start searching at
+ * @size: The maximum size to search
+ */
+static __inline__ int find_next_bit(void * addr, int size, int offset)
+{
 	unsigned int * p = ((unsigned int *) addr) + (offset >> 5);
 	int set = 0, bit = offset & 31, res;
 	
 	if (bit) {
 		/*
-		 * Look for zero in first byte
+		 * Look for nonzero in the first 32 bits:
 		 */
 		__asm__("bsfl %1,%0\n\t"
-			"jne 1f\n\t"
-			"movl $32, %0\n"
-			"1:"
+			"cmovel %2,%0\n\t"
 			: "=r" (set)
-			: "r" (~(*p >> bit)));
+			: "r" (*p >> bit), "r" (32));
 		if (set < (32 - bit))
 			return set + offset;
 		set = 32 - bit;
 		p++;
 	}
 	/*
-	 * No zero yet, search remaining full bytes for a zero
+	 * No set bit yet, search remaining full words for a bit
 	 */
-	res = find_first_zero_bit (p, size - 32 * (p - (unsigned int *) addr));
+	res = find_first_bit (p, size - 32 * (p - (unsigned int *) addr));
 	return (offset + set + res);
 }
 
@@ -370,9 +442,8 @@ static __inline__ int ffs(int x)
 	int r;
 
 	__asm__("bsfl %1,%0\n\t"
-		"jnz 1f\n\t"
-		"movl $-1,%0\n"
-		"1:" : "=r" (r) : "g" (x));
+		"cmovzl %2,%0" 
+		: "=r" (r) : "rm" (x), "r" (-1));
 	return r+1;
 }
 
@@ -404,6 +475,9 @@ static __inline__ int ffs(int x)
 #define minix_test_bit(nr,addr) test_bit(nr,addr)
 #define minix_find_first_zero_bit(addr,size) find_first_zero_bit(addr,size)
 
+/* find last set bit */
+#define fls(x) generic_fls(x)
+
 #endif /* __KERNEL__ */
 
 #endif /* _X86_64_BITOPS_H */
diff -urNp linux-130/include/asm-x86_64/desc.h linux-180/include/asm-x86_64/desc.h
--- linux-130/include/asm-x86_64/desc.h
+++ linux-180/include/asm-x86_64/desc.h
@@ -99,12 +99,12 @@ static inline void _set_gate(void *adr, 
 
 static inline void set_intr_gate(int nr, void *func) 
 { 
-	_set_gate(&idt_table[nr], GATE_INTERRUPT, (unsigned long) func, 3, 0); 
+	_set_gate(&idt_table[nr], GATE_INTERRUPT, (unsigned long) func, 0, 0); 
 } 
 
 static inline void set_intr_gate_ist(int nr, void *func, unsigned ist) 
 { 
-	_set_gate(&idt_table[nr], GATE_INTERRUPT, (unsigned long) func, 3, ist); 
+	_set_gate(&idt_table[nr], GATE_INTERRUPT, (unsigned long) func, 0, ist); 
 } 
 
 static inline void set_system_gate(int nr, void *func) 
diff -urNp linux-130/include/asm-x86_64/i387.h linux-180/include/asm-x86_64/i387.h
--- linux-130/include/asm-x86_64/i387.h
+++ linux-180/include/asm-x86_64/i387.h
@@ -120,7 +120,7 @@ static inline void kernel_fpu_begin(void
 
 static inline void save_init_fpu( struct task_struct *tsk )
 {
-	asm volatile( "fxsave %0 ; fnclex"
+	asm volatile( "rex64; fxsave %0 ; fnclex"
 		      : "=m" (tsk->thread.i387.fxsave));
 	tsk->flags &= ~PF_USEDFPU;
 	stts();
diff -urNp linux-130/include/asm-x86_64/ioctls.h linux-180/include/asm-x86_64/ioctls.h
--- linux-130/include/asm-x86_64/ioctls.h
+++ linux-180/include/asm-x86_64/ioctls.h
@@ -67,6 +67,7 @@
 #define TIOCGICOUNT	0x545D	/* read serial port inline interrupt counts */
 #define TIOCGHAYESESP   0x545E  /* Get Hayes ESP configuration */
 #define TIOCSHAYESESP   0x545F  /* Set Hayes ESP configuration */
+#define FIOQSIZE	0x5460
 
 /* Used for packet mode */
 #define TIOCPKT_DATA		 0
diff -urNp linux-130/include/asm-x86_64/irq.h linux-180/include/asm-x86_64/irq.h
--- linux-130/include/asm-x86_64/irq.h
+++ linux-180/include/asm-x86_64/irq.h
@@ -36,4 +36,8 @@ extern void disable_irq(unsigned int);
 extern void disable_irq_nosync(unsigned int);
 extern void enable_irq(unsigned int);
 
+#ifdef CONFIG_X86_LOCAL_APIC
+#define ARCH_HAS_NMI_WATCHDOG		/* See include/linux/nmi.h */
+#endif
+
 #endif /* _ASM_IRQ_H */
diff -urNp linux-130/include/asm-x86_64/page.h linux-180/include/asm-x86_64/page.h
--- linux-130/include/asm-x86_64/page.h
+++ linux-180/include/asm-x86_64/page.h
@@ -62,9 +62,13 @@ typedef struct { unsigned long pgprot; }
 #define __pte(x) ((pte_t) { (x) } )
 #define __pmd(x) ((pmd_t) { (x) } )
 #define __pgd(x) ((pgd_t) { (x) } )
-#define __level4(x) ((level4_t) { (x) } )
+#define __pml4(x) ((pml4_t) { (x) } )
 #define __pgprot(x)	((pgprot_t) { (x) } )
  
+extern unsigned long vm_stack_flags, vm_stack_flags32;
+extern unsigned long vm_data_default_flags, vm_data_default_flags32;
+extern unsigned long vm_force_exec32;
+
 #endif /* !__ASSEMBLY__ */
 
 /* to align the pointer to the (next) page boundary */
@@ -87,9 +91,9 @@ struct bug_frame { 
        char *filename;    /* should use 32bit offset instead, but the assembler doesn't like it */ 
        unsigned short line; 
 } __attribute__((packed)); 
-#define BUG() asm volatile("ud2 ; .quad %c1 ; .short %c0" :: "i"(__LINE__), \
+#define BUG() asm volatile("ud2 ; .quad %P1 ; .short %P0" :: "i"(__LINE__), \
 		"i" (__stringify(KBUILD_BASENAME)))
-#define HEADER_BUG() asm volatile("ud2 ; .quad %c1 ; .short %c0" :: "i"(__LINE__), \
+#define HEADER_BUG() asm volatile("ud2 ; .quad %P1 ; .short %P0" :: "i"(__LINE__), \
 		"i" (__stringify(__FILE__)))
 #define PAGE_BUG(page) BUG()
 
@@ -114,31 +118,41 @@ extern __inline__ int get_order(unsigned
 /* Note: __pa(&symbol_visible_to_c) should be always replaced with __pa_symbol.
    Otherwise you risk miscompilation. */ 
 #define __pa(x)			(((unsigned long)(x)>=__START_KERNEL_map)?(unsigned long)(x) - (unsigned long)__START_KERNEL_map:(unsigned long)(x) - PAGE_OFFSET)
-/* __pa_symbol should use for C visible symbols, but only for them. 
+/* __pa_symbol should be used for C visible symbols.
    This seems to be the official gcc blessed way to do such arithmetic. */ 
 #define __pa_symbol(x)		\
 	({unsigned long v;  \
 	  asm("" : "=r" (v) : "0" (x)); \
-	 v - __START_KERNEL_map; })
-#define __pa_maybe_symbol(x)		\
-	({unsigned long v;  \
-	  asm("" : "=r" (v) : "0" (x)); \
 	  __pa(v); })
 
 #define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
 #ifndef CONFIG_DISCONTIGMEM
-#define virt_to_page(kaddr)	(mem_map + (__pa(kaddr) >> PAGE_SHIFT))
-#define pfn_to_page(pfn)	(mem_map + (pfn)) 
-#define page_to_pfn(page)   ((page) - mem_map)
 #define page_to_phys(page)	(((page) - mem_map) << PAGE_SHIFT)
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define pfn_to_page(pfn)	(mem_map + (pfn))
+#define page_to_pfn(page)	((unsigned long)((page) - mem_map))
+#define pfn_valid(pfn)		((pfn) < max_mapnr)
 #define VALID_PAGE(page)	(((page) - mem_map) < max_mapnr)
+#else
+#include <asm/mmzone.h>
 #endif
 
+#define pfn_to_phys(pfn)	((unsigned long)(pfn) << PAGE_SHIFT)
 #define phys_to_pfn(phys)	((phys) >> PAGE_SHIFT)
 
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
+#define __VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define __VM_STACK_FLAGS 	(VM_GROWSDOWN | VM_READ | VM_WRITE | VM_EXEC | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC | \
+				 VM_ACCOUNT)
+
+#define VM_DATA_DEFAULT_FLAGS \
+	((current->thread.flags & THREAD_IA32) ? vm_data_default_flags32 : \
+	  vm_data_default_flags) 
+#define VM_STACK_FLAGS	vm_stack_flags
 
 #endif /* __KERNEL__ */
 
diff -urNp linux-130/include/asm-x86_64/pci.h linux-180/include/asm-x86_64/pci.h
--- linux-130/include/asm-x86_64/pci.h
+++ linux-180/include/asm-x86_64/pci.h
@@ -6,6 +6,9 @@
 
 #ifdef __KERNEL__
 
+#include <linux/mm.h> /* for struct page */
+
+
 extern dma_addr_t bad_dma_address;
 
 /* Can be used to override the logic in pci_scan_bus for skipping
@@ -37,6 +40,10 @@ int pcibios_set_irq_routing(struct pci_d
 
 struct pci_dev;
 
+extern int iommu_setup(char *opt);
+
+extern void pci_iommu_init(void);
+
 /* Allocate and map kernel buffer using consistent mode DMA for a device.
  * hwdev should be valid struct pci_dev pointer for PCI devices,
  * NULL for PCI-like buses (ISA, EISA).
diff -urNp linux-130/include/asm-x86_64/pgtable.h linux-180/include/asm-x86_64/pgtable.h
--- linux-130/include/asm-x86_64/pgtable.h
+++ linux-180/include/asm-x86_64/pgtable.h
@@ -433,6 +433,7 @@ extern int change_page_attr(struct page 
  */
 extern void __map_kernel_range(void *, int, pgprot_t);
 #define map_kernel_range(adr,size) __map_kernel_range(adr,size,PAGE_KERNEL_LARGE)
+extern void clear_kernel_mapping(unsigned long addr, unsigned long size);
 
 #endif /* !__ASSEMBLY__ */
 
diff -urNp linux-130/include/asm-x86_64/processor.h linux-180/include/asm-x86_64/processor.h
--- linux-130/include/asm-x86_64/processor.h
+++ linux-180/include/asm-x86_64/processor.h
@@ -77,15 +77,17 @@ extern struct cpuinfo_x86 cpu_data[];
 #define current_cpu_data boot_cpu_data
 #endif
 
-#define cpu_has_pge 1
-#define cpu_has_pse 1
-#define cpu_has_pae 1
-#define cpu_has_tsc 1
-#define cpu_has_de 1
-#define cpu_has_vme 1
-#define cpu_has_fxsr 1
-#define cpu_has_xmm 1
-#define cpu_has_apic 1
+#define cpu_has_pge	(test_bit(X86_FEATURE_PGE,  boot_cpu_data.x86_capability))
+#define cpu_has_pse	(test_bit(X86_FEATURE_PSE,  boot_cpu_data.x86_capability))
+#define cpu_has_pae	(test_bit(X86_FEATURE_PAE,  boot_cpu_data.x86_capability))
+#define cpu_has_tsc	(test_bit(X86_FEATURE_TSC,  boot_cpu_data.x86_capability))
+#define cpu_has_de	(test_bit(X86_FEATURE_DE,   boot_cpu_data.x86_capability))
+#define cpu_has_vme	(test_bit(X86_FEATURE_VME,  boot_cpu_data.x86_capability))
+#define cpu_has_fxsr	(test_bit(X86_FEATURE_FXSR, boot_cpu_data.x86_capability))
+#define cpu_has_xmm	(test_bit(X86_FEATURE_XMM,  boot_cpu_data.x86_capability))
+#define cpu_has_fpu	(test_bit(X86_FEATURE_FPU,  boot_cpu_data.x86_capability))
+#define cpu_has_apic	(test_bit(X86_FEATURE_APIC, boot_cpu_data.x86_capability))
+#define cpu_has_nx	(test_bit(X86_FEATURE_NX, boot_cpu_data.x86_capability))
 
 extern char ignore_irq13;
 
@@ -404,7 +406,8 @@ extern inline void sync_core(void)
 	asm volatile("cpuid" : "=a" (tmp) : "0" (1) : "ebx","ecx","edx","memory");
 } 
 
-#define cpu_has_fpu 1
+#define USE_PREFETCH 0
+#if USE_PREFETCH
 
 #define ARCH_HAS_PREFETCH
 #define ARCH_HAS_PREFETCHW
@@ -413,6 +416,9 @@ extern inline void sync_core(void)
 #define prefetch(x) __builtin_prefetch((x),0,1)
 #define prefetchw(x) __builtin_prefetch((x),1,1)
 #define spin_lock_prefetch(x)  prefetchw(x)
+
+#endif /* USE_PREFETCH */
+
 #define cpu_relax()   rep_nop()
 
 
diff -urNp linux-130/include/asm-x86_64/smp.h linux-180/include/asm-x86_64/smp.h
--- linux-130/include/asm-x86_64/smp.h
+++ linux-180/include/asm-x86_64/smp.h
@@ -7,7 +7,6 @@
 #ifndef __ASSEMBLY__
 #include <linux/config.h>
 #include <linux/threads.h>
-#include <linux/ptrace.h>
 #endif
 
 #ifdef CONFIG_X86_LOCAL_APIC
diff -urNp linux-130/include/asm-x86_64/spinlock.h linux-180/include/asm-x86_64/spinlock.h
--- linux-130/include/asm-x86_64/spinlock.h
+++ linux-180/include/asm-x86_64/spinlock.h
@@ -138,6 +138,8 @@ typedef struct {
 
 #define rwlock_init(x)	do { *(x) = RW_LOCK_UNLOCKED; } while(0)
 
+#define rwlock_is_locked(x) ((x)->lock != RW_LOCK_BIAS)
+
 /*
  * On x86, we implement read-write locks as a 32-bit counter
  * with the high bit (sign) being the "contended" bit.
diff -urNp linux-130/include/asm-x86_64/system.h linux-180/include/asm-x86_64/system.h
--- linux-130/include/asm-x86_64/system.h
+++ linux-180/include/asm-x86_64/system.h
@@ -3,6 +3,7 @@
 
 #include <linux/config.h>
 #include <linux/kernel.h>
+#include <linux/init.h>
 #include <asm/segment.h>
 
 #ifdef __KERNEL__
@@ -274,6 +275,14 @@ static inline unsigned long __cmpxchg(vo
 #define local_irq_disable()	__cli()
 #define local_irq_enable()	__sti()
 
+#define irqs_disabled()			\
+({					\
+	unsigned long flags;		\
+	__save_flags(flags);		\
+	!(flags & (1<<9));		\
+})
+
+
 #ifdef CONFIG_SMP
 
 extern void __global_cli(void);
diff -urNp linux-130/include/asm-x86_64/unistd.h linux-180/include/asm-x86_64/unistd.h
--- linux-130/include/asm-x86_64/unistd.h
+++ linux-180/include/asm-x86_64/unistd.h
@@ -497,8 +497,14 @@ __SYSCALL(__NR_remap_file_pages, sys_ni_
 __SYSCALL(__NR_getdents64, sys_getdents64)
 #define __NR_exit_group 231
 __SYSCALL(__NR_exit_group, sys_exit_group)
+#define __NR_UNDEF_1 232
+__SYSCALL(__NR_UNDEF_1, sys_ni_syscall)
+#define __NR_UNDEF_2 233
+__SYSCALL(__NR_UNDEF_2, sys_ni_syscall)
+#define __NR_tgkill 234
+__SYSCALL(__NR_tgkill, sys_tgkill)
 
-#define __NR_syscall_max __NR_exit_group
+#define __NR_syscall_max __NR_tgkill
 
 #ifndef __NO_STUBS
 
@@ -631,7 +637,7 @@ static inline pid_t setsid(void)
 	return sys_setsid();
 }
 
-extern ssize_t sys_write(unsigned int, char *, size_t);
+extern ssize_t sys_write(unsigned int, const char *, size_t);
 static inline ssize_t write(unsigned int fd, char * buf, size_t count)
 {
 	return sys_write(fd, buf, count);
diff -urNp linux-130/include/asm-x86_64/vsyscall.h linux-180/include/asm-x86_64/vsyscall.h
--- linux-130/include/asm-x86_64/vsyscall.h
+++ linux-180/include/asm-x86_64/vsyscall.h
@@ -30,6 +30,7 @@ struct vxtime_data {
 	int mode;
 };
 
+#define VXTIME_STUPID	0
 #define VXTIME_TSC	1
 #define	VXTIME_HPET	2
 
diff -urNp linux-130/mm/bootmem.c linux-180/mm/bootmem.c
--- linux-130/mm/bootmem.c
+++ linux-180/mm/bootmem.c
@@ -190,9 +190,10 @@ restart_scan:
 		}
 		start = i;
 		goto found;
-	fail_block:;
+fail_block:
+		i = ALIGN(j, incr);
 	}
-	if (preferred) {
+	if (preferred > offset) {
 		preferred = offset;
 		goto restart_scan;
 	}
