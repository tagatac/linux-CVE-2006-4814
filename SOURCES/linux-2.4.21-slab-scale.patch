diff -urNp linux-7065/mm/slab.c linux-7070/mm/slab.c
--- linux-7065/mm/slab.c
+++ linux-7070/mm/slab.c
@@ -74,6 +74,7 @@
 #include	<linux/init.h>
 #include	<linux/compiler.h>
 #include	<linux/seq_file.h>
+#include	<linux/bootmem.h>
 #include	<asm/uaccess.h>
 
 /*
@@ -97,12 +98,6 @@
 #define	FORCED_DEBUG	0
 #endif
 
-/*
- * Parameters for kmem_cache_reap
- */
-#define REAP_SCANLEN	10
-#define REAP_PERFECT	10
-
 /* Shouldn't this be in a header file somewhere? */
 #define	BYTES_PER_WORD		sizeof(void *)
 
@@ -759,6 +754,9 @@ cal_wastage:
 
 		if ((left_over*8) <= (PAGE_SIZE<<cachep->gfporder))
 			break;	/* Acceptable internal fragmentation. */
+
+		if (size < PAGE_SIZE)
+			break;  /* no larger than single-page slabs for kernel structs */
 next:
 		cachep->gfporder++;
 	} while (1);
@@ -1514,17 +1512,21 @@ static inline void __kmem_cache_free (km
 
 	CHECK_PAGE(virt_to_page(objp));
 	if (cc) {
-		int batchcount;
+		int tofree;
 		if (cc->avail < cc->limit) {
 			STATS_INC_FREEHIT(cachep);
 			cc_entry(cc)[cc->avail++] = objp;
 			return;
 		}
 		STATS_INC_FREEMISS(cachep);
-		batchcount = cachep->batchcount;
-		cc->avail -= batchcount;
-		free_block(cachep,
-					&cc_entry(cc)[cc->avail],batchcount);
+		tofree = cachep->batchcount;
+		cc->avail -= tofree;
+		/*
+		 * True LIFO - zap the cache-cold entries:
+		 */
+		free_block(cachep, &cc_entry(cc)[0],tofree);
+		memmove(&cc_entry(cc)[0],
+				&cc_entry(cc)[tofree],sizeof(void*)*cc->avail);
 		cc_entry(cc)[cc->avail++] = objp;
 		return;
 	} else {
@@ -1710,19 +1712,35 @@ oom:
 static void enable_cpucache (kmem_cache_t *cachep)
 {
 	int err;
-	int limit;
+	int limit, scale;
 
-	/* FIXME: optimize */
+	/* higher-order objects are rare. */
 	if (cachep->objsize > PAGE_SIZE)
 		return;
+	/*
+	 * Up to 256 MB of lowmem RAM we use a scale of 1,
+	 * then for the next 256 MB of RAM a scale of 2 ...
+	 * 4 GB lowmem means a scale of 16.
+	 */
+	scale = min(max_low_pfn, num_physpages) / (256*1024*1024 / PAGE_SIZE) + 1;
+
 	if (cachep->objsize > 1024)
-		limit = 60;
+		limit = 60 * scale;
 	else if (cachep->objsize > 256)
-		limit = 124;
+		limit = 124 * scale;
 	else
-		limit = 252;
+		limit = 252 * scale;
+
 
-	err = kmem_tune_cpucache(cachep, limit, limit/2);
+	/*
+	 * Cap at less than 128KB/sizeof(void *) due to limits in
+	 * kmalloc.
+	 */
+
+	if (limit > (128000/sizeof(void *)))
+		limit = 128000/sizeof(void *);
+
+	err = kmem_tune_cpucache(cachep, limit, limit/4);
 	if (err)
 		printk(KERN_ERR "enable_cpucache failed for %s, error %d.\n",
 					cachep->name, -err);
@@ -1768,7 +1786,6 @@ int kmem_cache_reap (int gfp_mask)
 		if (down_trylock(&cache_chain_sem))
 			return 0;
 
-	scan = REAP_SCANLEN;
 	best_len = 0;
 	best_pages = 0;
 	best_cachep = NULL;
@@ -1824,17 +1841,12 @@ int kmem_cache_reap (int gfp_mask)
 			best_cachep = searchp;
 			best_len = full_free;
 			best_pages = pages;
-			if (pages >= REAP_PERFECT) {
-				clock_searchp = list_entry(searchp->next.next,
-							kmem_cache_t,next);
-				goto perfect;
-			}
 		}
 next_unlock:
 		spin_unlock_irq(&searchp->spinlock);
 next:
 		searchp = list_entry(searchp->next.next,kmem_cache_t,next);
-	} while (--scan && searchp != clock_searchp);
+	} while (searchp != clock_searchp);
 
 	clock_searchp = searchp;
 
@@ -1843,7 +1855,6 @@ next:
 		goto out;
 
 	spin_lock_irq(&best_cachep->spinlock);
-perfect:
 	/* free only 50% of the free slabs */
 	best_len = (best_len + 1)/2;
 	for (scan = 0; scan < best_len; scan++) {
