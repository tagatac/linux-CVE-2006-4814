diff -urNp linux-5980/drivers/md/md.c linux-5990/drivers/md/md.c
--- linux-5980/drivers/md/md.c
+++ linux-5990/drivers/md/md.c
@@ -34,6 +34,8 @@
 #include <linux/sysctl.h>
 #include <linux/raid/xor.h>
 #include <linux/devfs_fs_kernel.h>
+#include <linux/poll.h>
+#include <linux/rwsem.h>
 
 #include <linux/init.h>
 
@@ -130,16 +132,69 @@ static struct gendisk md_gendisk=
 
 /*
  * Enables to iterate over all existing md arrays
+ *
+ * Locking rules:
+ * - access to all_mddevs requires all_mddevs_sem.
+ * - an mddev can be locked while all_mddevs_sem is held.
+ * - When removing an mddev, we
+ *     lock the mddev
+ *     check that ->active is 1 (us).
+ *     set "dying"
+ *     unlock the mddev
+ *     claim all_mddevs_sem
+ *     actually remove device
+ *     release all_mddevs_sem
+ * - to get a reference to an mddev, we
+ *     claim all_mddevs_sem
+ *     find the mddev in the list
+ *     check that it isn't "dying"
+ *     increase ->active or take a lock
  */
 static MD_LIST_HEAD(all_mddevs);
+static DECLARE_RWSEM(all_mddevs_sem);
 
 /*
- * The mapping between kdev and mddev is not necessary a simple
+ * The mapping between kdev and mddev is not necessarily a simple
  * one! Eg. HSM uses several sub-devices to implement Logical
  * Volumes. All these sub-devices map to the same mddev.
  */
 dev_mapping_t mddev_map[MAX_MD_DEVS];
 
+
+static inline mddev_t * kdev_to_mddev (kdev_t dev)
+{
+	mddev_t *mddev;
+	if (MAJOR(dev) != MD_MAJOR)
+		BUG();
+	down_read(&all_mddevs_sem);
+	mddev = mddev_map[MINOR(dev)].mddev;
+	if (mddev &&  !mddev->dying)
+		atomic_inc(&mddev->active);
+	else
+		mddev = NULL;
+	up_read(&all_mddevs_sem);
+	return mddev;
+}
+
+static inline mddev_t * kdev_to_mddev_lock_interruptible (kdev_t dev, int *err)
+{
+	mddev_t *mddev;
+	if (MAJOR(dev) != MD_MAJOR)
+		BUG();
+	down_read(&all_mddevs_sem);
+	mddev = mddev_map[MINOR(dev)].mddev;
+	*err = 0;
+	if (mddev) {
+		if (mddev->dying) {
+			*err = -EBUSY;
+			mddev = NULL;
+		} else
+			*err = lock_mddev_interruptible(mddev);
+	}
+	up_read(&all_mddevs_sem);
+	return mddev;
+}
+
 void add_mddev_mapping(mddev_t * mddev, kdev_t dev, void *data)
 {
 	unsigned int minor = MINOR(dev);
@@ -175,13 +230,22 @@ void del_mddev_mapping(mddev_t * mddev, 
 static int md_make_request(request_queue_t *q, int rw, struct buffer_head * bh)
 {
 	mddev_t *mddev = kdev_to_mddev(bh->b_rdev);
+	int rv;
 
 	if (mddev && mddev->pers)
-		return mddev->pers->make_request(mddev, rw, bh);
+		rv =  mddev->pers->make_request(mddev, rw, bh);
 	else {
 		buffer_IO_error(bh);
-		return 0;
+		rv = 0;
 	}
+	if (mddev)
+		/* should really drop count when request completes... */
+		/* Any time we are resyncing and not mounted, this will
+		 * be 0, so don't BUG(); on 0 
+		if (atomic_dec_and_test(&mddev->active))
+			BUG(); */
+		atomic_dec(&mddev->active);
+	return rv;
 }
 
 static mddev_t * alloc_mddev(kdev_t dev)
@@ -199,26 +263,76 @@ static mddev_t * alloc_mddev(kdev_t dev)
 	memset(mddev, 0, sizeof(*mddev));
 
 	mddev->__minor = MINOR(dev);
-	init_MUTEX(&mddev->reconfig_sem);
+	init_MUTEX_LOCKED(&mddev->reconfig_sem);
 	init_MUTEX(&mddev->recovery_sem);
 	init_MUTEX(&mddev->resync_sem);
 	MD_INIT_LIST_HEAD(&mddev->disks);
 	MD_INIT_LIST_HEAD(&mddev->all_mddevs);
-	atomic_set(&mddev->active, 0);
+	atomic_set(&mddev->active, 1);
 
 	/*
 	 * The 'base' mddev is the one with data NULL.
 	 * personalities can create additional mddevs
 	 * if necessary.
 	 */
+	down_write(&all_mddevs_sem);
 	add_mddev_mapping(mddev, dev, 0);
 	md_list_add(&mddev->all_mddevs, &all_mddevs);
+	up_write(&all_mddevs_sem);
 
 	MOD_INC_USE_COUNT;
 
 	return mddev;
 }
 
+
+/*
+ * We have a system wide 'event count' that is incremented
+ * on any 'interesting' event, and readers of /proc/mdstat
+ * can use 'poll' or 'select' to find out when the event
+ * count increases
+ *
+ * Events are:
+ *  start array, stop array, error, add device, remove device,
+ *  start build, activate spare
+ */
+DECLARE_WAIT_QUEUE_HEAD(md_event_waiters);
+static atomic_t md_event_count;
+int md_new_event(void)
+{
+	atomic_inc(&md_event_count);
+	wake_up(&md_event_waiters);
+	return atomic_read(&md_event_count);
+}
+struct mdstat_info {
+	struct list_head list;	/* all active files linked together */
+	unsigned long	seen, reading, acknowledged;
+};
+static LIST_HEAD(readers);
+static spinlock_t readers_lock = SPIN_LOCK_UNLOCKED;
+
+/*
+ * WARNING: There is no guarantee that this will ever return true.  If a
+ * daemon were to signal that it wants to read events, then get an event
+ * signal, then go into some sort of infinite loop (possibly intentionally)
+ * where it didn't ever call poll again but also didn't close the file
+ * handle, then this forever return false.  That in turn makes every place
+ * that does wait_event(md_event_waiters, md_event_reached(some_event)); a
+ * possible hang condition.
+ */
+int md_event_reached(unsigned long ev)
+{
+	/* returns true when all readers have acknowledged event 'ev' */
+	struct mdstat_info *mi;
+	int rv  = 1;
+	spin_lock(&readers_lock);
+	list_for_each_entry(mi, &readers, list)
+		if (mi->reading > 0 && mi->acknowledged < ev)
+			rv = 0;
+	spin_unlock(&readers_lock);
+	return rv;
+}
+
 mdk_rdev_t * find_rdev_nr(mddev_t *mddev, int nr)
 {
 	mdk_rdev_t * rdev;
@@ -744,18 +858,10 @@ static void free_mddev(mddev_t *mddev)
 	md_size[mdidx(mddev)] = 0;
 	md_hd_struct[mdidx(mddev)].nr_sects = 0;
 
-	/*
-	 * Make sure nobody else is using this mddev
-	 * (careful, we rely on the global kernel lock here)
-	 */
-	while (sem_getcount(&mddev->resync_sem) != 1)
-		schedule();
-	while (sem_getcount(&mddev->recovery_sem) != 1)
-		schedule();
-
+	down_write(&all_mddevs_sem);
 	del_mddev_mapping(mddev, MKDEV(MD_MAJOR, mdidx(mddev)));
 	md_list_del(&mddev->all_mddevs);
-	MD_INIT_LIST_HEAD(&mddev->all_mddevs);
+	up_write(&all_mddevs_sem);
 	kfree(mddev);
 	MOD_DEC_USE_COUNT;
 }
@@ -826,7 +932,9 @@ void md_print_devices(void)
 	printk("md:	**********************************\n");
 	printk("md:	* <COMPLETE RAID STATE PRINTOUT> *\n");
 	printk("md:	**********************************\n");
-	ITERATE_MDDEV(mddev,tmp) {
+
+	down_read(&all_mddevs_sem);
+	ITERATE_MDDEV/*_LOCK*/(mddev,tmp) {
 		printk("md%d: ", mdidx(mddev));
 
 		ITERATE_RDEV(mddev,rdev,tmp2)
@@ -841,6 +949,7 @@ void md_print_devices(void)
 		ITERATE_RDEV(mddev,rdev,tmp2)
 			print_rdev(rdev);
 	}
+	up_read(&all_mddevs_sem);
 	printk("md:	**********************************\n");
 	printk("\n");
 }
@@ -921,10 +1030,6 @@ static int write_disk_sb(mdk_rdev_t * rd
 		MD_BUG();
 		return 1;
 	}
-	if (rdev->faulty) {
-		MD_BUG();
-		return 1;
-	}
 	if (rdev->sb->md_magic != MD_SB_MAGIC) {
 		MD_BUG();
 		return 1;
@@ -1006,13 +1111,18 @@ static int sync_sbs(mddev_t * mddev)
 
 int md_update_sb(mddev_t * mddev)
 {
-	int err, count = 100;
+	int err, count = 100, unlock = 0;
 	struct md_list_head *tmp;
 	mdk_rdev_t *rdev;
 
+	if (!down_trylock(&mddev->reconfig_sem)) {
+		printk("hm, md_update_sb() called while unlocked from %p.\n", __builtin_return_address(0));
+		unlock = 1;
+	}
+		
 	if (!mddev->sb_dirty) {
 		printk("hm, md_update_sb() called without ->sb_dirty == 1, from %p.\n", __builtin_return_address(0));
-		return 0;
+		goto out;
 	}
 	mddev->sb_dirty = 0;
 repeat:
@@ -1036,7 +1146,7 @@ repeat:
 	 * nonpersistent superblocks
 	 */
 	if (mddev->sb->not_persistent)
-		return 0;
+		goto out;
 
 	printk(KERN_INFO "md: updating md%d RAID superblock on device\n",
 					mdidx(mddev));
@@ -1046,20 +1156,18 @@ repeat:
 		printk(KERN_INFO "md: ");
 		if (rdev->faulty)
 			printk("(skipping faulty ");
-		if (rdev->alias_device)
+		else if (rdev->alias_device)
 			printk("(skipping alias ");
-		if (!rdev->faulty && disk_faulty(&rdev->sb->this_disk)) {
-			printk("(skipping new-faulty %s )\n",
-			       partition_name(rdev->dev));
-			continue;
-		}
-		printk("%s ", partition_name(rdev->dev));
-		if (!rdev->faulty && !rdev->alias_device) {
-			printk("[events: %08lx]",
+		else if (disk_faulty(&rdev->sb->this_disk))
+			printk("(skipping new-faulty ");
+		else {
+			printk("%s [events: %08lx]\n",
+				partition_name(rdev->dev),
 				(unsigned long)rdev->sb->events_lo);
 			err += write_disk_sb(rdev);
-		} else
-			printk(")\n");
+			continue;
+		}
+		printk("%s)\n", partition_name(rdev->dev));
 	}
 	if (err) {
 		if (--count) {
@@ -1068,6 +1176,9 @@ repeat:
 		}
 		printk(KERN_ERR "md: excessive errors occurred during superblock update, exiting\n");
 	}
+out:
+	if (unlock)
+		unlock_mddev(mddev);
 	return 0;
 }
 
@@ -1271,6 +1382,47 @@ static int analyze_sbs(mddev_t * mddev)
 	memcpy (sb, freshest->sb, sizeof(*sb));
 
 	/*
+	 * For multipathing, lots of things are different from "true"
+	 * RAIDs.
+	 * All rdev's could be read, so they are no longer faulty.
+	 * As there is just one sb, trying to find changed devices via the
+	 * this_disk pointer is useless too.
+	 *
+	 * lmb@suse.de, 2002-09-12
+	 */
+
+	if (sb->level == -4) {
+		int desc_nr = 0;
+
+		/* ... and initialize from the current rdevs instead */
+		ITERATE_RDEV(mddev,rdev,tmp) {
+			mdp_disk_t *desc;
+
+			rdev->desc_nr=desc_nr;
+
+			desc = &sb->disks[rdev->desc_nr];
+
+			desc->number = desc_nr;
+			desc->major = MAJOR(rdev->dev);
+			desc->minor = MINOR(rdev->dev);
+			desc->raid_disk = desc_nr;
+
+			/* We could read from it, so it isn't faulty
+			 * any longer */
+			if (disk_faulty(desc))
+				mark_disk_spare(desc);
+
+			memcpy(&rdev->sb->this_disk,desc,sizeof(*desc));
+
+			desc_nr++;
+		}
+
+		/* Kick out all old info about disks we used to have,
+		 * if any */
+		for (i = desc_nr; i < MD_SB_DISKS; i++)
+			memset(&(sb->disks[i]),0,sizeof(mdp_disk_t));
+	} else {
+		/*
 	 * at this point we have picked the 'best' superblock
 	 * from all available superblocks.
 	 * now we validate this superblock and kick out possibly
@@ -1296,7 +1448,6 @@ static int analyze_sbs(mddev_t * mddev)
 	 * Fix up changed device names ... but only if this disk has a
 	 * recent update time. Use faulty checksum ones too.
 	 */
-	if (mddev->sb->level != -4)
 	ITERATE_RDEV(mddev,rdev,tmp) {
 		__u64 ev1, ev2, ev3;
 		if (rdev->faulty || rdev->alias_device) {
@@ -1348,13 +1499,8 @@ static int analyze_sbs(mddev_t * mddev)
 
 		/*
 		 * We kick faulty devices/descriptors immediately.
-		 *
-		 * Note: multipath devices are a special case.  Since we
-		 * were able to read the superblock on the path, we don't
-		 * care if it was previously marked as faulty, it's up now
-		 * so enable it.
 		 */
-		if (disk_faulty(desc) && mddev->sb->level != -4) {
+			if (disk_faulty(desc)) {
 			found = 0;
 			ITERATE_RDEV(mddev,rdev,tmp) {
 				if (rdev->desc_nr != desc->number)
@@ -1373,15 +1519,6 @@ static int analyze_sbs(mddev_t * mddev)
 			}
 			remove_descriptor(desc, sb);
 			continue;
-		} else if (disk_faulty(desc)) {
-			/*
-			 * multipath entry marked as faulty, unfaulty it
-			 */
-			rdev = find_rdev(mddev, dev);
-			if(rdev)
-				mark_disk_spare(desc);
-			else
-				remove_descriptor(desc, sb);
 		}
 
 		if (dev == MKDEV(0,0))
@@ -1391,17 +1528,6 @@ static int analyze_sbs(mddev_t * mddev)
 		 */
 		found = 0;
 		ITERATE_RDEV(mddev,rdev,tmp) {
-			/*
-			 * Multi-path IO special-case: since we have no
-			 * this_disk descriptor at auto-detect time,
-			 * we cannot check rdev->number.
-			 * We can check the device though.
-			 */
-			if ((sb->level == -4) && (rdev->dev ==
-					MKDEV(desc->major,desc->minor))) {
-				found = 1;
-				break;
-			}
 			if (rdev->desc_nr == desc->number) {
 				found = 1;
 				break;
@@ -1414,6 +1540,7 @@ static int analyze_sbs(mddev_t * mddev)
 		       mdidx(mddev), partition_name(dev));
 		remove_descriptor(desc, sb);
 	}
+	}
 
 	/*
 	 * Double check wether all devices mentioned in the
@@ -1743,6 +1870,8 @@ static int do_md_run(mddev_t * mddev)
 	mddev->sb_dirty = 1;
 	md_update_sb(mddev);
 
+	md_new_event();
+
 	/*
 	 * md_size has units of 1K blocks, which are
 	 * twice as large as sectors.
@@ -1823,6 +1952,9 @@ static int do_md_stop(mddev_t * mddev, i
 		if (mddev->recovery_running)
 			md_interrupt_thread(md_recovery_thread);
 
+		mddev->dying = 1; /* make sure nobody tries to use this */
+		unlock_mddev(mddev);
+
 		/*
 		 * This synchronizes with signal delivery to the
 		 * resync or reconstruction thread. It also nicely
@@ -1844,6 +1976,7 @@ static int do_md_stop(mddev_t * mddev, i
 			if (mddev->pers->stop(mddev)) {
 				if (mddev->ro)
 					set_device_ro(dev, 1);
+				mddev->dying = 0;
 				OUT(-EBUSY);
 			}
 			if (mddev->ro)
@@ -1859,10 +1992,15 @@ static int do_md_stop(mddev_t * mddev, i
 				mddev->sb->state |= 1 << MD_SB_CLEAN;
 			}
 			mddev->sb_dirty = 1;
+			lock_mddev(mddev);
 			md_update_sb(mddev);
+			unlock_mddev(mddev);
 		}
-		if (ro)
+		if (ro) {
 			set_device_ro(dev, 1);
+			lock_mddev(mddev);
+			mddev->dying = 0;
+		}
 	}
 
 	/*
@@ -1894,7 +2032,7 @@ int detect_old_array(mdp_super_t *sb)
 }
 
 
-static void autorun_array(mddev_t *mddev)
+static int autorun_array(mddev_t *mddev)
 {
 	mdk_rdev_t *rdev;
 	struct md_list_head *tmp;
@@ -1902,7 +2040,8 @@ static void autorun_array(mddev_t *mddev
 
 	if (mddev->disks.prev == &mddev->disks) {
 		MD_BUG();
-		return;
+		unlock_mddev(mddev);
+		return 0;
 	}
 
 	printk(KERN_INFO "md: running: ");
@@ -1920,7 +2059,10 @@ static void autorun_array(mddev_t *mddev
 		 */
 		mddev->sb_dirty = 0;
 		do_md_stop (mddev, 0);
+		return err;
 	}
+	unlock_mddev(mddev);
+	return 0;
 }
 
 /*
@@ -1976,6 +2118,7 @@ static void autorun_devices(kdev_t count
 			       mdidx(mddev), partition_name(rdev0->dev));
 			ITERATE_RDEV_GENERIC(candidates,pending,rdev,tmp)
 				export_rdev(rdev);
+			atomic_dec(&mddev->active);
 			continue;
 		}
 		mddev = alloc_mddev(md_kdev);
@@ -1983,15 +2126,15 @@ static void autorun_devices(kdev_t count
 			printk(KERN_ERR "md: cannot allocate memory for md drive.\n");
 			break;
 		}
-		if (md_kdev == countdev)
-			atomic_inc(&mddev->active);
 		printk(KERN_INFO "md: created md%d\n", mdidx(mddev));
 		ITERATE_RDEV_GENERIC(candidates,pending,rdev,tmp) {
 			bind_rdev_to_array(rdev, mddev);
 			md_list_del(&rdev->pending);
 			MD_INIT_LIST_HEAD(&rdev->pending);
 		}
-		autorun_array(mddev);
+		if (autorun_array(mddev)== 0
+		    && md_kdev != countdev)
+			atomic_dec(&mddev->active);
 	}
 	printk(KERN_INFO "md: ... autorun DONE.\n");
 }
@@ -2367,6 +2510,7 @@ static int hot_remove_disk(mddev_t * mdd
 	kick_rdev_from_array(rdev);
 	mddev->sb_dirty = 1;
 	md_update_sb(mddev);
+	md_new_event();
 
 	return 0;
 busy:
@@ -2486,7 +2630,7 @@ static int hot_add_disk(mddev_t * mddev,
 	 * array immediately.
 	 */
 	md_recover_arrays();
-
+	md_new_event();
 	return 0;
 
 abort_unbind_export:
@@ -2636,7 +2780,9 @@ static int md_ioctl(struct inode *inode,
 	 * Commands creating/starting a new array:
 	 */
 
-	mddev = kdev_to_mddev(dev);
+	mddev = kdev_to_mddev_lock_interruptible(dev, &err);
+	if (mddev == NULL && err)
+		goto abort;
 
 	switch (cmd)
 	{
@@ -2646,7 +2792,7 @@ static int md_ioctl(struct inode *inode,
 				printk(KERN_WARNING "md: array md%d already exists!\n",
 								mdidx(mddev));
 				err = -EEXIST;
-				goto abort;
+				goto abort_unlock;
 			}
 		default:;
 	}
@@ -2658,17 +2804,6 @@ static int md_ioctl(struct inode *inode,
 				err = -ENOMEM;
 				goto abort;
 			}
-			atomic_inc(&mddev->active);
-
-			/*
-			 * alloc_mddev() should possibly self-lock.
-			 */
-			err = lock_mddev(mddev);
-			if (err) {
-				printk(KERN_WARNING "md: ioctl, reason %d, cmd %d\n",
-				       err, cmd);
-				goto abort;
-			}
 
 			if (mddev->sb) {
 				printk(KERN_WARNING "md: array md%d already has a superblock!\n",
@@ -2691,14 +2826,11 @@ static int md_ioctl(struct inode *inode,
 			goto done_unlock;
 
 		case START_ARRAY:
-			/*
-			 * possibly make it lock the array ...
-			 */
 			err = autostart_array((kdev_t)arg, dev);
 			if (err) {
 				printk(KERN_WARNING "md: autostart %s failed!\n",
 					partition_name((kdev_t)arg));
-				goto abort;
+				goto abort_unlock;
 			}
 			goto done;
 
@@ -2713,13 +2845,11 @@ static int md_ioctl(struct inode *inode,
 		err = -ENODEV;
 		goto abort;
 	}
-	err = lock_mddev(mddev);
-	if (err) {
-		printk(KERN_INFO "md: ioctl lock interrupted, reason %d, cmd %d\n",err, cmd);
-		goto abort;
-	}
-	/* if we don't have a superblock yet, only ADD_NEW_DISK or STOP_ARRAY is allowed */
-	if (!mddev->sb && cmd != ADD_NEW_DISK && cmd != STOP_ARRAY && cmd != RUN_ARRAY) {
+
+	/* if we don't have a superblock yet, only ADD_NEW_DISK,
+	 * STOP_ARRAY, and RUN_ARRAY are allowed */
+	if (!mddev->sb && cmd != ADD_NEW_DISK && cmd != STOP_ARRAY
+	    && cmd != RUN_ARRAY) {
 		err = -ENODEV;
 		goto abort_unlock;
 	}
@@ -2881,17 +3011,17 @@ static int md_open(struct inode *inode, 
 	/*
 	 * Always succeed, but increment the usage count
 	 */
-	mddev_t *mddev = kdev_to_mddev(inode->i_rdev);
-	if (mddev)
-		atomic_inc(&mddev->active);
+	kdev_to_mddev(inode->i_rdev);
 	return (0);
 }
 
 static int md_release(struct inode *inode, struct file * file)
 {
 	mddev_t *mddev = kdev_to_mddev(inode->i_rdev);
-	if (mddev)
+	if (mddev) {
+		atomic_dec(&mddev->active);
 		atomic_dec(&mddev->active);
+	}
 	return 0;
 }
 
@@ -3062,6 +3192,7 @@ int md_error(mddev_t *mddev, kdev_t rdev
 		md_interrupt_thread(md_recovery_thread);
 	md_recover_arrays();
 
+	md_new_event();
 	return 0;
 }
 
@@ -3154,6 +3285,8 @@ static void *md_seq_start(struct seq_fil
 	loff_t l = *pos;
 	mddev_t *mddev;
 
+	down_read(&all_mddevs_sem);
+
 	if (l >= 0x10000)
 		return NULL;
 	if (!l--)
@@ -3174,7 +3307,7 @@ static void *md_seq_next(struct seq_file
 {
 	struct list_head *tmp;
 	mddev_t *next_mddev, *mddev = v;
-	
+
 	++*pos;
 	if (v == (void*)2)
 		return NULL;
@@ -3196,14 +3329,17 @@ static void *md_seq_next(struct seq_file
 
 static void md_seq_stop(struct seq_file *seq, void *v)
 {
-
+	up_read(&all_mddevs_sem);
 }
 
 static int md_seq_show(struct seq_file *seq, void *v)
 {
 	int j, size;
+	int err;
+
 	struct md_list_head *tmp2;
 	mdk_rdev_t *rdev;
+	struct mdstat_info *mi = seq->private;
 	mddev_t *mddev = v;
 
 	if (v == (void*)1) {
@@ -3218,13 +3354,21 @@ static int md_seq_show(struct seq_file *
 			seq_printf(seq, "not set\n");
 		else
 			seq_printf(seq, "%d sectors\n", read_ahead[MD_MAJOR]);
+
+		mi->reading = atomic_read(&md_event_count);
+		seq_printf(seq, "Event: %-20ld\n", mi->reading);
 		return 0;
 	}
 	if (v == (void*)2) {
 		status_unused(seq);
+		mi->seen = mi->reading;
 		return 0;
 	}
 
+	/* Lock device before reading its information */
+	lock_mddev(mddev);
+
+	/* Lock acquired, proceed */
 	seq_printf(seq, "md%d : %sactive", mdidx(mddev),
 		   mddev->pers ? "" : "in");
 	if (mddev->pers) {
@@ -3251,6 +3395,8 @@ static int md_seq_show(struct seq_file *
 		else
 			seq_printf(seq, "\n      %d blocks", size);
 	}
+	if (mddev->dying)
+		return 0;
 
 	if (mddev->pers) {
 
@@ -3266,6 +3412,8 @@ static int md_seq_show(struct seq_file *
 	}
 	seq_printf(seq, "\n");
 
+	unlock_mddev(mddev);
+
 	return 0;
 }
 
@@ -3280,16 +3428,70 @@ static struct seq_operations md_seq_ops 
 static int md_seq_open(struct inode *inode, struct file *file)
 {
 	int error;
+	struct mdstat_info *mi = kmalloc(sizeof(*mi), GFP_KERNEL);
+	if (mi == NULL)
+		return -ENOMEM;
 
 	error = seq_open(file, &md_seq_ops);
+	if (error)
+		kfree(mi);
+	else {
+		struct seq_file *p = file->private_data;
+		p->private = mi;
+		mi->acknowledged = mi->seen = mi->reading = 0;
+		if (file->f_mode & FMODE_WRITE) {
+			spin_lock(&readers_lock);
+			list_add(&mi->list, &readers);
+			spin_unlock(&readers_lock);
+		} else
+			INIT_LIST_HEAD(&mi->list);
+	}
 	return error;
 }
 
+static int md_seq_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *m = (struct seq_file*)file->private_data;
+	struct mdstat_info *mi = m->private;
+
+	m->private = NULL;
+	if (!list_empty(&mi->list)) {
+		spin_lock(&readers_lock);
+		list_del(&mi->list);
+		spin_unlock(&readers_lock);
+	}
+	kfree(mi);
+	wake_up(&md_event_waiters);
+	return seq_release(inode, file);
+}
+
+static unsigned int
+mdstat_poll(struct file *filp, poll_table *wait)
+{
+	struct seq_file *m = (struct seq_file*)filp->private_data;
+	struct mdstat_info*mi = m->private;
+	int mask;
+
+	if (mi->acknowledged != mi->seen) {
+		mi->acknowledged = mi->seen;
+		wake_up(&md_event_waiters);
+	}
+	poll_wait(filp, &md_event_waiters, wait);
+
+	/* always allow read */
+	mask = POLLIN | POLLRDNORM;
+
+	if (mi->seen < atomic_read(&md_event_count))
+		mask |= POLLPRI;
+	return mask;
+}
+
 static struct file_operations md_seq_fops = {
 	.open           = md_seq_open,
 	.read           = seq_read,
 	.llseek         = seq_lseek,
-	.release	= seq_release,
+	.release	= md_seq_release,
+	.poll		= mdstat_poll,
 };
 
 
@@ -3423,6 +3625,7 @@ int md_do_sync(mddev_t *mddev, mdp_disk_
 
 recheck:
 	serialize = 0;
+	down_read(&all_mddevs_sem);
 	ITERATE_MDDEV(mddev2,tmp) {
 		if (mddev2 == mddev)
 			continue;
@@ -3434,6 +3637,7 @@ recheck:
 			break;
 		}
 	}
+	up_read(&all_mddevs_sem);
 	if (serialize) {
 		interruptible_sleep_on(&resync_wait);
 		if (md_signal_pending(current)) {
@@ -3572,37 +3776,40 @@ void md_do_recovery(void *data)
 	struct md_list_head *tmp;
 
 	printk(KERN_INFO "md: recovery thread got woken up ...\n");
+
 restart:
+	down_read(&all_mddevs_sem);
 	ITERATE_MDDEV(mddev,tmp) {
 		sb = mddev->sb;
 		if (!sb)
 			continue;
-		if (mddev->recovery_running)
-			continue;
 		if (sb->active_disks == sb->raid_disks)
 			continue;
-		if (mddev->sb_dirty)
+		if (mddev->recovery_running)
+			continue;
+		if (mddev->sb_dirty) {
+			lock_mddev(mddev);
 			md_update_sb(mddev);
-		if (!sb->spare_disks) {
+			unlock_mddev(mddev);
+		}
+		if (!sb->spare_disks || ((spare = get_spare(mddev)) == NULL)) {
 			printk(KERN_ERR "md%d: no spare disk to reconstruct array! "
 			       "-- continuing in degraded mode\n", mdidx(mddev));
 			continue;
 		}
-		/*
-		 * now here we get the spare and resync it.
-		 */
-		spare = get_spare(mddev);
-		if (!spare)
-			continue;
 		printk(KERN_INFO "md%d: resyncing spare disk %s to replace failed disk\n",
 		       mdidx(mddev), partition_name(MKDEV(spare->major,spare->minor)));
-		if (!mddev->pers->diskop)
-			continue;
-		if (mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_WRITE))
+		if (!mddev->pers->diskop || 
+		    mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_WRITE)) {
 			continue;
+		}
+		up_read(&all_mddevs_sem);
+		md_new_event();
 		down(&mddev->recovery_sem);
 		mddev->recovery_running = 1;
 		err = md_do_sync(mddev, spare);
+		lock_mddev(mddev);
+
 		if (err == -EIO) {
 			printk(KERN_INFO "md%d: spare disk %s failed, skipping to next spare.\n",
 			       mdidx(mddev), partition_name(MKDEV(spare->major,spare->minor)));
@@ -3626,28 +3833,30 @@ restart:
 			 */
 			mddev->pers->diskop(mddev, &spare,
 							 DISKOP_SPARE_INACTIVE);
-			up(&mddev->recovery_sem);
 			mddev->recovery_running = 0;
-			continue;
+			up(&mddev->recovery_sem);
 		} else {
+			if (!disk_faulty(spare)) {
+				/*
+				 * the SPARE_ACTIVE diskop possibly changes the
+				 * pointer too
+				 */
+				mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_ACTIVE);
+				mark_disk_sync(spare);
+				mark_disk_active(spare);
+				sb->active_disks++;
+				sb->spare_disks--;
+			}
+			mddev->sb_dirty = 1;
+			md_update_sb(mddev);
 			mddev->recovery_running = 0;
 			up(&mddev->recovery_sem);
 		}
-		if (!disk_faulty(spare)) {
-			/*
-			 * the SPARE_ACTIVE diskop possibly changes the
-			 * pointer too
-			 */
-			mddev->pers->diskop(mddev, &spare, DISKOP_SPARE_ACTIVE);
-			mark_disk_sync(spare);
-			mark_disk_active(spare);
-			sb->active_disks++;
-			sb->spare_disks--;
-		}
-		mddev->sb_dirty = 1;
-		md_update_sb(mddev);
+		md_new_event();
+		unlock_mddev(mddev);
 		goto restart;
 	}
+	up_read(&all_mddevs_sem);
 	printk(KERN_INFO "md: recovery thread finished ...\n");
 
 }
@@ -3663,8 +3872,13 @@ int md_notify_reboot(struct notifier_blo
 
 		printk(KERN_INFO "md: stopping all md devices.\n");
 
-		ITERATE_MDDEV(mddev,tmp)
-			do_md_stop (mddev, 1);
+		ITERATE_MDDEV(mddev,tmp) {
+			lock_mddev(mddev);
+			if (!mddev->dying)
+				do_md_stop (mddev, 1);
+			unlock_mddev(mddev);
+		}
+
 		/*
 		 * certain more exotic SCSI devices are known to be
 		 * volatile wrt too early system reboots. While the
@@ -3701,7 +3915,7 @@ static void md_geninit(void)
 	dprintk("md: sizeof(mdp_super_t) = %d\n", (int)sizeof(mdp_super_t));
 
 #ifdef CONFIG_PROC_FS
-	p = create_proc_entry("mdstat", S_IRUGO, NULL);
+	p = create_proc_entry("mdstat", S_IRUGO|S_IWUSR, NULL);
 	if (p)
 		p->proc_fops = &md_seq_fops;
 #endif
@@ -4000,6 +4214,7 @@ void md__init md_setup_drive(void)
 				dinfo.minor = MINOR(dev);
 				add_new_disk (mddev, &dinfo);
 			}
+			md_new_event();
 		}
 		if (!err)
 			err = do_md_run(mddev);
@@ -4007,6 +4222,9 @@ void md__init md_setup_drive(void)
 			mddev->sb_dirty = 0;
 			do_md_stop(mddev, 0);
 			printk(KERN_WARNING "md: starting md%d failed\n", minor);
+		} else {
+			unlock_mddev(mddev);
+			atomic_dec(&mddev->active);
 		}
 	}
 }
@@ -4110,4 +4328,7 @@ MD_EXPORT_SYMBOL(md_interrupt_thread);
 MD_EXPORT_SYMBOL(mddev_map);
 MD_EXPORT_SYMBOL(md_check_ordering);
 MD_EXPORT_SYMBOL(get_spare);
+MD_EXPORT_SYMBOL(md_event_reached);
+MD_EXPORT_SYMBOL(md_event_waiters);
+MD_EXPORT_SYMBOL(md_new_event);
 MODULE_LICENSE("GPL");
diff -urNp linux-5980/drivers/md/multipath.c linux-5990/drivers/md/multipath.c
--- linux-5980/drivers/md/multipath.c
+++ linux-5990/drivers/md/multipath.c
@@ -145,7 +145,7 @@ static int multipath_map (mddev_t *mddev
 	 * Later we do read balancing on the read side 
 	 * now we use the first available disk.
 	 */
-
+ retry:
 	for (i = 0; i < disks; i++) {
 		if (conf->multipaths[i].operational) {
 			*rdev = conf->multipaths[i].dev;
@@ -153,6 +153,18 @@ static int multipath_map (mddev_t *mddev
 		}
 	}
 
+	/* Oops, no device available.  Make sure our failover event is
+	 * actually complete first. */
+	wait_event(md_event_waiters, conf->multipath_error_running == 0);
+	/* Now see if a disk was made operational by multipath_error */
+	if (conf->working_disks)
+		goto retry;
+	/* Last ditch effort, see if some daemon reading events might solve our
+	 * problem */
+	wait_event(md_event_waiters,
+		   md_event_reached(conf->last_failover_event));
+	if (conf->working_disks)
+		goto retry;
 	printk (KERN_ERR "multipath_map(): no more operational IO paths?\n");
 	return (-1);
 }
@@ -231,10 +243,26 @@ static int multipath_read_balance (multi
 {
 	int disk;
 
+retry:
 	for (disk = 0; disk < conf->raid_disks; disk++)	
 		if (conf->multipaths[disk].operational)
 			return disk;
-	BUG();
+	/* Oops, no device available.  Make sure our failover event is
+	 * actually complete first. */
+	wait_event(md_event_waiters, conf->multipath_error_running == 0);
+	/* Now see if a disk was made operational by multipath_error */
+	if (conf->working_disks)
+		goto retry;
+	/* Last ditch effort, see if some daemon reading events might solve our
+	 * problem */
+	wait_event(md_event_waiters,
+		   md_event_reached(conf->last_failover_event));
+	if (conf->working_disks)
+		goto retry;
+
+	printk (KERN_ERR "multipath_read_balance(): no more operational IO paths?\n");
+	/* returning 0 just uses a dead path device, which will eventually
+	 * make the I/O error out. */
 	return 0;
 }
 
@@ -334,57 +362,65 @@ static int multipath_error (mddev_t *mdd
 	int disks = MD_SB_DISKS;
 	int other_paths = 1;
 	int i;
+	unsigned long flags;
 
-	if (conf->working_disks == 1) {
-		other_paths = 0;
-		for (i = 0; i < disks; i++) {
-			if (multipaths[i].spare) {
-				other_paths = 1;
-				break;
-			}
+	/*
+	 * Only want to fail a device once, take the spinlock to avoid races.
+	 */
+	md_spin_lock_irqsave(&conf->device_lock, flags);
+	for (i = 0; i < disks; i++) {
+		if (multipaths[i].dev==dev && !multipaths[i].operational) {
+			/*
+			 * The first failed command starts a failover event.
+			 * All other commands on the failed path will trickle
+			 * in afterwards.  This is just another command
+			 * trickling in.
+			 */
+			md_spin_unlock_irqrestore(&conf->device_lock, flags);
+			return 0;
+		} else if (multipaths[i].dev==dev) {
+			/*
+			 * This is our failure event.
+			 */
+			conf->last_failover_event = md_new_event();
+			conf->multipath_error_running = 1;
+			mark_disk_bad(mddev, i);
+			break;
 		}
 	}
-
-	if (!other_paths) {
-		/*
-		 * Uh oh, we can do nothing if this is our last path, but
-		 * first check if this is a queued request for a device
-		 * which has just failed.
-		 */
-		for (i = 0; i < disks; i++) {
-			if (multipaths[i].dev==dev && !multipaths[i].operational)
-				return 0;
+	md_spin_unlock_irqrestore(&conf->device_lock, flags);
+		
+	other_paths = 0;
+	for (i = 0; i < disks; i++) {
+		if (multipaths[i].spare) {
+			other_paths = 1;
+			break;
 		}
+	}
+
+	if (!conf->working_disks && !other_paths) {
 		printk (LAST_DISK);
-	} else {
-		/*
-		 * Mark disk as unusable
-		 */
-		for (i = 0; i < disks; i++) {
-			if (multipaths[i].dev==dev && multipaths[i].operational) {
-				mark_disk_bad(mddev, i);
-				break;
-			}
-		}
-		if (!conf->working_disks) {
-			int err = 1;
-			mdp_disk_t *spare;
-			mdp_super_t *sb = mddev->sb;
-
-			spare = get_spare(mddev);
-			if (spare) {
-				err = multipath_diskop(mddev, &spare, DISKOP_SPARE_WRITE);
-				printk("got DISKOP_SPARE_WRITE err: %d. (spare_faulty(): %d)\n", err, disk_faulty(spare));
-			}
-			if (!err && !disk_faulty(spare)) {
-				multipath_diskop(mddev, &spare, DISKOP_SPARE_ACTIVE);
-				mark_disk_sync(spare);
-				mark_disk_active(spare);
-				sb->active_disks++;
-				sb->spare_disks--;
-			}
+	}
+	if (!conf->working_disks) {
+		int err = 1;
+		mdp_disk_t *spare;
+		mdp_super_t *sb = mddev->sb;
+
+		spare = get_spare(mddev);
+		if (spare) {
+			err = multipath_diskop(mddev, &spare, DISKOP_SPARE_WRITE);
+			printk("got DISKOP_SPARE_WRITE err: %d. (spare_faulty(): %d)\n", err, disk_faulty(spare));
+		}
+		if (!err && !disk_faulty(spare)) {
+			multipath_diskop(mddev, &spare, DISKOP_SPARE_ACTIVE);
+			mark_disk_sync(spare);
+			mark_disk_active(spare);
+			sb->active_disks++;
+			sb->spare_disks--;
 		}
 	}
+	conf->multipath_error_running = 0;
+	wake_up(&md_event_waiters);
 	return 0;
 }
 
@@ -700,8 +736,12 @@ static void multipathd (void *data)
 		md_spin_unlock_irqrestore(&retry_list_lock, flags);
 
 		mddev = mp_bh->mddev;
-		if (mddev->sb_dirty)
-			md_update_sb(mddev);
+		if (mddev->sb_dirty) {
+			lock_mddev(mddev);
+			if (mddev->sb_dirty)
+				md_update_sb(mddev);
+			unlock_mddev(mddev);
+		}
 		bh = &mp_bh->bh_req;
 		dev = bh->b_dev;
 		
diff -urNp linux-5980/drivers/md/raid1.c linux-5990/drivers/md/raid1.c
--- linux-5980/drivers/md/raid1.c
+++ linux-5990/drivers/md/raid1.c
@@ -1160,8 +1160,12 @@ static void raid1d (void *data)
 	mddev_t *mddev = conf->mddev;
 	kdev_t dev;
 
-	if (mddev->sb_dirty)
-		md_update_sb(mddev);
+	if (mddev->sb_dirty) {
+		lock_mddev(mddev);
+		if (mddev->sb_dirty)
+			md_update_sb(mddev);
+		unlock_mddev(mddev);
+	}
 
 	for (;;) {
 		md_spin_lock_irqsave(&retry_list_lock, flags);
diff -urNp linux-5980/drivers/md/raid5.c linux-5990/drivers/md/raid5.c
--- linux-5980/drivers/md/raid5.c
+++ linux-5990/drivers/md/raid5.c
@@ -1301,8 +1301,13 @@ static void raid5d (void *data)
 
 	handled = 0;
 
-	if (mddev->sb_dirty)
-		md_update_sb(mddev);
+	if (mddev->sb_dirty) {
+		lock_mddev(mddev);
+		if (mddev->sb_dirty)
+			md_update_sb(mddev);
+		unlock_mddev(mddev);
+	}
+
 	md_spin_lock_irq(&conf->device_lock);
 	while (1) {
 		struct list_head *first;
diff -urNp linux-5980/include/linux/raid/md_k.h linux-5990/include/linux/raid/md_k.h
--- linux-5980/include/linux/raid/md_k.h
+++ linux-5990/include/linux/raid/md_k.h
@@ -75,13 +75,6 @@ typedef struct dev_mapping_s {
 
 extern dev_mapping_t mddev_map [MAX_MD_DEVS];
 
-static inline mddev_t * kdev_to_mddev (kdev_t dev)
-{
-	if (MAJOR(dev) != MD_MAJOR)
-		BUG();
-        return mddev_map[MINOR(dev)].mddev;
-}
-
 /*
  * options passed in raidrun:
  */
@@ -216,6 +209,9 @@ struct mddev_s
 	md_wait_queue_head_t		recovery_wait;
 
 	struct md_list_head		all_mddevs;
+#ifndef __GENKSYMS__ /* preserve KMI/ABI ksyms compatibility for mod linkage */
+	int				dying;
+#endif
 };
 
 struct mdk_personality_s
@@ -310,7 +306,12 @@ extern mdp_disk_t *get_spare(mddev_t *md
 			tmp = tmp->next, tmp->prev != &all_mddevs	\
 		; )
 
-static inline int lock_mddev (mddev_t * mddev)
+static inline void lock_mddev (mddev_t * mddev)
+{
+	return down(&mddev->reconfig_sem);
+}
+
+static inline int lock_mddev_interruptible (mddev_t * mddev)
 {
 	return down_interruptible(&mddev->reconfig_sem);
 }
@@ -396,5 +397,9 @@ do {									\
 	__wait_disk_event(wq, condition);				\
 } while (0)
 
+extern int md_new_event(void);
+extern wait_queue_head_t md_event_waiters;
+extern int md_event_reached(unsigned long ev);
+
 #endif 
 
diff -urNp linux-5980/include/linux/raid/multipath.h linux-5990/include/linux/raid/multipath.h
--- linux-5980/include/linux/raid/multipath.h
+++ linux-5990/include/linux/raid/multipath.h
@@ -37,6 +37,8 @@ struct multipath_private_data {
 	int			freer1_blocked;
 	int			freer1_cnt;
 	md_wait_queue_head_t	wait_buffer;
+	int			last_failover_event;
+	int			multipath_error_running;
 };
 
 typedef struct multipath_private_data multipath_conf_t;
