--- linux-2.4.20/include/linux/mm.h.=K0000=.orig
+++ linux-2.4.20/include/linux/mm.h
@@ -683,12 +683,35 @@ static inline void __vma_unlink(struct m
 		mm->mmap_cache = prev;
 }
 
-static inline int can_vma_merge(struct vm_area_struct * vma, unsigned long vm_flags)
+#define VM_SPECIAL (VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_RESERVED)
+#define can_vma_merge(vma, vm_flags) __can_vma_merge(vma, vm_flags, NULL, 0, 0)
+
+/*
+ * We don't check here for the merged mmap wrapping around the end of pagecache
+ * indices (16TB on ia32) because do_mmap_pgoff() does not permit mmap's which
+ * wrap, nor mmaps which cover the final page at index 0xffffffff.
+ *
+ * If the vma has a ->close operation then the driver probably needs to release
+ * per-vma resources, so we don't attempt to merge those.
+ */
+static inline int __can_vma_merge(struct vm_area_struct * vma, unsigned long vm_flags,
+				  struct file * file, unsigned long vm_pgoff, unsigned long offset)
 {
-	if (!vma->vm_file && vma->vm_flags == vm_flags)
-		return 1;
-	else
+	if (unlikely(vma->vm_file != file))
+		return 0;
+	if (unlikely(vma->vm_flags != vm_flags))
+		return 0;
+	if (unlikely(vma->vm_flags & VM_SPECIAL))
+		return 0;
+	if (unlikely(vma->vm_private_data != NULL))
+		return 0;
+	if (unlikely(vma->vm_ops && vma->vm_ops->close))
 		return 0;
+	if (file) {
+		if (unlikely(vma->vm_pgoff != vm_pgoff + offset))
+			return 0;
+	}
+	return 1;
 }
 
 struct zone_t;
--- linux-2.4.20/mm/mmap.c.=K0000=.orig
+++ linux-2.4.20/mm/mmap.c
@@ -431,40 +431,56 @@ static inline void vma_link(struct mm_st
 }
 
 static int vma_merge(struct mm_struct * mm, struct vm_area_struct * prev,
-		     rb_node_t * rb_parent, unsigned long addr, unsigned long end, unsigned long vm_flags)
+		     rb_node_t * rb_parent, unsigned long addr, unsigned long end,
+		     unsigned long vm_flags, struct file * file, unsigned long pgoff)
 {
 	spinlock_t * lock = &mm->page_table_lock;
 	if (!prev) {
 		prev = rb_entry(rb_parent, struct vm_area_struct, vm_rb);
 		goto merge_next;
 	}
-	if (prev->vm_end == addr && can_vma_merge(prev, vm_flags)) {
+	if (prev->vm_end == addr && __can_vma_merge(prev, vm_flags, file,
+						    pgoff, -((prev->vm_end - prev->vm_start) >> PAGE_SHIFT))) {
 		struct vm_area_struct * next;
+		int need_unlock = 0;
 
+		if (unlikely(file && prev->vm_next && prev->vm_next->vm_file == file)) {
+			lock_vma_mappings(prev->vm_next);
+			need_unlock = 1;
+		}
 		spin_lock(lock);
 		prev->vm_end = end;
 		next = prev->vm_next;
-		if (next && prev->vm_end == next->vm_start && can_vma_merge(next, vm_flags)) {
+		if (next && prev->vm_end == next->vm_start && __can_vma_merge(next, vm_flags, file,
+									      pgoff, (end - addr) >> PAGE_SHIFT)) {
 			prev->vm_end = next->vm_end;
 			__vma_unlink(mm, next, prev);
+			__remove_shared_vm_struct(next);
 			spin_unlock(lock);
+			if (need_unlock)
+				unlock_vma_mappings(next);
+			if (file)
+				fput(file);
 
 			mm->map_count--;
 			kmem_cache_free(vm_area_cachep, next);
 			return 1;
 		}
 		spin_unlock(lock);
+		if (need_unlock)
+			unlock_vma_mappings(next);
 		return 1;
 	}
 
 	prev = prev->vm_next;
 	if (prev) {
  merge_next:
-		if (!can_vma_merge(prev, vm_flags))
+		if (!__can_vma_merge(prev, vm_flags, file, pgoff, (end - addr) >> PAGE_SHIFT))
 			return 0;
 		if (end == prev->vm_start) {
 			spin_lock(lock);
 			prev->vm_start = addr;
+			prev->vm_pgoff = pgoff;
 			spin_unlock(lock);
 			return 1;
 		}
@@ -591,7 +607,7 @@ munmap_back:
 
 	/* Can we just expand an old anonymous mapping? */
 	if (!file && !(vm_flags & VM_SHARED) && rb_parent)
-		if (vma_merge(mm, prev, rb_parent, addr, addr + len, vm_flags))
+		if (vma_merge(mm, prev, rb_parent, addr, addr + len, vm_flags, NULL, 0))
 			goto out;
 
 	/* Determine the object being mapped and call the appropriate
@@ -664,10 +680,15 @@ munmap_back:
 		}
 	}
 
-	vma_link(mm, vma, prev, rb_link, rb_parent);
 	if (correct_wcount)
 		atomic_inc(&file->f_dentry->d_inode->i_writecount);
 
+	if (file && rb_parent && vma_merge(mm, prev, rb_parent, addr, addr + len, vma->vm_flags, file, pgoff)) {
+		fput(file);
+		kmem_cache_free(vm_area_cachep, vma);
+	} else
+		vma_link(mm, vma, prev, rb_link, rb_parent);
+
 out:	
 	mm->total_vm += len >> PAGE_SHIFT;
 	if (vm_flags & VM_LOCKED) {
@@ -1285,7 +1306,7 @@ unsigned long do_brk(unsigned long addr,
 	flags = VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags; 
 
 	/* Can we just expand an old anonymous mapping? */
-	if (rb_parent && vma_merge(mm, prev, rb_parent, addr, addr + len, flags))
+	if (rb_parent && vma_merge(mm, prev, rb_parent, addr, addr + len, flags, NULL, 0))
 		goto out;
 
 	/*
