diff -urNp linux-6020/fs/binfmt_elf.c linux-6030/fs/binfmt_elf.c
--- linux-6020/fs/binfmt_elf.c
+++ linux-6030/fs/binfmt_elf.c
@@ -499,6 +499,7 @@ static int load_elf_binary(struct linux_
 	struct elfhdr elf_ex;
 	struct elfhdr interp_elf_ex;
   	struct exec interp_ex;
+	struct files_struct *files;
 	char passed_fileno[6];
 	int exec_stack, relocexec, old_relocexec = current->flags & PF_RELOCEXEC;
 	unsigned long total_size;
@@ -536,10 +537,21 @@ static int load_elf_binary(struct linux_
 			retval = -EIO;
 		goto out_free_ph;
 	}
+		
+	files = current->files;		/* Refcounted so ok */
+	if(unshare_files() < 0)
+		goto out_free_ph;
+	if (files == current->files) {
+		put_files_struct(files);
+		files = NULL;
+	}
 
+	/* exec will make our files private anyway, but for the a.out
+	   loader stuff we need to do it earlier */
+	   
 	retval = get_unused_fd();
 	if (retval < 0)
-		goto out_free_ph;
+		goto out_free_fh;
 	get_file(bprm->file);
 	fd_install(elf_exec_fileno = retval, bprm->file);
 
@@ -710,6 +722,13 @@ static int load_elf_binary(struct linux_
 		arch_add_exec_range(current->mm, -1);
 #endif
 
+	/* Discard our unneeded old files struct */
+	if (files) {
+		steal_locks(files);
+		put_files_struct(files);
+		files = NULL;
+	}
+
 	/* OK, This is the point of no return */
 	current->mm->start_data = 0;
 	current->mm->end_data = 0;
@@ -964,6 +983,11 @@ out_free_interp:
 		kfree(elf_interpreter);
 out_free_file:
 	sys_close(elf_exec_fileno);
+out_free_fh:
+	if (files) {
+		put_files_struct(current->files);
+		current->files = files;
+	}
 out_free_ph:
 	kfree(elf_phdata);
 	current->flags &= ~PF_RELOCEXEC;
diff -urNp linux-6020/fs/exec.c linux-6030/fs/exec.c
--- linux-6020/fs/exec.c
+++ linux-6030/fs/exec.c
@@ -721,7 +721,19 @@ int flush_old_exec(struct linux_binprm *
 {
 	char * name;
 	int i, ch, retval;
+	struct files_struct * files;
 
+	/*
+	 * Make sure we have private file handles. Ask the
+	 * fork helper to do the work for us and the exit
+	 * helper to do the cleanup of the old one.
+	 */
+	 
+	files = current->files;		/* refcounted so safe to hold */
+	retval = unshare_files();
+	if (retval)
+		goto out;
+	
 	/* 
 	 * Release all of the old mmap stuff
 	 */
@@ -737,6 +749,9 @@ int flush_old_exec(struct linux_binprm *
 		goto out;
 
 	/* This is the point of no return */
+	
+	steal_locks(files);
+	put_files_struct(files);
 
 	current->sas_ss_sp = current->sas_ss_size = 0;
 
@@ -777,6 +792,7 @@ int flush_old_exec(struct linux_binprm *
 
 	return 0;
 
+
 out:
 	return retval;
 }
diff -urNp linux-6020/fs/locks.c linux-6030/fs/locks.c
--- linux-6020/fs/locks.c
+++ linux-6030/fs/locks.c
@@ -1956,6 +1956,53 @@ done:
 	return length;
 }
 
+static inline void __steal_locks(struct file *file, fl_owner_t from)
+{
+	struct inode *inode = file->f_dentry->d_inode;
+	struct file_lock *fl = inode->i_flock;
+
+	while (fl) {
+		if (fl->fl_file == file && fl->fl_owner == from)
+			fl->fl_owner = current->files;
+		fl = fl->fl_next;
+	}
+}
+
+/* When getting ready for executing a binary, we make sure that current
+ * has a files_struct on its own. Before dropping the old files_struct,
+ * we take over ownership of all locks for all file descriptors we own.
+ * Note that we may accidentally steal a lock for a file that a sibling
+ * has created since the unshare_files() call.
+ */
+void steal_locks(fl_owner_t from)
+{
+	struct files_struct *files = current->files;
+	int i, j;
+
+	if (from == files)
+		return;
+
+	lock_kernel();
+	j = 0;
+	for (;;) {
+		unsigned long set;
+		i = j * __NFDBITS;
+		if (i >= files->max_fdset || i >= files->max_fds)
+			break;
+		set = files->open_fds->fds_bits[j++];
+		while (set) {
+			if (set & 1) {
+				struct file *file = files->fd[i];
+				if (file)
+					__steal_locks(file, from);
+			}
+			i++;
+			set >>= 1;
+		}
+	}
+	unlock_kernel();
+}
+
 #ifdef MSNFS
 /**
  *	lock_may_read - checks that the region is free of locks
diff -urNp linux-6020/fs/proc/base.c linux-6030/fs/proc/base.c
--- linux-6020/fs/proc/base.c
+++ linux-6030/fs/proc/base.c
@@ -124,20 +124,56 @@ static int proc_root_link(struct inode *
 	return result;
 }
 
+#define MAY_PTRACE(task) \
+	(task == current || \
+	(task->parent == current && \
+	(task->ptrace & PT_PTRACED) && task->state == TASK_STOPPED))
+
+static int may_ptrace_attach(struct task_struct *task)
+{
+	int retval = 0;
+
+	task_lock(task);
+
+	if (((current->uid != task->euid) ||
+	    (current->uid != task->suid) ||
+	    (current->uid != task->uid) ||
+	    (current->gid != task->egid) ||
+	    (current->gid != task->sgid) ||
+	    (!cap_issubset(task->cap_permitted, current->cap_permitted)) ||
+	    (current->gid != task->gid)) && !capable(CAP_SYS_PTRACE))
+		goto out;
+	rmb();
+	if (!is_dumpable(task) && !capable(CAP_SYS_PTRACE))
+		goto out;
+
+	retval = 1;
+
+out:
+	task_unlock(task);
+	return retval;
+}
+
 static int proc_pid_environ(struct task_struct *task, char * buffer)
 {
 	struct mm_struct *mm;
 	int res = 0;
+
+	if (!may_ptrace_attach(task))
+		return -ESRCH;
+
 	task_lock(task);
 	mm = task->mm;
 	if (mm)
 		atomic_inc(&mm->mm_users);
 	task_unlock(task);
 	if (mm) {
-		int len = mm->env_end - mm->env_start;
+		unsigned int len = mm->env_end - mm->env_start;
 		if (len > PAGE_SIZE)
 			len = PAGE_SIZE;
 		res = access_process_vm(task, mm->env_start, buffer, len, 0);
+		if (!may_ptrace_attach(task))
+			res = -ESRCH;
 		mmput(mm);
 	}
 	return res;
@@ -328,9 +364,6 @@ static struct file_operations proc_info_
 	read:		proc_info_read,
 };
 
-#define MAY_PTRACE(p) \
-(p==current||(p->parent==current&&(p->ptrace & PT_PTRACED)&&p->state==TASK_STOPPED))
-
 
 static int mem_open(struct inode* inode, struct file* file)
 {
@@ -347,8 +380,7 @@ static ssize_t mem_read(struct file * fi
 	int copied = 0;
 	struct mm_struct *mm;
 
-
-	if (!MAY_PTRACE(task))
+	if (!MAY_PTRACE(task) || !may_ptrace_attach(task))
 		return -ESRCH;
 
 	page = (char *)__get_free_page(GFP_USER);
@@ -370,14 +402,13 @@ static ssize_t mem_read(struct file * fi
 		copied = -EIO;
 		goto out_free;
 	}
-		
 
 	while (count > 0) {
 		int this_len, retval;
 
 		this_len = (count > PAGE_SIZE) ? PAGE_SIZE : count;
 		retval = access_process_vm(task, src, page, this_len, 0);
-		if (!retval) {
+		if (!retval || !MAY_PTRACE(task) || !may_ptrace_attach(task)) {
 			if (!copied)
 				copied = -EIO;
 			break;
@@ -411,7 +442,7 @@ static ssize_t mem_write(struct file * f
 	struct task_struct *task = file->f_dentry->d_inode->u.proc_i.task;
 	unsigned long dst = *ppos;
 
-	if (!MAY_PTRACE(task))
+	if (!MAY_PTRACE(task) || !may_ptrace_attach(task))
 		return -ESRCH;
 
 	page = (char *)__get_free_page(GFP_USER);
diff -urNp linux-6020/fs/read_write.c linux-6030/fs/read_write.c
--- linux-6020/fs/read_write.c
+++ linux-6030/fs/read_write.c
@@ -322,7 +322,7 @@ out_nofree:
 	/* VERIFY_WRITE actually means a read, as we write to user space */
 	if ((ret + (type == VERIFY_WRITE)) > 0)
 		dnotify_parent(file->f_dentry,
-			(type == VERIFY_WRITE) ? DN_MODIFY : DN_ACCESS);
+			(type == VERIFY_WRITE) ? DN_ACCESS : DN_MODIFY);
 	return ret;
 }
 
diff -urNp linux-6020/fs/stat.c linux-6030/fs/stat.c
--- linux-6020/fs/stat.c
+++ linux-6030/fs/stat.c
@@ -43,6 +43,8 @@ static int cp_old_stat(struct inode * in
 	static int warncount = 5;
 	struct __old_kernel_stat tmp;
 
+	memset(&tmp, 0, sizeof(struct __old_kernel_stat));
+	
 	if (warncount > 0) {
 		warncount--;
 		printk(KERN_WARNING "VFS: Warning: %s using old stat() call. Recompile your binary.\n",
diff -urNp linux-6020/include/linux/fs.h linux-6030/include/linux/fs.h
--- linux-6020/include/linux/fs.h
+++ linux-6030/include/linux/fs.h
@@ -703,6 +703,7 @@ extern int __get_lease(struct inode *ino
 extern time_t lease_get_mtime(struct inode *);
 extern int lock_may_read(struct inode *, loff_t start, unsigned long count);
 extern int lock_may_write(struct inode *, loff_t start, unsigned long count);
+extern void steal_locks(fl_owner_t from);
 
 struct fasync_struct {
 	int	magic;
@@ -1656,6 +1657,9 @@ extern int inode_change_ok(struct inode 
 extern int inode_setattr(struct inode *, struct iattr *);
 extern int setattr_mask(unsigned int ia_valid);
 
+/* kernel/fork.c */
+extern int unshare_files(void);
+
 /*
  * Common dentry functions for inclusion in the VFS
  * or in other stackable file systems.  Some of these
diff -urNp linux-6020/include/linux/sched.h linux-6030/include/linux/sched.h
--- linux-6020/include/linux/sched.h
+++ linux-6030/include/linux/sched.h
@@ -912,6 +912,8 @@ extern fd_set *alloc_fdset(int);
 extern int expand_fdset(struct files_struct *, int nr);
 extern void free_fdset(fd_set *, int);
 
+extern int unshare_files(void);
+
 extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, task_t *, struct pt_regs *);
 extern void flush_thread(void);
 extern void exit_thread(void);
diff -urNp linux-6020/init/main.c linux-6030/init/main.c
--- linux-6020/init/main.c
+++ linux-6030/init/main.c
@@ -26,6 +26,7 @@
 #include <linux/hdreg.h>
 #include <linux/iobuf.h>
 #include <linux/bootmem.h>
+#include <linux/file.h>
 #include <linux/tty.h>
 #include <linux/profile.h>
 
@@ -552,6 +553,7 @@ extern void prepare_namespace(void);
 
 static int init(void * unused)
 {
+	struct files_struct *files;
 	lock_kernel();
 
 	smp_init();
@@ -569,7 +571,17 @@ static int init(void * unused)
 	 */
 	free_initmem();
 	unlock_kernel();
-
+	
+	/*
+	 * Right now we are a thread sharing with a ton of kernel
+	 * stuff. We don't want to end up in user space in that state
+	 */
+	 
+	files = current->files;
+	if(unshare_files())
+		panic("unshare");
+	put_files_struct(files);
+	
 	if (open("/dev/console", O_RDWR, 0) < 0)
 		printk("Warning: unable to open an initial console.\n");
 
diff -urNp linux-6020/kernel/fork.c linux-6030/kernel/fork.c
--- linux-6020/kernel/fork.c
+++ linux-6030/kernel/fork.c
@@ -533,6 +533,11 @@ static int copy_files(unsigned long clon
 		goto out;
 	}
 
+	/*
+	 * Note: we may be using current for both targets (See exec.c)
+	 * This works because we cache current->files (old) as oldf. Don't
+	 * break this.
+	 */
 	tsk->files = NULL;
 	error = -ENOMEM;
 	newf = kmem_cache_alloc(files_cachep, SLAB_KERNEL);
@@ -622,6 +627,33 @@ out_release:
 	goto out;
 }
 
+/*
+ *	Helper to unshare the files of the current task. 
+ *	We don't want to expose copy_files internals to 
+ *	the exec layer of the kernel.
+ */
+
+int unshare_files(void)
+{
+	struct files_struct *files  = current->files;
+	int rc;
+	
+	if(!files)
+		BUG();
+		
+	/* This can race but the race causes us to copy when we don't
+	   need to and drop the copy */
+	if(atomic_read(&files->count) == 1)
+	{
+		atomic_inc(&files->count);
+		return 0;
+	}
+	rc = copy_files(0, current);
+	if(rc)
+		current->files = files;
+	return rc;
+}		
+
 static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct sighand_struct *sig;
diff -urNp linux-6020/kernel/ksyms.c linux-6030/kernel/ksyms.c
--- linux-6020/kernel/ksyms.c
+++ linux-6030/kernel/ksyms.c
@@ -112,6 +112,7 @@ EXPORT_SYMBOL(exit_mm);
 EXPORT_SYMBOL(exit_files);
 EXPORT_SYMBOL(exit_fs);
 EXPORT_SYMBOL(exit_sighand);
+EXPORT_SYMBOL(unshare_files);
 
 /* internal kernel memory management */
 EXPORT_SYMBOL(_alloc_pages);
