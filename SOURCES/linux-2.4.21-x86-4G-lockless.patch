diff -urNp linux-1160/arch/i386/mm/hugetlbpage.c linux-1170/arch/i386/mm/hugetlbpage.c
--- linux-1160/arch/i386/mm/hugetlbpage.c
+++ linux-1170/arch/i386/mm/hugetlbpage.c
@@ -157,7 +157,7 @@ nomem:
 }
 
 int
-follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+follow_pin_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
 		    unsigned long *position, int *length, int i)
 {
@@ -208,25 +208,30 @@ follow_huge_addr(struct mm_struct *mm,
 	return NULL;
 }
 
-struct vm_area_struct *hugepage_vma(struct mm_struct *mm, unsigned long addr)
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
 {
-	return NULL;
-}
+	struct page *page;
 
-int pmd_huge(pmd_t pmd)
-{
-	return !!(pmd_val(pmd) & _PAGE_PSE);
+	page = pte_page(*(pte_t *)pmd);
+	if (page)
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+
+	return page;
 }
 
 struct page *
-follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+follow_pin_huge_pmd(struct mm_struct *mm, unsigned long address,
 		pmd_t *pmd, int write)
 {
 	struct page *page;
 
 	page = pte_page(*(pte_t *)pmd);
-	if (page)
+	if (page) {
 		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+		get_page(page);
+	}
 
 	return page;
 }
diff -urNp linux-1160/arch/ia64/mm/hugetlbpage.c linux-1170/arch/ia64/mm/hugetlbpage.c
--- linux-1160/arch/ia64/mm/hugetlbpage.c
+++ linux-1170/arch/ia64/mm/hugetlbpage.c
@@ -233,7 +233,7 @@ nomem:
 }
 
 int
-follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+follow_pin_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
 		    unsigned long *st, int *length, int i)
 {
@@ -287,6 +287,18 @@ struct page *follow_huge_addr(struct mm_
 	ptep = huge_pte_offset(mm, addr);
 	page = pte_page(*ptep);
 	page += ((addr & ~HPAGE_MASK) >> PAGE_SHIFT);
+
+	return page;
+}
+
+struct page *follow_pin_huge_addr(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr, int write)
+{
+	struct page *page;
+	pte_t *ptep;
+
+	ptep = huge_pte_offset(mm, addr);
+	page = pte_page(*ptep);
+	page += ((addr & ~HPAGE_MASK) >> PAGE_SHIFT);
 	get_page(page);
 	return page;
 }
@@ -294,9 +306,16 @@ int pmd_huge(pmd_t pmd)
 {
 	return 0;
 }
+
 struct page *
 follow_huge_pmd(struct mm_struct *mm, unsigned long address, pmd_t *pmd, int write)
 {
+        return NULL;
+}
+
+struct page *
+follow_pin_huge_pmd(struct mm_struct *mm, unsigned long address, pmd_t *pmd, int write)
+{
 	return NULL;
 }
 
diff -urNp linux-1160/arch/x86_64/mm/hugetlbpage.c linux-1170/arch/x86_64/mm/hugetlbpage.c
--- linux-1160/arch/x86_64/mm/hugetlbpage.c
+++ linux-1170/arch/x86_64/mm/hugetlbpage.c
@@ -156,7 +156,7 @@ nomem:
 }
 
 int
-follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+follow_pin_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
 		    unsigned long *position, int *length, int i)
 {
@@ -207,6 +207,13 @@ follow_huge_addr(struct mm_struct *mm,
 	return NULL;
 }
 
+struct page *
+follow_pin_huge_addr(struct mm_struct *mm,
+	struct vm_area_struct *vma, unsigned long address, int write)
+{
+	return NULL;
+}
+
 struct vm_area_struct *hugepage_vma(struct mm_struct *mm, unsigned long addr)
 {
 	return NULL;
@@ -230,6 +237,21 @@ follow_huge_pmd(struct mm_struct *mm, un
 	return page;
 }
 
+struct page *
+follow_pin_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
+{
+	struct page *page;
+
+	page = pte_page(*(pte_t *)pmd);
+	if (page) {
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+		get_page(page);
+	}
+
+	return page;
+}
+
 
 void free_huge_page(struct page *page0)
 {
diff -urNp linux-1160/include/asm-i386/highmem.h linux-1170/include/asm-i386/highmem.h
--- linux-1160/include/asm-i386/highmem.h
+++ linux-1170/include/asm-i386/highmem.h
@@ -77,8 +77,16 @@ static inline void kunmap(struct page *p
 
 static inline void *kmap_atomic(struct page *page, enum km_type type)
 {
+/*
+ * The same code, just different probability:
+ */
+#if CONFIG_X86_4G_VM_LAYOUT
+	if (unlikely(page < highmem_start_page))
+		return page_address(page);
+#else
 	if (page < highmem_start_page)
 		return page_address(page);
+#endif
 	return __kmap_atomic(page, type);
 }
 
diff -urNp linux-1160/include/asm-i386/pgtable.h linux-1170/include/asm-i386/pgtable.h
--- linux-1160/include/asm-i386/pgtable.h
+++ linux-1170/include/asm-i386/pgtable.h
@@ -234,7 +234,7 @@ extern void pgtable_cache_init(void);
 #define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY)
 #define _PAGE_CHG_MASK	(PTE_MASK | _PAGE_ACCESSED | _PAGE_DIRTY)
 
-#define PAGE_NONE	__pgprot(_PAGE_PROTNONE | _PAGE_ACCESSED)
+#define PAGE_NONE	__pgprot(_PAGE_PROTNONE | _PAGE_USER | _PAGE_ACCESSED)
 #define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER | _PAGE_ACCESSED)
 
 #define PAGE_SHARED_EXEC  __pgprot(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER | _PAGE_ACCESSED)
@@ -369,6 +369,12 @@ static inline void ptep_mkdirty(pte_t *p
 #define mk_pte(page, pgprot)	__mk_pte((page) - mem_map, (pgprot))
 #define mk_pte_huge(entry) ((entry).pte_low |= _PAGE_PRESENT | _PAGE_PSE)
 
+#define HAVE_ARCH_INLINE_HUGEPAGE_VMA
+#define hugepage_vma(mm, addr) NULL
+
+#define HAVE_ARCH_INLINE_PMD_HUGE
+#define pmd_huge(pmd) (pmd_val(pmd) & _PAGE_PSE)
+
 /* This takes a physical page address that is used by the remapping functions */
 #define mk_pte_phys(physpage, pgprot)	__mk_pte((physpage) >> PAGE_SHIFT, pgprot)
 
diff -urNp linux-1160/include/linux/hugetlb.h linux-1170/include/linux/hugetlb.h
--- linux-1160/include/linux/hugetlb.h
+++ linux-1170/include/linux/hugetlb.h
@@ -22,7 +22,7 @@ static inline int is_vm_hugetlb_page(str
 
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void *, size_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+int follow_pin_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
 void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
@@ -31,12 +31,20 @@ int hugetlb_report_meminfo(char *);
 int is_hugepage_mem_enough(size_t);
 struct page *follow_huge_addr(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write);
+struct page *follow_pin_huge_addr(struct mm_struct *mm, struct vm_area_struct *vma,
+			unsigned long address, int write);
+#ifndef HAVE_ARCH_INLINE_HUGEPAGE_VMA
 struct vm_area_struct *hugepage_vma(struct mm_struct *mm,
 					unsigned long address);
+#endif
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int write);
+struct page *follow_pin_huge_pmd(struct mm_struct *mm, unsigned long address,
+				pmd_t *pmd, int write);
 int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
+#ifndef HAVE_ARCH_INLINE_PMD_HUGE
 int pmd_huge(pmd_t pmd);
+#endif
 int zap_one_hugepage(struct vm_area_struct *vma, unsigned long address, unsigned long size);
 
 extern int htlbpage_max, htlbpool_max;
diff -urNp linux-1160/include/linux/mm.h linux-1170/include/linux/mm.h
--- linux-1160/include/linux/mm.h
+++ linux-1170/include/linux/mm.h
@@ -554,11 +554,7 @@ struct page;
 #define get_page(p)							\
 do {									\
 	struct page *___page = (struct page *)(p);			\
- \
- \
- \
- \
-	if (PageCompound(___page)) {					\
+	if (unlikely(PageCompound(___page))) {				\
 		___page = (struct page *)___page->lru.next;		\
 		BUG_ON(!PageCompound(___page));				\
 	}								\
@@ -571,10 +567,6 @@ do {									\
 #define get_page(p)							\
 do {									\
 	struct page *___page = (struct page *)(p);			\
- \
- \
- \
- \
 	atomic_inc(&___page->count);					\
 } while (0)
 
@@ -673,6 +665,7 @@ extern int make_pages_present(unsigned l
 extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
 
 extern struct page * follow_page(struct mm_struct *mm, unsigned long address, int write);
+extern struct page * follow_pin_page(struct mm_struct *mm, unsigned long address, int write);
 int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start,
 		int len, int write, int force, struct page **pages, struct vm_area_struct **vmas);
 
diff -urNp linux-1160/mm/memory.c linux-1170/mm/memory.c
--- linux-1160/mm/memory.c
+++ linux-1170/mm/memory.c
@@ -650,37 +650,147 @@ follow_page(struct mm_struct *mm, unsign
 		return follow_huge_addr(mm, vma, address, write);
 
 	pgd = pgd_offset(mm, address);
-	if (pgd_none(*pgd) || pgd_bad(*pgd))
+	if (unlikely(pgd_none(*pgd) || pgd_bad(*pgd)))
 		goto out;
 
 	pmd = pmd_offset(pgd, address);
-	if (pmd_none(*pmd))
+	if (unlikely(pmd_none(*pmd)))
 		goto out;
-	if (pmd_huge(*pmd))
+	if (unlikely(pmd_huge(*pmd)))
 		return follow_huge_pmd(mm, address, pmd, write);
-	if (pmd_bad(*pmd))
+	if (unlikely(pmd_bad(*pmd)))
 		goto out;
 
 	ptep = pte_offset_map(pmd, address);
-	if (!ptep)
+	if (unlikely(!ptep))
 		goto out;
 
 	pte = *ptep;
+#ifdef __i386__
+	if (unlikely(!pte_user(pte)))
+		goto out_pte_unmap;
+#endif
+
+	if (likely(pte_present(pte))) {
+		if (!write || pte_write(pte)) {
+			unsigned long pfn = pte_pfn(pte);
+			if (likely(pfn_valid(pfn))) {
+				struct page *page = pfn_to_page(pfn);
+
+				if (write && !pte_dirty(pte))
+					ptep_mkdirty(ptep);
+				pte_unmap(ptep);
+				mark_page_accessed(page);
+				return page;
+			}
+		}
+	}
+out_pte_unmap:
 	pte_unmap(ptep);
-	if (pte_present(pte) && pte_user(pte)) {
-		struct page *page = pte_page(pte);
-		prefetch(page);
-		if (!write ||
-		    (pte_write(pte) && pte_dirty(pte))) {
-			BUG_ON(!pfn_valid(pte_pfn(pte)));
-			return pte_page(pte);
+out:
+	return 0;
+}
+
+/*
+ * Do a quick page-table lookup for a single page.
+ * mm->page_table_lock must be held.
+ */
+struct page *
+follow_pin_page(struct mm_struct *mm, unsigned long address, int write) 
+{
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+	struct vm_area_struct *vma;
+	struct page *page1, *page2;
+
+	vma = hugepage_vma(mm, address);
+	if (unlikely(vma != NULL)) {
+repeat1:
+		page1 = follow_pin_huge_addr(mm, vma, address, write);
+	       	/* make sure we reload the pte */
+		barrier();
+		page2 = follow_pin_huge_addr(mm, vma, address, write);
+		if (unlikely(page1 != page2)) {
+			if (page1)
+				put_page(page1);
+			if (page2)
+				put_page(page2);
+			barrier();
+			goto repeat1;
 		}
+		// one too many counts;
+		if (page1)
+			put_page(page1);
+
+		return page1;
 	}
 
+	pgd = pgd_offset(mm, address);
+	if (unlikely(pgd_none(*pgd) || pgd_bad(*pgd)))
+		goto out;
+
+	pmd = pmd_offset(pgd, address);
+	if (unlikely(pmd_none(*pmd)))
+		goto out;
+	if (unlikely(pmd_huge(*pmd))) {
+repeat2:
+		page1 = follow_pin_huge_pmd(mm, address, pmd, write);
+		barrier();
+		page2 = follow_pin_huge_pmd(mm, address, pmd, write);
+		if (unlikely(page1 != page2)) {
+			if (page1)
+				put_page(page1);
+			if (page2)
+				put_page(page2);
+			barrier();
+			goto repeat2;
+		}
+		if (page1)
+			put_page(page1);
+
+		return page1;
+	}
+	if (unlikely(pmd_bad(*pmd)))
+		goto out;
+
+	ptep = pte_offset_map(pmd, address);
+	if (unlikely(!ptep))
+		goto out;
+
+repeat3:
+	pte = *ptep;
+	page1 = NULL;
+	if (likely(pte_present(pte) && pte_user(pte))) {
+		if (likely(!write || (pte_write(pte) && pte_dirty(pte))))
+			page1 = pte_page(pte);
+	}
+	if (likely(page1 && !PageReserved(page1)))
+		get_page(page1);
+	else
+		rmb();
+	barrier();
+	pte = *ptep;
+	page2 = NULL;
+	if (likely(pte_present(pte) && pte_user(pte))) {
+		if (likely(!write || (pte_write(pte) && pte_dirty(pte))))
+			page2 = pte_page(pte);
+	}
+	if (unlikely(page1 != page2)) {
+		if (page1)
+			put_page(page1);
+		barrier();
+		goto repeat3;
+	}
+	pte_unmap(ptep);
+
+	return page1;
 out:
 	return 0;
 }
 
+
+
 /* 
  * Given a physical address, is there a useful struct page pointing to
  * it?  This may become more complex in the future if we start dealing
@@ -723,7 +833,7 @@ int get_user_pages(struct task_struct *t
 		if ( !vma || (pages && vma->vm_flags & VM_IO) || !(flags & vma->vm_flags) )
 			return i ? : -EFAULT;
 		if (is_vm_hugetlb_page(vma)) {
-			i = follow_hugetlb_page(mm, vma, pages, vmas,
+			i = follow_pin_hugetlb_page(mm, vma, pages, vmas,
 						&start, &len, i);
 			continue;
 		}
diff -urNp linux-1160/mm/usercopy.c linux-1170/mm/usercopy.c
--- linux-1160/mm/usercopy.c
+++ linux-1170/mm/usercopy.c
@@ -21,42 +21,24 @@
 #include <asm/atomic_kmap.h>
 
 /*
- * Get kernel address of the user page and pin it.
+ * Get the kernel address of the user page and make it present.
  */
-static inline struct page *pin_page(unsigned long addr, int write)
+static struct page *fault_in_page(struct mm_struct *mm, unsigned long addr, int write)
 {
-	struct mm_struct *mm = current->mm ? : &init_mm;
 	struct page *page = NULL;
 	int ret;
 
-	spin_lock(&mm->page_table_lock);
-	/*
-	 * Do a quick atomic lookup first - this is the fastpath.
-	 */
-	page = follow_page(mm, addr, write);
-	if (likely(page != NULL)) {	
-		if (!PageReserved(page))
-			get_page(page);
-		spin_unlock(&mm->page_table_lock);
-		return page;
-	}
-
-	/*
-	 * No luck - bad address or need to fault in the page:
-	 */
 	spin_unlock(&mm->page_table_lock);
-
 	down_read(&mm->mmap_sem);
 	ret = get_user_pages(current, mm, addr, 1, write, 0, &page, NULL);
 	up_read(&mm->mmap_sem);
+	spin_lock(&mm->page_table_lock);
+
 	if (ret <= 0)
 		return NULL;
-	return page;
-}
-
-static inline void unpin_page(struct page *page)
-{
 	put_page(page);
+
+	return page;
 }
 
 /*
@@ -66,22 +48,37 @@ static inline void unpin_page(struct pag
  */
 static int rw_vm(unsigned long addr, void *buf, int len, int write)
 {
-	if (!len)
-		return 0;
+	struct mm_struct *mm = current->mm ? : &init_mm;
 
+	spin_lock(&mm->page_table_lock);
 	/* ignore errors, just check how much was sucessfully transfered */
 	while (len) {
-		struct page *page = NULL;
 		int bytes, offset;
+		struct page *page;
 		void *maddr;
 
-		page = pin_page(addr, write);
-		if (!page)
-			break;
+		/*
+		 * Do a quick atomic lookup first - this is the fastpath.
+		 */
+repeat:
+		page = follow_page(mm, addr, write);
+		/*
+		 * Slowpath - bad address or needs to fault in the page:
+		 */
+		if (unlikely(!page)) {
+			page = fault_in_page(mm, addr, write);
+			if (unlikely(!page))
+				break;
+			/*
+			 * Re-check the pagetable, with the spinlock
+			 * held:
+			 */
+			goto repeat;
+		}
 
 		bytes = len;
 		offset = addr & (PAGE_SIZE-1);
-		if (bytes > PAGE_SIZE-offset)
+		if (unlikely(bytes > PAGE_SIZE-offset))
 			bytes = PAGE_SIZE-offset;
 
 		maddr = kmap_atomic(page, KM_USER_COPY);
@@ -111,12 +108,11 @@ static int rw_vm(unsigned long addr, voi
 #undef HANDLE_TYPE
 		}
 		kunmap_atomic(maddr, KM_USER_COPY);
-		unpin_page(page);
 		len -= bytes;
 		buf += bytes;
 		addr += bytes;
 	}
-	
+	spin_unlock(&mm->page_table_lock);
 	return len;
 }
 
@@ -125,22 +121,39 @@ static int str_vm(unsigned long addr, vo
 	struct mm_struct *mm = current->mm ? : &init_mm;
 	struct page *page;
 	void *buf = buf0;
+	int write = (copy == 2);
 
 	if (!len)
 		return len;
 
-	down_read(&mm->mmap_sem);
+	spin_lock(&mm->page_table_lock);
 	/* ignore errors, just check how much was sucessfully transfered */
 	while (len) {
 		int bytes, ret, offset, left, copied;
 		char *maddr;
 
-		ret = get_user_pages(current, mm, addr, 1, copy == 2, 0, &page, NULL);
-		if (ret <= 0) {
-			up_read(&mm->mmap_sem);
-			return -EFAULT;
+		/*
+		 * Do a quick atomic lookup first - this is the fastpath.
+		 */
+repeat:
+		page = follow_page(mm, addr, write);
+		/*
+		 * Slowpath - bad address or needs to fault in the page:
+		 */
+		if (unlikely(!page)) {
+			page = fault_in_page(mm, addr, write);
+			if (unlikely(!page))
+				goto bad_page;
+			/*
+			 * Re-check the pagetable, with the spinlock
+			 * held:
+			 */
+			goto repeat;
 		}
 
+		if (unlikely(!page))
+			goto bad_page;
+
 		bytes = len;
 		offset = addr & (PAGE_SIZE-1);
 		if (bytes > PAGE_SIZE-offset)
@@ -160,16 +173,18 @@ static int str_vm(unsigned long addr, vo
 		}
 		BUG_ON(bytes < 0 || copied < 0);
 		kunmap_atomic(maddr, KM_USER_COPY);
-		page_cache_release(page);
 		len -= copied;
 		buf += copied;
 		addr += copied;
 		if (left)
 			break;
 	}
-	up_read(&mm->mmap_sem);
-	
+	spin_unlock(&mm->page_table_lock);
 	return len;
+
+bad_page:
+	spin_unlock(&mm->page_table_lock);
+	return -EFAULT;
 }
 
 /*
