diff -urNp linux-1120/arch/i386/kernel/entry.S linux-1130/arch/i386/kernel/entry.S
--- linux-1120/arch/i386/kernel/entry.S
+++ linux-1130/arch/i386/kernel/entry.S
@@ -784,7 +784,7 @@ ENTRY(sys_call_table)
 	.long SYMBOL_NAME(sys_ni_syscall)	/* sys_epoll_create */
 	.long SYMBOL_NAME(sys_ni_syscall)	/* sys_epoll_ctl 255 */
 	.long SYMBOL_NAME(sys_ni_syscall)	/* sys_epoll_wait */
- 	.long SYMBOL_NAME(sys_ni_syscall)	/* sys_remap_file_pages */
+ 	.long sys_remap_file_pages
  	.long SYMBOL_NAME(sys_set_tid_address)
  	.long SYMBOL_NAME(sys_ni_syscall)	/* sys_timer_create */
  	.long SYMBOL_NAME(sys_ni_syscall)	/* 260 sys_timer_settime */
diff -urNp linux-1120/fs/ramfs/inode.c linux-1130/fs/ramfs/inode.c
--- linux-1120/fs/ramfs/inode.c
+++ linux-1130/fs/ramfs/inode.c
@@ -37,7 +37,7 @@
 
 static struct super_operations ramfs_ops;
 static struct address_space_operations ramfs_aops;
-static struct file_operations ramfs_file_operations;
+struct file_operations ramfs_file_operations;
 static struct inode_operations ramfs_dir_inode_operations;
 
 static int ramfs_statfs(struct super_block *sb, struct statfs *buf)
@@ -264,17 +264,80 @@ static int ramfs_sync_file(struct file *
 	return 0;
 }
 
+static int ramfs_populate(struct vm_area_struct *vma,
+			unsigned long addr,
+			unsigned long len,
+			pgprot_t prot,
+			unsigned long pgoff,
+			int nonblock)
+{
+	struct file *file = vma->vm_file;
+	struct address_space *mapping = file->f_dentry->d_inode->i_mapping;
+	struct inode *inode = mapping->host;
+	unsigned long size;
+	struct mm_struct *mm = vma->vm_mm;
+	struct page *page;
+	int err;
+
+repeat:
+	size = (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	if (pgoff + (len >> PAGE_CACHE_SHIFT) > size)
+		return -EINVAL;
+
+	page = find_get_page(mapping, pgoff);
+	if (!page)
+		return -ENOMEM;
+	if (page) {
+		err = install_page(mm, vma, addr, page, prot);
+		if (err) {
+			page_cache_release(page);
+			return err;
+		}
+		/*
+		 * Zap the page from the LRU:
+		 */
+		if (PageLRU(page))
+			lru_cache_del(page);
+	}
+
+	len -= PAGE_SIZE;
+	addr += PAGE_SIZE;
+	pgoff++;
+	if (len)
+		goto repeat;
+
+	return 0;
+}
+
+static struct vm_operations_struct ramfs_vm_ops = {
+	nopage: filemap_nopage,
+	populate: ramfs_populate,
+};
+
 static struct address_space_operations ramfs_aops = {
 	readpage:	ramfs_readpage,
 	writepage:	fail_writepage,
 	prepare_write:	ramfs_prepare_write,
-	commit_write:	ramfs_commit_write
+	commit_write:	ramfs_commit_write,
 };
 
-static struct file_operations ramfs_file_operations = {
+static int ramfs_mmap(struct file * file, struct vm_area_struct * vma)
+{
+	struct vm_operations_struct * ops;
+	struct inode *inode = file->f_dentry->d_inode;
+
+	ops = &ramfs_vm_ops;
+	if (!inode->i_sb || !S_ISREG(inode->i_mode))
+		return -EACCES;
+	UPDATE_ATIME(inode);
+	vma->vm_ops = ops;
+	return 0;
+}
+
+struct file_operations ramfs_file_operations = {
 	read:		generic_file_read,
 	write:		generic_file_write,
-	mmap:		generic_file_mmap,
+	mmap:		ramfs_mmap,
 	fsync:		ramfs_sync_file,
 };
 
@@ -300,6 +363,7 @@ static struct super_block *ramfs_read_su
 	struct inode * inode;
 	struct dentry * root;
 
+	sb->s_maxbytes = MAX_LFS_FILESIZE;
 	sb->s_blocksize = PAGE_CACHE_SIZE;
 	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
 	sb->s_magic = RAMFS_MAGIC;
diff -urNp linux-1120/include/asm-i386/unistd.h linux-1130/include/asm-i386/unistd.h
--- linux-1120/include/asm-i386/unistd.h
+++ linux-1130/include/asm-i386/unistd.h
@@ -258,6 +258,7 @@
 #define __NR_free_hugepages	251
 #define __NR_exit_group		252
 #define __NR_lookup_dcookie	253
+#define __NR_remap_file_pages   257
 #define __NR_set_tid_address	258
 #define __NR_tgkill		270
 
diff -urNp linux-1120/include/linux/mm.h linux-1130/include/linux/mm.h
--- linux-1120/include/linux/mm.h
+++ linux-1130/include/linux/mm.h
@@ -120,6 +120,7 @@ struct vm_area_struct {
 #define VM_RESERVED	0x00080000	/* Don't unmap it from swap_out */
 
 #define VM_ACCOUNT	0x00100000	/* Memory is a vm accounted object */
+#define VM_NO_UNLOCK	0x00800000	/* do not unlock */
 
 /* arches may define VM_STACK_FLAGS for their own purposes */
 #ifndef VM_STACK_FLAGS
@@ -156,6 +157,7 @@ struct vm_operations_struct {
 	void (*open)(struct vm_area_struct * area);
 	void (*close)(struct vm_area_struct * area);
 	struct page * (*nopage)(struct vm_area_struct * area, unsigned long address, int unused);
+	int (*populate)(struct vm_area_struct * area, unsigned long address, unsigned long len, pgprot_t prot, unsigned long pgoff, int nonblock);
 };
 
 /* forward declaration; pte_chain is meant to be internal to rmap.c */
@@ -616,6 +618,8 @@ extern int vmtruncate(struct inode * ino
 extern pmd_t *FASTCALL(__pmd_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address));
 extern pte_t *FASTCALL(pte_alloc_kernel(struct mm_struct *mm, pmd_t *pmd, unsigned long address));
 extern pte_t *FASTCALL(pte_alloc_map(struct mm_struct *mm, pmd_t *pmd, unsigned long address));
+extern int install_page(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr, struct page *page, pgprot_t prot);
+extern long sys_remap_file_pages(unsigned long start, unsigned long size, unsigned long prot, unsigned long pgoff, unsigned long nonblock);
 extern int handle_mm_fault(struct mm_struct *mm,struct vm_area_struct *vma, unsigned long address, int write_access);
 extern int make_pages_present(unsigned long addr, unsigned long end);
 extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
diff -urNp linux-1120/include/linux/mman.h linux-1130/include/linux/mman.h
--- linux-1120/include/linux/mman.h
+++ linux-1130/include/linux/mman.h
@@ -10,4 +10,12 @@ extern int vm_enough_memory(long pages);
 extern void vm_unacct_memory(long pages);
 extern void vm_validate_enough(char *x);
 
+#ifndef MAP_POPULATE
+# define MAP_POPULATE 0x8000
+#endif
+
+#ifndef MAP_NONBLOCK
+# define MAP_NONBLOCK 0x10000
+#endif
+
 #endif /* _LINUX_MMAN_H */
diff -urNp linux-1120/kernel/fork.c linux-1130/kernel/fork.c
--- linux-1120/kernel/fork.c
+++ linux-1130/kernel/fork.c
@@ -228,7 +228,8 @@ static inline int dup_mmap(struct mm_str
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
-		tmp->vm_flags &= ~VM_LOCKED;
+		if (likely(!(tmp->vm_flags & VM_NO_UNLOCK)))
+			tmp->vm_flags &= ~VM_LOCKED;
 		tmp->vm_mm = mm;
 		tmp->vm_next = NULL;
 		file = tmp->vm_file;
diff -urNp linux-1120/mm/Makefile linux-1130/mm/Makefile
--- linux-1120/mm/Makefile
+++ linux-1130/mm/Makefile
@@ -14,7 +14,7 @@ export-objs := shmem.o filemap.o memory.
 obj-y	 := memory.o mmap.o filemap.o mprotect.o mlock.o mremap.o \
 	    vmalloc.o slab.o bootmem.o swap.o vmscan.o page_io.o \
 	    page_alloc.o swap_state.o swapfile.o numa.o oom_kill.o \
-	    shmem.o mempool.o vcache.o rmap.o
+	    shmem.o mempool.o vcache.o rmap.o fremap.o
 
 obj-$(CONFIG_HIGHMEM) += highmem.o
 obj-$(CONFIG_X86_UACCESS_INDIRECT) += usercopy.o
diff -urNp linux-1120/mm/fremap.c linux-1130/mm/fremap.c
--- linux-1120/mm/fremap.c
+++ linux-1130/mm/fremap.c
@@ -0,0 +1,146 @@
+/*
+ *   linux/mm/fremap.c
+ * 
+ * Explicit pagetable population and nonlinear (random) mappings support.
+ *
+ * started by Ingo Molnar, Copyright (C) 2002
+ */
+
+#include <linux/mm.h>
+#include <linux/file.h>
+#include <linux/mman.h>
+#include <linux/pagemap.h>
+#include <asm/mmu_context.h>
+
+static inline int zap_pte(struct mm_struct *mm, pte_t *ptep)
+{
+	pte_t pte = *ptep;
+
+	if (pte_none(pte))
+		return 0;
+	if (pte_present(pte)) {
+		unsigned long pfn = pte_pfn(pte);
+
+		pte = ptep_get_and_clear(ptep);
+		if (pfn_valid(pfn)) {
+			struct page *page = pfn_to_page(pfn);
+			if (!PageReserved(page)) {
+				if (!PageDirty(page) && pte_dirty(pte))
+					set_page_dirty(page);
+				/*
+				 * An rmap-ed page might exist already,
+				 * if we zap it here then remove the pte
+				 * chain entry as well:
+				 */
+				if (page->pte.direct)
+					page_remove_rmap(page, ptep);
+				page_cache_release(page);
+				mm->rss--;
+			}
+		}
+		return 1;
+	} else {
+		free_swap_and_cache(pte_to_swp_entry(pte));
+		pte_clear(ptep);
+		return 0;
+	}
+}
+
+/*
+ * Install a page to a given virtual memory address, release any
+ * previously existing mapping.
+ */
+int install_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		unsigned long addr, struct page *page, pgprot_t prot)
+{
+	int err = -ENOMEM, flush;
+	pte_t *pte, entry;
+	pgd_t *pgd;
+	pmd_t *pmd;
+
+	pgd = pgd_offset(mm, addr);
+	spin_lock(&mm->page_table_lock);
+
+	pmd = pmd_alloc(mm, pgd, addr);
+	if (!pmd)
+		goto err_unlock;
+
+	pte = pte_alloc_map(mm, pmd, addr);
+	if (!pte)
+		goto err_unlock;
+
+	flush = zap_pte(mm, pte);
+
+	mm->rss++;
+	flush_page_to_ram(page);
+	flush_icache_page(vma, page);
+	entry = mk_pte(page, prot);
+	set_pte(pte, entry);
+	pte_unmap(pte);
+	if (flush)
+		flush_tlb_page(vma, addr);
+
+	spin_unlock(&mm->page_table_lock);
+	return 0;
+
+err_unlock:
+	spin_unlock(&mm->page_table_lock);
+err:
+	return err;
+}
+
+/***
+ * sys_remap_file_pages - remap arbitrary pages of a shared backing store
+ *                        file within an existing vma.
+ * @start: start of the remapped virtual memory range
+ * @size: size of the remapped virtual memory range
+ * @prot: new protection bits of the range
+ * @pgoff: to be mapped page of the backing store file
+ * @flags: 0 or MAP_NONBLOCK - the later will cause no IO.
+ *
+ * this syscall works purely via pagetables, so it's the most efficient
+ * way to map the same (large) file into a given virtual window. Unlike
+ * mmap()/mremap() it does not create any new vmas. The new mappings are
+ * also safe across swapout.
+ *
+ * NOTE: the 'prot' parameter right now is ignored, and the vma's default
+ * protection is used. Arbitrary protections might be implemented in the
+ * future.
+ */
+long sys_remap_file_pages(unsigned long start, unsigned long size,
+	unsigned long __prot, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long end = start + size;
+	struct vm_area_struct *vma;
+	int err = -EINVAL;
+	extern struct file_operations ramfs_file_operations;
+
+	if (__prot || (start & ~PAGE_MASK) || (size & ~PAGE_MASK))
+		return err;
+
+	/* Does the address range wrap, or is the span zero-sized? */
+	if (start + size <= start)
+		return err;
+
+	down_read(&mm->mmap_sem);
+
+	vma = find_vma(mm, start);
+	/*
+	 * Make sure the vma is shared, that it supports prefaulting or
+	 * it represents a ramfs mapping and that the remapped range is 
+	 * valid and fully within the single existing vma:
+	 */
+	if (vma && (vma->vm_flags & VM_SHARED) && 
+	    ((vma->vm_flags & VM_LOCKED) || 
+	     (vma->vm_file && vma->vm_file->f_op == &ramfs_file_operations)) &&
+		vma->vm_ops && vma->vm_ops->populate &&
+			end > start && start >= vma->vm_start &&
+				end <= vma->vm_end)
+		err = vma->vm_ops->populate(vma, start, size, vma->vm_page_prot,
+				pgoff, flags & MAP_NONBLOCK);
+
+	up_read(&mm->mmap_sem);
+
+	return err;
+}
diff -urNp linux-1120/mm/mlock.c linux-1130/mm/mlock.c
--- linux-1120/mm/mlock.c
+++ linux-1130/mm/mlock.c
@@ -169,8 +169,11 @@ static int do_mlock(unsigned long start,
 		/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */
 
 		newflags = vma->vm_flags | VM_LOCKED;
-		if (!on)
+		if (!on) {
 			newflags &= ~VM_LOCKED;
+			if (vma->vm_flags & VM_NO_UNLOCK)
+				return -EINVAL; 
+		}
 
 		if (vma->vm_end >= end) {
 			error = mlock_fixup(vma, nstart, end, newflags);
@@ -257,8 +260,11 @@ static int do_mlockall(int flags)
 		unsigned int newflags;
 
 		newflags = vma->vm_flags | VM_LOCKED;
-		if (!(flags & MCL_CURRENT))
+		if (!(flags & MCL_CURRENT)) {
 			newflags &= ~VM_LOCKED;
+			if (vma->vm_flags & VM_NO_UNLOCK)
+				continue;
+		}
 		error = mlock_fixup(vma, vma->vm_start, vma->vm_end, newflags);
 		if (error)
 			break;
diff -urNp linux-1120/mm/mmap.c linux-1130/mm/mmap.c
--- linux-1120/mm/mmap.c
+++ linux-1130/mm/mmap.c
@@ -708,6 +708,12 @@ out:	
 		mm->locked_vm += len >> PAGE_SHIFT;
 		make_pages_present(addr, addr + len);
 	}
+	if (flags & MAP_POPULATE) {
+		up_write(&mm->mmap_sem);
+		sys_remap_file_pages(addr, len, prot,
+					pgoff, flags & MAP_NONBLOCK);
+		down_write(&mm->mmap_sem);
+	}
 	vm_validate_enough("out from do_mmap_pgoff");
 	return addr;
 
diff -urNp linux-1120/mm/shmem.c linux-1130/mm/shmem.c
--- linux-1120/mm/shmem.c
+++ linux-1130/mm/shmem.c
@@ -711,6 +711,44 @@ struct page * shmem_nopage(struct vm_are
 	return(page);
 }
 
+static int shmem_populate(struct vm_area_struct *vma,
+	unsigned long addr, unsigned long len,
+	pgprot_t prot, unsigned long pgoff, int nonblock)
+{
+	struct inode *inode = vma->vm_file->f_dentry->d_inode;
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long size;
+
+	size = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	if (pgoff >= size || pgoff + (len >> PAGE_SHIFT) > size)
+		return -EINVAL;
+	
+	vma->vm_flags |= VM_NO_UNLOCK;
+
+	while ((long) len > 0) {
+		struct page *page = NULL;
+		int err;
+		/*
+		 * Will need changing if PAGE_CACHE_SIZE != PAGE_SIZE
+		 */
+		err = shmem_getpage(inode, pgoff, &page);
+		if (err)
+			return err;
+		if (page) {
+			mark_page_accessed(page);
+			err = install_page(mm, vma, addr, page, prot);
+			if (err) {
+				page_cache_release(page);
+				return err;
+			}
+		}
+		len -= PAGE_SIZE;
+		addr += PAGE_SIZE;
+		pgoff++;
+	}
+	return 0;
+}
+
 int shmem_lock(struct file *file, int lock,
 		struct mm_struct **locker_mm_p, pid_t *locker_pid_p)
 {
@@ -1585,6 +1623,7 @@ static struct super_operations shmem_ops
 
 static struct vm_operations_struct shmem_vm_ops = {
 	nopage:	shmem_nopage,
+	populate: shmem_populate,
 };
 
 #ifdef CONFIG_TMPFS
