diff -urNp linux-210/arch/i386/kernel/mpparse.c linux-211/arch/i386/kernel/mpparse.c
--- linux-210/arch/i386/kernel/mpparse.c
+++ linux-211/arch/i386/kernel/mpparse.c
@@ -67,7 +67,6 @@ static unsigned int num_processors;
 
 /* Bitmask of physically existing CPUs */
 unsigned long phys_cpu_present_map;
-unsigned long logical_cpu_present_map;
 
 #ifdef CONFIG_X86_CLUSTERED_APIC
 unsigned char esr_disable = 0;
@@ -157,10 +156,19 @@ static struct mpc_config_translation *tr
 void __init MP_processor_info (struct mpc_config_processor *m)
 {
  	int ver, quad, logical_apicid;
+	extern unsigned int sibling_ht_mask;
  	
 	if (!(m->mpc_cpuflag & CPU_ENABLED))
 		return;
 
+	/* If the "noht" boot option is set, then enable only the
+	 * first CPU of a sibling set...
+	 */
+        if (m->mpc_apicid & sibling_ht_mask) {
+		m->mpc_cpuflag &= ~CPU_ENABLED;
+		return;
+	}
+
 	logical_apicid = m->mpc_apicid;
 	if (clustered_apic_mode == CLUSTERED_APIC_NUMAQ) {
 		quad = translation_table[mpc_record]->trans_quad;
@@ -245,7 +253,6 @@ void __init MP_processor_info (struct mp
 	if (APIC_XAPIC_SUPPORT(ver) && !xapic_support_disabled)
 		xapic_support = 1;
 
-	logical_cpu_present_map |= 1 << (num_processors-1);
  	phys_cpu_present_map |= apicid_to_phys_cpu_present(m->mpc_apicid);
  
 	/*
@@ -588,10 +595,21 @@ static int __init smp_read_mpc(struct mp
 		++mpc_record;
 	}
 
-	if (clustered_apic_mode){
-		phys_cpu_present_map = logical_cpu_present_map;
-	}
+	if (clustered_apic_mode) {
+		int i;
+		unsigned long new_map = 0;
+
+		/* Since we're in clustered APIC mode, rebuild
+		 * phys_cpu_present_map from the actual APIC IDs
+		 * already scanned.
+		 */
 
+		for (i = 0; i < NR_CPUS; i++)
+			if (raw_phys_apicid[i] != BAD_APICID)
+				new_map |= apicid_to_phys_cpu_present(raw_phys_apicid[i]);
+
+		phys_cpu_present_map = new_map;
+	}
 
 	if (!num_processors)
 		printk(KERN_ERR "SMP mptable: no processors registered!\n");
@@ -838,12 +856,24 @@ void __init get_smp_config (void)
 	if ((clustered_apic_mode == CLUSTERED_APIC_NONE) &&
 	    (xapic_support) &&
 	    (num_processors > FLAT_APIC_CPU_MAX)) {
+		int i;
+		unsigned long new_map = 0;
+
 		clustered_apic_mode = CLUSTERED_APIC_XAPIC;
 		apic_broadcast_id = APIC_BROADCAST_ID_XAPIC;
 		int_dest_addr_mode = APIC_DEST_PHYSICAL;
 		int_delivery_mode = dest_Fixed;
 		esr_disable = 1;
-		phys_cpu_present_map = logical_cpu_present_map;
+
+		/* Since we switched from flat to clustered mode,
+		 * we need to reinterepret the APIC IDs we already
+		 * scanned.
+		 */
+		for (i = 0; i < NR_CPUS; i++)
+			if (raw_phys_apicid[i] != BAD_APICID)
+				new_map |= apicid_to_phys_cpu_present(raw_phys_apicid[i]);
+
+		phys_cpu_present_map = new_map;
 	}
 #endif
 
diff -urNp linux-210/arch/i386/kernel/setup.c linux-211/arch/i386/kernel/setup.c
--- linux-210/arch/i386/kernel/setup.c
+++ linux-211/arch/i386/kernel/setup.c
@@ -121,6 +121,9 @@
 #include <asm/mpspec.h>
 #include <asm/mmu_context.h>
 #include <asm/edd.h>
+#ifdef CONFIG_SMP
+#include <asm/pci-direct.h>
+#endif
 /*
  * Machine setup..
  */
@@ -178,6 +181,7 @@ static u32 disabled_x86_caps[NCAPINTS] _
 extern int blk_nohighio;
 
 int enable_acpi_smp_table;
+unsigned int sibling_ht_mask;
 
 /*
  * This is set up by the setup-routine at boot-time
@@ -1108,6 +1112,62 @@ static void __init register_memory(unsig
 		pci_mem_start = low_mem_size;
 }
 
+/*
+ * find out the number of processor cores on the die
+ */
+static int __init num_cpu_cores(struct cpuinfo_x86 *c)
+{
+	unsigned int eax, level;
+
+	level = c ? c->cpuid_level : cpuid_eax(0);
+
+	if (level < 4)
+		return 1;
+
+	__asm__("cpuid"
+		: "=a" (eax)
+		: "0" (4), "c" (0)
+		: "bx", "dx");
+	if (eax & 0x1f)
+		return ((eax >> 26) + 1);
+	else
+		return 1;
+}
+
+static void __init noht_init(void)
+{
+	unsigned int cpuid_level;
+	char vendorid[13];
+	vendorid[12] = 0;
+
+	/* Have to get CPUID level and vendor ID here since
+	 * we haven't populated boot_cpu_data yet
+	 */
+
+	cpuid(0, &cpuid_level, (int *)&vendorid[0], (int *)&vendorid[8], (int *)&vendorid[4]);
+
+	if (!strncmp(vendorid, "GenuineIntel", 12)) {
+		unsigned int siblings, eax, ebx;
+		/* If "noht" specified, calculate a mask that
+		 * can be used to determine which APIC IDs
+		 * correspond to the first CPU in a sibling
+		 * set.
+		 */
+		printk("Disabling hyperthreading\n");
+		ebx = cpuid_ebx(1);
+		siblings = (ebx & 0xff0000) >> 16;
+		if (!siblings)
+			siblings = 1;
+		else
+			siblings /= num_cpu_cores(NULL);
+
+		sibling_ht_mask = 1;
+		while (sibling_ht_mask < siblings)
+			sibling_ht_mask <<= 1;
+		sibling_ht_mask--;
+	}
+}
+
 void __init setup_arch(char **cmdline_p)
 {
 	unsigned long max_low_pfn;
@@ -1164,6 +1224,7 @@ void __init setup_arch(char **cmdline_p)
 	 * parsing ACPI SMP table might prove useful on some non-HT cpu.
 	 */
 	if (disable_x86_ht) {
+		noht_init();
 		clear_bit(X86_FEATURE_HT, &boot_cpu_data.x86_capability[0]);
 		set_bit(X86_FEATURE_HT, disabled_x86_caps);
 		enable_acpi_smp_table = 0;
@@ -1337,11 +1398,47 @@ static void __init display_cacheinfo(str
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
+#ifdef CONFIG_SMP
+/* Defines for disabling C1 clock ramping */
+#define NB_PCI_ADDR    0x18
+#define NB_PM_DEV      3
+#define NB_C1_REG      0x84
+#define NB_C1_MASK     0xfcffffff
+
+/* processor's cpuid instruction support */
+#define CPUID_PROCESSOR_SIGNATURE	1
+#define CPUID_XMOD 			0x000f0000
+#define CPUID_XMOD_REV_E 		0x00020000
+#endif
 static int __init init_amd(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
 	int mbytes = max_mapnr >> (20-PAGE_SHIFT);
 	int r;
+#ifdef CONFIG_SMP
+	int tmp, cpu;
+	unsigned int coreshift = 0;
+	u32 reg;
+	u32 eax;
+
+	/* Figure out what physical package we're on. */
+	if (cpuid_ebx(1) & 0xff0000) {
+		/* nonzero sibling count */
+		if (cpuid_eax(0x80000000) >= 0x80000008 &&
+		    (tmp = (cpuid_ecx(0x80000008) & 0xff)) > 0) {
+			smp_num_cores = tmp + 1;
+			do {
+				coreshift++;
+				tmp >>= 1;
+			} while (tmp > 0);
+		}
+	}
+
+	cpu = smp_processor_id();
+	tmp = hard_smp_processor_id();
+	cpu_core_id[cpu] = tmp & ((1 << coreshift) - 1);
+	phys_proc_id[cpu] = tmp >> coreshift;
+#endif
 
 	/*
 	 *	FIXME: We should handle the K5 here. Set up the write
@@ -1481,6 +1578,24 @@ static int __init init_amd(struct cpuinf
 				}
 			}
 			break;
+#ifdef CONFIG_SMP
+		case 0x0f:
+			eax = cpuid_eax(CPUID_PROCESSOR_SIGNATURE);
+			if ((eax & CPUID_XMOD) >= CPUID_XMOD_REV_E) {
+				/* Disable C1 clock ramping to avoid TSC
+				   jitter */
+				printk(KERN_INFO
+				       "Disabled C1 clock ramping...\n");
+				reg = read_pci_config(0, NB_PCI_ADDR +
+						      phys_proc_id[cpu],
+						      NB_PM_DEV, NB_C1_REG);
+				write_pci_config(0, NB_PCI_ADDR +
+						 phys_proc_id[cpu],
+						 NB_PM_DEV, NB_C1_REG,
+						 reg & NB_C1_MASK);
+			}
+			break;
+#endif
 	}
 
 	display_cacheinfo(c);
@@ -2567,7 +2682,6 @@ static void __init init_intel(struct cpu
 	
 #ifdef CONFIG_SMP
 	if (test_bit(X86_FEATURE_HT, &c->x86_capability) && !disable_x86_ht) {
-		extern	int phys_proc_id[NR_CPUS];
 		extern int acpi_proc_id[NR_CPUS];
 		extern int acpi_provides_cpus;
 		static int warned_mismatch;
@@ -2582,12 +2696,14 @@ static void __init init_intel(struct cpu
 		if (smp_num_siblings == 1) {
 			printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
 		} else if (smp_num_siblings > 1 ) {
+			smp_num_cores = num_cpu_cores(c);
+			smp_num_siblings /= smp_num_cores;
 			/*
 			 * At this point we only support two siblings per
 			 * processor package.
 			 */
 #define NR_SIBLINGS	2
-			if (smp_num_siblings != NR_SIBLINGS) {
+			if (smp_num_siblings > NR_SIBLINGS) {
 				printk(KERN_WARNING "CPU: Unsupported number of the siblings %d", smp_num_siblings);
 				smp_num_siblings = 1;
 				return;
@@ -2618,13 +2734,34 @@ static void __init init_intel(struct cpu
 				if (index_lsb != index_msb )
 					index_msb++;
 				initial_apic_id = ebx >> 24 & 0xff;
+				cpu_core_id[cpu] = initial_apic_id >> index_msb;
+
+				index_lsb = 0;
+				index_msb = 31;
+
+				tmp = smp_num_siblings * smp_num_cores;
+				while ((tmp & 1) == 0) {
+					tmp >>=1 ;
+					index_lsb++;
+				}
+				tmp = smp_num_siblings * smp_num_cores;
+				while ((tmp & 0x80000000 ) == 0) {
+					tmp <<=1 ;
+					index_msb--;
+				}
+				if (index_lsb != index_msb )
+					index_msb++;
 				phys_proc_id[cpu] = initial_apic_id >> index_msb;
 			} else {
-				phys_proc_id[cpu] = hard_smp_processor_id() & ~(smp_num_siblings - 1);
+				cpu_core_id[cpu] = hard_smp_processor_id() & ~(smp_num_siblings - 1);
+				phys_proc_id[cpu] = hard_smp_processor_id() & ~(smp_num_siblings * smp_num_cores - 1);
 			}
 
 			printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-                               phys_proc_id[cpu]);
+			       phys_proc_id[cpu]);
+			if (smp_num_cores > 1)
+				printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
+				       cpu_core_id[cpu]);
 		}
 
 	}
@@ -3181,7 +3318,6 @@ static int show_cpuinfo(struct seq_file 
 	 * applications want to get the raw CPUID data, they should access
 	 * /dev/cpu/<cpu_nr>/cpuid instead.
 	 */
-	extern	int phys_proc_id[NR_CPUS];
 	static char *x86_cap_flags[] = {
 		/* Intel-defined */
 	        "fpu", "vme", "de", "pse", "tsc", "msr", "pae", "mce",
@@ -3242,7 +3378,11 @@ static int show_cpuinfo(struct seq_file 
 
 #ifdef CONFIG_SMP
 	seq_printf(m, "physical id\t: %d\n",phys_proc_id[n]);
-	seq_printf(m, "siblings\t: %d\n",smp_num_siblings);
+	seq_printf(m, "siblings\t: %d\n",smp_num_siblings * smp_num_cores);
+	if (smp_num_cores > 1) {
+		seq_printf(m, "core id\t\t: %d\n",cpu_core_id[n]);
+		seq_printf(m, "cpu cores\t: %d\n",smp_num_cores);
+	}
 #if CONFIG_SHARE_RUNQUEUE
 {
 	extern long __rq_idx[NR_CPUS];
diff -urNp linux-210/arch/i386/kernel/smpboot.c linux-211/arch/i386/kernel/smpboot.c
--- linux-210/arch/i386/kernel/smpboot.c
+++ linux-211/arch/i386/kernel/smpboot.c
@@ -59,7 +59,9 @@ int smp_num_cpus = 1;
 
 /* Number of siblings per CPU package */
 int smp_num_siblings = 1;
+int smp_num_cores = 1;
 int phys_proc_id[NR_CPUS]; /* Package ID of each logical CPU */
+int cpu_core_id[NR_CPUS]; /* Core ID of each logical CPU */
 
 /* Bitmask of currently online CPUs */
 unsigned long cpu_online_map;
@@ -445,7 +447,7 @@ void __init smp_callin(void)
 	/*
 	 *      Synchronize the TSC with the BP
 	 */
-	if (cpu_has_tsc > 1)
+	if (cpu_has_tsc && (boot_cpu_data.x86_vendor == X86_VENDOR_AMD))
 		synchronize_tsc_ap();
 }
 
@@ -1117,14 +1119,19 @@ void __init smp_boot_cpus(void)
 
 	for (bit = 0; bit < NR_CPUS; bit++) {
 		apicid = cpu_present_to_apicid(bit);
+
 		/*
 		 * Don't even attempt to start the boot CPU!
 		 */
 		if (apicid == boot_cpu_apicid)
 			continue;
 
-		if (!(phys_cpu_present_map & (1ul << bit)))
+		if (apicid == BAD_APICID)
+			continue;
+
+		if (!(phys_cpu_present_map & apicid_to_phys_cpu_present(apicid)))
 			continue;
+
 		if ((max_cpus >= 0) && (max_cpus <= cpucount+1))
 			continue;
 
@@ -1134,7 +1141,7 @@ void __init smp_boot_cpus(void)
 		 * Make sure we unmap all failed CPUs
 		 */
 		if ((boot_apicid_to_cpu(apicid) == -1) &&
-				(phys_cpu_present_map & (1ul << bit)))
+		    (phys_cpu_present_map & apicid_to_phys_cpu_present(apicid)))
 			printk("CPU #%d/0x%02x not responding - cannot use it.\n",
 								bit, apicid);
 	}
@@ -1202,7 +1209,8 @@ void __init smp_boot_cpus(void)
 	/*
 	 * Synchronize the TSC with the AP
 	 */
-	if (cpu_has_tsc > 1 && cpucount && cpu_khz)
+	if (cpu_has_tsc && cpucount && cpu_khz &&
+	    (boot_cpu_data.x86_vendor == X86_VENDOR_AMD))
 		synchronize_tsc_bp();
 
 	/*
@@ -1220,7 +1228,7 @@ void __init smp_boot_cpus(void)
 			for (i = 0; i < smp_num_cpus; i++) {
 				if (i == cpu)
 					continue;
-				if (phys_proc_id[cpu] == phys_proc_id[i]) {
+				if (cpu_core_id[cpu] == cpu_core_id[i]) {
 					cpu_sibling_map[cpu] = i;
 					printk("cpu_sibling_map[%d] = %d\n", cpu, cpu_sibling_map[cpu]);
 					break;
diff -urNp linux-210/arch/x86_64/kernel/Makefile linux-211/arch/x86_64/kernel/Makefile
--- linux-210/arch/x86_64/kernel/Makefile
+++ linux-211/arch/x86_64/kernel/Makefile
@@ -39,7 +39,8 @@ obj-$(CONFIG_X86_CPUID)	+= cpuid.o
 obj-$(CONFIG_ACPI)	+= acpi.o
 obj-$(CONFIG_SMP)	+= smp.o smpboot.o trampoline.o
 obj-$(CONFIG_X86_LOCAL_APIC)	+= apic.o  nmi.o
-obj-$(CONFIG_X86_IO_APIC)	+= io_apic.o mpparse.o
+obj-$(CONFIG_X86_IO_APIC)	+= io_apic.o mpparse.o \
+		genapic.o genapic_cluster.o genapic_flat.o
 obj-$(CONFIG_EARLY_PRINTK) +=  early_printk.o
 obj-$(CONFIG_GART_IOMMU) += pci-gart.o aperture.o
 obj-$(CONFIG_DUMMY_IOMMU) += pci-nommu.o
diff -urNp linux-210/arch/x86_64/kernel/apic.c linux-211/arch/x86_64/kernel/apic.c
--- linux-210/arch/x86_64/kernel/apic.c
+++ linux-211/arch/x86_64/kernel/apic.c
@@ -30,6 +30,7 @@
 #include <asm/mpspec.h>
 #include <asm/pgalloc.h>
 #include <asm/timex.h>
+#include <asm/mach_apic.h>
 
 extern spinlock_t i8253_lock;
 
@@ -283,10 +284,8 @@ void __init setup_local_APIC (void)
 
 	/*
 	 * Double-check wether this APIC is really registered.
-	 * This is meaningless in clustered apic mode, so we skip it.
 	 */
-	if (!clustered_apic_mode &&
-	    !test_bit(GET_APIC_ID(apic_read(APIC_ID)), &phys_cpu_present_map))
+	if (!apic_id_registered())
 		BUG();
 
 	/*
@@ -294,23 +293,7 @@ void __init setup_local_APIC (void)
 	 * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel
 	 * document number 292116).  So here it goes...
 	 */
-
-	if (!clustered_apic_mode) {
-		/*
-		 * In clustered apic mode, the firmware does this for us
-		 * Put the APIC into flat delivery mode.
-		 * Must be "all ones" explicitly for 82489DX.
-		 */
-		apic_write_around(APIC_DFR, 0xffffffff);
-
-		/*
-		 * Set up the logical destination ID.
-		 */
-		value = apic_read(APIC_LDR);
-		value &= ~APIC_LDR_MASK;
-		value |= (1<<(smp_processor_id()+24));
-		apic_write_around(APIC_LDR, value);
-	}
+	init_apic_ldr();
 
 	/*
 	 * Set Task Priority to 'accept all'. We never change this
diff -urNp linux-210/arch/x86_64/kernel/e820.c linux-211/arch/x86_64/kernel/e820.c
--- linux-210/arch/x86_64/kernel/e820.c
+++ linux-211/arch/x86_64/kernel/e820.c
@@ -16,6 +16,7 @@
 #include <asm/bootsetup.h>
 #include <asm/smp.h>
 
+extern int disable_timer_pin_1;
 extern unsigned long table_start, table_end;
 extern char _end[];
 
@@ -515,7 +516,7 @@ extern int fallback_aper_order;
 extern int iommu_setup(char *opt);
 extern int acpi_disabled;
 
-void __init parse_mem_cmdline (char ** cmdline_p)
+void __init parse_cmdline_early (char ** cmdline_p)
 {
 	char c = ' ', *to = command_line, *from = COMMAND_LINE;
 	int len = 0;
@@ -578,6 +579,8 @@ void __init parse_mem_cmdline (char ** c
 		else if (!memcmp(from, "acpi_sci=low", 12))
 			acpi_sci_flags.polarity = 3;
 #endif
+		else if (!memcmp(from, "disable_timer_pin_1", 19))
+			disable_timer_pin_1 = 1;
 #ifdef CONFIG_SWIOTLB
 		else if (!memcmp(from, "swiotlb=", 8)) {
 			if (to != command_line)
@@ -598,6 +601,10 @@ void __init parse_mem_cmdline (char ** c
 			use_pmtmr = 1;
 		}
 #endif
+		else if (!memcmp(from, "noht", 4)) {
+			extern int disable_x86_ht;
+			disable_x86_ht = 1;
+		}
 	next:
 		c = *(from++);
 		if (!c)
diff -urNp linux-210/arch/x86_64/kernel/genapic.c linux-211/arch/x86_64/kernel/genapic.c
--- linux-210/arch/x86_64/kernel/genapic.c
+++ linux-211/arch/x86_64/kernel/genapic.c
@@ -0,0 +1,74 @@
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Generic APIC sub-arch probe layer.
+ *
+ * Hacked for x86-64 by James Cleverdon from i386 architecture code by
+ * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
+ * James Cleverdon.
+ */
+#include <linux/config.h>
+#include <linux/threads.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/ctype.h>
+#include <linux/init.h>
+#include <asm/smp.h>
+#include <asm/ipi.h>
+
+extern struct genapic apic_cluster;
+extern struct genapic apic_flat;
+
+struct genapic *genapic;
+
+
+/*
+ * Check the APIC IDs in bios_cpu_apicid and choose the APIC mode.
+ */
+void __init clustered_apic_check(void)
+{
+	long i;
+	u8 clusters, max_cluster;
+	u8 id;
+	u8 cluster_cnt[NUM_APIC_CLUSTERS];
+
+	memset(cluster_cnt, 0, sizeof(cluster_cnt));
+
+	for (i = 0; i < NR_CPUS; i++) {
+		id = bios_cpu_apicid[i];
+		if (id != BAD_APICID)
+			cluster_cnt[APIC_CLUSTERID(id)]++;
+	}
+
+	clusters = 0;
+	max_cluster = 0;
+	for (i = 0; i < NUM_APIC_CLUSTERS; i++) {
+		if (cluster_cnt[i] > 0) {
+			++clusters;
+			if (cluster_cnt[i] > max_cluster)
+				max_cluster = cluster_cnt[i];
+		}
+	}
+
+	/*
+	 * If we have clusters <= 1 and CPUs <= 8 in cluster 0, then flat mode,
+	 * else if max_cluster <= 4 and cluster_cnt[15] == 0, clustered logical
+	 * else physical mode.
+	 * (We don't use lowest priority delivery + HW APIC IRQ steering, so
+	 * can ignore the clustered logical case and go straight to physical.)
+	 */
+	if (clusters <= 1 && max_cluster <= 8 && cluster_cnt[0] == max_cluster)
+		genapic = &apic_flat;
+	else
+		genapic = &apic_cluster;
+
+	printk(KERN_INFO "Setting APIC routing to %s\n", genapic->name);
+}
+
+/* Same for both flat and clustered. */
+
+void send_IPI_self(int vector)
+{
+	__send_IPI_shortcut(APIC_DEST_SELF, vector, APIC_DEST_PHYSICAL);
+}
diff -urNp linux-210/arch/x86_64/kernel/genapic_cluster.c linux-211/arch/x86_64/kernel/genapic_cluster.c
--- linux-210/arch/x86_64/kernel/genapic_cluster.c
+++ linux-211/arch/x86_64/kernel/genapic_cluster.c
@@ -0,0 +1,146 @@
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Clustered APIC subarch code.  Up to 255 CPUs, physical delivery.
+ * (A more realistic maximum is around 230 CPUs.)
+ *
+ * Hacked for x86-64 by James Cleverdon from i386 architecture code by
+ * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
+ * James Cleverdon.
+ */
+#include <linux/config.h>
+#include <linux/threads.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/ctype.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <asm/smp.h>
+#include <asm/ipi.h>
+
+
+/*
+ * Set up the logical destination ID.
+ *
+ * Intel recommends to set DFR, LDR and TPR before enabling
+ * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel
+ * document number 292116).  So here it goes...
+ */
+static void cluster_init_apic_ldr(void)
+{
+	unsigned long val, id;
+	long i, count;
+	u8 lid;
+	u8 my_id = hard_smp_processor_id();
+	u8 my_cluster = APIC_CLUSTER(my_id);
+
+	/* Create logical APIC IDs by counting CPUs already in cluster. */
+	for (count = 0, i = NR_CPUS; --i >= 0; ) {
+		lid = x86_cpu_to_log_apicid[i];
+		if (lid != BAD_APICID && APIC_CLUSTER(lid) == my_cluster)
+			++count;
+	}
+	/*
+	 * We only have a 4 wide bitmap in cluster mode.  There's no way
+	 * to get above 60 CPUs and still give each one it's own bit.
+	 * But, we're using physical IRQ delivery, so we don't care.
+	 * Use bit 3 for the 4th through Nth CPU in each cluster.
+	 */
+	if (count >= XAPIC_DEST_CPUS_SHIFT)
+		count = 3;
+	id = my_cluster | (1UL << count);
+	x86_cpu_to_log_apicid[smp_processor_id()] = id;
+	apic_write_around(APIC_DFR, APIC_DFR_CLUSTER);
+	val = apic_read(APIC_LDR) & ~APIC_LDR_MASK;
+	val |= SET_APIC_LOGICAL_ID(id);
+	apic_write_around(APIC_LDR, val);
+}
+
+
+/* Mapping from cpu number to logical apicid */
+static int cluster_cpu_to_logical_apicid(int cpu)
+{
+       if ((unsigned)cpu >= NR_CPUS)
+	       return BAD_APICID;
+	return x86_cpu_to_log_apicid[cpu];
+}
+
+static int cluster_cpu_present_to_apicid(int mps_cpu)
+{
+	if ((unsigned)mps_cpu < NR_CPUS)
+		return (int)bios_cpu_apicid[mps_cpu];
+	else
+		return BAD_APICID;
+}
+
+/* Distribute IRQ load with round-robin allocation */
+
+static u8 cluster_target_cpus(void)
+{
+	unsigned long	i;
+	static unsigned long	last_cpu = 0;
+
+	i = last_cpu;
+	do {
+		if (++i >= NR_CPUS)
+			i = 0;
+	} while (x86_cpu_to_apicid[i] == BAD_APICID);
+	last_cpu = i;
+
+	return x86_cpu_to_apicid[i];
+}
+
+static void cluster_send_IPI_mask(unsigned long mask, int vector)
+{
+	send_IPI_mask_sequence(mask, vector);
+}
+
+static void cluster_send_IPI_allbutself(int vector)
+{
+	unsigned long	mask;
+
+	mask = ~(1ul << smp_processor_id()) & cpu_online_map;
+	if (mask != 0)
+		cluster_send_IPI_mask(mask, vector);
+}
+
+static void cluster_send_IPI_all(int vector)
+{
+	cluster_send_IPI_mask(cpu_online_map, vector);
+}
+
+static int cluster_apic_id_registered(void)
+{
+	return 1;
+}
+
+static unsigned int cluster_cpu_mask_to_apicid(unsigned long cpumask)
+{
+	long cpu;
+
+	/*
+	 * We're using fixed IRQ delivery, can only return one phys APIC ID.
+	 * May as well be the first.
+	 */
+	cpu = ffs(cpumask) - 1;
+	if (cpu >= 0 && cpu < NR_CPUS)
+		return x86_cpu_to_apicid[cpu];
+	else
+		return x86_cpu_to_apicid[0];
+}
+
+
+struct genapic apic_cluster = {
+	.name = "clustered",
+	.int_delivery_mode = dest_Fixed,
+	.int_dest_mode = (APIC_DEST_PHYSICAL != 0),
+	.int_delivery_dest = APIC_DEST_PHYSICAL | APIC_DM_FIXED,
+	.target_cpus = cluster_target_cpus,
+	.apic_id_registered = cluster_apic_id_registered,
+	.init_apic_ldr = cluster_init_apic_ldr,
+	.send_IPI_all = cluster_send_IPI_all,
+	.send_IPI_allbutself = cluster_send_IPI_allbutself,
+	.send_IPI_mask = cluster_send_IPI_mask,
+	.cpu_mask_to_apicid = cluster_cpu_mask_to_apicid,
+};
diff -urNp linux-210/arch/x86_64/kernel/genapic_flat.c linux-211/arch/x86_64/kernel/genapic_flat.c
--- linux-210/arch/x86_64/kernel/genapic_flat.c
+++ linux-211/arch/x86_64/kernel/genapic_flat.c
@@ -0,0 +1,123 @@
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Flat APIC subarch code.  Maximum 8 CPUs, logical delivery.
+ *
+ * Hacked for x86-64 by James Cleverdon from i386 architecture code by
+ * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
+ * James Cleverdon.
+ */
+#include <linux/config.h>
+#include <linux/threads.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/ctype.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <asm/smp.h>
+#include <asm/ipi.h>
+
+
+static u8 flat_target_cpus(void)
+{
+	return cpu_online_map;
+}
+
+/*
+ * Set up the logical destination ID.
+ *
+ * Intel recommends to set DFR, LDR and TPR before enabling
+ * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel
+ * document number 292116).  So here it goes...
+ */
+static void flat_init_apic_ldr(void)
+{
+	unsigned long val;
+	unsigned long num, id;
+
+	num = smp_processor_id();
+	id = 1UL << num;
+	x86_cpu_to_log_apicid[num] = id;
+	apic_write_around(APIC_DFR, APIC_DFR_FLAT);
+	val = apic_read(APIC_LDR) & ~APIC_LDR_MASK;
+	val |= SET_APIC_LOGICAL_ID(id);
+	apic_write_around(APIC_LDR, val);
+}
+
+static void flat_send_IPI_allbutself(int vector)
+{
+	/*
+	 * if there are no other CPUs in the system then
+	 * we get an APIC send error if we try to broadcast.
+	 * thus we have to avoid sending IPIs in this case.
+	 */
+//	if (~(1ul << smp_processor_id()) & cpu_online_map)
+	if (smp_num_cpus > 1)
+		__send_IPI_shortcut(APIC_DEST_ALLBUT, vector, APIC_DEST_LOGICAL);
+}
+
+static void flat_send_IPI_all(int vector)
+{
+	__send_IPI_shortcut(APIC_DEST_ALLINC, vector, APIC_DEST_LOGICAL);
+}
+
+static void flat_send_IPI_mask(unsigned long cpumask, int vector)
+{
+	unsigned long mask = cpumask;
+	unsigned long cfg;
+	unsigned long flags;
+
+	__save_flags(flags);
+	__cli();
+
+	/*
+	 * Wait for idle.
+	 */
+	apic_wait_icr_idle();
+
+	/*
+	 * prepare target chip field
+	 */
+	cfg = __prepare_ICR2(mask);
+	apic_write_around(APIC_ICR2, cfg);
+
+	/*
+	 * program the ICR
+	 */
+	cfg = __prepare_ICR(0, vector, APIC_DEST_LOGICAL);
+
+	/*
+	 * Send the IPI. The write to APIC_ICR fires this off.
+	 */
+	apic_write_around(APIC_ICR, cfg);
+	__restore_flags(flags);
+}
+
+static int flat_apic_id_registered(void)
+{
+	return test_bit(GET_APIC_ID(apic_read(APIC_ID)), &phys_cpu_present_map);
+}
+
+static unsigned int flat_cpu_mask_to_apicid(unsigned long cpumask)
+{
+	cpumask &= APIC_ALL_CPUS;
+	if (cpumask == 0)
+		cpumask = 0x01UL;
+	return cpumask;
+}
+
+
+struct genapic apic_flat =  {
+	.name = "flat",
+	.int_delivery_mode = dest_LowestPrio,
+	.int_dest_mode = (APIC_DEST_LOGICAL != 0),
+	.int_delivery_dest = APIC_DEST_LOGICAL | APIC_DM_LOWEST,
+	.target_cpus = flat_target_cpus,
+	.apic_id_registered = flat_apic_id_registered,
+	.init_apic_ldr = flat_init_apic_ldr,
+	.send_IPI_all = flat_send_IPI_all,
+	.send_IPI_allbutself = flat_send_IPI_allbutself,
+	.send_IPI_mask = flat_send_IPI_mask,
+	.cpu_mask_to_apicid = flat_cpu_mask_to_apicid,
+};
diff -urNp linux-210/arch/x86_64/kernel/io_apic.c linux-211/arch/x86_64/kernel/io_apic.c
--- linux-210/arch/x86_64/kernel/io_apic.c
+++ linux-211/arch/x86_64/kernel/io_apic.c
@@ -33,13 +33,16 @@
 #include <asm/smp.h>
 #include <asm/desc.h>
 #include <asm/acpi.h>
-
-#undef APIC_LOCKUP_DEBUG
-
-#define APIC_LOCKUP_DEBUG
+#include <asm/mach_apic.h>
+#include <asm/pci-direct.h>
+#include <linux/pci_ids.h>
+#include <linux/pci.h>
 
 static spinlock_t ioapic_lock = SPIN_LOCK_UNLOCKED;
 
+int disable_timer_pin_1 __initdata;
+int timer_over_8254 __initdata = 1;
+
 /*
  * # of IRQ routing registers
  */
@@ -210,6 +213,68 @@ static int __init ioapic_setup(char *str
 
 __setup("apic", ioapic_setup);
 
+static int __init setup_disable_8254_timer(char *s)
+{
+	timer_over_8254 = -1;
+	return 1;
+}
+static int __init setup_enable_8254_timer(char *s)
+{
+	timer_over_8254 = 2;
+	return 1;
+}
+
+__setup("disable_8254_timer", setup_disable_8254_timer);
+__setup("enable_8254_timer", setup_enable_8254_timer);
+
+
+void __init check_ioapic(void) 
+{
+	int num,slot,func; 
+	/* Poor man's PCI discovery */
+	for (num = 0; num < 32; num++) { 
+		for (slot = 0; slot < 32; slot++) { 
+			for (func = 0; func < 8; func++) { 
+				u32 class;
+				u32 vendor;
+				u8 type;
+				class = read_pci_config(num,slot,func,
+							PCI_CLASS_REVISION);
+				if (class == 0xffffffff)
+					break; 
+
+		       		if ((class >> 16) != PCI_CLASS_BRIDGE_PCI)
+					continue; 
+
+				vendor = read_pci_config(num, slot, func, 
+							 PCI_VENDOR_ID);
+				vendor &= 0xffff;
+				switch (vendor) { 
+				case PCI_VENDOR_ID_ATI:
+					if (timer_over_8254 == 1) {	
+						timer_over_8254 = 0;	
+						printk(KERN_INFO
+	"ATI chipset detected. Disabling timer routing over 8254.\n");
+					}	
+					return;
+				case PCI_VENDOR_ID_SERVERWORKS:
+					if (timer_over_8254 == 1) {	
+						timer_over_8254 = 0;	
+						printk(KERN_INFO
+	"ServerWorks chipset detected. Disabling timer routing over 8254.\n");
+					}	
+					return;
+				} 
+
+				/* No multi-function device? */
+				type = read_pci_config_byte(num,slot,func,
+							    PCI_HEADER_TYPE);
+				if (!(type & 0x80))
+					break;
+			} 
+		}
+	}
+}
 
 static int __init ioapic_pirq_setup(char *str)
 {
@@ -627,8 +692,8 @@ void __init setup_IO_APIC_irqs(void)
 		 */
 		memset(&entry,0,sizeof(entry));
 
-		entry.delivery_mode = dest_LowestPrio;
-		entry.dest_mode = INT_DELIVERY_MODE;
+		entry.delivery_mode = INT_DELIVERY_MODE;
+		entry.dest_mode = INT_DEST_MODE;
 		entry.mask = 0;				/* enable IRQ */
 		entry.dest.logical.logical_dest = TARGET_CPUS;
 
@@ -702,10 +767,10 @@ void __init setup_ExtINT_IRQ0_pin(unsign
 	 * We use logical delivery to get the timer IRQ
 	 * to the first CPU.
 	 */
-	entry.dest_mode = INT_DELIVERY_MODE;
+	entry.dest_mode = INT_DEST_MODE;
 	entry.mask = 0;					/* unmask IRQ now */
 	entry.dest.logical.logical_dest = TARGET_CPUS;
-	entry.delivery_mode = dest_LowestPrio;
+	entry.delivery_mode = INT_DELIVERY_MODE;
 	entry.polarity = 0;
 	entry.trigger = 0;
 	entry.vector = vector;
@@ -1041,7 +1106,6 @@ void disable_IO_APIC(void)
 static void __init setup_ioapic_ids_from_mpc (void)
 {
 	struct IO_APIC_reg_00 reg_00;
-	unsigned long phys_id_present_map = phys_cpu_present_map;
 	int apic;
 	int i;
 	unsigned char old_id;
@@ -1063,35 +1127,7 @@ static void __init setup_ioapic_ids_from
 		
 		old_id = mp_ioapics[apic].mpc_apicid;
 
-		if (mp_ioapics[apic].mpc_apicid >= 0xf) {
-			printk(KERN_ERR "BIOS bug, IO-APIC#%d ID is %d in the MPC table!...\n",
-				apic, mp_ioapics[apic].mpc_apicid);
-			printk(KERN_ERR "... fixing up to %d. (tell your hw vendor)\n",
-				reg_00.ID);
-			mp_ioapics[apic].mpc_apicid = reg_00.ID;
-		}
-
-		/*
-		 * Sanity check, is the ID really free? Every APIC in a
-		 * system must have a unique ID or we get lots of nice
-		 * 'stuck on smp_invalidate_needed IPI wait' messages.
-	 	 */
-		if (phys_id_present_map & (1 << mp_ioapics[apic].mpc_apicid)) {
-			printk(KERN_ERR "BIOS bug, IO-APIC#%d ID %d is already used!...\n",
-				apic, mp_ioapics[apic].mpc_apicid);
-			for (i = 0; i < 0xf; i++)
-				if (!(phys_id_present_map & (1 << i)))
-					break;
-			if (i >= 0xf)
-				panic("Max APIC ID exceeded!\n");
-			printk(KERN_ERR "... fixing up to %d. (tell your hw vendor)\n",
-				i);
-			phys_id_present_map |= 1 << i;
-			mp_ioapics[apic].mpc_apicid = i;
-		} else {
-			printk("Setting %d in the phys_id_present_map\n", mp_ioapics[apic].mpc_apicid);
-			phys_id_present_map |= 1 << mp_ioapics[apic].mpc_apicid;
-		}
+		printk(KERN_INFO "Using IO-APIC %d\n", mp_ioapics[apic].mpc_apicid);
 
 
 		/*
@@ -1274,30 +1310,11 @@ static void end_level_ioapic_irq (unsign
 	ack_APIC_irq();
 
 	if (!(v & (1 << (i & 0x1f)))) {
-#ifdef APIC_LOCKUP_DEBUG
-		struct irq_pin_list *entry;
-#endif
-
 #ifdef APIC_MISMATCH_DEBUG
 		atomic_inc(&irq_mis_count);
 #endif
 		spin_lock(&ioapic_lock);
 		__mask_and_edge_IO_APIC_irq(irq);
-#ifdef APIC_LOCKUP_DEBUG
-		for (entry = irq_2_pin + irq;;) {
-			unsigned int reg;
-
-			if (entry->pin == -1)
-				break;
-			reg = io_apic_read(entry->apic, 0x10 + entry->pin * 2);
-			if (reg & 0x00004000)
-				printk(KERN_CRIT "Aieee!!!  Remote IRR"
-					" still set after unlock!\n");
-			if (!entry->next)
-				break;
-			entry = irq_2_pin + entry->next;
-		}
-#endif
 		__unmask_and_level_IO_APIC_irq(irq);
 		spin_unlock(&ioapic_lock);
 	}
@@ -1314,10 +1331,12 @@ static void set_ioapic_affinity (unsigne
 	if (no_valid_irqaffinity)
 		return;
 
+	mask = cpu_mask_to_apicid(mask);
+
 	/*
-	 * Only the first 8 bits are valid.
+	 * Only the high 8 bits are valid.
 	 */
-	mask = mask << 24;
+	mask = SET_APIC_LOGICAL_ID(mask);
 
 	spin_lock_irqsave(&ioapic_lock, flags);
 	__DO_ACTION(1, = mask, )
@@ -1541,7 +1560,8 @@ static inline void check_timer(void)
 	 */
 	apic_write_around(APIC_LVT0, APIC_LVT_MASKED | APIC_DM_EXTINT);
 	init_8259A(1);
-	enable_8259A_irq(0);
+	if (timer_over_8254 > 0)
+		enable_8259A_irq(0);
 
 	pin1 = find_isa_irq_pin(0, mp_INT);
 	pin2 = find_isa_irq_pin(0, mp_ExtINT);
@@ -1560,6 +1580,8 @@ static inline void check_timer(void)
 				enable_8259A_irq(0);
 				check_nmi_watchdog();
 			}
+			if (disable_timer_pin_1 > 0)
+				clear_IO_APIC_pin(0, pin1);
 			return;
 		}
 		clear_IO_APIC_pin(0, pin1);
@@ -1715,8 +1737,8 @@ int io_apic_set_pci_routing (int ioapic,
 
 	memset(&entry,0,sizeof(entry));
 
-	entry.delivery_mode = dest_LowestPrio;
-	entry.dest_mode = INT_DELIVERY_MODE;
+	entry.delivery_mode = INT_DELIVERY_MODE;
+	entry.dest_mode = INT_DEST_MODE;
 	entry.dest.logical.logical_dest = TARGET_CPUS;
 	entry.mask = 1;					 /* Disabled (masked) */
 	entry.trigger = edge_level;
diff -urNp linux-210/arch/x86_64/kernel/mpparse.c linux-211/arch/x86_64/kernel/mpparse.c
--- linux-210/arch/x86_64/kernel/mpparse.c
+++ linux-211/arch/x86_64/kernel/mpparse.c
@@ -82,6 +82,10 @@ extern int acpi_parse_ioapic (acpi_table
 
 u8 bios_cpu_apicid[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
 
+/* which logical CPU number maps to which CPU (physical APIC ID) */
+u8 x86_cpu_to_apicid[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
+u8 x86_cpu_to_log_apicid[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
+
 
 /*
  * Intel MP BIOS table parsing routines:
@@ -104,10 +108,19 @@ static int __init mpf_checksum(unsigned 
 static void __init MP_processor_info (struct mpc_config_processor *m)
 {
 	int ver;
+	extern unsigned int sibling_ht_mask;
 
 	if (!(m->mpc_cpuflag & CPU_ENABLED))
 		return;
 
+	/* if the "noht" boot option is set, then use sibling_ht_mask
+	 * to enable only the first processor of a sibling set.
+	 */
+	if (m->mpc_apicid & sibling_ht_mask) {
+		m->mpc_cpuflag &= ~CPU_ENABLED;
+		return;
+	}
+
 	printk(KERN_INFO "Processor #%d %d:%d APIC version %d\n",
 		m->mpc_apicid,
 	       (m->mpc_cpufeature & CPU_FAMILY_MASK)>>8,
@@ -123,6 +136,7 @@ static void __init MP_processor_info (st
 	if (m->mpc_apicid > MAX_APICS) {
 		printk(KERN_ERR "Processor #%d INVALID. (Max ID: %d).\n",
 			m->mpc_apicid, MAX_APICS);
+		--num_processors;
 		return;
 	}
 	ver = m->mpc_apicver;
@@ -486,6 +500,7 @@ static struct intel_mp_floating *mpf_fou
 void __init get_smp_config (void)
 {
 	struct intel_mp_floating *mpf = mpf_found;
+	extern void clustered_apic_check(void);
 
 	/*
  	 * ACPI may be used to obtain the entire SMP configuration or just to 
@@ -495,6 +510,7 @@ void __init get_smp_config (void)
  	 */
  	if (acpi_lapic && acpi_ioapic) {
  		printk(KERN_INFO "Using ACPI (MADT) for SMP configuration information\n");
+		clustered_apic_check();
  		return;
 	}
  	else if (acpi_lapic)
@@ -554,6 +570,7 @@ void __init get_smp_config (void)
 	/*
 	 * Only use the first configuration found.
 	 */
+	clustered_apic_check();
 }
 
 static int __init smp_scan_config (unsigned long base, unsigned long length)
diff -urNp linux-210/arch/x86_64/kernel/pci-pc.c linux-211/arch/x86_64/kernel/pci-pc.c
--- linux-210/arch/x86_64/kernel/pci-pc.c
+++ linux-211/arch/x86_64/kernel/pci-pc.c
@@ -540,14 +540,14 @@ static void __devinit pci_scan_mptable(v
 		return;
 	} 
 
-	pcibios_last_bus = 0xfe;
-
 	for (i = 0; i < MAX_MP_BUSSES; i++) {
 		int n = mp_bus_id_to_pci_bus[i]; 
 		if (n < 0 || n >= 0xff)
 			continue; 
 		if (pci_bus_exists(&pci_root_buses, n))
 			continue;
+		if (n > pcibios_last_bus)
+			pcibios_last_bus = n;
 		printk(KERN_INFO "PCI: Scanning bus %02x from mptable\n", n); 
 		pci_scan_bus(n, pci_root_ops, NULL); 
 	} 			
diff -urNp linux-210/arch/x86_64/kernel/setup.c linux-211/arch/x86_64/kernel/setup.c
--- linux-210/arch/x86_64/kernel/setup.c
+++ linux-211/arch/x86_64/kernel/setup.c
@@ -73,6 +73,10 @@ static int __init acpioff(char *str)
 __setup("acpi=off", acpioff);
 
 
+/* For enabling/disabling HT */
+unsigned int sibling_ht_mask;
+int disable_x86_ht;
+
 /* For PCI or other memory-mapped resources */
 unsigned long pci_mem_start = 0x10000000;
 
@@ -203,6 +207,62 @@ static void __init contig_initmem_init(v
 }
 #endif
 
+/*
+ * find out the number of processor cores on the die
+ */
+static int __init num_cpu_cores(struct cpuinfo_x86 *c)
+{
+	unsigned int eax, level;
+
+	level = c ? c->cpuid_level : cpuid_eax(0);
+
+	if (level < 4)
+		return 1;
+
+	__asm__("cpuid"
+		: "=a" (eax)
+		: "0" (4), "c" (0)
+		: "bx", "dx");
+	if (eax & 0x1f)
+		return ((eax >> 26) + 1);
+	else
+		return 1;
+}
+
+static void __init noht_init(void)
+{
+	unsigned int cpuid_level;
+	char vendorid[13];
+	vendorid[12] = 0;
+
+	/* Have to get CPUID level and vendor ID here since
+ 	 * we haven't populated boot_cpu_data yet
+	 */
+
+	cpuid(0, &cpuid_level, (int *)&vendorid[0], (int *)&vendorid[8], (int *)&vendorid[4]);
+
+	if (!strncmp(vendorid, "GenuineIntel", 12)) {
+		unsigned int siblings, eax, ebx;
+		/* If "noht" specified, calculate a mask that
+		 * can be used to determine which APIC IDs
+		 * correspond to the first CPU in a sibling
+		 * set.
+		 */
+		printk("Disabling hyperthreading\n");
+		ebx = cpuid_ebx(1);
+		siblings = (ebx & 0xff0000) >> 16;
+		if (!siblings)
+			siblings = 1;
+		else
+			siblings /= num_cpu_cores(NULL);
+
+		sibling_ht_mask = 1;
+		while (sibling_ht_mask < siblings)
+			sibling_ht_mask <<= 1;
+		sibling_ht_mask--;
+	}
+}
+
 void __init setup_arch(char **cmdline_p)
 {
 	int i;
@@ -232,12 +292,15 @@ void __init setup_arch(char **cmdline_p)
 	data_resource.start = virt_to_bus(&_etext);
 	data_resource.end = virt_to_bus(&_edata)-1;
 
-	parse_mem_cmdline(cmdline_p);
+	parse_cmdline_early(cmdline_p);
 
 	e820_end_of_ram();
 
 	init_memory_mapping(); 
 
+	if (disable_x86_ht)
+		noht_init();
+
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (LOADER_TYPE && INITRD_START) {
 		if (INITRD_START + INITRD_SIZE <= (end_pfn << PAGE_SHIFT)) {
@@ -307,6 +370,11 @@ void __init setup_arch(char **cmdline_p)
 #endif
 
 	paging_init();
+
+#ifdef CONFIG_X86_IO_APIC
+	check_ioapic();
+#endif
+
 #ifdef CONFIG_ACPI_BOOT
 	/*
 	 * Initialize the ACPI boot-time table parser (gets the RSDP and SDT).
@@ -701,8 +769,6 @@ static void __init init_intel(struct cpu
 	
 #ifdef CONFIG_SMP
 	if (test_bit(X86_FEATURE_HT, &c->x86_capability)) {
-		extern	int phys_proc_id[NR_CPUS];
-		
 		int 	index_lsb, index_msb, tmp;
 		int	initial_apic_id;
 		int 	cpu = smp_processor_id();
@@ -711,9 +777,14 @@ static void __init init_intel(struct cpu
 		cpuid(1, &eax, &ebx, &ecx, &edx);
 		smp_num_siblings = (ebx & 0xff0000) >> 16;
 
-		if (smp_num_siblings == 1) {
+		if (disable_x86_ht || smp_num_siblings == 1) {
 			printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
-		} else if (smp_num_siblings > 1 ) {
+			clear_bit(X86_FEATURE_HT, &c->x86_capability);
+		} else if (smp_num_siblings > 1) {
+
+			smp_num_cores = num_cpu_cores(c);
+
+			smp_num_siblings /= smp_num_cores;
 			index_lsb = 0;
 			index_msb = 31;
 			/*
@@ -721,7 +792,7 @@ static void __init init_intel(struct cpu
 			 * processor package.
 			 */
 #define NR_SIBLINGS	2
-			if (smp_num_siblings != NR_SIBLINGS) {
+			if (smp_num_siblings > NR_SIBLINGS) {
 				printk(KERN_WARNING "CPU: Unsupported number of the siblings %d", smp_num_siblings);
 				smp_num_siblings = 1;
 				return;
@@ -739,13 +810,42 @@ static void __init init_intel(struct cpu
 			if (index_lsb != index_msb )
 				index_msb++;
 			initial_apic_id = ebx >> 24 & 0xff;
-			phys_proc_id[cpu] = initial_apic_id >> index_msb;
+			cpu_core_id[cpu] = initial_apic_id >> index_msb;
+
+			if (smp_num_cores == 1) {
+				phys_proc_id[cpu] = cpu_core_id[cpu];
+				printk(KERN_INFO  "CPU%d: Initial APIC ID: %d, Physical Processor ID: %d\n",
+						   cpu, initial_apic_id, phys_proc_id[cpu]);
+				goto end;
+			}
 
-			printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-                               phys_proc_id[cpu]);
+			index_lsb = 0;
+			index_msb = 31;
+
+			tmp = smp_num_siblings * smp_num_cores;
+			while ((tmp & 1) == 0) {
+				tmp >>=1 ;
+				index_lsb++;
+			}
+			tmp = smp_num_siblings * smp_num_cores;
+			while ((tmp & 0x80000000 ) == 0) {
+				tmp <<=1 ;
+				index_msb--;
+			}
+			if (index_lsb != index_msb )
+				index_msb++;
+
+			phys_proc_id[cpu] = initial_apic_id >> index_msb;
+			printk(KERN_INFO  "CPU%d: Physical Processor ID: %d\n",
+                               cpu, phys_proc_id[cpu]);
+			printk(KERN_INFO  "CPU%d: Processor Core ID: %d\n",
+                               cpu, cpu_core_id[cpu]);
+			printk(KERN_INFO  "CPU%d: Initial APIC ID: %d\n",
+				cpu, initial_apic_id);
 		}
 
 	}
+end:
 #endif
 
 	n = cpuid_eax(0x80000000);
@@ -760,6 +860,28 @@ static void __init init_intel(struct cpu
 static int __init init_amd(struct cpuinfo_x86 *c)
 {
 	int r;
+#ifdef CONFIG_SMP
+	int tmp, cpu;
+	unsigned int coreshift = 0;
+
+	/* Figure out what physical package we're on. */
+	if (cpuid_ebx(1) & 0xff0000) {
+		/* nonzero sibling count */
+		if (cpuid_eax(0x80000000) >= 0x80000008 &&
+		    (tmp = (cpuid_ecx(0x80000008) & 0xff)) > 0) {
+			smp_num_cores = tmp + 1;
+			do {
+				coreshift++;
+				tmp >>= 1;
+			} while (tmp > 0);
+		}
+	}
+
+	cpu = smp_processor_id();
+	tmp = hard_smp_processor_id();
+	cpu_core_id[cpu] = tmp & ((1 << coreshift) - 1);
+	phys_proc_id[cpu] = tmp >> coreshift;
+#endif
 
 	/* Bit 31 in normal CPUID used for nonstandard 3DNow ID;
 	   3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway */
@@ -935,9 +1057,6 @@ static int show_cpuinfo(struct seq_file 
 {
 	struct cpuinfo_x86 *c = v;
 	int n = c - cpu_data;
-#ifdef CONFIG_SMP
-	extern	int phys_proc_id[NR_CPUS];
-#endif
 
 	/* 
 	 * These flag bits must match the definitions in <asm/cpufeature.h>.
@@ -1003,7 +1122,11 @@ static int show_cpuinfo(struct seq_file 
 	
 #ifdef CONFIG_SMP
 	seq_printf(m, "physical id\t: %d\n",phys_proc_id[n]);
-	seq_printf(m, "siblings\t: %d\n",smp_num_siblings);
+	seq_printf(m, "siblings\t: %d\n",smp_num_siblings*smp_num_cores);
+	if (smp_num_cores > 1) {
+		seq_printf(m, "core id\t\t: %d\n",cpu_core_id[n]);
+		seq_printf(m, "cpu cores\t: %d\n",smp_num_cores);
+	}
 #if CONFIG_SHARE_RUNQUEUE
 {
 	extern long __rq_idx[NR_CPUS];
diff -urNp linux-210/arch/x86_64/kernel/smp.c linux-211/arch/x86_64/kernel/smp.c
--- linux-210/arch/x86_64/kernel/smp.c
+++ linux-211/arch/x86_64/kernel/smp.c
@@ -21,6 +21,7 @@
 
 #include <asm/mtrr.h>
 #include <asm/pgalloc.h>
+#include <asm/mach_apic.h>
 
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
@@ -107,102 +108,6 @@ spinlock_cacheline_t kernel_flag_cacheli
 struct tlb_state cpu_tlbstate[NR_CPUS] __cacheline_aligned = {[0 ... NR_CPUS-1] = { &init_mm, 0, }};
 
 /*
- * the following functions deal with sending IPIs between CPUs.
- *
- * We use 'broadcast', CPU->CPU IPIs and self-IPIs too.
- */
-
-static inline unsigned int __prepare_ICR (unsigned int shortcut, int vector)
-{
-	unsigned int icr =  APIC_DM_FIXED | shortcut | vector | APIC_DEST_LOGICAL;
-	return icr;
-}
-
-static inline int __prepare_ICR2 (unsigned int mask)
-{
-	return SET_APIC_DEST_FIELD(mask);
-}
-
-static inline void __send_IPI_shortcut(unsigned int shortcut, int vector)
-{
-	/*
-	 * Subtle. In the case of the 'never do double writes' workaround
-	 * we have to lock out interrupts to be safe.  As we don't care
-	 * of the value read we use an atomic rmw access to avoid costly
-	 * cli/sti.  Otherwise we use an even cheaper single atomic write
-	 * to the APIC.
-	 */
-	unsigned int cfg;
-
-	/*
-	 * Wait for idle.
-	 */
-	apic_wait_icr_idle();
-
-	/*
-	 * No need to touch the target chip field
-	 */
-	cfg = __prepare_ICR(shortcut, vector);
-
-	/*
-	 * Send the IPI. The write to APIC_ICR fires this off.
-	 */
-	apic_write_around(APIC_ICR, cfg);
-}
-
-static inline void send_IPI_allbutself(int vector)
-{
-	/*
-	 * if there are no other CPUs in the system then
-	 * we get an APIC send error if we try to broadcast.
-	 * thus we have to avoid sending IPIs in this case.
-	 */
-	if (smp_num_cpus > 1)
-		__send_IPI_shortcut(APIC_DEST_ALLBUT, vector);
-}
-
-static inline void send_IPI_all(int vector)
-{
-	__send_IPI_shortcut(APIC_DEST_ALLINC, vector);
-}
-
-void send_IPI_self(int vector)
-{
-	__send_IPI_shortcut(APIC_DEST_SELF, vector);
-}
-
-static inline void send_IPI_mask(int mask, int vector)
-{
-	unsigned long cfg;
-	unsigned long flags;
-
-	__save_flags(flags);
-	__cli();
-
-	/*
-	 * Wait for idle.
-	 */
-	apic_wait_icr_idle();
-
-	/*
-	 * prepare target chip field
-	 */
-	cfg = __prepare_ICR2(mask);
-	apic_write_around(APIC_ICR2, cfg);
-
-	/*
-	 * program the ICR 
-	 */
-	cfg = __prepare_ICR(0, vector);
-	
-	/*
-	 * Send the IPI. The write to APIC_ICR fires this off.
-	 */
-	apic_write_around(APIC_ICR, cfg);
-	__restore_flags(flags);
-}
-
-/*
  *	Smarter SMP flushing macros. 
  *		c/o Linus Torvalds.
  *
diff -urNp linux-210/arch/x86_64/kernel/smpboot.c linux-211/arch/x86_64/kernel/smpboot.c
--- linux-210/arch/x86_64/kernel/smpboot.c
+++ linux-211/arch/x86_64/kernel/smpboot.c
@@ -60,9 +60,14 @@ static int cpu_mask = -1; 
 /* Total count of live CPUs */
 int smp_num_cpus = 1;
 
-/* Number of siblings per CPU package */
+/* Number of siblings per CPU core */
 int smp_num_siblings = 1;
-int phys_proc_id[NR_CPUS]; /* Package ID of each logical CPU */
+/* Number of cores per CPU package */
+int smp_num_cores = 1;
+/* Package ID of each logical CPU */
+u8 phys_proc_id[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
+/* Core ID of each logical CPU */
+u8 cpu_core_id[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
 int cpu_sibling_map[NR_CPUS] __cacheline_aligned;
 
 static int test_ht;
@@ -70,9 +75,6 @@ static int test_ht;
 /* Bitmask of currently online CPUs */
 unsigned long cpu_online_map;
 
-/* which logical CPU number maps to which CPU (physical APIC ID) */
-volatile int x86_cpu_to_apicid[NR_CPUS];
-
 static volatile unsigned long cpu_callin_map;
 static volatile unsigned long cpu_callout_map;
 
@@ -766,7 +768,8 @@ static int __init do_boot_cpu (int apici
 		}
 	}
 	if (send_status || accept_status || boot_status) {
-		x86_cpu_to_apicid[cpu] = -1;
+		x86_cpu_to_apicid[cpu] = BAD_APICID;
+		x86_cpu_to_log_apicid[cpu] = BAD_APICID;
 		cpucount--;
 	}
 
@@ -939,9 +942,9 @@ void __init smp_boot_cpus(void)
 		if (apicid == boot_cpu_id || (apicid == BAD_APICID))
 			continue;
 
-		if (!(phys_cpu_present_map & (1 << apicid)))
+		if (!(phys_cpu_present_map & (1u << apicid)))
 			continue;
-		if (((1<<apicid) & cpu_mask) == 0) 
+		if (((1u<<apicid) & cpu_mask) == 0)
 			continue;
 		if ((max_cpus >= 0) && (max_cpus <= cpucount+1))
 			continue;
@@ -951,8 +954,7 @@ void __init smp_boot_cpus(void)
 		/*
 		 * Make sure we unmap all failed CPUs
 		 */
-		if ((x86_cpu_to_apicid[cpu] == -1) && 
-				(phys_cpu_present_map & (1 << apicid))) {
+		if (cpu < 0) {
 			printk("phys CPU #%d not responding - cannot use it.\n",apicid);
 			continue;
 		} else if (cpu > maxcpu) 
@@ -1015,7 +1017,7 @@ void __init smp_boot_cpus(void)
 			for (i = 0; i < smp_num_cpus; i++) {
 				if (i == cpu)
 					continue;
-				if (phys_proc_id[cpu] == phys_proc_id[i]) {
+				if (cpu_core_id[cpu] == cpu_core_id[i]) {
 					cpu_sibling_map[cpu] = i;
 					printk("cpu_sibling_map[%d] = %d\n", cpu, cpu_sibling_map[cpu]);
 					break;
diff -urNp linux-210/include/asm-i386/apic.h linux-211/include/asm-i386/apic.h
--- linux-210/include/asm-i386/apic.h
+++ linux-211/include/asm-i386/apic.h
@@ -96,6 +96,4 @@ extern unsigned int nmi_watchdog;
 
 #endif /* CONFIG_X86_LOCAL_APIC */
 
-extern int phys_proc_id[NR_CPUS];
-
 #endif /* __ASM_APIC_H */
diff -urNp linux-210/include/asm-i386/pci-direct.h linux-211/include/asm-i386/pci-direct.h
--- linux-210/include/asm-i386/pci-direct.h
+++ linux-211/include/asm-i386/pci-direct.h
@@ -0,0 +1 @@
+#include "asm-x86_64/pci-direct.h"
diff -urNp linux-210/include/asm-i386/smp.h linux-211/include/asm-i386/smp.h
--- linux-210/include/asm-i386/smp.h
+++ linux-211/include/asm-i386/smp.h
@@ -34,7 +34,10 @@ extern unsigned long cpu_online_map;
 extern volatile unsigned long smp_invalidate_needed;
 extern int pic_mode;
 extern int smp_num_siblings;
+extern int smp_num_cores;
 extern int cpu_sibling_map[];
+extern int phys_proc_id[];
+extern int cpu_core_id[];
 
 extern void smp_flush_tlb(void);
 extern void smp_message_irq(int cpl, void *dev_id, struct pt_regs *regs);
diff -urNp linux-210/include/asm-x86_64/apic.h linux-211/include/asm-x86_64/apic.h
--- linux-210/include/asm-x86_64/apic.h
+++ linux-211/include/asm-x86_64/apic.h
@@ -98,7 +98,6 @@ extern unsigned int nmi_watchdog;
 
 #endif /* CONFIG_X86_LOCAL_APIC */
 
-#define clustered_apic_mode 0
 #define esr_disable 0
 extern unsigned boot_cpu_id;
 
diff -urNp linux-210/include/asm-x86_64/apicdef.h linux-211/include/asm-x86_64/apicdef.h
--- linux-210/include/asm-x86_64/apicdef.h
+++ linux-211/include/asm-x86_64/apicdef.h
@@ -11,27 +11,29 @@
 #define		APIC_DEFAULT_PHYS_BASE	0xfee00000
  
 #define		APIC_ID		0x20
-#define			APIC_ID_MASK		(0x0F<<24)
-#define			GET_APIC_ID(x)		(((x)>>24)&0x0F)
+#define			APIC_ID_MASK		(0xFFu<<24)
+#define			GET_APIC_ID(x)		(((x)>>24)&0xFFu)
 #define		APIC_LVR	0x30
 #define			APIC_LVR_MASK		0xFF00FF
-#define			GET_APIC_VERSION(x)	((x)&0xFF)
-#define			GET_APIC_MAXLVT(x)	(((x)>>16)&0xFF)
-#define			APIC_INTEGRATED(x)	((x)&0xF0)
+#define			GET_APIC_VERSION(x)	((x)&0xFFu)
+#define			GET_APIC_MAXLVT(x)	(((x)>>16)&0xFFu)
+#define			APIC_INTEGRATED(x)	((x)&0xF0u)
 #define		APIC_TASKPRI	0x80
-#define			APIC_TPRI_MASK		0xFF
+#define			APIC_TPRI_MASK		0xFFu
 #define		APIC_ARBPRI	0x90
-#define			APIC_ARBPRI_MASK	0xFF
+#define			APIC_ARBPRI_MASK	0xFFu
 #define		APIC_PROCPRI	0xA0
 #define		APIC_EOI	0xB0
 #define			APIC_EIO_ACK		0x0		/* Write this to the EOI register */
 #define		APIC_RRR	0xC0
 #define		APIC_LDR	0xD0
-#define			APIC_LDR_MASK		(0xFF<<24)
-#define			GET_APIC_LOGICAL_ID(x)	(((x)>>24)&0xFF)
+#define			APIC_LDR_MASK		(0xFFu<<24)
+#define			GET_APIC_LOGICAL_ID(x)	(((x)>>24)&0xFFu)
 #define			SET_APIC_LOGICAL_ID(x)	(((x)<<24))
-#define			APIC_ALL_CPUS		0xFF
+#define			APIC_ALL_CPUS		0xFFu
 #define		APIC_DFR	0xE0
+#define			APIC_DFR_CLUSTER	0x0FFFFFFFu
+#define			APIC_DFR_FLAT		0xFFFFFFFFu
 #define		APIC_SPIV	0xF0
 #define			APIC_SPIV_FOCUS_DISABLED	(1<<9)
 #define			APIC_SPIV_APIC_ENABLED		(1<<8)
@@ -58,6 +60,7 @@
 #define			APIC_INT_ASSERT		0x04000
 #define			APIC_ICR_BUSY		0x01000
 #define			APIC_DEST_LOGICAL	0x00800
+#define			APIC_DEST_PHYSICAL	0x00000
 #define			APIC_DM_FIXED		0x00000
 #define			APIC_DM_LOWEST		0x00100
 #define			APIC_DM_SMI		0x00200
@@ -110,6 +113,20 @@
 #define MAX_IO_APICS 32
 
 /*
+ * All x86-64 systems are xAPIC compatible.
+ * In the following, "apicid" is a physical APIC ID.
+ */
+#define XAPIC_DEST_CPUS_SHIFT	4
+#define XAPIC_DEST_CPUS_MASK	((1u << XAPIC_DEST_CPUS_SHIFT) - 1)
+#define XAPIC_DEST_CLUSTER_MASK	(XAPIC_DEST_CPUS_MASK << XAPIC_DEST_CPUS_SHIFT)
+#define APIC_CLUSTER(apicid)	((apicid) & XAPIC_DEST_CLUSTER_MASK)
+#define APIC_CLUSTERID(apicid)	(APIC_CLUSTER(apicid) >> XAPIC_DEST_CPUS_SHIFT)
+#define APIC_CPUID(apicid)	((apicid) & XAPIC_DEST_CPUS_MASK)
+#define NUM_APIC_CLUSTERS	((BAD_APICID + 1) >> XAPIC_DEST_CPUS_SHIFT)
+
+#define BAD_APICID	0xFFu
+
+/*
  * the local APIC register structure, memory mapped. Not terribly well
  * tested, but we might eventually use this one in the future - the
  * problem why we cannot use it right now is the P5 APIC, it has an
diff -urNp linux-210/include/asm-x86_64/e820.h linux-211/include/asm-x86_64/e820.h
--- linux-210/include/asm-x86_64/e820.h
+++ linux-211/include/asm-x86_64/e820.h
@@ -54,7 +54,7 @@ extern int e820_mapped(unsigned long sta
 
 extern void e820_bootmem_free(pg_data_t *pgdat, unsigned long start,unsigned long end);
 
-extern void parse_mem_cmdline (char ** cmdline_p);
+extern void parse_cmdline_early(char ** cmdline_p);
 
 extern struct e820map e820;
 #endif/*!__ASSEMBLY__*/
diff -urNp linux-210/include/asm-x86_64/genapic.h linux-211/include/asm-x86_64/genapic.h
--- linux-210/include/asm-x86_64/genapic.h
+++ linux-211/include/asm-x86_64/genapic.h
@@ -0,0 +1,34 @@
+#ifndef _ASM_GENAPIC_H
+#define _ASM_GENAPIC_H 1
+
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Generic APIC sub-arch data struct.
+ *
+ * Hacked for x86-64 by James Cleverdon from i386 architecture code by
+ * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
+ * James Cleverdon.
+ */
+
+struct genapic {
+	char *name;
+	u32 int_delivery_mode;
+	u32 int_dest_mode;
+	u32 int_delivery_dest;	/* for quick IPIs */
+	int (*apic_id_registered)(void);
+	u8 (*target_cpus)(void);
+	void (*init_apic_ldr)(void);
+	/* ipi */
+	void (*send_IPI_mask)(unsigned long mask, int vector);
+	void (*send_IPI_allbutself)(int vector);
+	void (*send_IPI_all)(int vector);
+	/* */
+	unsigned int (*cpu_mask_to_apicid)(unsigned long cpumask);
+};
+
+
+extern struct genapic *genapic;
+
+#endif
diff -urNp linux-210/include/asm-x86_64/io_apic.h linux-211/include/asm-x86_64/io_apic.h
--- linux-210/include/asm-x86_64/io_apic.h
+++ linux-211/include/asm-x86_64/io_apic.h
@@ -132,6 +132,11 @@ static inline void io_apic_sync(unsigned
 	(void) *(IO_APIC_BASE(apic)+4);
 }
 
+/*
+ * Miscellaneous IO-APIC function prototypes
+ */
+void check_ioapic(void);
+
 /* 1 if "noapic" boot option passed */
 extern int skip_ioapic_setup;
 
diff -urNp linux-210/include/asm-x86_64/ipi.h linux-211/include/asm-x86_64/ipi.h
--- linux-210/include/asm-x86_64/ipi.h
+++ linux-211/include/asm-x86_64/ipi.h
@@ -0,0 +1,110 @@
+#ifndef __ASM_IPI_H
+#define __ASM_IPI_H
+
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Generic APIC InterProcessor Interrupt code.
+ *
+ * Moved to include file by James Cleverdon from
+ * arch/x86-64/kernel/smp.c
+ *
+ * Copyrights from kernel/smp.c:
+ *
+ * (c) 1995 Alan Cox, Building #3 <alan@redhat.com>
+ * (c) 1998-99, 2000 Ingo Molnar <mingo@redhat.com>
+ * (c) 2002,2003 Andi Kleen, SuSE Labs.
+ * Subject to the GNU Public License, v.2
+ */
+
+#include <asm/fixmap.h>
+#include <asm/hw_irq.h>
+#include <asm/apicdef.h>
+#include <asm/genapic.h>
+
+/*
+ * the following functions deal with sending IPIs between CPUs.
+ *
+ * We use 'broadcast', CPU->CPU IPIs and self-IPIs too.
+ */
+
+static inline unsigned int __prepare_ICR (unsigned int shortcut, int vector, unsigned int dest)
+{
+	return APIC_DM_FIXED | shortcut | vector | dest;
+}
+
+static inline int __prepare_ICR2 (unsigned int mask)
+{
+	return SET_APIC_DEST_FIELD(mask);
+}
+
+static inline void __send_IPI_shortcut(unsigned int shortcut, int vector, unsigned int dest)
+{
+	/*
+	 * Subtle. In the case of the 'never do double writes' workaround
+	 * we have to lock out interrupts to be safe.  As we don't care
+	 * of the value read we use an atomic rmw access to avoid costly
+	 * cli/sti.  Otherwise we use an even cheaper single atomic write
+	 * to the APIC.
+	 */
+	unsigned int cfg;
+
+	/*
+	 * Wait for idle.
+	 */
+	apic_wait_icr_idle();
+
+	/*
+	 * No need to touch the target chip field
+	 */
+	cfg = __prepare_ICR(shortcut, vector, dest);
+
+	/*
+	 * Send the IPI. The write to APIC_ICR fires this off.
+	 */
+	apic_write_around(APIC_ICR, cfg);
+}
+
+
+static inline void send_IPI_mask_sequence(unsigned long mask, int vector)
+{
+	unsigned long cfg, flags;
+	unsigned long query_cpu;
+
+	/*
+	 * Hack. The clustered APIC addressing mode doesn't allow us to send
+	 * to an arbitrary mask, so I do a unicast to each CPU instead.
+	 * - mbligh
+	 */
+	local_irq_save(flags);
+
+	for (query_cpu = 0; query_cpu < NR_CPUS; ++query_cpu) {
+		if ((1ul << query_cpu) & mask) {
+
+			/*
+			 * Wait for idle.
+			 */
+			apic_wait_icr_idle();
+
+			/*
+			 * prepare target chip field
+			 */
+			cfg = __prepare_ICR2(x86_cpu_to_apicid[query_cpu]);
+			apic_write_around(APIC_ICR2, cfg);
+
+			/*
+			 * program the ICR
+			 */
+			cfg = __prepare_ICR(0, vector, APIC_DEST_PHYSICAL);
+
+			/*
+			 * Send the IPI. The write to APIC_ICR fires this off.
+			 */
+			apic_write_around(APIC_ICR, cfg);
+		}
+	}
+	local_irq_restore(flags);
+}
+
+#endif /* __ASM_IPI_H */
diff -urNp linux-210/include/asm-x86_64/irq.h linux-211/include/asm-x86_64/irq.h
--- linux-210/include/asm-x86_64/irq.h
+++ linux-211/include/asm-x86_64/irq.h
@@ -23,8 +23,10 @@
  */
 #ifdef CONFIG_X86_IO_APIC
 #define NR_IRQS 224
+#define NR_IRQ_VECTORS 1024
 #else
 #define NR_IRQS 16
+#define NR_IRQ_VECTORS NR_IRQS
 #endif
 
 static __inline__ int irq_cannonicalize(int irq)
diff -urNp linux-210/include/asm-x86_64/mach_apic.h linux-211/include/asm-x86_64/mach_apic.h
--- linux-210/include/asm-x86_64/mach_apic.h
+++ linux-211/include/asm-x86_64/mach_apic.h
@@ -0,0 +1,28 @@
+#ifndef __ASM_MACH_APIC_H
+#define __ASM_MACH_APIC_H
+
+/*
+ * Copyright 2004 James Cleverdon, IBM.
+ * Subject to the GNU Public License, v.2
+ *
+ * Generic APIC sub-arch defines.
+ *
+ * Hacked for x86-64 by James Cleverdon from i386 architecture code by
+ * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
+ * James Cleverdon.
+ */
+
+#include <asm/genapic.h>
+
+#define INT_DELIVERY_MODE (genapic->int_delivery_mode)
+#define INT_DEST_MODE (genapic->int_dest_mode)
+#define INT_DELIVERY_DEST (genapic->int_delivery_dest)
+#define TARGET_CPUS	  (genapic->target_cpus())
+#define apic_id_registered (genapic->apic_id_registered)
+#define init_apic_ldr (genapic->init_apic_ldr)
+#define send_IPI_mask (genapic->send_IPI_mask)
+#define send_IPI_allbutself (genapic->send_IPI_allbutself)
+#define send_IPI_all (genapic->send_IPI_all)
+#define cpu_mask_to_apicid (genapic->cpu_mask_to_apicid)
+
+#endif /* __ASM_MACH_APIC_H */
diff -urNp linux-210/include/asm-x86_64/mpspec.h linux-211/include/asm-x86_64/mpspec.h
--- linux-210/include/asm-x86_64/mpspec.h
+++ linux-211/include/asm-x86_64/mpspec.h
@@ -14,9 +14,9 @@
 #define SMP_MAGIC_IDENT	(('_'<<24)|('P'<<16)|('M'<<8)|'_')
 
 /*
- * a maximum of 16 APICs with the current APIC ID architecture.
+ * A maximum of 255 APICs with the current APIC ID architecture.
  */
-#define MAX_APICS 16
+#define MAX_APICS 128
 
 struct intel_mp_floating
 {
@@ -156,7 +156,11 @@ struct mpc_config_lintsrc
  *	7	2 CPU MCA+PCI
  */
 
-#define MAX_MP_BUSSES 257
+/*
+ * Assume max PCI and 8 chassis x366 system (with 8 ISA):  256 + 8 = 264.
+ * Call it 270 for safety.
+ */
+#define MAX_MP_BUSSES	270
 #define MAX_IRQ_SOURCES (MAX_MP_BUSSES*4)
 enum mp_bustype {
 	MP_BUS_ISA = 1,
diff -urNp linux-210/include/asm-x86_64/pci-direct.h linux-211/include/asm-x86_64/pci-direct.h
--- linux-210/include/asm-x86_64/pci-direct.h
+++ linux-211/include/asm-x86_64/pci-direct.h
@@ -18,6 +18,15 @@ static inline u32 read_pci_config(u8 bus
 	return v;
 }
 
+static inline u8 read_pci_config_byte(u8 bus, u8 slot, u8 func, u8 offset)
+{
+	u8 v; 
+	outl(0x80000000 | (bus<<16) | (slot<<11) | (func<<8) | offset, 0xcf8);
+	v = inb(0xcfc + (offset&3)); 
+	PDprintk("%x reading 1 from %x: %x\n", slot, offset, v);
+	return v;
+}
+
 static inline void write_pci_config(u8 bus, u8 slot, u8 func, u8 offset,
 				    u32 val)
 {
diff -urNp linux-210/include/asm-x86_64/smp.h linux-211/include/asm-x86_64/smp.h
--- linux-210/include/asm-x86_64/smp.h
+++ linux-211/include/asm-x86_64/smp.h
@@ -21,6 +21,17 @@
 #endif
 #endif
 
+#ifndef ASSEMBLY
+extern u8 bios_cpu_apicid[];
+
+/*
+ * Some lowlevel functions might want to know about
+ * the real APIC ID <-> CPU # mapping.
+ */
+extern u8 x86_cpu_to_log_apicid[NR_CPUS];
+extern u8 x86_cpu_to_apicid[NR_CPUS];
+#endif
+
 #ifdef CONFIG_SMP
 #ifndef ASSEMBLY
 
@@ -37,7 +48,10 @@ extern unsigned long cpu_online_map;
 extern volatile unsigned long smp_invalidate_needed;
 extern int pic_mode;
 extern int smp_num_siblings;
+extern int smp_num_cores;
 extern int cpu_sibling_map[];
+extern u8 phys_proc_id[];
+extern u8 cpu_core_id[];
 
 extern void smp_flush_tlb(void);
 extern void smp_message_irq(int cpl, void *dev_id, struct pt_regs *regs);
@@ -67,12 +81,6 @@ extern inline int cpu_number_map(int cpu
 	return cpu;
 }
 
-/*
- * Some lowlevel functions might want to know about
- * the real APIC ID <-> CPU # mapping.
- */
-extern volatile int x86_cpu_to_apicid[NR_CPUS];
-
 static inline char x86_apicid_to_cpu(char apicid)
 {
 	int i;
@@ -85,8 +93,6 @@ static inline char x86_apicid_to_cpu(cha
 }
 
 
-extern u8 bios_cpu_apicid[];
-
 static inline int cpu_present_to_apicid(int mps_cpu)
 {
 	if (mps_cpu < NR_CPUS)
@@ -140,9 +146,7 @@ extern int slow_smp_processor_id(void);
 
 
 
-#endif
-#define INT_DELIVERY_MODE 1     /* logical delivery */
-#define TARGET_CPUS cpu_online_map
+#endif /* CONFIG_SMP */
 
 #ifndef CONFIG_SMP
 #define stack_smp_processor_id() 0
