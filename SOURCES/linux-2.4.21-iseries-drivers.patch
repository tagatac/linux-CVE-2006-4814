diff -urNp linux-341/Makefile linux-342/Makefile
--- linux-341/Makefile
+++ linux-342/Makefile
@@ -157,6 +157,7 @@ DRIVERS-$(CONFIG_FC4) += drivers/fc4/fc4
 DRIVERS-$(CONFIG_SCSI) += drivers/scsi/scsidrv.o
 DRIVERS-$(CONFIG_FUSION_BOOT) += drivers/message/fusion/fusion.o
 DRIVERS-$(CONFIG_IEEE1394) += drivers/ieee1394/ieee1394drv.o
+DRIVERS-$(CONFIG_PPC_ISERIES) += drivers/iseries/iseries.o
 
 ifneq ($(CONFIG_CD_NO_IDESCSI)$(CONFIG_BLK_DEV_IDECD)$(CONFIG_BLK_DEV_SR)$(CONFIG_PARIDE_PCD),)
 DRIVERS-y += drivers/cdrom/driver.o
diff -urNp linux-341/arch/ppc64/kernel/HvLpEvent.c linux-342/arch/ppc64/kernel/HvLpEvent.c
--- linux-341/arch/ppc64/kernel/HvLpEvent.c
+++ linux-342/arch/ppc64/kernel/HvLpEvent.c
@@ -37,6 +37,7 @@ int HvLpEvent_unregisterHandler( HvLpEve
 		if ( !lpEventHandlerPaths[eventType] ) {
 			lpEventHandler[eventType] = NULL;
 			rc = 0;
+			synchronize_irq();
 		}
 	}
 	return rc;
diff -urNp linux-341/drivers/Makefile linux-342/drivers/Makefile
--- linux-341/drivers/Makefile
+++ linux-342/drivers/Makefile
@@ -8,7 +8,7 @@
 
 mod-subdirs :=	dio hil mtd sbus video macintosh usb input telephony sgi ide \
 		message/i2o message/fusion scsi md ieee1394 pnp isdn atm \
-		fc4 net/hamradio i2c acpi bluetooth
+		fc4 net/hamradio i2c acpi bluetooth iseries
 
 subdir-y :=	parport char block net sound misc media cdrom hotplug
 subdir-m :=	$(subdir-y)
@@ -26,6 +26,7 @@ subdir-$(CONFIG_NUBUS)		+= nubus
 subdir-$(CONFIG_TC)		+= tc
 subdir-$(CONFIG_VT)		+= video
 subdir-$(CONFIG_MAC)		+= macintosh
+subdir-$(CONFIG_PPC_ISERIES)    += iseries
 subdir-$(CONFIG_PPC32)		+= macintosh
 subdir-$(CONFIG_USB)		+= usb
 subdir-$(CONFIG_INPUT)		+= input
diff -urNp linux-341/drivers/cdrom/Makefile linux-342/drivers/cdrom/Makefile
--- linux-341/drivers/cdrom/Makefile
+++ linux-342/drivers/cdrom/Makefile
@@ -27,6 +27,7 @@ obj-		:=
 obj-$(CONFIG_BLK_DEV_IDECD)	+=              cdrom.o
 obj-$(CONFIG_BLK_DEV_SR)	+=              cdrom.o
 obj-$(CONFIG_PARIDE_PCD)	+=		cdrom.o
+obj-$(CONFIG_VIOCD)		+=		cdrom.o
 
 obj-$(CONFIG_AZTCD)		+= aztcd.o
 obj-$(CONFIG_CDU31A)		+= cdu31a.o     cdrom.o
diff -urNp linux-341/drivers/char/tty_io.c linux-342/drivers/char/tty_io.c
--- linux-341/drivers/char/tty_io.c
+++ linux-342/drivers/char/tty_io.c
@@ -158,6 +158,8 @@ extern void tx3912_console_init(void);
 extern void tx3912_rs_init(void);
 extern void txx927_console_init(void);
 extern void sb1250_serial_console_init(void);
+extern void viocons_init(void);
+extern int viocons_init2(void);
 
 #ifndef MIN
 #define MIN(a,b)	((a) < (b) ? (a) : (b))
@@ -2314,6 +2316,9 @@ void __init console_init(void)
 #ifdef CONFIG_TXX927_SERIAL_CONSOLE
 	txx927_console_init();
 #endif
+#ifdef CONFIG_VIOCONS
+       viocons_init();
+#endif
 #ifdef CONFIG_SIBYTE_SB1250_DUART_CONSOLE
 	sb1250_serial_console_init();
 #endif
@@ -2367,6 +2372,9 @@ void __init tty_init(void)
 	/* console calls tty_register_driver() before kmalloc() works.
 	 * Thus, we can't devfs_register() then.  Do so now, instead. 
 	 */
+#ifdef CONFIG_VIOCONS
+        viocons_init2();
+#endif
 #ifdef CONFIG_VT
 	con_init_devfs();
 #endif
diff -urNp linux-341/drivers/iseries/Makefile linux-342/drivers/iseries/Makefile
--- linux-341/drivers/iseries/Makefile
+++ linux-342/drivers/iseries/Makefile
@@ -0,0 +1,43 @@
+#
+# Makefile for the iSeries-specific device drivers.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now inherited from the
+# parent makes..
+#
+
+# The target object and module list name.
+
+# O_TARGET	:= macintosh.o
+
+O_TARGET  := iseries.o
+
+# Objects that export symbols.
+
+# export-objs	:= adb.o rtc.o mac_hid.o via-pmu.o
+
+export-objs := veth.o viocons.o viotape.o viodasd.o viocd.o viopath.o
+
+# Object file lists.
+
+obj-y	:=
+obj-m	:=
+obj-n	:=
+obj-	:=
+
+# Each configuration option enables a list of files.
+
+obj-$(CONFIG_VETH) += veth.o
+obj-$(CONFIG_VIOCONS) += viocons.o
+obj-$(CONFIG_VIOPATH) += viopath.o
+obj-$(CONFIG_VIOTAPE) += viotape.o
+obj-$(CONFIG_VIODASD) += viodasd.o
+obj-$(CONFIG_VIOCD)   += viocd.o
+
+# The global Rules.make.
+
+include $(TOPDIR)/Rules.make
+
diff -urNp linux-341/drivers/iseries/veth.c linux-342/drivers/iseries/veth.c
--- linux-341/drivers/iseries/veth.c
+++ linux-342/drivers/iseries/veth.c
@@ -0,0 +1,1746 @@
+/* File veth.c created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/**************************************************************************/
+/*                                                                        */
+/* IBM eServer iSeries Virtual Ethernet Device Driver                     */
+/* Copyright (C) 2001 Kyle A. Lucke (klucke@us.ibm.com), IBM Corp.        */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the implementation of a virtual ethernet device   */
+/* for use with iSeries LPAR Linux.  It utilizes low-level message passing*/
+/* provided by the hypervisor to enable an ethernet-like network device   */
+/* that can be used to enable inter-partition communications on the same  */
+/* physical iSeries.                                                      */
+/*                                                                        */
+/* The iSeries LPAR hypervisor has currently defined the ability for a    */
+/* partition to communicate on up to 16 different virtual ethernets, all  */
+/* dynamically configurable, at least for an OS/400 partition.  The       */
+/* dynamic nature is not supported for Linux yet.                         */
+/*                                                                        */
+/* Each virtual ethernet a given Linux partition participates in will     */
+/* cause a network device with the form ethXX to be created,              */
+/*                                                                        */
+/* The virtual ethernet a given ethXX virtual ethernet device talks on    */
+/* can be determined either by dumping /proc/iSeries/veth/vethX, where    */
+/* X is the virtual ethernet number, and the netdevice name will be       */
+/* printed out.  The virtual ethernet a given ethX device communicates on */
+/* is also printed to the printk() buffer at module load time.            */
+/*                                                                        */
+/* This driver (and others like it on other partitions) is responsible for*/
+/* routing packets to and from other partitions.  The MAC addresses used  */
+/* by the virtual ethernets contain meaning, and should not be modified.  */
+/* Doing so could disable the ability of your Linux partition to          */
+/* communicate with the other OS/400 partitions on your physical iSeries. */
+/* Similarly, setting the MAC address to something other than the         */
+/* "virtual burned-in" address is not allowed, for the same reason.       */
+/*                                                                        */
+/* Notes:                                                                 */
+/*                                                                        */
+/* 1. Although there is the capability to talk on multiple shared         */
+/*    ethernets to communicate to the same partition, each shared         */
+/*    ethernet to a given partition X will use a finite, shared amount    */
+/*    of hypervisor messages to do the communication.  So having 2 shared */
+/*    ethernets to the same remote partition DOES NOT double the          */
+/*    available bandwidth.  Each of the 2 shared ethernets will share the */
+/*    same bandwidth available to another.                                */
+/*                                                                        */
+/* 2. It is allowed to have a virtual ethernet that does not communicate  */
+/*    with any other partition.  It won't do anything, but it's allowed.  */
+/*                                                                        */
+/* 3. There is no "loopback" mode for a virtual ethernet device.  If you  */
+/*    send a packet to your own mac address, it will just be dropped, you */
+/*    won't get it on the receive side.  Such a thing could be done,      */
+/*    but my default driver DOES NOT do so.                               */
+/*                                                                        */
+/* 4. Multicast addressing is implemented via broadcasting the multicast  */
+/*    frames to other partitions.  It is the responsibility of the        */
+/*    receiving partition to filter the addresses desired.                */
+/*                                                                        */
+/* 5. This module utilizes several different bottom half handlers for     */
+/*    non-high-use path function (setup, error handling, etc.).  Multiple */
+/*    bottom halves were used because only one would not keep up to the   */
+/*    much faster iSeries device drivers this Linux driver is talking to. */
+/*    All hi-priority work (receiving frames, handling frame acks) is done*/
+/*    in the interrupt handler for maximum performance.                   */
+/*                                                                        */
+/* Tunable parameters:                                                    */
+/*                                                                        */
+/* VethBuffersToAllocate: This compile time option defaults to 120. It can*/
+/* be safely changed to something greater or less than the default.  It   */
+/* controls how much memory Linux will allocate per remote partition it is*/
+/* communicating with.  The user can play with this to see how it affects */
+/* performance, packets dropped, etc.  Without trying to understand the   */
+/* complete driver, it can be thought of as the maximum number of packets */
+/* outstanding to a remote partition at a time.                           */
+/*                                                                        */
+/**************************************************************************/
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#include <asm/iSeries/mf.h>
+#include <asm/uaccess.h>
+
+#include "veth.h"
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/veth-proc.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/iSeries_proc.h>
+#include <asm/iSeries/iSeries_dma.h>
+#include <asm/semaphore.h>
+#include <linux/proc_fs.h>
+
+
+#define veth_printk(fmt, args...) \
+printk(KERN_INFO "%s: " fmt, __FILE__, ## args)
+
+#define veth_error_printk(fmt, args...) \
+printk(KERN_ERR "(%s:%3.3d) ERROR: " fmt, __FILE__, __LINE__ , ## args)
+
+static char *version __initdata = "v1.06 05/04/2003  Kyle Lucke, klucke@us.ibm.com\n";
+
+static int probed __initdata = 0;
+#define VethBuffersToAllocate 120
+
+static struct VethFabricMgr *mFabricMgr = NULL;
+static struct proc_dir_entry *veth_proc_root = NULL;
+static atomic_t pending_callbacks;
+
+DECLARE_MUTEX_LOCKED(VethProcSemaphore);
+
+static int veth_open(struct net_device *dev);
+static int veth_close(struct net_device *dev);
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static void veth_handleEvent(struct HvLpEvent *, struct pt_regs *);
+static void veth_handleAck(struct HvLpEvent *);
+static void veth_handleInt(struct HvLpEvent *);
+static void veth_openConnections(void);
+static void veth_openConnection(u8);
+static void veth_closeConnection(u8);
+static void veth_intFinishOpeningConnections(void *, int number);
+static void veth_finishOpeningConnections(void *);
+static void veth_finishOpeningConnectionsLocked(struct VethLpConnection *);
+static int veth_multicast_wanted(struct VethPort *port, u64 dest);
+static void veth_set_multicast_list(struct net_device *dev);
+
+static void veth_sendCap(struct VethLpConnection *);
+static void veth_sendMonitor(struct VethLpConnection *);
+static void veth_takeCap(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_takeCapAck(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_takeMonitorAck(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_msgsInit(struct VethLpConnection *connection);
+static void veth_recycleMsgByNum(struct VethLpConnection *, u16);
+static void veth_recycleMsg(struct VethLpConnection *, struct VethMsg *);
+static void veth_capTask(void *);
+static void veth_capAckTask(void *);
+static void veth_monitorAckTask(void *);
+static void veth_takeFrames(struct VethLpConnection *, struct VethLpEvent *);
+static int veth_pTransmit(struct sk_buff *skb, HvLpIndex remoteLp, struct net_device *dev);
+static struct net_device_stats *veth_get_stats(struct net_device *dev);
+static void veth_intFinishMsgsInit(void *, int);
+static void veth_finishMsgsInit(struct VethLpConnection *connection);
+static void veth_intFinishCapTask(void *, int);
+static void veth_finishCapTask(struct VethLpConnection *connection);
+static void veth_finishCapTaskLocked(struct VethLpConnection *connection);
+static void veth_finishSendCap(struct VethLpConnection *connection);
+static void veth_timedAck(unsigned long connectionPtr);
+static void veth_startQueues(void);
+static void veth_failMe(struct VethLpConnection *connection);
+
+extern struct pci_dev *iSeries_veth_dev;
+static struct net_device *veth_devices[16];
+static int veth_dev_queue_stopped[16];
+static int veth_num_devices;
+static int veth_num_fragged;
+
+int __init veth_probe(void)
+{
+	struct net_device *dev = NULL;
+	struct VethPort *port = NULL;
+	int vlansFound = 0;
+	int displayVersion = 0;
+	int rc;
+
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	int vlanIndex = 0;
+	veth_num_fragged = 0;
+
+	memset(veth_devices, 0, sizeof(struct net_device *) * 16);
+	memset(veth_dev_queue_stopped, 0, sizeof(int) * 16);
+	veth_num_devices = 0;
+	if (probed)
+		return -ENODEV;
+	probed = 1;
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			vlansFound++;
+			
+			dev = alloc_etherdev(sizeof(struct VethPort));
+
+			if (dev == NULL) {
+				veth_error_printk("Unable to allocate net_device structure!\n");
+				break;
+			}
+
+			port = mFabricMgr->mPorts[vlanIndex] = (struct VethPort *) dev->priv;
+			memset(port, 0, sizeof(struct VethPort));
+			rwlock_init(&(port->mMcastGate));
+			mFabricMgr->mPorts[vlanIndex]->mDev = dev;
+			veth_devices[veth_num_devices] = dev;
+			veth_num_devices++;
+
+			dev->dev_addr[0] = 0x02;
+			dev->dev_addr[1] = 0x01;
+			dev->dev_addr[2] = 0xFF;
+			dev->dev_addr[3] = vlanIndex;
+			dev->dev_addr[4] = 0xFF;
+			dev->dev_addr[5] = HvLpConfig_getLpIndex_outline();
+			dev->mtu = 9000;
+
+			memcpy(&(port->mMyAddress), dev->dev_addr, 6);
+
+			dev->open = &veth_open;
+			dev->hard_start_xmit = &veth_start_xmit;
+			dev->stop = &veth_close;
+			dev->get_stats = veth_get_stats;
+			dev->set_multicast_list = &veth_set_multicast_list;
+			dev->do_ioctl = &veth_ioctl;
+
+			/* display version info if adapter is found */
+			if (!displayVersion) {
+				/* set display flag to TRUE so that */
+				/* we only display this string ONCE */
+				displayVersion = 1;
+				veth_printk("%s", version);
+			}
+
+			rc = register_netdev(dev);
+
+			if(rc) {
+				veth_printk("Failed to register an ethernet device (veth=%d)\n", vlanIndex);
+				mFabricMgr->mPorts[vlanIndex] = NULL;
+				veth_num_devices--;
+				veth_devices[veth_num_devices] = NULL;
+				kfree(dev);
+			} else {
+				veth_printk("Found an ethernet device %s (veth=%d) (addr=%p)\n", dev->name, vlanIndex, dev);
+			}
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	if (vlansFound > 0)
+		return 0;
+	else
+		return -ENODEV;
+}
+
+MODULE_AUTHOR("Kyle Lucke <klucke@us.ibm.com>");
+MODULE_DESCRIPTION("iSeries Virtual ethernet driver");
+MODULE_LICENSE("GPL");
+
+int VethModuleReopen = 1;
+
+void veth_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	int i = 0;
+	HvLpIndex thisLp = HvLpConfig_getLpIndex_outline();
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	int vlanIndex = 0;
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		if (i != thisLp) {
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(thisLp, i)) {
+			        char name[10];
+				sprintf(name, "lpar%d", i);
+				remove_proc_entry(name, veth_proc_root);
+			}
+		}
+	}
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			char name[10];
+			sprintf(name, "veth%d", vlanIndex);
+			remove_proc_entry(name, veth_proc_root);
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	remove_proc_entry("veth", iSeries_proc);
+
+	up(&VethProcSemaphore);
+}
+
+void __exit veth_module_cleanup(void)
+{
+        if(mFabricMgr != NULL) {
+
+	    int i;
+	    struct VethFabricMgr *myFm = mFabricMgr;
+	    struct net_device *thisOne = NULL;
+
+	    VethModuleReopen = 0;
+
+	    for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		    struct VethLpConnection *connection = &(mFabricMgr->mConnection[i]);
+		    unsigned long flags;
+		    spin_lock_irqsave(&connection->mStatusGate, flags);
+		    veth_closeConnection(i);
+		    spin_unlock_irqrestore(&connection->mStatusGate, flags);
+	    }
+
+	    /* Stop interrupts coming from the Hypervisor */
+	    HvLpEvent_unregisterHandler(HvLpEvent_Type_VirtualLan);
+
+	    /* Stop rescheduling of tasks in ack callbacks */
+	    mFabricMgr = NULL;
+	    mb();
+
+	    /* Wait for any scheduled tasks to complete */
+	    flush_scheduled_tasks();
+
+	    /* Wait for any outstanding callbacks to arrive */
+	    while (atomic_read(&pending_callbacks));
+
+	    /* Ensure all callbacks have finished running */
+	    synchronize_irq();
+
+	    down(&VethProcSemaphore);
+
+	    iSeries_proc_callback(&veth_proc_delete);
+
+	    down(&VethProcSemaphore);
+
+	    for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		    if (myFm->mConnection[i].mNumberAllocated + myFm->mConnection[i].mNumberRcvMsgs > 0) {
+			    mf_deallocateLpEvents(myFm->mConnection[i].mRemoteLp,
+						  HvLpEvent_Type_VirtualLan,
+						  myFm->mConnection[i].mNumberAllocated + myFm->mConnection[i].mNumberRcvMsgs,
+						  NULL, NULL);
+		    }
+
+		    kfree(myFm->mConnection[i].mMsgs);
+	    }
+
+	    for (i = 0; i < HvMaxArchitectedVirtualLans; ++i) {
+		    if (myFm->mPorts[i] != NULL) {
+			    thisOne = myFm->mPorts[i]->mDev;
+			    myFm->mPorts[i] = NULL;
+
+			    mb();
+
+			    if (thisOne != NULL) {
+				    veth_printk("Unregistering %s (veth=%d)\n", thisOne->name, i);
+				    unregister_netdev(thisOne);
+				    kfree(thisOne);
+			    }
+		    }
+	    }
+
+	    kfree(myFm);
+	}
+}
+
+module_exit(veth_module_cleanup);
+
+void veth_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+        long i = 0;
+	HvLpIndex thisLp = HvLpConfig_getLpIndex_outline();
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	long vlanIndex = 0;
+
+
+	veth_proc_root = proc_mkdir("veth", iSeries_proc);
+	if (!veth_proc_root)
+		return;
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		if (i != thisLp) {
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(thisLp, i)) {
+				struct proc_dir_entry *ent;
+				char name[10];
+				sprintf(name, "lpar%d", (int) i);
+				ent = create_proc_entry(name, S_IFREG | S_IRUSR, veth_proc_root);
+				if (!ent)
+					return;
+				ent->nlink = 1;
+				ent->owner = THIS_MODULE;
+				ent->data = (void *) i;
+				ent->read_proc = proc_veth_dump_connection;
+				ent->write_proc = NULL;
+			}
+		}
+	}
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			struct proc_dir_entry *ent;
+			char name[10];
+			sprintf(name, "veth%d", (int) vlanIndex);
+			ent = create_proc_entry(name, S_IFREG | S_IRUSR, veth_proc_root);
+			if (!ent)
+				return;
+			ent->nlink = 1;
+			ent->owner = THIS_MODULE;
+			ent->data = (void *) vlanIndex;
+			ent->read_proc = proc_veth_dump_port;
+			ent->write_proc = NULL;
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	up(&VethProcSemaphore);
+}
+
+int __init veth_module_init(void)
+{
+	int status;
+	int i;
+
+	mFabricMgr = kmalloc(sizeof(struct VethFabricMgr), GFP_KERNEL);
+
+	if(mFabricMgr == NULL) {
+	        veth_printk("Unable to allocate fabric manager\n");
+		return -ENOMEM;
+	}
+
+	memset(mFabricMgr, 0, sizeof(struct VethFabricMgr));
+	veth_printk("Initializing veth module, fabric mgr (address=%p)\n", mFabricMgr);
+
+	mFabricMgr->mEyecatcher = 0x56455448464D4752ULL;
+	mFabricMgr->mThisLp = HvLpConfig_getLpIndex_outline();
+	atomic_set(&pending_callbacks, 0);
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		mFabricMgr->mConnection[i].mEyecatcher = 0x564554484C50434EULL;
+		veth_failMe(mFabricMgr->mConnection + i);
+		spin_lock_init(&mFabricMgr->mConnection[i].mAckGate);
+		spin_lock_init(&mFabricMgr->mConnection[i].mStatusGate);
+	}
+
+	status = veth_probe();
+
+	if (status == 0) {
+		veth_openConnections();
+		iSeries_proc_callback(&veth_proc_init);
+	}
+
+	return status;
+}
+
+module_init(veth_module_init);
+
+static void veth_failMe(struct VethLpConnection *connection)
+{
+	connection->mConnectionStatus.mSentCap = 0;
+	connection->mConnectionStatus.mCapAcked = 0;
+	connection->mConnectionStatus.mGotCap = 0;
+	connection->mConnectionStatus.mGotCapAcked = 0;
+	connection->mConnectionStatus.mSentMonitor = 0;
+	connection->mConnectionStatus.mFailed = 1;
+}
+
+static int veth_open(struct net_device *dev)
+{
+	struct VethPort *port = (struct VethPort *) dev->priv;
+
+	memset(&port->mStats, 0, sizeof(port->mStats));
+	MOD_INC_USE_COUNT;
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int veth_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+
+	MOD_DEC_USE_COUNT;
+
+	return 0;
+}
+
+static struct net_device_stats *veth_get_stats(struct net_device *dev)
+{
+	struct VethPort *port = (struct VethPort *) dev->priv;
+
+	return (&port->mStats);
+}
+
+
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned char *frame = skb->data;
+	HvLpIndex remoteLp = frame[5];
+	int i = 0;
+	int rc = 1;
+	int individual_rc;
+	int skb_len = skb->len;
+	struct VethPort *port = (struct VethPort *) dev->priv;
+
+	if (mFabricMgr == NULL) {
+		veth_error_printk("NULL fabric manager with active ports!\n");
+		netif_stop_queue(dev);
+		return 1;
+	}
+
+	mb();
+
+	if ((*frame & 0x01) != 0x01) {	/* broadcast or multicast */
+		if ((remoteLp != mFabricMgr->mThisLp) && (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, remoteLp)))
+			rc = veth_pTransmit(skb, remoteLp, dev);
+		else
+		{
+		        dev_kfree_skb(skb);
+			rc = 0;
+		}
+	} else {
+		for (i = 0; i < HvMaxArchitectedLps; ++i) {
+			if (i != mFabricMgr->mThisLp) {
+				if (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, i)) {
+				        struct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);
+
+					if(clone == NULL) {
+						veth_error_printk("skb_clone failed %p\n", skb);
+						rc = 0;
+						break;
+					}
+
+					/* the ack handles deleting the skb */
+					individual_rc = veth_pTransmit(clone, i, dev);
+
+					/* tx failed, we need to free the sbk */
+					if (individual_rc != 0)
+					    dev_kfree_skb(clone);
+
+					/* if we didn't fail from lack of buffers, the tx as a whole is successful */
+					if(individual_rc != 1)
+					    rc = 0;
+				}
+			}
+		}
+
+                /* broadcast/multicast - If every connection is out of buffers (highly unlikely) then we leave rc
+                   set to  1 and stop the queue. If any connection fails for any reason other than out of buffers,
+                   then we say the tx succeeded. 
+                */ 
+		if(rc == 0)
+		    dev_kfree_skb(skb);
+	}
+
+	if (rc != 0)
+	{
+	    if (rc == 1) {
+		/* reasons for stopping the queue:
+		     - a non broadcast/multicast packet was destined for a connection that is out of buffers
+                     - a broadcast/multicast packet and every connection was out of buffers
+                */ 
+		  
+		int number = dev->dev_addr[3];
+		++veth_dev_queue_stopped[number];
+		netif_stop_queue(dev);
+	    } else {
+		/* reasons for not stopping the queue:
+		     - a non broadcast/multicast packet was destined for a failed connection
+		     - a broadcast/multicast packet and at least one connection had available buffers
+                */
+		dev_kfree_skb(skb);
+		rc = 0;
+	    }
+	}
+	else
+	{
+	    port->mStats.tx_packets++;
+	    port->mStats.tx_bytes += skb_len;
+	}
+
+	return rc;
+}
+
+static int veth_pTransmit(struct sk_buff *skb, HvLpIndex remoteLp, struct net_device *dev)
+{
+	struct VethLpConnection *connection = mFabricMgr->mConnection + remoteLp;
+	HvLpEvent_Rc returnCode;
+	u32 dma_address, dma_length;
+	int transmit_rc = 0;
+
+	if (connection->mConnectionStatus.mFailed != 1) {
+		struct VethMsg *msg = NULL;
+		VETHSTACKPOP(&(connection->mMsgStack), msg);
+
+		if (msg != NULL && ((skb->len - 14) <= 9000)) {
+		        dma_length = skb->len;
+		        dma_address = pci_map_single(iSeries_veth_dev, skb->data, dma_length, PCI_DMA_TODEVICE);
+
+			/* Is it really necessary to check the length and address fields of the 
+			   first entry here? */
+			if (dma_address != NO_TCE) {
+			        msg->mSkb = skb;
+			        msg->mEvent.mSendData.mAddress[0] = dma_address;
+			        msg->mEvent.mSendData.mLength[0]  = dma_length;
+			        msg->mEvent.mSendData.mEofMask = 1;
+			        set_bit(0, &(msg->mInUse));
+			        returnCode = HvCallEvent_signalLpEventFast(remoteLp,
+									   HvLpEvent_Type_VirtualLan,
+									   VethEventTypeFrames,
+									   HvLpEvent_AckInd_NoAck,
+									   HvLpEvent_AckType_ImmediateAck,
+									   connection->mSourceInst,
+									   connection->mTargetInst,
+									   msg->mIndex,
+									   msg->mEvent.mFpData.mData1,
+									   msg->mEvent.mFpData.mData2,
+									   msg->mEvent.mFpData.mData3,
+									   msg->mEvent.mFpData.mData4,
+									   msg->mEvent.mFpData.mData5);
+			} else {
+			        struct VethPort *port = (struct VethPort *) dev->priv;
+			        returnCode = -1;	/* Bad return code */
+			        port->mStats.tx_errors++;
+			}
+
+			if (returnCode != HvLpEvent_Rc_Good) {
+			        msg->mSkb = NULL;
+				/* need to set in use to make veth_recycleMsg in case this was a mapping failure */
+				set_bit(0, &(msg->mInUse));
+			        veth_recycleMsg(connection, msg);
+			        transmit_rc = 2;
+			} else {
+			        transmit_rc = 0;
+			}
+		} else {
+		        if (msg != NULL) {
+			        veth_recycleMsg(connection, msg);
+			        transmit_rc = 2;
+			} else {
+			        transmit_rc = 1;
+			}
+		}
+	} else {
+		transmit_rc = 2;
+	}
+
+	return transmit_rc;
+}
+
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+    struct ethtool_cmd ecmd;
+
+    if (cmd != SIOCETHTOOL)
+	return -EOPNOTSUPP;
+    if (copy_from_user(&ecmd, ifr->ifr_data, sizeof(ecmd)))
+	return -EFAULT;
+    switch (ecmd.cmd) {
+	case ETHTOOL_GSET:
+	    ecmd.supported =
+	      (SUPPORTED_1000baseT_Full |
+	       SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+	    ecmd.advertising =
+	      (SUPPORTED_1000baseT_Full |
+	       SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+
+	    ecmd.port = PORT_FIBRE;
+	    ecmd.transceiver = XCVR_INTERNAL;
+	    ecmd.phy_address = 0;
+	    ecmd.speed = SPEED_1000;
+	    ecmd.duplex = DUPLEX_FULL;
+	    ecmd.autoneg = AUTONEG_ENABLE;
+	    ecmd.maxtxpkt = 120;
+	    ecmd.maxrxpkt = 120;
+	    if(copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+		return -EFAULT;
+	    return 0;
+
+	case ETHTOOL_GDRVINFO: {
+		struct ethtool_drvinfo info = {ETHTOOL_GDRVINFO};
+		strncpy(info.driver, "veth", sizeof(info.driver) - 1);
+		info.driver[sizeof(info.driver) - 1] = '\0';
+		strncpy(info.version, "1.0", sizeof(info.version) - 1);
+		if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+		    return -EFAULT;
+		return 0;
+	    }
+	/* get link status */
+	case ETHTOOL_GLINK: {
+		struct ethtool_value edata = {ETHTOOL_GLINK};
+		edata.data = 1;
+		if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+		    return -EFAULT;
+		return 0;
+	    }
+
+	default:
+	    break;
+    }
+
+#endif
+	return -EOPNOTSUPP;
+}
+
+static void veth_set_multicast_list(struct net_device *dev)
+{
+	char *addrs;
+	struct VethPort *port = (struct VethPort *) dev->priv;
+	u64 newAddress = 0;
+	unsigned long flags;
+
+	write_lock_irqsave(&port->mMcastGate, flags);
+
+	if (dev->flags & IFF_PROMISC) {	/* set promiscuous mode */
+		port->mPromiscuous = 1;
+	} else {
+		struct dev_mc_list *dmi = dev->mc_list;
+
+		if ((dev->flags & IFF_ALLMULTI) || (dev->mc_count > 12)) {
+			port->mAllMcast = 1;
+		} else {
+			int i;
+			/* Update table */
+			port->mNumAddrs = 0;
+
+			for (i = 0; ((i < dev->mc_count) && (i < 12)); i++) {	/* for each address in the list */
+				addrs = dmi->dmi_addr;
+				dmi = dmi->next;
+				if ((*addrs & 0x01) == 1) {	/* multicast address? */
+					memcpy(&newAddress, addrs, 6);
+					newAddress &= 0xFFFFFFFFFFFF0000;
+
+					port->mMcasts[port->mNumAddrs] = newAddress;
+					mb();
+					port->mNumAddrs = port->mNumAddrs + 1;
+				}
+			}
+		}
+	}
+
+	write_unlock_irqrestore(&port->mMcastGate, flags);
+}
+
+
+static void veth_handleEvent(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack) {
+		veth_handleAck(event);
+	} else if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		veth_handleInt(event);
+	}
+}
+
+static void veth_handleAck(struct HvLpEvent *event)
+{
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[event->xTargetLp]);
+	struct VethLpEvent *vethEvent = (struct VethLpEvent *) event;
+
+	switch (event->xSubtype) {
+	case VethEventTypeCap:
+		{
+			veth_takeCapAck(connection, vethEvent);
+			break;
+		}
+	case VethEventTypeMonitor:
+		{
+			veth_takeMonitorAck(connection, vethEvent);
+			break;
+		}
+	default:
+		{
+			veth_error_printk("Unknown ack type %d from lpar %d\n", event->xSubtype, connection->mRemoteLp);
+		}
+	};
+}
+
+static void veth_handleInt(struct HvLpEvent *event)
+{
+	int i = 0;
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[event->xSourceLp]);
+	struct VethLpEvent *vethEvent = (struct VethLpEvent *) event;
+
+	switch (event->xSubtype) {
+	case VethEventTypeCap:
+		{
+			veth_takeCap(connection, vethEvent);
+			break;
+		}
+	case VethEventTypeMonitor:
+		{
+			/* do nothing... this'll hang out here til we're dead, and the hypervisor will return it for us. */
+			break;
+		}
+	case VethEventTypeFramesAck:
+		{
+			for (i = 0; i < VethMaxFramesMsgsAcked; ++i) {
+				u16 msg = vethEvent->mDerivedData.mFramesAckData.mToken[i];
+				veth_recycleMsgByNum(connection, msg);
+			}
+			break;
+		}
+	case VethEventTypeFrames:
+		{
+			veth_takeFrames(connection, vethEvent);
+			break;
+		}
+	default:
+		{
+			veth_error_printk("Unknown interrupt type %d from lpar %d\n", event->xSubtype, connection->mRemoteLp);
+		}
+	};
+}
+
+static void veth_openConnections()
+{
+	int i = 0;
+
+	HvLpEvent_registerHandler(HvLpEvent_Type_VirtualLan, &veth_handleEvent);
+
+	/* Now I need to run through the active lps and open connections to the ones I'm supposed to
+	   open to. */
+
+	for (i = HvMaxArchitectedLps - 1; i >= 0; --i) {
+		if (i != mFabricMgr->mThisLp) {
+    		        struct VethLpConnection *connection = &(mFabricMgr->mConnection[i]);
+		        unsigned long flags;
+
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, i)) {
+			        spin_lock_irqsave(&connection->mStatusGate, flags);
+			        veth_openConnection(i);
+			        spin_unlock_irqrestore(&connection->mStatusGate, flags);
+			} else {
+			        spin_lock_irqsave(&connection->mStatusGate, flags);
+			        veth_closeConnection(i);
+			        spin_unlock_irqrestore(&connection->mStatusGate, flags);
+			}
+		}
+	}
+}
+
+static void veth_intFinishOpeningConnections(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocTaskTq.data = parm;
+	connection->mNumberAllocated = number;
+	atomic_dec(&pending_callbacks);
+	if (!mFabricMgr)
+		return;
+	schedule_task(&connection->mAllocTaskTq);
+}
+
+static void veth_finishOpeningConnections(void *parm)
+{
+	unsigned long flags;
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	veth_finishOpeningConnectionsLocked(connection);
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_finishOpeningConnectionsLocked(struct VethLpConnection *connection)
+{
+	if (connection->mNumberAllocated >= 2) {
+		connection->mConnectionStatus.mCapMonAlloced = 1;
+		veth_sendCap(connection);
+	} else {
+		veth_error_printk("Couldn't allocate base msgs for lpar %d, only got %d\n", connection->mRemoteLp,
+				  connection->mNumberAllocated);
+		veth_failMe(connection);
+	}
+}
+
+static void veth_openConnection(u8 remoteLp)
+{
+	unsigned long flags;
+	HvLpInstanceId source;
+	HvLpInstanceId target;
+	u64 i = 0;
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[remoteLp]);
+	
+	INIT_TQUEUE(&connection->mCapTaskTq, veth_capTask, NULL);
+	INIT_TQUEUE(&connection->mCapAckTaskTq, veth_capAckTask, NULL);
+	INIT_TQUEUE(&connection->mMonitorAckTaskTq, veth_monitorAckTask, NULL);
+	INIT_TQUEUE(&connection->mAllocTaskTq, veth_finishOpeningConnections, NULL);
+
+	connection->mRemoteLp = remoteLp;
+
+	spin_lock_irqsave(&connection->mAckGate, flags);
+
+	memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+	connection->mNumAcks = 0;
+
+	HvCallEvent_openLpEventPath(remoteLp, HvLpEvent_Type_VirtualLan);
+
+	/* clean up non-acked msgs */
+	for (i = 0; i < connection->mNumMsgs; ++i) {
+		veth_recycleMsgByNum(connection, i);
+	}
+
+	connection->mConnectionStatus.mOpen = 1;
+
+	source = connection->mSourceInst = HvCallEvent_getSourceLpInstanceId(remoteLp, HvLpEvent_Type_VirtualLan);
+	target = connection->mTargetInst = HvCallEvent_getTargetLpInstanceId(remoteLp, HvLpEvent_Type_VirtualLan);
+
+	if (connection->mConnectionStatus.mCapMonAlloced != 1) {
+		connection->mAllocTaskTq.routine = (void *) (void *) veth_finishOpeningConnections;
+		atomic_inc(&pending_callbacks);
+		mf_allocateLpEvents(remoteLp,
+				    HvLpEvent_Type_VirtualLan,
+				    sizeof(struct VethLpEvent), 2, &veth_intFinishOpeningConnections, connection);
+	} else {
+		veth_finishOpeningConnectionsLocked(connection);
+	}
+
+	spin_unlock_irqrestore(&connection->mAckGate, flags);
+}
+
+static void veth_closeConnection(u8 remoteLp)
+{
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[remoteLp]);
+	unsigned long flags;
+
+	del_timer_sync(&connection->mAckTimer);
+
+	if (connection->mConnectionStatus.mOpen == 1) {
+		HvCallEvent_closeLpEventPath(remoteLp, HvLpEvent_Type_VirtualLan);
+		connection->mConnectionStatus.mOpen = 0;
+		veth_failMe(connection);
+
+		/* reset ack data */
+		spin_lock_irqsave(&connection->mAckGate, flags);
+
+		memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+		connection->mNumAcks = 0;
+
+		spin_unlock_irqrestore(&connection->mAckGate, flags);
+	}
+
+}
+
+static void veth_msgsInit(struct VethLpConnection *connection)
+{
+	connection->mAllocTaskTq.routine = (void *) (void *) veth_finishMsgsInit;
+	atomic_inc(&pending_callbacks);
+	mf_allocateLpEvents(connection->mRemoteLp,
+			    HvLpEvent_Type_VirtualLan,
+			    sizeof(struct VethLpEvent),
+			    connection->mMyCap.mUnionData.mFields.mNumberBuffers, &veth_intFinishMsgsInit, connection);
+}
+
+static void veth_intFinishMsgsInit(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocTaskTq.data = parm;
+	connection->mNumberRcvMsgs = number;
+	atomic_dec(&pending_callbacks);
+	if (!mFabricMgr)
+		return;
+	schedule_task(&connection->mAllocTaskTq);
+}
+
+static void veth_intFinishCapTask(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocTaskTq.data = parm;
+	if (number > 0)
+		connection->mNumberLpAcksAlloced += number;
+	atomic_dec(&pending_callbacks);
+	if (!mFabricMgr)
+		return;
+	schedule_task(&connection->mAllocTaskTq);
+}
+
+static void veth_finishMsgsInit(struct VethLpConnection *connection)
+{
+	int i = 0;
+	unsigned int numberGotten = 0;
+	u64 amountOfHeapToGet = connection->mMyCap.mUnionData.mFields.mNumberBuffers * sizeof(struct VethMsg);
+	char *msgs = NULL;
+	unsigned long flags;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	if (connection->mNumberRcvMsgs >= connection->mMyCap.mUnionData.mFields.mNumberBuffers) {
+		msgs = kmalloc(amountOfHeapToGet, GFP_ATOMIC);
+
+		connection->mMsgs = (struct VethMsg *) msgs;
+
+		if (msgs != NULL) {
+			memset(msgs, 0, amountOfHeapToGet);
+
+			for (i = 0; i < connection->mMyCap.mUnionData.mFields.mNumberBuffers; ++i) {
+				connection->mMsgs[i].mIndex = i;
+				++numberGotten;
+				VETHSTACKPUSH(&(connection->mMsgStack), (connection->mMsgs + i));
+			}
+			if (numberGotten > 0) {
+				connection->mNumMsgs = numberGotten;
+			}
+		} else {
+			connection->mMsgs = NULL;
+		}
+	}
+
+	connection->mMyCap.mUnionData.mFields.mNumberBuffers = connection->mNumMsgs;
+
+	if (connection->mNumMsgs < 10)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 1;
+	else if (connection->mNumMsgs < 20)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 4;
+	else if (connection->mNumMsgs < 40)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 10;
+	else
+		connection->mMyCap.mUnionData.mFields.mThreshold = 20;
+
+	connection->mMyCap.mUnionData.mFields.mTimer = VethAckTimeoutUsec;
+
+	veth_finishSendCap(connection);
+
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_sendCap(struct VethLpConnection *connection)
+{
+	if (connection->mMsgs == NULL) {
+		connection->mMyCap.mUnionData.mFields.mNumberBuffers = VethBuffersToAllocate;
+		veth_msgsInit(connection);
+	} else {
+		veth_finishSendCap(connection);
+	}
+}
+
+static void veth_finishSendCap(struct VethLpConnection *connection)
+{
+	HvLpEvent_Rc returnCode = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+								HvLpEvent_Type_VirtualLan,
+								VethEventTypeCap,
+								HvLpEvent_AckInd_DoAck,
+								HvLpEvent_AckType_ImmediateAck,
+								connection->mSourceInst,
+								connection->mTargetInst,
+								0,
+								connection->mMyCap.mUnionData.mNoFields.mReserved1,
+								connection->mMyCap.mUnionData.mNoFields.mReserved2,
+								connection->mMyCap.mUnionData.mNoFields.mReserved3,
+								connection->mMyCap.mUnionData.mNoFields.mReserved4,
+								connection->mMyCap.mUnionData.mNoFields.mReserved5);
+
+	if ((returnCode == HvLpEvent_Rc_PartitionDead) || (returnCode == HvLpEvent_Rc_PathClosed)) {
+		connection->mConnectionStatus.mSentCap = 0;
+	} else if (returnCode != HvLpEvent_Rc_Good) {
+		veth_error_printk("Couldn't send cap to lpar %d, rc %x\n", connection->mRemoteLp, (int) returnCode);
+		veth_failMe(connection);
+	} else {
+		connection->mConnectionStatus.mSentCap = 1;
+	}
+}
+
+static void veth_takeCap(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mCapTaskPending))) {
+		connection->mCapTaskTq.data = connection;
+		memcpy(&connection->mCapEvent, event, sizeof(connection->mCapEvent));
+		schedule_task(&connection->mCapTaskTq);
+	} else {
+		veth_error_printk("Received a capabilities from lpar %d while already processing one\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+}
+
+static void veth_takeCapAck(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mCapAckTaskPending))) {
+		connection->mCapAckTaskTq.data = connection;
+		memcpy(&connection->mCapAckEvent, event, sizeof(connection->mCapAckEvent));
+		schedule_task(&connection->mCapAckTaskTq);
+	} else {
+		veth_error_printk("Received a capabilities ack from lpar %d while already processing one\n",
+				  connection->mRemoteLp);
+	}
+}
+
+static void veth_takeMonitorAck(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mMonitorAckTaskPending))) {
+		connection->mMonitorAckTaskTq.data = connection;
+		memcpy(&connection->mMonitorAckEvent, event, sizeof(connection->mMonitorAckEvent));
+		schedule_task(&connection->mMonitorAckTaskTq);
+	} else {
+		veth_error_printk("Received a monitor ack from lpar %d while already processing one\n", connection->mRemoteLp);
+	}
+}
+
+static void veth_recycleMsgByNum(struct VethLpConnection *connection, u16 msg)
+{
+	if (msg < connection->mNumMsgs) {
+		struct VethMsg *myMsg = connection->mMsgs + msg;
+		veth_recycleMsg(connection, myMsg);
+	}
+}
+
+static void veth_recycleMsg(struct VethLpConnection *connection, struct VethMsg *myMsg)
+{
+	u32 dma_address, dma_length;
+
+	if (test_and_clear_bit(0, &(myMsg->mInUse))) {
+	        dma_address = myMsg->mEvent.mSendData.mAddress[0];
+	        dma_length = myMsg->mEvent.mSendData.mLength[0];
+	    
+	        pci_unmap_single(iSeries_veth_dev, dma_address, dma_length, PCI_DMA_TODEVICE);
+
+	        if (myMsg->mSkb != NULL) {
+		    dev_kfree_skb_any(myMsg->mSkb);
+		    myMsg->mSkb = NULL;
+	        }
+
+	        memset(&(myMsg->mEvent.mSendData), 0, sizeof(struct VethFramesData));
+	        VETHSTACKPUSH(&connection->mMsgStack, myMsg);
+	} else {
+	        if (connection->mConnectionStatus.mOpen) {
+		        veth_error_printk("Received a frames ack for msg %d from lpar %d while not outstanding\n", myMsg->mIndex,
+				  connection->mRemoteLp);
+		}
+	}
+
+	veth_startQueues();
+}
+
+static void veth_startQueues(void)
+{
+    int i;
+    for (i=0; i < veth_num_devices; ++i)
+    {
+	netif_wake_queue(veth_devices[i]);
+    }
+}
+
+static void veth_capTask(void *parm)
+{
+        struct VethLpConnection *connection = (struct VethLpConnection*)parm;
+	struct VethLpEvent *event = &connection->mCapEvent;
+	unsigned long flags;
+	struct VethCapData *remoteCap = &(connection->mRemoteCap);
+	u64 numAcks = 0;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	connection->mConnectionStatus.mGotCap = 1;
+
+	memcpy(remoteCap, &(event->mDerivedData.mCapabilitiesData), sizeof(connection->mRemoteCap));
+
+	if ((remoteCap->mUnionData.mFields.mNumberBuffers <= VethMaxFramesMsgs) &&
+	    (remoteCap->mUnionData.mFields.mNumberBuffers != 0) &&
+	    (remoteCap->mUnionData.mFields.mThreshold <= VethMaxFramesMsgsAcked) &&
+	    (remoteCap->mUnionData.mFields.mThreshold != 0)) {
+		numAcks = (remoteCap->mUnionData.mFields.mNumberBuffers / remoteCap->mUnionData.mFields.mThreshold) + 1;
+
+		if (connection->mNumberLpAcksAlloced < numAcks) {
+			numAcks = numAcks - connection->mNumberLpAcksAlloced;
+			connection->mAllocTaskTq.routine = (void *) (void *) veth_finishCapTask;
+			atomic_inc(&pending_callbacks);
+			mf_allocateLpEvents(connection->mRemoteLp,
+					    HvLpEvent_Type_VirtualLan,
+					    sizeof(struct VethLpEvent), numAcks, &veth_intFinishCapTask, connection);
+		} else
+			veth_finishCapTaskLocked(connection);
+	} else {
+		veth_error_printk("Received incompatible capabilities from lpar %d\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_InvalidSubtypeData;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+
+	clear_bit(0, &(connection->mCapTaskPending));
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_capAckTask(void *parm)
+{
+        struct VethLpConnection *connection = (struct VethLpConnection*)parm;
+	struct VethLpEvent *event = &connection->mCapAckEvent;
+	unsigned long flags;
+
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	if (event->mBaseEvent.xRc == HvLpEvent_Rc_Good) {
+		connection->mConnectionStatus.mCapAcked = 1;
+
+		if ((connection->mConnectionStatus.mGotCap == 1) && (connection->mConnectionStatus.mGotCapAcked == 1)) {
+			if (connection->mConnectionStatus.mSentMonitor != 1)
+				veth_sendMonitor(connection);
+		}
+	} else {
+		veth_printk("Bad rc(%d) from lpar %d on capabilities\n", event->mBaseEvent.xRc, connection->mRemoteLp);
+		veth_failMe(connection);
+	}
+
+	clear_bit(0, &(connection->mCapAckTaskPending));
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_monitorAckTask(void *parm)
+{
+        struct VethLpConnection *connection = (struct VethLpConnection*)parm;
+	unsigned long flags;
+
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	veth_failMe(connection);
+
+	veth_printk("Monitor ack returned for lpar %d\n", connection->mRemoteLp);
+
+	if (connection->mConnectionStatus.mOpen) {
+		veth_closeConnection(connection->mRemoteLp);
+
+		udelay(100);
+		
+		schedule_task(&connection->mMonitorAckTaskTq);
+	} else {
+		if (VethModuleReopen)
+			veth_openConnection(connection->mRemoteLp);
+		else {
+			int i = 0;
+
+			for (i = 0; i < connection->mNumMsgs; ++i) {
+				veth_recycleMsgByNum(connection, i);
+			}
+		}
+		clear_bit(0, &(connection->mMonitorAckTaskPending));
+	}
+
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+#define number_of_pages(v, l) ((((unsigned long)(v) & ((1 << 12) - 1)) + (l) + 4096 - 1) / 4096)
+#define page_offset(v) ((unsigned long)(v) & ((1 << 12) - 1))
+
+static void veth_takeFrames(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	int i = 0;
+	struct VethPort *port = NULL;
+	struct BufList {
+		union {
+			struct {
+				u32 token2;
+				u32 garbage;
+			} token1;
+			u64 address;
+		} addr;
+		u64 size;
+	};
+
+	struct BufList myBufList[4];	/* max pages per frame */
+	struct BufList remoteList[VethMaxFramesPerMsg];	/* max frags per frame */
+	memset(myBufList, 0, sizeof(struct BufList) * 4);
+	memset(remoteList, 0, sizeof(struct BufList) * VethMaxFramesPerMsg);
+
+	do {
+		int nfrags = 0;
+		u16 length = 0;
+
+		/* a 0 address marks the end of the valid entries */
+		if (event->mDerivedData.mSendData.mAddress[i] == 0)
+			break;
+
+		/* make sure that we have at least 1 EOF entry in the remaining entries */
+		if (!(event->mDerivedData.mSendData.mEofMask >> i)) {
+			veth_printk("bad lp event: missing EOF frag in event mEofMask 0x%x i %d\n",
+				    event->mDerivedData.mSendData.mEofMask, i);
+			break;
+		}
+
+		/* add up length of non-EOF frags */
+		do {
+			remoteList[nfrags].addr.token1.token2 = event->mDerivedData.mSendData.mAddress[i + nfrags];
+			remoteList[nfrags].addr.token1.garbage = 0;
+			length += remoteList[nfrags].size = event->mDerivedData.mSendData.mLength[i + nfrags];
+		}
+		while (!(event->mDerivedData.mSendData.mEofMask & (1 << (i + nfrags++))));
+
+
+		/* length == total length of all framgents */
+		/* nfrags == # of fragments in this frame */
+
+		if ((length - 14) <= 9000) {	/* save as 13 < length <= 9014 */
+			struct sk_buff *skb = alloc_skb(length, GFP_ATOMIC);
+			if (skb != NULL) {
+				HvLpDma_Rc returnCode = HvLpDma_Rc_Good;
+
+				/* build the buffer list for the dma operation */
+				int numPages = number_of_pages((skb->data), length);	/* number of pages in this fragment of the complete buffer */
+				myBufList[0].addr.address =
+				    (0x8000000000000000LL | (virt_to_absolute((unsigned long) skb->data)));
+				myBufList[0].size = (numPages > 1) ? (4096 - page_offset(skb->data)) : length;
+				if (numPages > 1) {
+					myBufList[1].addr.address =
+					    (0x8000000000000000LL |
+					     (virt_to_absolute((unsigned long) skb->data + myBufList[0].size)));
+					myBufList[1].size = (numPages > 2) ? 4096 : length - myBufList[0].size;
+					if (numPages > 2) {
+						myBufList[2].addr.address =
+						    (0x8000000000000000LL |
+						     (virt_to_absolute
+						      ((unsigned long) skb->data + myBufList[0].size + myBufList[1].size)));
+						myBufList[2].size =
+						    (numPages > 3) ? 4096 : length - myBufList[0].size - myBufList[1].size;
+						if (numPages > 3) {
+							myBufList[3].addr.address =
+							    0x8000000000000000LL |
+							    (virt_to_absolute
+							     ((unsigned long) skb->data + myBufList[0].size + myBufList[1].size +
+							      myBufList[2].size));
+							myBufList[3].size =
+							    length - myBufList[0].size - myBufList[1].size - myBufList[2].size;
+						}
+					}
+				}
+				returnCode = HvCallEvent_dmaBufList(HvLpEvent_Type_VirtualLan,
+								    event->mBaseEvent.xSourceLp,
+								    HvLpDma_Direction_RemoteToLocal,
+								    connection->mSourceInst,
+								    connection->mTargetInst,
+								    HvLpDma_AddressType_RealAddress,
+								    HvLpDma_AddressType_TceIndex,
+								    0x8000000000000000LL |
+								    (virt_to_absolute((unsigned long) &myBufList)),
+								    0x8000000000000000LL |
+								    (virt_to_absolute((unsigned long) &remoteList)), length);
+
+				if (returnCode == HvLpDma_Rc_Good) {
+					HvLpVirtualLanIndex vlan = skb->data[9];
+					u64 dest = *((u64 *) skb->data) & 0xFFFFFFFFFFFF0000;
+
+					if (((vlan < HvMaxArchitectedVirtualLans) && ((port = mFabricMgr->mPorts[vlan]) != NULL)) && ((dest == port->mMyAddress) ||	/* it's for me */
+																      (dest == 0xFFFFFFFFFFFF0000) ||	/* it's a broadcast */
+																      (veth_multicast_wanted(port, dest)) ||	/* it's one of my multicasts */
+																      (port->mPromiscuous == 1))) {	/* I'm promiscuous */
+						skb_put(skb, length);
+						skb->dev = port->mDev;
+						skb->protocol = eth_type_trans(skb, port->mDev);
+						skb->ip_summed = CHECKSUM_NONE;
+						netif_rx(skb);	/* send it up */
+						port->mStats.rx_packets++;
+						port->mStats.rx_bytes += length;
+
+					} else {
+						dev_kfree_skb_irq(skb);
+					}
+				} else {
+					dev_kfree_skb_irq(skb);
+				}
+			}
+		} else {
+			break;
+		}
+		i += nfrags;
+	} while (i < VethMaxFramesPerMsg);
+
+	/* Ack it */
+
+	{
+		unsigned long flags;
+		spin_lock_irqsave(&connection->mAckGate, flags);
+
+		if (connection->mNumAcks < VethMaxFramesMsgsAcked) {
+			connection->mEventData.mAckData.mToken[connection->mNumAcks] = event->mBaseEvent.xCorrelationToken;
+			++connection->mNumAcks;
+
+			if (connection->mNumAcks == connection->mRemoteCap.mUnionData.mFields.mThreshold) {
+				HvLpEvent_Rc rc = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+										HvLpEvent_Type_VirtualLan,
+										VethEventTypeFramesAck,
+										HvLpEvent_AckInd_NoAck,
+										HvLpEvent_AckType_ImmediateAck,
+										connection->mSourceInst,
+										connection->mTargetInst,
+										0,
+										connection->mEventData.mFpData.mData1,
+										connection->mEventData.mFpData.mData2,
+										connection->mEventData.mFpData.mData3,
+										connection->mEventData.mFpData.mData4,
+										connection->mEventData.mFpData.mData5);
+
+				if (rc != HvLpEvent_Rc_Good) {
+					veth_error_printk("Bad lp event return code(%x) acking frames from lpar %d\n", (int) rc,
+							  connection->mRemoteLp);
+				}
+
+				connection->mNumAcks = 0;
+
+				memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+			}
+
+		}
+
+		spin_unlock_irqrestore(&connection->mAckGate, flags);
+	}
+}
+
+#undef number_of_pages
+#undef page_offset
+
+static void veth_timedAck(unsigned long connectionPtr)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+	struct VethLpConnection *connection = (struct VethLpConnection *) connectionPtr;
+	/* Ack all the events */
+	spin_lock_irqsave(&connection->mAckGate, flags);
+
+	if (connection->mNumAcks > 0) {
+		rc = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+						   HvLpEvent_Type_VirtualLan,
+						   VethEventTypeFramesAck,
+						   HvLpEvent_AckInd_NoAck,
+						   HvLpEvent_AckType_ImmediateAck,
+						   connection->mSourceInst,
+						   connection->mTargetInst,
+						   0,
+						   connection->mEventData.mFpData.mData1,
+						   connection->mEventData.mFpData.mData2,
+						   connection->mEventData.mFpData.mData3,
+						   connection->mEventData.mFpData.mData4, connection->mEventData.mFpData.mData5);
+
+		if (rc != HvLpEvent_Rc_Good) {
+			veth_error_printk("Bad lp event return code(%x) acking frames from lpar %d!\n", (int) rc,
+					  connection->mRemoteLp);
+		}
+
+		connection->mNumAcks = 0;
+
+		memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+	}
+
+	spin_unlock_irqrestore(&connection->mAckGate, flags);
+
+	veth_startQueues();
+
+	/* Reschedule the timer */
+	connection->mAckTimer.expires = jiffies + connection->mTimeout;
+	add_timer(&connection->mAckTimer);
+}
+
+static int veth_multicast_wanted(struct VethPort *port, u64 thatAddr)
+{
+	int returnParm = 0;
+	int i;
+	unsigned long flags;
+
+	if ((*((char *) &thatAddr) & 0x01) != 1)
+		return 0;
+
+	read_lock_irqsave(&port->mMcastGate, flags);
+	if (port->mAllMcast) {
+		read_unlock_irqrestore(&port->mMcastGate, flags);
+		return 1;
+	}
+
+	for (i = 0; i < port->mNumAddrs; ++i) {
+		u64 thisAddr = port->mMcasts[i];
+
+		if (thisAddr == thatAddr) {
+			returnParm = 1;
+			break;
+		}
+	}
+	read_unlock_irqrestore(&port->mMcastGate, flags);
+
+	return returnParm;
+}
+
+static void veth_sendMonitor(struct VethLpConnection *connection)
+{
+	HvLpEvent_Rc returnCode = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+								HvLpEvent_Type_VirtualLan,
+								VethEventTypeMonitor,
+								HvLpEvent_AckInd_DoAck,
+								HvLpEvent_AckType_DeferredAck,
+								connection->mSourceInst,
+								connection->mTargetInst,
+								0, 0, 0, 0, 0, 0);
+
+	if (returnCode == HvLpEvent_Rc_Good) {
+		connection->mConnectionStatus.mSentMonitor = 1;
+		connection->mConnectionStatus.mFailed = 0;
+
+		/* Start the ACK timer */
+		init_timer(&connection->mAckTimer);
+		connection->mAckTimer.function = veth_timedAck;
+		connection->mAckTimer.data = (unsigned long) connection;
+		connection->mAckTimer.expires = jiffies + connection->mTimeout;
+		add_timer(&connection->mAckTimer);
+
+	} else {
+		veth_error_printk("Monitor send to lpar %d failed with rc %x\n", connection->mRemoteLp, (int) returnCode);
+		veth_failMe(connection);
+	}
+}
+
+static void veth_finishCapTask(struct VethLpConnection *connection)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	veth_finishCapTaskLocked(connection);
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_finishCapTaskLocked(struct VethLpConnection *connection)
+{
+	struct VethLpEvent *event = &connection->mCapEvent;
+	struct VethCapData *remoteCap = &(connection->mRemoteCap);
+	int numAcks = (remoteCap->mUnionData.mFields.mNumberBuffers / remoteCap->mUnionData.mFields.mThreshold) + 1;
+
+	/* Convert timer to jiffies */
+	if (connection->mMyCap.mUnionData.mFields.mTimer)
+		connection->mTimeout = remoteCap->mUnionData.mFields.mTimer * HZ / 1000000;
+	else
+		connection->mTimeout = VethAckTimeoutUsec * HZ / 1000000;
+
+	if (connection->mNumberLpAcksAlloced >= numAcks) {
+		HvLpEvent_Rc returnCode = HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+
+		if (returnCode == HvLpEvent_Rc_Good) {
+			connection->mConnectionStatus.mGotCapAcked = 1;
+
+			if (connection->mConnectionStatus.mSentCap != 1) {
+				connection->mTargetInst =
+				    HvCallEvent_getTargetLpInstanceId(connection->mRemoteLp, HvLpEvent_Type_VirtualLan);
+
+				veth_sendCap(connection);
+			} else if (connection->mConnectionStatus.mCapAcked == 1) {
+				if (connection->mConnectionStatus.mSentMonitor != 1)
+					veth_sendMonitor(connection);
+			}
+		} else {
+			veth_error_printk("Failed to ack remote cap for lpar %d with rc %x\n", connection->mRemoteLp,
+					  (int) returnCode);
+			veth_failMe(connection);
+		}
+	} else {
+		veth_error_printk("Couldn't allocate all the frames ack events for lpar %d\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+}
+
+int proc_veth_dump_connection(char *page, char **start, off_t off, int count, int *eof, void *data) {
+	char *out = page;
+	long whichConnection = (long) data;
+	int len = 0;
+	struct VethLpConnection *connection = NULL;
+
+	if ((whichConnection < 0) || (whichConnection > HvMaxArchitectedLps) || (mFabricMgr == NULL)) {
+		veth_error_printk("Got bad data from /proc file system\n");
+		len = sprintf(page, "ERROR\n");
+	} else {
+		int thereWasStuffBefore = 0;
+		connection = &(mFabricMgr->mConnection[whichConnection]);
+
+		out += sprintf(out, "Remote Lp:\t%d\n", connection->mRemoteLp);
+		out += sprintf(out, "Source Inst:\t%04X\n", connection->mSourceInst);
+		out += sprintf(out, "Target Inst:\t%04X\n", connection->mTargetInst);
+		out += sprintf(out, "Num Msgs:\t%d\n", connection->mNumMsgs);
+		out += sprintf(out, "Num Lp Acks:\t%d\n", connection->mNumberLpAcksAlloced);
+		out += sprintf(out, "Num Acks:\t%d\n", connection->mNumAcks);
+
+		if (connection->mConnectionStatus.mOpen) {
+			out += sprintf(out, "<Open");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mCapMonAlloced) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "CapMonAlloced");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mBaseMsgsAlloced) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "BaseMsgsAlloced");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mSentCap) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "SentCap");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mCapAcked) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "CapAcked");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mGotCap) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "GotCap");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mGotCapAcked) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "GotCapAcked");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mSentMonitor) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "SentMonitor");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mPopulatedRings) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "PopulatedRings");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mFailed) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "Failed");
+			thereWasStuffBefore = 1;
+		}
+
+		if (thereWasStuffBefore)
+			out += sprintf(out, ">");
+
+		out += sprintf(out, "\n");
+
+		out += sprintf(out, "Capabilities (System:<Version/Buffers/Threshold/Timeout>):\n");
+		out += sprintf(out, "\tLocal:<");
+		out += sprintf(out, "%d/%d/%d/%d>\n",
+			       connection->mMyCap.mUnionData.mFields.mVersion,
+			       connection->mMyCap.mUnionData.mFields.mNumberBuffers,
+			       connection->mMyCap.mUnionData.mFields.mThreshold, connection->mMyCap.mUnionData.mFields.mTimer);
+		out += sprintf(out, "\tRemote:<");
+		out += sprintf(out, "%d/%d/%d/%d>\n",
+			       connection->mRemoteCap.mUnionData.mFields.mVersion,
+			       connection->mRemoteCap.mUnionData.mFields.mNumberBuffers,
+			       connection->mRemoteCap.mUnionData.mFields.mThreshold,
+			       connection->mRemoteCap.mUnionData.mFields.mTimer);
+		len = out - page;
+	}
+	len -= off;
+	if (len < count) {
+		*eof = 1;
+		if (len <= 0)
+			return 0;
+	} else
+		len = count;
+	*start = page + off;
+	return len;
+}
+
+int proc_veth_dump_port(char *page, char **start, off_t off, int count, int *eof, void *data) {
+	char *out = page;
+	long whichPort = (long) data;
+	int len = 0;
+	struct VethPort *port = NULL;
+
+	if ((whichPort < 0) || (whichPort > HvMaxArchitectedVirtualLans) || (mFabricMgr == NULL))
+		len = sprintf(page, "Virtual ethernet is not configured.\n");
+	else {
+		int i = 0;
+		u32 *myAddr;
+		u16 *myEndAddr;
+		port = mFabricMgr->mPorts[whichPort];
+
+		if (port != NULL) {
+			myAddr = (u32 *) & (port->mMyAddress);
+			myEndAddr = (u16 *) (myAddr + 1);
+			out += sprintf(out, "Net device:\t%p\n", port->mDev);
+			out += sprintf(out, "Net device name:\t%s\n", port->mDev->name);
+			out += sprintf(out, "Address:\t%08X%04X\n", myAddr[0], myEndAddr[0]);
+			out += sprintf(out, "Promiscuous:\t%d\n", port->mPromiscuous);
+			out += sprintf(out, "All multicast:\t%d\n", port->mAllMcast);
+			out += sprintf(out, "Number multicast:\t%d\n", port->mNumAddrs);
+
+			for (i = 0; i < port->mNumAddrs; ++i) {
+				u32 *multi = (u32 *) & (port->mMcasts[i]);
+				u16 *multiEnd = (u16 *) (multi + 1);
+				out += sprintf(out, "   %08X%04X\n", multi[0], multiEnd[0]);
+			}
+		} else {
+			out += sprintf(page, "veth%d is not configured.\n", (int) whichPort);
+		}
+
+		len = out - page;
+	}
+	len -= off;
+	if (len < count) {
+		*eof = 1;
+		if (len <= 0)
+			return 0;
+	} else
+		len = count;
+	*start = page + off;
+	return len;
+}
diff -urNp linux-341/drivers/iseries/veth.h linux-342/drivers/iseries/veth.h
--- linux-341/drivers/iseries/veth.h
+++ linux-342/drivers/iseries/veth.h
@@ -0,0 +1,241 @@
+/* File veth.h created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/* Change Activity: */
+/* End Change Activity */
+
+#ifndef _VETH_H
+#define _VETH_H
+
+#ifndef _HVTYPES_H
+#include <asm/iSeries/HvTypes.h>
+#endif
+#ifndef _HVLPEVENT_H
+#include <asm/iSeries/HvLpEvent.h>
+#endif
+#include <linux/netdevice.h>
+
+#define VethEventNumTypes (4)
+#define VethEventTypeCap (0)
+#define VethEventTypeFrames (1)
+#define VethEventTypeMonitor (2)
+#define VethEventTypeFramesAck (3)
+
+#define VethMaxFramesMsgsAcked (20)
+#define VethMaxFramesMsgs (0xFFFF)
+#define VethMaxFramesPerMsg (6)
+#define VethAckTimeoutUsec (1000000)
+
+#define VETHSTACKTYPE(T) struct VethStack##T
+#define VETHSTACK(T) \
+VETHSTACKTYPE(T) \
+{ \
+struct T *head; \
+spinlock_t lock; \
+}
+#define VETHSTACKCTOR(s) do { (s)->head = NULL; spin_lock_init(&(s)->lock); } while(0)
+#define VETHSTACKPUSH(s, p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(s)->lock,flags); \
+(p)->next = (s)->head; \
+(s)->head = (p); \
+spin_unlock_irqrestore(&(s)->lock, flags); \
+} while(0)
+
+#define VETHSTACKPOP(s,p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(s)->lock,flags); \
+(p) = (s)->head; \
+if ((s)->head != NULL) \
+{ \
+(s)->head = (s)->head->next; \
+} \
+spin_unlock_irqrestore(&(s)->lock, flags); \
+} while(0)
+
+#define VETHQUEUE(T) \
+struct VethQueue##T \
+{ \
+T *head; \
+T *tail; \
+spinlock_t lock; \
+}
+#define VETHQUEUECTOR(q) do { (q)->head = NULL; (q)->tail = NULL; spin_lock_init(&(q)->lock); } while(0)
+#define VETHQUEUEENQ(q, p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(q)->lock,flags); \
+(p)->next = NULL; \
+if ((q)->head != NULL) \
+{ \
+(q)->head->next = (p); \
+(q)->head = (p); \
+} \
+else \
+{ \
+(q)->tail = (q)->head = (p); \
+} \
+spin_unlock_irqrestore(&(q)->lock, flags); \
+} while(0)
+
+#define VETHQUEUEDEQ(q,p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(q)->lock,flags); \
+(p) = (q)->tail; \
+if ((p) != NULL) \
+{ \
+(q)->tail = (p)->next; \
+(p)->next = NULL; \
+} \
+if ((q)->tail == NULL) \
+(q)->head = NULL; \
+spin_unlock_irqrestore(&(q)->lock, flags); \
+} while(0)
+
+struct VethFramesData {
+	u32 mAddress[6];
+	u16 mLength[6];
+	u32 mEofMask:6;
+	u32 mReserved:26;
+};
+
+struct VethFramesAckData {
+	u16 mToken[VethMaxFramesMsgsAcked];
+};
+
+struct VethCapData {
+	union {
+		struct Fields {
+			u8 mVersion;
+			u8 mReserved1;
+			u16 mNumberBuffers;
+			u16 mThreshold;
+			u16 mReserved2;
+			u32 mTimer;
+			u32 mReserved3;
+			u64 mReserved4;
+			u64 mReserved5;
+			u64 mReserved6;
+		} mFields;
+		struct NoFields {
+			u64 mReserved1;
+			u64 mReserved2;
+			u64 mReserved3;
+			u64 mReserved4;
+			u64 mReserved5;
+		} mNoFields;
+	} mUnionData;
+};
+
+struct VethFastPathData {
+	u64 mData1;
+	u64 mData2;
+	u64 mData3;
+	u64 mData4;
+	u64 mData5;
+};
+
+struct VethLpEvent {
+	struct HvLpEvent mBaseEvent;
+	union {
+		struct VethFramesData mSendData;
+		struct VethCapData mCapabilitiesData;
+		struct VethFramesAckData mFramesAckData;
+		struct VethFastPathData mFastPathData;
+	} mDerivedData;
+
+};
+
+struct VethMsg {
+	struct VethMsg *next;
+	union {
+		struct VethFramesData mSendData;
+		struct VethFastPathData mFpData;
+	} mEvent;
+	int mIndex;
+	unsigned long mInUse;
+	struct sk_buff *mSkb;
+};
+
+
+struct VethControlBlock {
+	struct net_device *mDev;
+	struct VethControlBlock *mNext;
+	HvLpVirtualLanIndex mVlanId;
+};
+
+struct VethLpConnection {
+	u64 mEyecatcher;
+	HvLpIndex mRemoteLp;
+	HvLpInstanceId mSourceInst;
+	HvLpInstanceId mTargetInst;
+	u32 mNumMsgs;
+	struct VethMsg *mMsgs;
+	int mNumberRcvMsgs;
+	int mNumberLpAcksAlloced;
+	union {
+		struct VethFramesAckData mAckData;
+		struct VethFastPathData mFpData;
+	} mEventData;
+	spinlock_t mAckGate;
+	u32 mNumAcks;
+	spinlock_t mStatusGate;
+	struct {
+		u64 mOpen:1;
+		u64 mCapMonAlloced:1;
+		u64 mBaseMsgsAlloced:1;
+		u64 mSentCap:1;
+		u64 mCapAcked:1;
+		u64 mGotCap:1;
+		u64 mGotCapAcked:1;
+		u64 mSentMonitor:1;
+		u64 mPopulatedRings:1;
+		u64 mReserved:54;
+		u64 mFailed:1;
+	} mConnectionStatus;
+	struct VethCapData mMyCap;
+	struct VethCapData mRemoteCap;
+	unsigned long mCapAckTaskPending;
+	struct tq_struct mCapAckTaskTq;
+	struct VethLpEvent mCapAckEvent;
+	unsigned long mCapTaskPending;
+	struct tq_struct mCapTaskTq;
+	struct VethLpEvent mCapEvent;
+	unsigned long mMonitorAckTaskPending;
+	struct tq_struct mMonitorAckTaskTq;
+	struct VethLpEvent mMonitorAckEvent;
+	unsigned long mAllocTaskPending;
+	struct tq_struct mAllocTaskTq;
+	int mNumberAllocated;
+	struct timer_list mAckTimer;
+	u32 mTimeout;
+	 VETHSTACK(VethMsg) mMsgStack;
+};
+#define HVMAXARCHITECTEDVIRTUALLANS 16
+struct VethPort {
+	struct net_device *mDev;
+	struct net_device_stats mStats;
+	int mLock;
+	u64 mMyAddress;
+	int mPromiscuous;
+	int mAllMcast;
+	rwlock_t mMcastGate;
+	int mNumAddrs;
+	u64 mMcasts[12];
+};
+
+struct VethFabricMgr {
+	u64 mEyecatcher;
+	HvLpIndex mThisLp;
+	struct VethLpConnection mConnection[HVMAXARCHITECTEDLPS];
+	spinlock_t mPortListGate;
+	u64 mNumPorts;
+	struct VethPort *mPorts[HVMAXARCHITECTEDVIRTUALLANS];
+};
+
+int proc_veth_dump_connection(char *page, char **start, off_t off, int count, int *eof, void *data);
+int proc_veth_dump_port(char *page, char **start, off_t off, int count, int *eof, void *data);
+
+#endif				/* _VETH_H */
diff -urNp linux-341/drivers/iseries/vio.h linux-342/drivers/iseries/vio.h
--- linux-341/drivers/iseries/vio.h
+++ linux-342/drivers/iseries/vio.h
@@ -0,0 +1,130 @@
+/* -*- linux-c -*-
+ *  drivers/char/vio.h
+ *
+ *  iSeries Virtual I/O Message Path header
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This header file is used by the iSeries virtual I/O device
+ * drivers.  It defines the interfaces to the common functions
+ * (implemented in drivers/char/viopath.h) as well as defining
+ * common functions and structures.  Currently (at the time I 
+ * wrote this comment) the iSeries virtual I/O device drivers
+ * that use this are 
+ *   drivers/block/viodasd.c 
+ *   drivers/char/viocons.c
+ *   drivers/char/viotape.c
+ *   drivers/cdrom/viocd.c
+ *
+ * The iSeries virtual ethernet support (veth.c) uses a whole
+ * different set of functions.
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef _VIO_H
+#define _VIO_H
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+
+/* iSeries virtual I/O events use the subtype field in
+ * HvLpEvent to figure out what kind of vio event is coming
+ * in.  We use a table to route these, and this defines
+ * the maximum number of distinct subtypes
+ */
+#define VIO_MAX_SUBTYPES 7
+
+/* Each subtype can register a handler to process their events.
+ * The handler must have this interface.
+ */
+typedef void (vio_event_handler_t) (struct HvLpEvent * event);
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq);
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq);
+int vio_setHandler(int subtype, vio_event_handler_t * beh);
+int vio_clearHandler(int subtype);
+int viopath_isactive(HvLpIndex lp);
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp);
+HvLpInstanceId viopath_targetinst(HvLpIndex lp);
+void vio_set_hostlp(void);
+void *vio_get_event_buffer(int subtype);
+void vio_free_event_buffer(int subtype, void *buffer);
+
+extern HvLpIndex viopath_hostLp;
+extern HvLpIndex viopath_ourLp;
+
+#define VIO_MESSAGE "iSeries virtual I/O: "
+#define KERN_DEBUG_VIO KERN_DEBUG VIO_MESSAGE
+#define KERN_INFO_VIO KERN_INFO VIO_MESSAGE
+#define KERN_WARNING_VIO KERN_WARNING VIO_MESSAGE
+
+#define VIOCHAR_MAX_DATA 200
+
+#define VIOMAJOR_SUBTYPE_MASK 0xff00
+#define VIOMINOR_SUBTYPE_MASK 0x00ff
+#define VIOMAJOR_SUBTYPE_SHIFT 8
+
+#define VIOVERSION            0x0101
+
+/*
+This is the general structure for VIO errors; each module should have a table
+of them, and each table should be terminated by an entry of { 0, 0, NULL }.
+Then, to find a specific error message, a module should pass its local table
+and the return code.
+*/
+struct vio_error_entry {
+	u16 rc;
+	int errno;
+	const char *msg;
+};
+const struct vio_error_entry *vio_lookup_rc(const struct vio_error_entry
+					    *local_table, u16 rc);
+
+enum viosubtypes {
+	viomajorsubtype_monitor = 0x0100,
+	viomajorsubtype_blockio = 0x0200,
+	viomajorsubtype_chario = 0x0300,
+	viomajorsubtype_config = 0x0400,
+	viomajorsubtype_cdio = 0x0500,
+	viomajorsubtype_tape = 0x0600
+};
+
+
+enum vioconfigsubtype {
+	vioconfigget = 0x0001,
+};
+
+enum viorc {
+	viorc_good = 0x0000,
+	viorc_noConnection = 0x0001,
+	viorc_noReceiver = 0x0002,
+	viorc_noBufferAvailable = 0x0003,
+	viorc_invalidMessageType = 0x0004,
+	viorc_invalidRange = 0x0201,
+	viorc_invalidToken = 0x0202,
+	viorc_DMAError = 0x0203,
+	viorc_useError = 0x0204,
+	viorc_releaseError = 0x0205,
+	viorc_invalidDisk = 0x0206,
+	viorc_openRejected = 0x0301
+};
+
+
+#endif				/* _VIO_H */
diff -urNp linux-341/drivers/iseries/viocd.c linux-342/drivers/iseries/viocd.c
--- linux-341/drivers/iseries/viocd.c
+++ linux-342/drivers/iseries/viocd.c
@@ -0,0 +1,859 @@
+/* -*- linux-c -*-
+ *  drivers/cdrom/viocd.c
+ *
+ ***************************************************************************
+ *  iSeries Virtual CD Rom
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *************************************************************************** 
+ * This routine provides access to CD ROM drives owned and managed by an 
+ * OS/400 partition running on the same box as this Linux partition.
+ *
+ * All operations are performed by sending messages back and forth to 
+ * the OS/400 partition.  
+ *
+ * 
+ * This device driver can either use it's own major number, or it can
+ * pretend to be an AZTECH drive. This is controlled with a 
+ * CONFIG option.  You can either call this an elegant solution to the 
+ * fact that a lot of software doesn't recognize a new CD major number...
+ * or you can call this a really ugly hack.  Your choice.
+ *
+ */
+
+#include <linux/major.h>
+#include <linux/config.h>
+
+/* Decide on the proper naming convention to use for our device */
+#ifdef CONFIG_DEVFS_FS
+#define VIOCD_DEVICE "cdroms/cdrom%d"
+#define VIOCD_DEVICE_OFFSET 0
+#else
+#ifdef CONFIG_VIOCD_AZTECH
+#define VIOCD_DEVICE "aztcd"
+#define VIOCD_DEVICE_OFFSET 0
+#else
+#define VIOCD_DEVICE "iseries/vcd%c"
+#define VIOCD_DEVICE_OFFSET 'a'
+#endif
+#endif
+
+/***************************************************************************
+ * Decide if we are using our own major or pretending to be an AZTECH drive
+ ***************************************************************************/
+#ifdef CONFIG_VIOCD_AZTECH
+#define MAJOR_NR AZTECH_CDROM_MAJOR
+#define do_viocd_request do_aztcd_request
+#else
+#define MAJOR_NR VIOCD_MAJOR
+#endif
+
+#define VIOCD_VERS "1.04"
+
+#include <linux/blk.h>
+#include <linux/cdrom.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include "vio.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+extern struct pci_dev * iSeries_vio_dev;
+
+#define signalLpEventFast HvCallEvent_signalLpEventFast
+
+struct viocdlpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	u32 mToken;
+	u64 mOffset;		// On open, the max number of disks
+	u64 mLen;		// On open, the size of the disk
+	u32 mBlockSize;		// Only set on open
+	u32 mMediaSize;		// Only set on open
+};
+
+enum viocdsubtype {
+	viocdopen = 0x0001,
+	viocdclose = 0x0002,
+	viocdread = 0x0003,
+	viocdwrite = 0x0004,
+	viocdlockdoor = 0x0005,
+	viocdgetinfo = 0x0006,
+	viocdcheck = 0x0007
+};
+
+/* Should probably make this a module parameter....sigh
+ */
+#define VIOCD_MAX_CD 8
+int viocd_blocksizes[VIOCD_MAX_CD];
+int viocd_mediasizes[VIOCD_MAX_CD];
+static u64 viocd_size_in_bytes[VIOCD_MAX_CD];
+
+static const struct vio_error_entry viocd_err_table[] = {
+	{0x0201, EINVAL, "Invalid Range"},
+	{0x0202, EINVAL, "Invalid Token"},
+	{0x0203, EIO, "DMA Error"},
+	{0x0204, EIO, "Use Error"},
+	{0x0205, EIO, "Release Error"},
+	{0x0206, EINVAL, "Invalid CD"},
+	{0x020C, EROFS, "Read Only Device"},
+	{0x020D, EIO, "Changed or Missing Volume (or Varied Off?)"},
+	{0x020E, EIO, "Optical System Error (Varied Off?)"},
+	{0x02FF, EIO, "Internal Error"},
+	{0x3010, EIO, "Changed Volume"},
+	{0xC100, EIO, "Optical System Error"},
+	{0x0000, 0, NULL},
+};
+
+/* This is the structure we use to exchange info between driver and interrupt
+ * handler
+ */
+struct viocd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	u16 subtypeRc;
+	int changed;
+};
+
+/* this is a lookup table for the true capabilities of a device */
+struct capability_entry {
+	char *type;
+	int capability;
+};
+
+static struct capability_entry capability_table[] = {
+	{ "6330", CDC_LOCK | CDC_DVD_RAM },
+	{ "6321", CDC_LOCK },
+	{ "632B", 0 },
+	{ NULL  , CDC_LOCK },
+};
+
+struct block_device_operations viocd_fops =
+{
+	owner:			THIS_MODULE,
+	open:			cdrom_open,
+	release:		cdrom_release,
+	ioctl:			cdrom_ioctl,
+	check_media_change:	cdrom_media_changed,
+};
+
+/* These are our internal structures for keeping track of devices
+ */
+static int viocd_numdev;
+
+struct cdrom_info {
+	char rsrcname[10];
+	char type[4];
+	char model[3];
+};
+static struct cdrom_info *viocd_unitinfo;
+static dma_addr_t viocd_unitinfo_token;
+
+struct disk_info{
+	u32 useCount;
+	u32 blocksize;
+	u32 mediasize;
+};
+static struct disk_info viocd_diskinfo[VIOCD_MAX_CD];
+
+static struct cdrom_device_info viocd_info[VIOCD_MAX_CD];
+
+static spinlock_t viocd_lock = SPIN_LOCK_UNLOCKED;
+
+#define MAX_CD_REQ 1
+static LIST_HEAD(reqlist);
+
+/* End a request
+ */
+static int viocd_end_request(struct request *req, int uptodate)
+{
+	if (end_that_request_first(req, uptodate, DEVICE_NAME))
+		return 0;
+	end_that_request_last(req);
+	return 1;
+}
+
+
+/* Get info on CD devices from OS/400
+ */
+static int get_viocd_info(void)
+{
+	HvLpEvent_Rc hvrc;
+	int i;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we;
+
+	// If we don't have a host, bail out
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return -ENODEV;
+
+	if (viocd_unitinfo == NULL)
+		viocd_unitinfo = pci_alloc_consistent(iSeries_vio_dev, sizeof(viocd_unitinfo[0]) * VIOCD_MAX_CD, &viocd_unitinfo_token);
+
+	if (viocd_unitinfo == NULL) {
+		printk(KERN_WARNING_VIO "viocd_unitinfo couldn't be allocated\n");
+		return -ENOMEM;
+	}
+
+	memset(viocd_unitinfo, 0x00, sizeof(struct cdrom_info) * VIOCD_MAX_CD);
+
+	we.sem = &Semaphore;
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+				 HvLpEvent_Type_VirtualIo,
+				 viomajorsubtype_cdio | viocdgetinfo,
+				 HvLpEvent_AckInd_DoAck,
+				 HvLpEvent_AckType_ImmediateAck,
+				 viopath_sourceinst(viopath_hostLp),
+				 viopath_targetinst(viopath_hostLp),
+				 (u64) (unsigned long) &we,
+				 VIOVERSION << 16,
+				 viocd_unitinfo_token,
+				 0,
+				 sizeof(struct cdrom_info) * VIOCD_MAX_CD,
+				 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO
+		       "cdrom error sending event. rc %d\n", (int) hvrc);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (we.rc) {
+		const struct vio_error_entry *err =
+		    vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO
+		       "bad rc %d:0x%04X on getinfo: %s\n", we.rc,
+		       we.subtypeRc, err->msg);
+		return -err->errno;
+	}
+
+
+	for (i = 0; (i < VIOCD_MAX_CD) && (viocd_unitinfo[i].rsrcname[0]);
+	     i++) {
+		viocd_numdev++;
+	}
+
+	return 0;
+}
+
+/* Open a device
+ */
+static int viocd_open(struct cdrom_device_info *cdi, int purpose)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int device_no = MINOR(cdi->dev);
+	HvLpEvent_Rc hvrc;
+	struct viocd_waitevent we;
+	struct disk_info *diskinfo = &viocd_diskinfo[device_no];
+
+	// If we don't have a host, bail out
+	if (viopath_hostLp == HvLpIndexInvalid || device_no >= viocd_numdev)
+		return -ENODEV;
+
+	we.sem = &Semaphore;
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdopen,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (we.rc) {
+		const struct vio_error_entry *err = vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on open: %s\n", we.rc, we.subtypeRc, err->msg);
+		return -err->errno;
+	}
+
+	if (diskinfo->useCount == 0) {
+		if(diskinfo->blocksize > 0) {
+			viocd_blocksizes[device_no] = diskinfo->blocksize;
+			viocd_size_in_bytes[device_no] = (u64)diskinfo->blocksize * (u64)diskinfo->mediasize;
+			viocd_mediasizes[device_no] = viocd_size_in_bytes[device_no] / 1024;
+		} else {
+			viocd_size_in_bytes[device_no] = 0xFFFFFFFFFFFFFFFFUL;
+		}
+	}
+	return 0;
+}
+
+/* Release a device
+ */
+static void viocd_release(struct cdrom_device_info *cdi)
+{
+	int device_no = MINOR(cdi->dev);
+	HvLpEvent_Rc hvrc;
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid
+	    || device_no >= viocd_numdev)
+		return;
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdclose,
+			     HvLpEvent_AckInd_NoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     0,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return;
+	}
+}
+
+/* Send a read or write request to OS/400
+ */
+static int send_request(struct request *req)
+{
+	HvLpEvent_Rc hvrc;
+	dma_addr_t dmaaddr;
+	int device_no = DEVICE_NR(req->rq_dev);
+	u64 start = req->sector * 512,
+	    len = req->current_nr_sectors * 512;
+	char reading = (req->cmd == READ || req->cmd == READA);
+	char pci_dir = reading ? PCI_DMA_FROMDEVICE : PCI_DMA_TODEVICE;
+	u16 command = reading ? viocdread : viocdwrite;
+
+
+	if(start + len > viocd_size_in_bytes[device_no]) {
+		printk(KERN_WARNING_VIO "viocd%d; access position %lx, past size %lx\n",
+		       device_no, start + len, viocd_size_in_bytes[device_no]);
+		return -1;
+	}
+
+	if(!req->bh) {
+		dmaaddr = pci_map_single(iSeries_vio_dev, req->buffer, len, pci_dir);
+	}
+	else {
+		enum { MAX_SCATTER = 30 };
+		struct scatterlist sg[MAX_SCATTER];
+		struct buffer_head *bh;
+		int nbh = 0, datalen = 0, ntce;
+
+		memset(&sg, 0x00, sizeof(sg));
+		for(bh = req->bh; bh && nbh < MAX_SCATTER && datalen <= (15 * PAGE_SIZE); bh = bh->b_reqnext) {
+			sg[nbh].address = bh->b_data;
+			sg[nbh].length = bh->b_size;
+			datalen += bh->b_size;
+			++nbh;
+		}
+		ntce = pci_map_sg(iSeries_vio_dev, sg, nbh, pci_dir);
+		if(!ntce) {
+			printk(KERN_WARNING_VIO "error allocating bh tces\n");
+			return -1;
+		}
+		else {
+			dmaaddr = sg[0].dma_address;
+			len = sg[0].dma_length;
+			if(ntce > 1) {
+				printk("viocd: unmapping %d extra sg entries\n", ntce - 1);
+				pci_unmap_sg(iSeries_vio_dev, &sg[1], ntce - 1, pci_dir);
+			}
+		}
+	}
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | command,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) req->buffer,
+	                     VIOVERSION << 16,
+			     ((u64) device_no << 48) | dmaaddr,
+			     start, len, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "hv error on op %d\n", (int) hvrc);
+		return -1;
+	}
+
+	return 0;
+}
+
+
+/* Do a request
+ */
+static int rwreq;
+static void do_viocd_request(request_queue_t * q)
+{
+	for (;;) {
+		struct request *req;
+		char err_str[80];
+		int device_no;
+
+
+		INIT_REQUEST;
+		if (rwreq >= MAX_CD_REQ) {
+			return;
+		}
+
+		device_no = CURRENT_DEV;
+
+		/* remove the current request from the queue */
+		req = CURRENT;
+		blkdev_dequeue_request(req);
+
+		/* check for any kind of error */
+		if (device_no > viocd_numdev)
+			sprintf(err_str, "Invalid device number %d", device_no);
+		else if (send_request(req) < 0)
+			strcpy(err_str, "unable to send message to OS/400!");
+		else
+			err_str[0] = '\0';
+
+		/* if we had any sort of error, log it and cancel the request */
+		if (*err_str) {
+			printk(KERN_WARNING_VIO "%s\n", err_str);
+			viocd_end_request(req, 0);
+		} else {
+			spin_lock(&viocd_lock);
+			list_add_tail(&req->queue, &reqlist);
+			++rwreq;
+			spin_unlock(&viocd_lock);
+		}
+	}
+}
+
+/* Check if the CD changed
+ */
+static int viocd_media_changed(struct cdrom_device_info *cdi, int disc_nr)
+{
+	struct viocd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = MINOR(cdi->dev);
+
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400                                         */
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdcheck,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc) {
+		const struct vio_error_entry *err = vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on check_change: %s; Assuming no change\n", we.rc, we.subtypeRc, err->msg);
+		return 0;
+	}
+
+	return we.changed;
+}
+
+static int viocd_lock_door(struct cdrom_device_info *cdi, int locking)
+{
+	HvLpEvent_Rc hvrc;
+	u64 device_no = MINOR(cdi->dev);
+	/* NOTE: flags is 1 or 0 so it won't overwrite the device_no             */
+	u64 flags = !!locking;
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we = { sem:&Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the lockdoor event to OS/400                                     */
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdlockdoor,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     (device_no << 48) | (flags << 32),
+			     0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc != 0) {
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/* This routine handles incoming CD LP events
+ */
+static void vioHandleCDEvent(struct HvLpEvent *event)
+{
+	struct viocdlpevent *bevent = (struct viocdlpevent *) event;
+	struct viocd_waitevent *pwe;
+
+	if (event == NULL) {
+		/* Notification that a partition went away! */
+		return;
+	}
+	/* First, we should NEVER get an int here...only acks */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO "Yikes! got an int in viocd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+	case viocdopen:
+		viocd_diskinfo[bevent->mDisk].blocksize = bevent->mBlockSize;
+		viocd_diskinfo[bevent->mDisk].mediasize = bevent->mMediaSize;
+		/* FALLTHROUGH !! */
+	case viocdgetinfo:
+	case viocdlockdoor:
+		pwe = (struct viocd_waitevent *) (unsigned long) event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		up(pwe->sem);
+		break;
+
+	case viocdclose:
+		break;
+
+	case viocdwrite:
+	case viocdread:
+		{
+			unsigned long flags;
+			char success;
+			int nsect;
+			int reading = ((event->xSubtype & VIOMINOR_SUBTYPE_MASK) == viocdread);
+			struct request *req;
+
+			/* Since this is running in interrupt mode, we need to make sure we're not
+			 * stepping on any global I/O operations
+			 */
+			spin_lock_irqsave(&io_request_lock, flags);
+			req = blkdev_entry_to_request(reqlist.next);
+
+			pci_unmap_single(iSeries_vio_dev,
+					 bevent->mToken,
+					 bevent->mLen,
+					 reading ? PCI_DMA_FROMDEVICE :
+					 PCI_DMA_TODEVICE);
+
+			/* find the event to which this is a response */
+			while ((&req->queue != &reqlist) &&
+			       ((u64) (unsigned long) req->buffer != bevent->event.xCorrelationToken))
+				req = blkdev_entry_to_request(req->queue.next);
+
+			/* if the event was not there, then what are we responding to?? */
+			if (&req->queue == &reqlist) {
+				printk(KERN_WARNING_VIO "Yikes! we never enqueued this guy!\n");
+				spin_unlock_irqrestore(&io_request_lock, flags);
+				break;
+			}
+
+			/* we don't need to keep it around anymore... */
+			spin_lock(&viocd_lock);
+			list_del(&req->queue);
+			--rwreq;
+			spin_unlock(&viocd_lock);
+
+			success = event->xRc == HvLpEvent_Rc_Good;
+			nsect = bevent->mLen >> 9;
+
+			if (!success) {
+				const struct vio_error_entry *err =
+				    vio_lookup_rc(viocd_err_table,
+						  bevent->mSubTypeRc);
+				printk(KERN_WARNING_VIO
+				       "request %p failed with rc %d:0x%04X: %s\n",
+				       req->buffer, event->xRc,
+				       bevent->mSubTypeRc, err->msg);
+			}
+			while ((nsect > 0) && (req->bh)) {
+				nsect -= req->current_nr_sectors;
+				viocd_end_request(req, success);
+			}
+			/* we weren't done yet */
+			if (req->bh) {
+				if (send_request(req) < 0) {
+					printk(KERN_WARNING_VIO "couldn't re-submit req %p\n", req->buffer);
+					viocd_end_request(req, 0);
+				} else {
+					spin_lock(&viocd_lock);
+					list_add_tail(&req->queue, &reqlist);
+					++rwreq;
+					spin_unlock(&viocd_lock);
+				}
+			}
+
+			/* restart handling of incoming requests */
+			do_viocd_request(NULL);
+			spin_unlock_irqrestore(&io_request_lock, flags);
+			break;
+		}
+	case viocdcheck:
+		pwe =
+		    (struct viocd_waitevent *) (unsigned long) event->
+		    xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		pwe->changed = bevent->mFlags;
+		up(pwe->sem);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO
+		       "message with invalid subtype %0x04X!\n",
+		       event->xSubtype & VIOMINOR_SUBTYPE_MASK);
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+/* Our file operations table
+ */
+static struct cdrom_device_ops viocd_dops = {
+	open:viocd_open,
+	release:viocd_release,
+	media_changed:viocd_media_changed,
+	lock_door:viocd_lock_door,
+	capability:CDC_CLOSE_TRAY | CDC_OPEN_TRAY | CDC_LOCK | CDC_SELECT_SPEED | CDC_SELECT_DISC | CDC_MULTI_SESSION | CDC_MCN | CDC_MEDIA_CHANGED | CDC_PLAY_AUDIO | CDC_RESET | CDC_IOCTLS | CDC_DRIVE_STATUS | CDC_GENERIC_PACKET | CDC_CD_R | CDC_CD_RW | CDC_DVD | CDC_DVD_R | CDC_DVD_RAM
+};
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+
+	for (i = 0; i < viocd_numdev; i++) {
+		len +=
+		    sprintf(buf + len,
+			    "viocd device %d is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			    i, viocd_unitinfo[i].rsrcname,
+			    viocd_unitinfo[i].type,
+			    viocd_unitinfo[i].model);
+	}
+	*eof = 1;
+	return len;
+}
+
+
+/* setup our proc file system entries
+ */
+void viocd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("viocd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->owner = THIS_MODULE;
+}
+
+/* clean up our proc file system entries
+ */
+void viocd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viocd", iSeries_proc);
+}
+
+static int find_capability(const char *type)
+{
+	struct capability_entry *entry;
+	for(entry = capability_table; entry->type; ++entry)
+		if(!strncmp(entry->type, type, 4))
+			break;
+	return entry->capability;
+}
+
+/* Initialize the whole device driver.  Handle module and non-module
+ * versions
+ */
+int __init viocd_init(void)
+{
+	int i, rc;
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return -ENODEV;
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+	if (rc) {
+		printk(KERN_WARNING_VIO "error opening path to host partition %d\n",
+			   viopath_hostLp);
+		return rc;
+	}
+
+	/* Initialize our request handler
+	 */
+	rwreq = 0;
+	vio_setHandler(viomajorsubtype_cdio, vioHandleCDEvent);
+
+	memset(&viocd_diskinfo, 0x00, sizeof(viocd_diskinfo));
+
+	if((rc = get_viocd_info()) < 0) {
+		viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+		vio_clearHandler(viomajorsubtype_cdio);
+		return rc;
+	}
+
+	if (viocd_numdev == 0) {
+		vio_clearHandler(viomajorsubtype_cdio);
+		viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+		return 0;
+	}
+
+	printk(KERN_INFO_VIO
+	       "%s: iSeries Virtual CD vers %s, major %d, max disks %d, hosting partition %d\n",
+	       DEVICE_NAME, VIOCD_VERS, MAJOR_NR, VIOCD_MAX_CD, viopath_hostLp);
+
+	if (devfs_register_blkdev(MAJOR_NR, "viocd", &viocd_fops) != 0) {
+		printk(KERN_WARNING_VIO "Unable to get major %d for viocd CD-ROM\n", MAJOR_NR);
+		return -EIO;
+	}
+
+	blk_size[MAJOR_NR] = viocd_mediasizes;
+	blksize_size[MAJOR_NR] = hardsect_size[MAJOR_NR] = viocd_blocksizes;
+	blk_init_queue(BLK_DEFAULT_QUEUE(MAJOR_NR), DEVICE_REQUEST);
+	read_ahead[MAJOR_NR] = 4;
+
+	memset(&viocd_info, 0x00, sizeof(viocd_info));
+	for (i = 0; i < viocd_numdev; i++) {
+		viocd_info[i].dev = MKDEV(MAJOR_NR, i);
+		viocd_info[i].ops = &viocd_dops;
+		viocd_info[i].speed = 4;
+		viocd_info[i].capacity = 1;
+		viocd_info[i].mask = ~find_capability(viocd_unitinfo[i].type);
+		sprintf(viocd_info[i].name, VIOCD_DEVICE, VIOCD_DEVICE_OFFSET + i);
+		if (register_cdrom(&viocd_info[i]) != 0) {
+			printk(KERN_WARNING_VIO "Cannot register viocd CD-ROM %s!\n", viocd_info[i].name);
+		} else {
+			printk(KERN_INFO_VIO 
+			       "cd %s is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			       viocd_info[i].name,
+			       viocd_unitinfo[i].rsrcname,
+			       viocd_unitinfo[i].type,
+			       viocd_unitinfo[i].model);
+		}
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viocd_proc_init);
+
+	return 0;
+}
+
+void __exit viocd_exit(void)
+{
+	int i;
+	for (i = 0; i < viocd_numdev; i++) {
+		if (unregister_cdrom(&viocd_info[i]) != 0) {
+			printk(KERN_WARNING_VIO "Cannot unregister viocd CD-ROM %s!\n", viocd_info[i].name);
+		}
+	}
+	if ((devfs_unregister_blkdev(MAJOR_NR, "viocd") == -EINVAL)) {
+		printk(KERN_WARNING_VIO "can't unregister viocd\n");
+		return;
+	}
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(MAJOR_NR));
+	if (viocd_unitinfo)
+		pci_free_consistent(iSeries_vio_dev, sizeof(viocd_unitinfo[0]) * VIOCD_MAX_CD, viocd_unitinfo, viocd_unitinfo_token);
+
+	blk_size[MAJOR_NR] = blksize_size[MAJOR_NR] = hardsect_size[MAJOR_NR] = NULL;
+
+	iSeries_proc_callback(&viocd_proc_delete);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+	vio_clearHandler(viomajorsubtype_cdio);
+}
+
+module_init(viocd_init);
+module_exit(viocd_exit);
+MODULE_LICENSE("GPL");
diff -urNp linux-341/drivers/iseries/viocons.c linux-342/drivers/iseries/viocons.c
--- linux-341/drivers/iseries/viocons.c
+++ linux-342/drivers/iseries/viocons.c
@@ -0,0 +1,1638 @@
+/*
+ *  drivers/char/viocons.c
+ *
+ *  iSeries Virtual Terminal
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000, 2001, 2002, 2003 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/console.h>
+#include <linux/module.h>
+#include <asm/uaccess.h>
+#include <linux/init.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <asm/ioctls.h>
+#include <linux/kd.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/sysrq.h>
+
+#include "vio.h"
+
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvCallEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvCall.h>
+#include <asm/iSeries/iSeries_proc.h>
+
+/* Check that the tty_driver_data actually points to our stuff
+ */
+#define VIOTTY_PARANOIA_CHECK 1
+#define VIOTTY_MAGIC (0x0DCB)
+
+static int debug;
+
+static DECLARE_WAIT_QUEUE_HEAD(viocons_wait_queue);
+
+#define VTTY_PORTS 10
+#define VIOTTY_SERIAL_START 65
+
+static u64 sndMsgSeq[VTTY_PORTS];
+static u64 sndMsgAck[VTTY_PORTS];
+
+static spinlock_t consolelock = SPIN_LOCK_UNLOCKED;
+static spinlock_t consoleloglock = SPIN_LOCK_UNLOCKED;
+
+/* This is a place where we handle the distribution of memory
+ * for copy_from_user() calls.  We use VIO_MAX_SUBTYPES because it
+ * seems as good a number as any.  The buffer_available array is to
+ * help us determine which buffer to use.
+ */
+static void *viocons_cfu_buffer[VIO_MAX_SUBTYPES];
+static atomic_t viocons_cfu_buffer_available[VIO_MAX_SUBTYPES];
+
+#ifdef CONFIG_MAGIC_SYSRQ
+static int vio_sysrq_pressed;
+extern struct sysrq_ctls_struct sysrq_ctls;
+#endif
+
+/* THe structure of the events that flow between us and OS/400.  You can't
+ * mess with this unless the OS/400 side changes too
+ */
+struct viocharlpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u8 virtualDevice;
+	u8 immediateDataLen;
+	u8 immediateData[VIOCHAR_MAX_DATA];
+};
+
+#define viochar_window (10)
+#define viochar_highwatermark (3)
+
+enum viocharsubtype {
+	viocharopen = 0x0001,
+	viocharclose = 0x0002,
+	viochardata = 0x0003,
+	viocharack = 0x0004,
+	viocharconfig = 0x0005
+};
+
+enum viochar_rc {
+	viochar_rc_ebusy = 1
+};
+
+/* When we get writes faster than we can send it to the partition,
+ * buffer the data here.  There is one set of buffers for each virtual
+ * port.
+ * Note that bufferUsed is a bit map of used buffers.
+ * It had better have enough bits to hold NUM_BUF
+ * the bitops assume it is a multiple of unsigned long
+ */
+#define NUM_BUF (8)
+#define OVERFLOW_SIZE VIOCHAR_MAX_DATA
+
+static struct overflowBuffers {
+	unsigned long bufferUsed;
+	u8 *buffer[NUM_BUF];
+	int bufferBytes[NUM_BUF];
+	int curbuf;
+	int bufferOverflow;
+	int overflowMessage;
+} overflow[VTTY_PORTS];
+
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp);
+
+static int viocons_init_cfu_buffer(void);
+static void *viocons_get_cfu_buffer(void);
+
+static struct tty_driver viotty_driver;
+static struct tty_driver viottyS_driver;
+static int viotty_refcount;
+
+static struct tty_struct *viotty_table[VTTY_PORTS];
+static struct tty_struct *viottyS_table[VTTY_PORTS];
+static struct termios *viotty_termios[VTTY_PORTS];
+static struct termios *viottyS_termios[VTTY_PORTS];
+static struct termios *viotty_termios_locked[VTTY_PORTS];
+static struct termios *viottyS_termios_locked[VTTY_PORTS];
+
+char viocons_hvlog_buffer[256];
+
+void hvlog(char *fmt, ...)
+{
+	int i;
+	unsigned long flags;
+	va_list args;
+
+	spin_lock_irqsave(&consoleloglock, flags);
+	va_start(args, fmt);
+	i = vsprintf(viocons_hvlog_buffer, fmt, args);
+	va_end(args);
+	HvCall_writeLogBuffer(viocons_hvlog_buffer, i);
+	HvCall_writeLogBuffer("\r", 1);
+	spin_unlock_irqrestore(&consoleloglock, flags);
+
+}
+
+void hvlogOutput( const char *buf, int count )
+{
+	unsigned long flags;
+	int begin;
+	int index;
+	char cr;
+
+	cr = '\r';
+	begin = 0;
+	spin_lock_irqsave(&consoleloglock, flags);
+	for( index = 0; index < count; ++index ) {
+		if( buf[index] == 0x0a ) {
+			/* Start right after the last 0x0a or at the zeroth
+			 * array position and output the number of characters
+			 * including the newline.
+			 */
+			HvCall_writeLogBuffer(&buf[begin], index-begin+1);
+			begin = index+1;
+			HvCall_writeLogBuffer(&cr, 1);
+		}
+	}
+	if(index-begin > 0) {
+		HvCall_writeLogBuffer(&buf[begin], index-begin);
+	}
+	index = 0;
+	begin = 0;
+
+	spin_unlock_irqrestore(&consoleloglock, flags);
+}
+
+
+/* Our port information.  We store a pointer to one entry in the
+ * tty_driver_data
+ */
+static struct port_info_tag {
+	int magic;
+	struct tty_struct *tty;
+	HvLpIndex lp;
+	u8 vcons;
+	u8 port;
+} port_info[VTTY_PORTS];
+
+/* Make sure we're pointing to a valid port_info structure.  Shamelessly
+ * plagerized from serial.c
+ */
+static inline int viotty_paranoia_check(struct port_info_tag *pi,
+					kdev_t device, const char *routine)
+{
+#ifdef VIOTTY_PARANOIA_CHECK
+	static const char *badmagic =
+	    "\n\rWarning: bad magic number for port_info struct (%s) in %s.";
+	static const char *badinfo =
+	    "\n\rWarning: null port_info for (%s) in %s.";
+
+	if (!pi) {
+		hvlog(badinfo, kdevname(device), routine);
+		return 1;
+	}
+	if (pi->magic != VIOTTY_MAGIC) {
+		hvlog(badmagic, kdevname(device), routine);
+		return 1;
+	}
+#endif
+	return 0;
+}
+
+/*
+ * Handle reads from the proc file system.  Right now we just dump the
+ * state of the first TTY
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	struct tty_struct *tty = viotty_table[0];
+	struct termios *termios;
+	if (tty == NULL) {
+		len += sprintf(buf + len, "no tty\n");
+		*eof = 1;
+		return len;
+	}
+
+	len +=
+	    sprintf(buf + len,
+		    "tty info: COOK_OUT %ld COOK_IN %ld, NO_WRITE_SPLIT %ld\n",
+		    tty->flags & TTY_HW_COOK_OUT,
+		    tty->flags & TTY_HW_COOK_IN,
+		    tty->flags & TTY_NO_WRITE_SPLIT);
+
+	termios = tty->termios;
+	if (termios == NULL) {
+		len += sprintf(buf + len, "no termios\n");
+		*eof = 1;
+		return len;
+	}
+	len += sprintf(buf + len, "INTR_CHAR     %2.2x\n", INTR_CHAR(tty));
+	len += sprintf(buf + len, "QUIT_CHAR     %2.2x\n", QUIT_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "ERASE_CHAR    %2.2x\n", ERASE_CHAR(tty));
+	len += sprintf(buf + len, "KILL_CHAR     %2.2x\n", KILL_CHAR(tty));
+	len += sprintf(buf + len, "EOF_CHAR      %2.2x\n", EOF_CHAR(tty));
+	len += sprintf(buf + len, "TIME_CHAR     %2.2x\n", TIME_CHAR(tty));
+	len += sprintf(buf + len, "MIN_CHAR      %2.2x\n", MIN_CHAR(tty));
+	len += sprintf(buf + len, "SWTC_CHAR     %2.2x\n", SWTC_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "START_CHAR    %2.2x\n", START_CHAR(tty));
+	len += sprintf(buf + len, "STOP_CHAR     %2.2x\n", STOP_CHAR(tty));
+	len += sprintf(buf + len, "SUSP_CHAR     %2.2x\n", SUSP_CHAR(tty));
+	len += sprintf(buf + len, "EOL_CHAR      %2.2x\n", EOL_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "REPRINT_CHAR  %2.2x\n", REPRINT_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "DISCARD_CHAR  %2.2x\n", DISCARD_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "WERASE_CHAR   %2.2x\n", WERASE_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "LNEXT_CHAR    %2.2x\n", LNEXT_CHAR(tty));
+	len += sprintf(buf + len, "EOL2_CHAR     %2.2x\n", EOL2_CHAR(tty));
+
+	len += sprintf(buf + len, "I_IGNBRK      %4.4x\n", I_IGNBRK(tty));
+	len += sprintf(buf + len, "I_BRKINT      %4.4x\n", I_BRKINT(tty));
+	len += sprintf(buf + len, "I_IGNPAR      %4.4x\n", I_IGNPAR(tty));
+	len += sprintf(buf + len, "I_PARMRK      %4.4x\n", I_PARMRK(tty));
+	len += sprintf(buf + len, "I_INPCK       %4.4x\n", I_INPCK(tty));
+	len += sprintf(buf + len, "I_ISTRIP      %4.4x\n", I_ISTRIP(tty));
+	len += sprintf(buf + len, "I_INLCR       %4.4x\n", I_INLCR(tty));
+	len += sprintf(buf + len, "I_IGNCR       %4.4x\n", I_IGNCR(tty));
+	len += sprintf(buf + len, "I_ICRNL       %4.4x\n", I_ICRNL(tty));
+	len += sprintf(buf + len, "I_IUCLC       %4.4x\n", I_IUCLC(tty));
+	len += sprintf(buf + len, "I_IXON        %4.4x\n", I_IXON(tty));
+	len += sprintf(buf + len, "I_IXANY       %4.4x\n", I_IXANY(tty));
+	len += sprintf(buf + len, "I_IXOFF       %4.4x\n", I_IXOFF(tty));
+	len += sprintf(buf + len, "I_IMAXBEL     %4.4x\n", I_IMAXBEL(tty));
+
+	len += sprintf(buf + len, "O_OPOST       %4.4x\n", O_OPOST(tty));
+	len += sprintf(buf + len, "O_OLCUC       %4.4x\n", O_OLCUC(tty));
+	len += sprintf(buf + len, "O_ONLCR       %4.4x\n", O_ONLCR(tty));
+	len += sprintf(buf + len, "O_OCRNL       %4.4x\n", O_OCRNL(tty));
+	len += sprintf(buf + len, "O_ONOCR       %4.4x\n", O_ONOCR(tty));
+	len += sprintf(buf + len, "O_ONLRET      %4.4x\n", O_ONLRET(tty));
+	len += sprintf(buf + len, "O_OFILL       %4.4x\n", O_OFILL(tty));
+	len += sprintf(buf + len, "O_OFDEL       %4.4x\n", O_OFDEL(tty));
+	len += sprintf(buf + len, "O_NLDLY       %4.4x\n", O_NLDLY(tty));
+	len += sprintf(buf + len, "O_CRDLY       %4.4x\n", O_CRDLY(tty));
+	len += sprintf(buf + len, "O_TABDLY      %4.4x\n", O_TABDLY(tty));
+	len += sprintf(buf + len, "O_BSDLY       %4.4x\n", O_BSDLY(tty));
+	len += sprintf(buf + len, "O_VTDLY       %4.4x\n", O_VTDLY(tty));
+	len += sprintf(buf + len, "O_FFDLY       %4.4x\n", O_FFDLY(tty));
+
+	len += sprintf(buf + len, "C_BAUD        %4.4x\n", C_BAUD(tty));
+	len += sprintf(buf + len, "C_CSIZE       %4.4x\n", C_CSIZE(tty));
+	len += sprintf(buf + len, "C_CSTOPB      %4.4x\n", C_CSTOPB(tty));
+	len += sprintf(buf + len, "C_CREAD       %4.4x\n", C_CREAD(tty));
+	len += sprintf(buf + len, "C_PARENB      %4.4x\n", C_PARENB(tty));
+	len += sprintf(buf + len, "C_PARODD      %4.4x\n", C_PARODD(tty));
+	len += sprintf(buf + len, "C_HUPCL       %4.4x\n", C_HUPCL(tty));
+	len += sprintf(buf + len, "C_CLOCAL      %4.4x\n", C_CLOCAL(tty));
+	len += sprintf(buf + len, "C_CRTSCTS     %4.4x\n", C_CRTSCTS(tty));
+
+	len += sprintf(buf + len, "L_ISIG        %4.4x\n", L_ISIG(tty));
+	len += sprintf(buf + len, "L_ICANON      %4.4x\n", L_ICANON(tty));
+	len += sprintf(buf + len, "L_XCASE       %4.4x\n", L_XCASE(tty));
+	len += sprintf(buf + len, "L_ECHO        %4.4x\n", L_ECHO(tty));
+	len += sprintf(buf + len, "L_ECHOE       %4.4x\n", L_ECHOE(tty));
+	len += sprintf(buf + len, "L_ECHOK       %4.4x\n", L_ECHOK(tty));
+	len += sprintf(buf + len, "L_ECHONL      %4.4x\n", L_ECHONL(tty));
+	len += sprintf(buf + len, "L_NOFLSH      %4.4x\n", L_NOFLSH(tty));
+	len += sprintf(buf + len, "L_TOSTOP      %4.4x\n", L_TOSTOP(tty));
+	len += sprintf(buf + len, "L_ECHOCTL     %4.4x\n", L_ECHOCTL(tty));
+	len += sprintf(buf + len, "L_ECHOPRT     %4.4x\n", L_ECHOPRT(tty));
+	len += sprintf(buf + len, "L_ECHOKE      %4.4x\n", L_ECHOKE(tty));
+	len += sprintf(buf + len, "L_FLUSHO      %4.4x\n", L_FLUSHO(tty));
+	len += sprintf(buf + len, "L_PENDIN      %4.4x\n", L_PENDIN(tty));
+	len += sprintf(buf + len, "L_IEXTEN      %4.4x\n", L_IEXTEN(tty));
+
+	*eof = 1;
+	return len;
+}
+
+/*
+ * Handle writes to our proc file system.  Right now just turns on and off
+ * our debug flag
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	if (count) {
+		if (buffer[0] == '1') {
+			printk(KERN_INFO_VIO "viocons: debugging on\n");
+			debug = 1;
+		} else {
+			printk(KERN_INFO_VIO "viocons: debugging off\n");
+			debug = 0;
+		}
+	}
+	return count;
+}
+
+/*
+ * setup our proc file system entries
+ */
+void viocons_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viocons", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+	ent->owner = THIS_MODULE;
+}
+
+/*
+ * clean up our proc file system entries
+ */
+void viocons_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viocons", iSeries_proc);
+}
+
+/*
+ * This function should ONLY be called once from viocons_init2
+ */
+static int viocons_init_cfu_buffer( )
+{
+	int i;
+
+	if (viocons_cfu_buffer[0] == NULL) {
+		if (VIO_MAX_SUBTYPES <= 16) {
+			viocons_cfu_buffer[0] = (void *) get_free_page(GFP_KERNEL);
+			if (viocons_cfu_buffer[0] == NULL) {
+				hvlog("\n\rviocons: get_free_page() for cfu_buffer FAILED.");
+				return -ENOMEM;
+			} else {
+				/* We can fit sixteen 256 byte characters in each page
+				 * of memory (4096).  This routine aligns the boundaries
+				 * of the 256 byte cfu buffers incrementally along the space
+				 * of the page we allocated earlier.  Start at index 1 because
+				 * we've already done index 0 when we fetched the free page.
+				 */
+				for(i=1;i<VIO_MAX_SUBTYPES; i++) {
+					viocons_cfu_buffer[i] = viocons_cfu_buffer[i-1] + 256;
+					atomic_set(&viocons_cfu_buffer_available[i], 1);
+				}
+			}
+		} else {
+			hvlog("\n\rviocons: VIO_MAX_SUBTYPES > 16. Need more space for cfu buffer.");
+			return -ENOMEM;
+		}
+	}
+	return 0;
+}
+
+static void *viocons_get_cfu_buffer()
+{
+	int i;
+
+	/* Grab the first available buffer.  It doesn't matter if we
+	 * are interrupted during this array traversal as long as we
+	 * get an available space.
+	 */
+	for(i = 0; i < VIO_MAX_SUBTYPES; i++) {
+		if( atomic_dec_if_positive(&viocons_cfu_buffer_available[i]) == 0 ) {
+			return viocons_cfu_buffer[i];
+		}
+	}
+	hvlog("\n\rviocons: viocons_get_cfu_buffer : no free buffers found");
+	return NULL;
+}
+
+static void viocons_free_cfu_buffer( void *buffer )
+{
+	int i;
+
+	/* Cycle through the cfu_buffer array and see which position
+	 * in the array matches the buffer pointer parameter.  This
+	 * is the index of the event_buffer_available array that we
+	 * want to reset to available.
+	 */
+	for(i = 0; i < VIO_MAX_SUBTYPES; i++) {
+		if( viocons_cfu_buffer[i] == buffer ) {
+			if (atomic_read(&viocons_cfu_buffer_available[i]) != 0) {
+				hvlog("\n\rviocons: WARNING : returning unallocated cfu buffer.");
+				return;
+			}
+			atomic_set(&viocons_cfu_buffer_available[i], 1);
+			return;
+		}
+	}
+	hvlog("\n\rviocons: viocons_free_cfu_buffer : buffer pointer not found in list.");
+}
+
+/*
+ * Add data to our pending-send buffers.
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+static int bufferAdd(u8 port, char *buf, size_t len)
+{
+	size_t bleft;
+	size_t curlen;
+	char *curbuf;
+	int nextbuf;
+	struct overflowBuffers *pov = &overflow[port];
+
+	curbuf = buf;
+	bleft = len;
+
+	while (bleft > 0) {
+		/* If there is no space left in the current buffer, we have
+		 * filled everything up, so return.  If we filled the previous
+		 * buffer we would already have moved to the next one.
+		 */
+		if (pov->bufferBytes[pov->curbuf] == OVERFLOW_SIZE) {
+			hvlog ("\n\rviocons: No overflow buffer available for memcpy().\n");
+			pov->bufferOverflow++;
+			pov->overflowMessage = 1;
+			break;
+		}
+
+		/*
+		 * Turn on the "used" bit for this buffer.  If it's already on,
+		 * that's fine.
+		 */
+		set_bit(pov->curbuf, &pov->bufferUsed);
+
+		/*
+		 * See if this buffer has been allocated.  If not, allocate it.
+		 */
+		if (pov->buffer[pov->curbuf] == NULL) {
+			pov->buffer[pov->curbuf] = kmalloc(OVERFLOW_SIZE, GFP_ATOMIC);
+			if (pov->buffer[pov->curbuf] == NULL) {
+				hvlog("\n\rviocons: kmalloc failed allocating spaces for buffer %d.",pov->curbuf);
+				break;
+			}
+		}
+
+		/* Figure out how much we can copy into this buffer. */
+		if (bleft < (OVERFLOW_SIZE - pov->bufferBytes[pov->curbuf]))
+			curlen = bleft;
+		else
+			curlen = OVERFLOW_SIZE - pov->bufferBytes[pov->curbuf];
+
+		/* Copy the data into the buffer. */
+		memcpy(pov->buffer[pov->curbuf] +
+			pov->bufferBytes[pov->curbuf], curbuf,
+			curlen);
+
+		pov->bufferBytes[pov->curbuf] += curlen;
+		curbuf += curlen;
+		bleft -= curlen;
+
+		/*
+		 * Now see if we've filled this buffer.  If not then
+		 * we'll try to use it again later.  If we've filled it
+		 * up then we'll advance the curbuf to the next in the
+		 * circular queue.
+		 */
+		if (pov->bufferBytes[pov->curbuf] == OVERFLOW_SIZE) {
+			nextbuf = (pov->curbuf + 1) % NUM_BUF;
+			/*
+			 * Move to the next buffer if it hasn't been used yet
+			 */
+			if (test_bit(nextbuf, &pov->bufferUsed) == 0) {
+				pov->curbuf = nextbuf;
+			}
+		}
+	}
+	return len - bleft;
+}
+
+/* Send pending data
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+void sendBuffers(u8 port, HvLpIndex lp)
+{
+	HvLpEvent_Rc hvrc;
+	int nextbuf;
+	struct viocharlpevent *viochar;
+	unsigned long flags;
+	struct overflowBuffers *pov = &overflow[port];
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	viochar = (struct viocharlpevent *)
+	    vio_get_event_buffer(viomajorsubtype_chario);
+
+	/* Make sure we got a buffer */
+	if (viochar == NULL) {
+		hvlog("\n\rviocons: Can't get viochar buffer in sendBuffers().");
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	if (pov->bufferUsed == 0) {
+		hvlog("\n\rviocons: in sendbuffers(), but no buffers used.\n");
+		vio_free_event_buffer(viomajorsubtype_chario, viochar);
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	/*
+	 * curbuf points to the buffer we're filling.  We want to start sending AFTER
+	 * this one.
+	 */
+	nextbuf = (pov->curbuf + 1) % NUM_BUF;
+
+	/*
+	 * Loop until we find a buffer with the bufferUsed bit on
+	 */
+	while (test_bit(nextbuf, &pov->bufferUsed) == 0)
+		nextbuf = (nextbuf + 1) % NUM_BUF;
+
+	initDataEvent(viochar, lp);
+
+	/*
+	 * While we have buffers with data, and our send window is open, send them
+	 */
+	while ((test_bit(nextbuf, &pov->bufferUsed)) &&
+	       ((sndMsgSeq[port] - sndMsgAck[port]) < viochar_window)) {
+		viochar->immediateDataLen = pov->bufferBytes[nextbuf];
+		viochar->event.xCorrelationToken = sndMsgSeq[port]++;
+		viochar->event.xSizeMinus1 =
+			offsetof(struct viocharlpevent, immediateData) +
+				viochar->immediateDataLen;
+
+		memcpy(viochar->immediateData, pov->buffer[nextbuf],
+		       viochar->immediateDataLen);
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			/*
+			 * MUST unlock the spinlock before doing a printk
+			 */
+			vio_free_event_buffer(viomajorsubtype_chario,
+					      viochar);
+			spin_unlock_irqrestore(&consolelock, flags);
+
+			printk(KERN_WARNING_VIO
+			       "console error sending event! return code %d\n",
+			       (int) hvrc);
+			return;
+		}
+
+		/*
+		 * clear the bufferUsed bit, zero the number of bytes in this buffer,
+		 * and move to the next buffer
+		 */
+		clear_bit(nextbuf, &pov->bufferUsed);
+		pov->bufferBytes[nextbuf] = 0;
+		nextbuf = (nextbuf + 1) % NUM_BUF;
+	}
+
+	/*
+	 * If we have emptied all the buffers, start at 0 again.
+	 * this will re-use any allocated buffers
+	 */
+	if (pov->bufferUsed == 0) {
+		pov->curbuf = 0;
+
+		if (pov->overflowMessage)
+			pov->overflowMessage = 0;
+
+		if (port_info[port].tty) {
+			if ((port_info[port].tty->
+			     flags & (1 << TTY_DO_WRITE_WAKEUP))
+			    && (port_info[port].tty->ldisc.write_wakeup))
+				(port_info[port].tty->ldisc.
+				 write_wakeup) (port_info[port].tty);
+			wake_up_interruptible(&port_info[port].tty->
+					      write_wait);
+		}
+	}
+
+	vio_free_event_buffer(viomajorsubtype_chario, viochar);
+	spin_unlock_irqrestore(&consolelock, flags);
+
+}
+
+/* Our internal writer.  Gets called both from the console device and
+ * the tty device.  the tty pointer will be NULL if called from the console.
+ * Return total number of bytes "written".
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+static int internal_write(HvLpIndex lp, u8 port, const char *buf,
+			  size_t len, struct viocharlpevent *viochar)
+{
+	HvLpEvent_Rc hvrc;
+	size_t bleft;
+	size_t curlen;
+	const char *curbuf;
+	unsigned long flags;
+	int copy_needed = (viochar == NULL);
+
+	/* Writes to the hvlog of inbound data are now done prior to
+	 * calling internal_write() since internal_write() is only called in
+	 * the event that an lp event path is active, which isn't the case for
+	 * logging attempts prior to console initialization.
+	 */
+
+	/*
+	 * If there is already data queued for this port, send it prior to
+	 * attempting to send any new data.
+	 */
+	if (overflow[port].bufferUsed)
+		sendBuffers(port, lp);
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	/* If the internal_write() was passed a pointer to a
+	 * viocharlpevent then we don't need to allocate a new one
+	 * (this is the case where we are internal_writing user space
+	 * data).  If we aren't writing user space data then we need
+	 * to get an event from viopath.
+	 */
+	if (copy_needed) {
+
+		/* This one is fetched from the viopath data structure */
+		viochar = (struct viocharlpevent *)
+			vio_get_event_buffer(viomajorsubtype_chario);
+
+		/* Make sure we got a buffer */
+		if (viochar == NULL) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			hvlog("\n\rviocons: Can't get viochar buffer in internal_write().");
+			return -EAGAIN;
+		}
+
+		initDataEvent(viochar, lp);
+	}
+
+	curbuf = buf;
+	bleft = len;
+
+	while ((bleft > 0) &&
+	       (overflow[port].bufferUsed == 0) &&
+	       ((sndMsgSeq[port] - sndMsgAck[port]) < viochar_window)) {
+
+		if (bleft > VIOCHAR_MAX_DATA)
+			curlen = VIOCHAR_MAX_DATA;
+		else
+			curlen = bleft;
+
+		viochar->event.xCorrelationToken = sndMsgSeq[port]++;
+
+		if (copy_needed) {
+			memcpy(viochar->immediateData, curbuf, curlen);
+			viochar->immediateDataLen = curlen;
+		}
+
+		viochar->event.xSizeMinus1 = offsetof(struct viocharlpevent,
+			immediateData) + viochar->immediateDataLen;
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			if (copy_needed) {
+				vio_free_event_buffer(viomajorsubtype_chario,
+					viochar);
+			}
+			hvlog("viocons: error sending event! %d\n", (int) hvrc);
+			return len - bleft;
+		}
+
+		curbuf += curlen;
+		bleft -= curlen;
+	}
+
+	/*
+	 * If we couldn't send it all, buffer as much of it as we can.
+	 */
+	if (bleft > 0) {
+		bleft -= bufferAdd(port, curbuf, bleft);
+	}
+
+	/* Since we grabbed it from the viopath data structure, return it to the
+	 * data structure
+	 */
+	if (copy_needed)
+		vio_free_event_buffer(viomajorsubtype_chario, viochar);
+
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return len - bleft;
+}
+
+static int get_port_data(struct tty_struct *tty, HvLpIndex *lp, u8 *port)
+{
+	unsigned long flags;
+	struct port_info_tag *pi = NULL;
+
+	spin_lock_irqsave(&consolelock, flags);
+	if (tty) {
+		pi = (struct port_info_tag *) tty->driver_data;
+
+		if (!pi
+		    || viotty_paranoia_check(pi, tty->device,
+					     "viotty_internal_write")) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			return -ENODEV;
+		}
+
+		*lp = pi->lp;
+		*port = pi->port;
+	} else {
+		/* If this is the console device, use the lp from the first port entry
+		 */
+		*port = 0;
+		*lp = port_info[0].lp;
+	}
+	spin_unlock_irqrestore(&consolelock, flags);
+	return 0;
+}
+
+/* Initialize the common fields in a charLpEvent */
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp)
+{
+	memset(viochar, 0x00, sizeof(struct viocharlpevent));
+
+	viochar->event.xFlags.xValid = 1;
+	viochar->event.xFlags.xFunction = HvLpEvent_Function_Int;
+	viochar->event.xFlags.xAckInd = HvLpEvent_AckInd_NoAck;
+	viochar->event.xFlags.xAckType = HvLpEvent_AckType_DeferredAck;
+	viochar->event.xType = HvLpEvent_Type_VirtualIo;
+	viochar->event.xSubtype = viomajorsubtype_chario | viochardata;
+	viochar->event.xSourceLp = HvLpConfig_getLpIndex();
+	viochar->event.xTargetLp = lp;
+	viochar->event.xSizeMinus1 = sizeof(struct viocharlpevent);
+	viochar->event.xSourceInstanceId = viopath_sourceinst(lp);
+	viochar->event.xTargetInstanceId = viopath_targetinst(lp);
+}
+
+/* console device write
+ */
+static void viocons_write(struct console *co, const char *s,
+			  unsigned count)
+{
+	int index;
+	char charptr[1];
+	int begin;
+	HvLpIndex lp;
+	u8 port;
+
+	/* Check port data first because the target LP might be valid but
+	 * simply not active, in which case we want to hvlog the output.
+	 */
+	if (get_port_data(NULL, &lp, &port)) {
+		hvlog("\n\rviocons: in viocons_write unable to get port data.");
+		return;
+	}
+
+	hvlogOutput(s,count);
+
+	if(!viopath_isactive(lp)) {
+		return;
+	}
+
+	/* 
+	 * Any newline character (0x0a == '\n') found will cause a
+	 * carriage return character to be emitted as well. 
+	 */
+	begin = 0;
+	for (index = 0; index < count; index++) {
+		if (s[index] == 0x0a) {
+			/* 
+			 * Newline found. Print everything up to and 
+			 * including the newline
+			 */
+			internal_write(lp, port, &s[begin], index-begin+1, 0);
+			begin = index + 1;
+			/* Emit a carriage return as well */
+			charptr[0] = '\r';
+			internal_write(lp, port, charptr, 1, 0);
+		}
+	}
+
+	/* If any characters left to write, write them now */
+	if (index - begin > 0)
+		internal_write(lp, port, &s[begin], index - begin, 0);
+}
+
+/* Work out a the device associate with this console
+ */
+static kdev_t viocons_device(struct console *c)
+{
+	return MKDEV(TTY_MAJOR, c->index + viotty_driver.minor_start);
+}
+
+/* Do console device setup
+ */
+static int __init viocons_setup(struct console *co, char *options)
+{
+	return 0;
+}
+
+/* console device I/O methods
+ */
+static struct console viocons = {
+	name:"ttyS",
+	write:viocons_write,
+	device:viocons_device,
+	setup:viocons_setup,
+	flags:CON_PRINTBUFFER,
+};
+
+/* TTY Open method
+ */
+static int viotty_open(struct tty_struct *tty, struct file *filp)
+{
+	int port;
+	unsigned long flags;
+	MOD_INC_USE_COUNT;
+	port = MINOR(tty->device) - tty->driver.minor_start;
+
+	/* NOTE: in the event that a user space program attempts to open tty
+	 * devices 2-x this viotty_open will succeed but will return an
+	 * invalid tty device as far as this device driver is concerned.  We
+	 * allow such behavior because many installers require additional tty
+	 * devices even if we don't support them.  Further method invocations
+	 * upon these invalid tty devices will simply fail gracefully.
+	 */
+
+	if (port >= VIOTTY_SERIAL_START)
+		port -= VIOTTY_SERIAL_START;
+
+	if ((port < 0) || (port >= VTTY_PORTS)) {
+		MOD_DEC_USE_COUNT;
+		return -ENODEV;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	/*
+	 * If some other TTY is already connected here, reject the open
+	 */
+	if ((port_info[port].tty) && (port_info[port].tty != tty)) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		MOD_DEC_USE_COUNT;
+		printk(KERN_INFO_VIO
+		       "console attempt to open device twice from different ttys\n");
+		return -EBUSY;
+	}
+	tty->driver_data = &port_info[port];
+	port_info[port].tty = tty;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return 0;
+}
+
+/* TTY Close method
+ */
+static void viotty_close(struct tty_struct *tty, struct file *filp)
+{
+	unsigned long flags;
+	struct port_info_tag *pi = NULL;
+
+	spin_lock_irqsave(&consolelock, flags);
+	pi = (struct port_info_tag *) tty->driver_data;
+
+	if (!pi || viotty_paranoia_check(pi, tty->device, "viotty_close")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	if (atomic_read(&tty->count) == 1) {
+		pi->tty = NULL;
+	}
+
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	MOD_DEC_USE_COUNT;
+}
+
+/* TTY Write method
+ */
+static int viotty_write(struct tty_struct *tty, int from_user,
+			const unsigned char *buf, int count)
+{
+	struct viocharlpevent *viochar;
+	int curlen;
+	const char *curbuf = buf;
+	int ret;
+	int total = 0;
+	HvLpIndex lp;
+	u8 port;
+
+	ret = get_port_data(tty, &lp, &port);
+	if (ret) {
+		hvlog("\n\rviocons: in viotty_write: no port data.");
+		return -ENODEV;
+	}
+
+	hvlogOutput(buf,count);
+
+	/* If the path to this LP is closed, don't bother doing anything more.
+	 * just dump the data on the floor and return count.  For some reason some
+	 * user level programs will attempt to probe available tty's and they'll
+	 * attempt a viotty_write on an invalid port which maps to an invalid target
+	 * lp.  If this is the case then ignore the viotty_write call and since
+	 * the viopath isn't active to this partition return count.
+	 */
+	if (!viopath_isactive(lp)) {
+		return count;
+	}
+
+	/* If the viotty_write is invoked from user space we want to do the
+	 * copy_from_user() into an event buffer from the cfu buffer before
+	 * internal_write() is called because internal_write may need to buffer
+	 * data which will need to grab a spin_lock and we shouldn't
+	 * copy_from_user() while holding a spin_lock.  Should internal_write()
+	 * not need to buffer data then it'll just use the event we created here
+	 * rather than checking one out from vio_get_event_buffer().
+	 */
+	if (from_user) {
+
+		viochar = (struct viocharlpevent *) viocons_get_cfu_buffer();
+
+		if (viochar == NULL)
+			return -EAGAIN;
+
+		initDataEvent(viochar, lp);
+
+		while (count > 0) {
+			if (count > VIOCHAR_MAX_DATA)
+				curlen = VIOCHAR_MAX_DATA;
+			else
+				curlen = count;
+			viochar->immediateDataLen = curlen;
+
+			ret = copy_from_user(viochar->immediateData, curbuf, curlen);
+			if (ret)
+				break;
+
+			ret = internal_write(lp, port, viochar->immediateData,
+					viochar->immediateDataLen, viochar);
+			total += ret;
+			if (ret != curlen)
+				break;
+			count -= curlen;
+			curbuf += curlen;
+		}
+		viocons_free_cfu_buffer(viochar);
+	}
+	else {
+		total = internal_write(lp, port, buf, count, NULL);
+	}
+	return total;
+}
+
+/* TTY put_char method */
+static void viotty_put_char(struct tty_struct *tty, unsigned char ch)
+{
+	HvLpIndex lp;
+	u8 port;
+
+	if (get_port_data(tty, &lp, &port)) {
+		return;
+	}
+
+	/* This will append \r as well if the char is 0x0A ('\n') */
+	if (port==0) {
+		hvlogOutput(&ch,1);
+	}
+
+	if(!viopath_isactive(lp)) {
+		return;
+	}
+
+	internal_write(lp, port, &ch, 1, 0);
+}
+
+/* TTY write_room method */
+static int viotty_write_room(struct tty_struct *tty)
+{
+	int i;
+	int room = 0;
+	struct port_info_tag *pi = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&consolelock, flags);
+	pi = (struct port_info_tag *) tty->driver_data;
+
+	if (!pi
+	    || viotty_paranoia_check(pi, tty->device,
+				     "viotty_sendbuffers")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return 0;
+	}
+
+	/*
+	 * If no buffers are used, return the max size.
+	 */
+	if (overflow[pi->port].bufferUsed == 0) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return VIOCHAR_MAX_DATA * NUM_BUF;
+	}
+
+	/* We retain the spinlock because we want to get an accurate
+	 * count and it can change on us between each operation if we
+	 * don't hold the spinlock.
+	 */
+	for (i = 0; ((i < NUM_BUF) && (room < VIOCHAR_MAX_DATA)); i++) {
+		room +=
+		    (OVERFLOW_SIZE - overflow[pi->port].bufferBytes[i]);
+	}
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (room > VIOCHAR_MAX_DATA)
+		return VIOCHAR_MAX_DATA;
+	else
+		return room;
+}
+
+/* TTY chars_in_buffer_room method
+ */
+static int viotty_chars_in_buffer(struct tty_struct *tty)
+{
+	return 0;
+}
+
+static int viotty_ioctl(struct tty_struct *tty, struct file *file,
+			unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+		/* the ioctls below read/set the flags usually shown in the leds */
+		/* don't use them - they will go away without warning */
+	case KDGETLED:
+	case KDGKBLED:
+		return put_user(0, (char *) arg);
+
+	case KDSKBLED:
+		return 0;
+	}
+
+	return n_tty_ioctl(tty, file, cmd, arg);
+}
+
+/* Handle an open charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleOpenEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	u8 port = cevent->virtualDevice;
+	int reject = 0;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack) {
+		if (port >= VTTY_PORTS)
+			return;
+
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			sndMsgSeq[port] = sndMsgAck[port] = 0;
+
+			/* This line allows connections from the primary
+			 * partition but once one is connected from the
+			 * primary partition nothing short of a reboot
+			 * of linux will allow access from the hosting
+			 * partition again without a required iSeries fix.
+			 */
+			port_info[port].lp = event->xTargetLp;
+		}
+
+		spin_unlock_irqrestore(&consolelock, flags);
+		if (event->xRc != HvLpEvent_Rc_Good)
+			printk(KERN_WARNING_VIO
+			       "viocons: event->xRc != HvLpEvent_Rc_Good, event->xRc == (%d).\n",
+			       event->xRc);
+
+		if (event->xCorrelationToken != 0) {
+			unsigned long semptr = event->xCorrelationToken;
+			up((struct semaphore *) semptr);
+		} else
+			printk(KERN_WARNING_VIO
+			       "viocons: wierd...got open ack without semaphore\n");
+	} else {
+		/* This had better require an ack, otherwise complain
+		 */
+		if (event->xFlags.xAckInd != HvLpEvent_AckInd_DoAck) {
+			printk(KERN_WARNING_VIO
+			       "viocons: viocharopen without ack bit!\n");
+			return;
+		}
+
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		/* Make sure this is a good virtual tty */
+		if (port >= VTTY_PORTS) {
+			event->xRc = HvLpEvent_Rc_SubtypeError;
+			cevent->mSubTypeRc = viorc_openRejected;
+			/*
+			 * Flag state here since we can't printk while holding
+			 * a spinlock.
+			 */
+			reject = 1;
+		} else if ((port_info[port].lp != HvLpIndexInvalid) &&
+			   (port_info[port].lp != event->xSourceLp)) {
+			/*
+			 * If this is tty is already connected to a different
+			 * partition, fail.
+			 */
+			event->xRc = HvLpEvent_Rc_SubtypeError;
+			cevent->mSubTypeRc = viorc_openRejected;
+			reject = 2;
+		} else {
+			port_info[port].lp = event->xSourceLp;
+			event->xRc = HvLpEvent_Rc_Good;
+			cevent->mSubTypeRc = viorc_good;
+			sndMsgSeq[port] = sndMsgAck[port] = 0;
+			reject = 0;
+		}
+
+		spin_unlock_irqrestore(&consolelock, flags);
+
+		if (reject == 1)
+			printk
+			    (KERN_WARNING_VIO "viocons: console open rejected : bad virtual tty.\n");
+		else if (reject == 2)
+			printk
+			    (KERN_WARNING_VIO "viocons: console open rejected : console in exclusive use by another partition.\n");
+		/*
+		 * Don't leave this here to clutter up the log unless it is
+		 * needed for debug.
+		 *
+		 * else
+		 *      printk(KERN_INFO_VIO "viocons: console open event Good!\n");
+		 *
+		 */
+
+		/* Return the acknowledgement */
+		HvCallEvent_ackLpEvent(event);
+	}
+}
+
+/* Handle a close charLpEvent.  This should ONLY be an Interrupt because the
+ * virtual console should never actually issue a close event to the hypervisor
+ * because the virtual console never goes away.  A close event coming from the
+ * hypervisor simply means that there are no client consoles connected to the
+ * virtual console.
+ *
+ * Regardless of the number of connections masqueraded on the other side of
+ * the hypervisor ONLY ONE close event should be called to accompany the ONE
+ * open event that is called.  The close event should ONLY be called when NO
+ * MORE connections (masqueraded or not) exist on the other side of the
+ * hypervisor.
+ */
+static void vioHandleCloseEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	u8 port = cevent->virtualDevice;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		if (port >= VTTY_PORTS) {
+			printk(KERN_WARNING_VIO "viocons: close message from invalid virtual device.\n");
+			return;
+		}
+
+		/* For closes, just mark the console partition invalid */
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		if (port_info[port].lp == event->xSourceLp)
+			port_info[port].lp = HvLpIndexInvalid;
+
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_INFO_VIO
+		       "console close from %d\n", event->xSourceLp);
+	} else {
+		printk(KERN_WARNING_VIO
+		       "console got unexpected close acknowlegement\n");
+	}
+}
+
+/* Handle a config charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleConfig(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	int len;
+
+	len = cevent->immediateDataLen;
+	HvCall_writeLogBuffer(cevent->immediateData,
+			      cevent->immediateDataLen);
+
+	if (cevent->immediateData[0] == 0x01) {
+		printk(KERN_INFO_VIO
+		       "console window resized to %d: %d: %d: %d\n",
+		       cevent->immediateData[1],
+		       cevent->immediateData[2],
+		       cevent->immediateData[3], cevent->immediateData[4]);
+	} else {
+		printk(KERN_WARNING_VIO "console unknown config event\n");
+	}
+	return;
+}
+
+/* Handle a data charLpEvent. */
+static void vioHandleData(struct HvLpEvent *event)
+{
+	struct tty_struct *tty;
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	struct port_info_tag *pi;
+	int index;
+	u8 port = cevent->virtualDevice;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "console data on invalid virtual device %d\n",
+		       port);
+		return;
+	}
+	/*
+	 * Change 05/01/2003 - Ryan Arnold: If a partition other than
+	 * the current exclusive partition tries to send us data
+	 * events then just drop them on the floor because we don't
+	 * want his stinking data.  He isn't authorized to receive
+	 * data because he wasn't the first one to get the console,
+	 * therefore he shouldn't be allowed to send data either.
+	 * This will work without an iSeries fix.
+	 */
+	if (port_info[port].lp != event->xSourceLp)
+		return;
+
+	/* Hold the spinlock so that we don't take an interrupt that
+	 * changes tty between the time we fetch the port_info_tag
+	 * pointer and the time we paranoia check.
+	 */
+	spin_lock_irqsave(&consolelock, flags);
+
+	tty = port_info[port].tty;
+
+	if (tty == NULL) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_WARNING_VIO
+		       "no tty for virtual device %d\n", port);
+		return;
+	}
+
+	if (tty->magic != TTY_MAGIC) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_WARNING_VIO "tty bad magic\n");
+		return;
+	}
+
+	/*
+	 * Just to be paranoid, make sure the tty points back to this port
+	 */
+	pi = (struct port_info_tag *) tty->driver_data;
+
+	if (!pi || viotty_paranoia_check(pi, tty->device, "vioHandleData")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	/* Change 07/21/2003 - Ryan Arnold: functionality added to support sysrq
+	 * utilizing ^O as the sysrq key.  The sysrq functionality will only work
+	 * if built into the kernel and then only if sysrq is enabled through the
+	 * proc filesystem.
+	 */
+	for ( index = 0; index < cevent->immediateDataLen; ++index ) {
+#ifdef CONFIG_MAGIC_SYSRQ /* Handle the SysRq */
+		if( sysrq_ctls.enabled ) {
+			/* 0x0f is the ascii character for ^O */
+			if(cevent->immediateData[index] == '\x0f') {
+				vio_sysrq_pressed = 1;
+				/* continue because we don't want to add the sysrq key into
+				 * the data string.*/
+				continue;
+			} else if (vio_sysrq_pressed) {
+				handle_sysrq(cevent->immediateData[index], NULL, NULL, tty);
+				vio_sysrq_pressed = 0;
+				/* continue because we don't want to add the sysrq
+				 * sequence into the data string.*/
+				continue;
+			}
+		}
+#endif
+		/* The sysrq sequence isn't included in this check if sysrq is enabled
+		 * and compiled into the kernel because the sequence will never get
+		 * inserted into the buffer.  Don't attempt to copy more data into the
+		 * buffer than we have room for because it would fail without
+		 * indication.
+		 */
+		if( tty->flip.count + 1 > TTY_FLIPBUF_SIZE ) {
+			printk(KERN_WARNING_VIO "console input buffer overflow!\n");
+			break;
+		}
+		else {
+			tty_insert_flip_char(tty, cevent->immediateData[index], TTY_NORMAL);
+		}
+	}
+
+	/* if cevent->immediateDataLen == 0 then no data was added to the buffer and flip.count == 0 */
+	if (tty->flip.count) {
+		/* The next call resets flip.count when the data is flushed. */
+		tty_flip_buffer_push(tty);
+	}
+}
+
+/* Handle an ack charLpEvent. */
+static void vioHandleAck(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	unsigned long flags;
+	u8 port = cevent->virtualDevice;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "viocons: data on invalid virtual device.\n");
+		return;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+	sndMsgAck[port] = event->xCorrelationToken;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (overflow[port].bufferUsed)
+		sendBuffers(port, port_info[port].lp);
+}
+
+/* Handle charLpEvents and route to the appropriate routine
+ */
+static void vioHandleCharEvent(struct HvLpEvent *event)
+{
+	int charminor;
+
+	if (event == NULL) {
+		return;
+	}
+	charminor = event->xSubtype & VIOMINOR_SUBTYPE_MASK;
+	switch (charminor) {
+	case viocharopen:
+		vioHandleOpenEvent(event);
+		break;
+	case viocharclose:
+		vioHandleCloseEvent(event);
+		break;
+	case viochardata:
+		vioHandleData(event);
+		break;
+	case viocharack:
+		vioHandleAck(event);
+		break;
+	case viocharconfig:
+		vioHandleConfig(event);
+		break;
+	default:
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+/* Send an open event
+ */
+static int viocons_sendOpen(HvLpIndex remoteLp, u8 port, void *sem)
+{
+	return HvCallEvent_signalLpEventFast(remoteLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_chario
+					     | viocharopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (remoteLp),
+					     viopath_targetinst
+					     (remoteLp),
+					     (u64) (unsigned long)
+					     sem, VIOVERSION << 16,
+					     ((u64) port << 48), 0, 0, 0);
+
+}
+
+int __init viocons_init2(void)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int rc;
+
+	/*
+	 * Now open to the primary LP
+	 */
+	printk(KERN_INFO_VIO "console open path to primary\n");
+	rc = viopath_open(HvLpConfig_getPrimaryLpIndex(), viomajorsubtype_chario, viochar_window + 2);	/* +2 for fudge */
+	if (rc) {
+		printk(KERN_WARNING_VIO
+		       "console error opening to primary %d\n", rc);
+	}
+
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		vio_set_hostlp();
+	}
+
+	/*
+	 * And if the primary is not the same as the hosting LP, open to the 
+	 * hosting lp
+	 */
+	if ((viopath_hostLp != HvLpIndexInvalid) &&
+	    (viopath_hostLp != HvLpConfig_getPrimaryLpIndex())) {
+		printk(KERN_INFO_VIO
+		       "console open path to hosting (%d)\n",
+		       viopath_hostLp);
+		rc = viopath_open(viopath_hostLp, viomajorsubtype_chario, viochar_window + 2);	/* +2 for fudge */
+		if (rc) {
+			printk(KERN_WARNING_VIO
+			       "console error opening to partition %d: %d\n",
+			       viopath_hostLp, rc);
+		}
+	}
+
+	if (vio_setHandler(viomajorsubtype_chario, vioHandleCharEvent) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Error seting handler for console events!\n");
+	}
+
+	printk(KERN_INFO_VIO "console major number is %d\n", TTY_MAJOR);
+
+	/* First, try to open the console to the hosting lp.
+	 * Wait on a semaphore for the response.
+	 */
+	if ((viopath_isactive(viopath_hostLp)) &&
+	    (viocons_sendOpen(viopath_hostLp, 0, &Semaphore) == 0)) {
+		printk(KERN_INFO_VIO
+		       "opening console to hosting partition %d\n",
+		       viopath_hostLp);
+		down(&Semaphore);
+	}
+
+	/*
+	 * If we don't have an active console, try the primary
+	 */
+	if ((!viopath_isactive(port_info[0].lp)) &&
+	    (viopath_isactive(HvLpConfig_getPrimaryLpIndex())) &&
+	    (viocons_sendOpen
+	     (HvLpConfig_getPrimaryLpIndex(), 0, &Semaphore) == 0)) {
+		printk(KERN_INFO_VIO
+		       "opening console to primary partition\n");
+		down(&Semaphore);
+	}
+
+	/* Initialize the tty_driver structure */
+	memset(&viotty_driver, 0, sizeof(struct tty_driver));
+	viotty_driver.magic = TTY_DRIVER_MAGIC;
+	viotty_driver.driver_name = "vioconsole";
+#if defined(CONFIG_DEVFS_FS)
+	viotty_driver.name = "tty%d";
+#else
+	viotty_driver.name = "tty";
+#endif
+	viotty_driver.major = TTY_MAJOR;
+	viotty_driver.minor_start = 1;
+	viotty_driver.name_base = 1;
+	viotty_driver.num = VTTY_PORTS;
+	viotty_driver.type = TTY_DRIVER_TYPE_CONSOLE;
+	viotty_driver.subtype = 1;
+	viotty_driver.init_termios = tty_std_termios;
+	viotty_driver.flags =
+	    TTY_DRIVER_REAL_RAW | TTY_DRIVER_RESET_TERMIOS;
+	viotty_driver.refcount = &viotty_refcount;
+	viotty_driver.table = viotty_table;
+	viotty_driver.termios = viotty_termios;
+	viotty_driver.termios_locked = viotty_termios_locked;
+
+	viotty_driver.open = viotty_open;
+	viotty_driver.close = viotty_close;
+	viotty_driver.write = viotty_write;
+	viotty_driver.put_char = viotty_put_char;
+	viotty_driver.flush_chars = NULL;
+	viotty_driver.write_room = viotty_write_room;
+	viotty_driver.chars_in_buffer = viotty_chars_in_buffer;
+	viotty_driver.flush_buffer = NULL;
+	viotty_driver.ioctl = NULL;
+	viotty_driver.throttle = NULL;
+	viotty_driver.unthrottle = NULL;
+	viotty_driver.set_termios = NULL;
+	viotty_driver.stop = NULL;
+	viotty_driver.start = NULL;
+	viotty_driver.hangup = NULL;
+	viotty_driver.break_ctl = NULL;
+	viotty_driver.send_xchar = NULL;
+	viotty_driver.wait_until_sent = NULL;
+
+	viottyS_driver = viotty_driver;
+#if defined(CONFIG_DEVFS_FS)
+	viottyS_driver.name = "ttyS%d";
+#else
+	viottyS_driver.name = "ttyS";
+#endif
+	viottyS_driver.major = TTY_MAJOR;
+	viottyS_driver.minor_start = VIOTTY_SERIAL_START;
+	viottyS_driver.type = TTY_DRIVER_TYPE_SERIAL;
+	viottyS_driver.table = viottyS_table;
+	viottyS_driver.termios = viottyS_termios;
+	viottyS_driver.termios_locked = viottyS_termios_locked;
+
+	if (tty_register_driver(&viotty_driver)) {
+		printk(KERN_WARNING_VIO
+		       "Couldn't register console driver\n");
+	}
+
+	if (tty_register_driver(&viottyS_driver)) {
+		printk(KERN_WARNING_VIO
+		       "Couldn't register console S driver\n");
+	}
+	/* Now create the vcs and vcsa devfs entries so mingetty works */
+#if defined(CONFIG_DEVFS_FS)
+	{
+		struct tty_driver temp_driver = viotty_driver;
+		int i;
+
+		temp_driver.name = "vcs%d";
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+
+		temp_driver.name = "vcsa%d";
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+
+		/* For compatibility with some earlier code only!
+		 * This will go away!!!
+		 */
+		temp_driver.name = "viocons/%d";
+		temp_driver.name_base = 0;
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+	}
+#endif
+
+	/* Create the proc entry */
+	iSeries_proc_callback(&viocons_proc_init);
+
+	/* Fetch memory for the cfu buffer */
+	viocons_init_cfu_buffer();
+
+	return 0;
+}
+
+void __init viocons_init(void)
+{
+	int i;
+	printk(KERN_INFO_VIO "registering console\n");
+
+	memset(&port_info, 0x00, sizeof(port_info));
+	for (i = 0; i < VTTY_PORTS; i++) {
+		sndMsgSeq[i] = sndMsgAck[i] = 0;
+		port_info[i].port = i;
+		port_info[i].lp = HvLpIndexInvalid;
+		port_info[i].magic = VIOTTY_MAGIC;
+	}
+
+	register_console(&viocons);
+	memset(overflow, 0x00, sizeof(overflow));
+	debug = 0;
+
+	HvCall_setLogBufferFormatAndCodepage(HvCall_LogBuffer_ASCII, 437);
+}
diff -urNp linux-341/drivers/iseries/viodasd.c linux-342/drivers/iseries/viodasd.c
--- linux-341/drivers/iseries/viodasd.c
+++ linux-342/drivers/iseries/viodasd.c
@@ -0,0 +1,1675 @@
+/* -*- linux-c -*-
+ * viodasd.c
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ * 
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA 
+ ***************************************************************************
+ * This routine provides access to disk space (termed "DASD" in historical
+ * IBM terms) owned and managed by an OS/400 partition running on the
+ * same box as this Linux partition.
+ *
+ * All disk operations are performed by sending messages back and forth to 
+ * the OS/400 partition. 
+ * 
+ * This device driver can either use its own major number, or it can
+ * pretend to be an IDE drive (grep 'IDE[0-9]_MAJOR' ../../include/linux/major.h).
+ * This is controlled with a CONFIG option.  You can either call this an
+ * elegant solution to the fact that a lot of software doesn't recognize
+ * a new disk major number...or you can call this a really ugly hack.
+ * Your choice.
+ */
+
+#include <linux/major.h>
+#include <linux/config.h>
+#include <linux/fs.h>
+#include <linux/blkpg.h>
+#include <linux/types.h>
+#include <asm/semaphore.h>
+#include <linux/seq_file.h>
+
+/* Changelog:
+	2001-11-27	devilbis	Added first pass at complete IDE emulation
+	2002-07-07      boutcher        Added randomness
+ */
+
+/* Decide if we are using our own major or pretending to be an IDE drive
+ *
+ * If we are using our own major, we only support 7 partitions per physical
+ * disk....so with minor numbers 0-255 we get a maximum of 32 disks.  If we
+ * are emulating IDE, we get 63 partitions per disk, with a maximum of 4
+ * disks per major, but common practice is to place only 2 devices in /dev
+ * for each IDE major, for a total of 20 (since there are 10 IDE majors).
+ */
+
+static const int major_table[] = {
+	VIODASD_MAJOR,
+};
+enum {
+	DEV_PER_MAJOR = 32,
+	PARTITION_SHIFT = 3,
+};
+static inline int major_to_index(int major)
+{
+	if(major != VIODASD_MAJOR)
+		return -1;
+	return 0;
+}
+#define VIOD_DEVICE_NAME "viod"
+#ifdef CONFIG_DEVFS_FS
+#define VIOD_GENHD_NAME "viod"
+#else
+#define VIOD_GENHD_NAME "iseries/vd"
+#endif
+
+#define DEVICE_NR(dev) (devt_to_diskno(dev))
+#define LOCAL_END_REQUEST
+
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/blk.h>
+#include <linux/genhd.h>
+#include <linux/hdreg.h>
+#include <linux/fd.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/pci.h>
+#include <linux/capability.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include "vio.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+MODULE_DESCRIPTION("iSeries Virtual DASD");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+#define VIODASD_VERS "1.60"
+
+enum {
+	NUM_MAJORS = sizeof(major_table) / sizeof(major_table[0]),
+	MAX_DISKNO = DEV_PER_MAJOR * NUM_MAJORS,
+	MAX_MAJOR_NAME = 16 + 1, /* maximum length of a gendisk->name */
+};
+
+static volatile int viodasd_max_disk = MAX_DISKNO - 1;
+
+static inline int diskno_to_major(int diskno)
+{
+	if (diskno >= MAX_DISKNO)
+		return -1;
+	return major_table[diskno / DEV_PER_MAJOR];
+}
+static inline int devt_to_diskno(kdev_t dev)
+{
+	return major_to_index(MAJOR(dev)) * DEV_PER_MAJOR +
+	    (MINOR(dev) >> PARTITION_SHIFT);
+}
+static inline int diskno_to_devt(int diskno, int partition)
+{
+	return MKDEV(diskno_to_major(diskno),
+		     ((diskno % DEV_PER_MAJOR) << PARTITION_SHIFT) +
+		     partition);
+}
+
+#define VIOMAXREQ 16
+#define VIOMAXBLOCKDMA        12
+
+extern struct pci_dev *iSeries_vio_dev;
+
+struct openData {
+	u64 mDiskLen;
+	u16 mMaxDisks;
+	u16 mCylinders;
+	u16 mTracks;
+	u16 mSectors;
+	u16 mBytesPerSector;
+};
+
+struct rwData {			// Used during rw
+	u64 mOffset;
+	struct {
+		u32 mToken;
+		u32 reserved;
+		u64 mLen;
+	} dmaInfo[VIOMAXBLOCKDMA];
+};
+
+struct vioblocklpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	union {
+		struct openData openData;
+		struct rwData rwData;
+		struct {
+			u64 changed;
+		} check;
+	} u;
+};
+
+#define vioblockflags_ro   0x0001
+
+enum vioblocksubtype {
+	vioblockopen = 0x0001,
+	vioblockclose = 0x0002,
+	vioblockread = 0x0003,
+	vioblockwrite = 0x0004,
+	vioblockflush = 0x0005,
+	vioblockcheck = 0x0007
+};
+
+/* In a perfect world we will perform better if we get page-aligned I/O
+ * requests, in multiples of pages.  At least peg our block size to the
+ * actual page size.
+ */
+static int blksize = HVPAGESIZE;	/* in bytes */
+
+static DECLARE_WAIT_QUEUE_HEAD(viodasd_wait);
+struct viodasd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	union {
+		int changed;	/* Used only for check_change */
+		u16 subRC;
+	} data;
+};
+
+static const struct vio_error_entry viodasd_err_table[] = {
+	{0x0201, EINVAL, "Invalid Range"},
+	{0x0202, EINVAL, "Invalid Token"},
+	{0x0203, EIO, "DMA Error"},
+	{0x0204, EIO, "Use Error"},
+	{0x0205, EIO, "Release Error"},
+	{0x0206, EINVAL, "Invalid Disk"},
+	{0x0207, EBUSY, "Cant Lock"},
+	{0x0208, EIO, "Already Locked"},
+	{0x0209, EIO, "Already Unlocked"},
+	{0x020A, EIO, "Invalid Arg"},
+	{0x020B, EIO, "Bad IFS File"},
+	{0x020C, EROFS, "Read Only Device"},
+	{0x02FF, EIO, "Internal Error"},
+	{0x0000, 0, NULL},
+};
+
+/* Our gendisk table
+ */
+static struct gendisk viodasd_gendisk[NUM_MAJORS];
+
+static inline struct gendisk *major_to_gendisk(int major)
+{
+	int index = major_to_index(major);
+	return index < 0 ? NULL : &viodasd_gendisk[index];
+}
+static inline struct hd_struct *devt_to_partition(kdev_t dev)
+{
+	return &major_to_gendisk(MAJOR(dev))->part[MINOR(dev)];
+}
+
+/* Figure out the biggest I/O request (in sectors) we can accept
+ */
+#define VIODASD_MAXSECTORS (4096 / 512 * VIOMAXBLOCKDMA)
+
+/* Keep some statistics on what's happening for the PROC file system
+ */
+static struct {
+	long tot;
+	long nobh;
+	long ntce[VIOMAXBLOCKDMA];
+} viod_stats[MAX_DISKNO][2];
+
+/* Number of disk I/O requests we've sent to OS/400
+ */
+static int num_req_outstanding;
+
+/* This is our internal structure for keeping track of disk devices
+ */
+struct viodasd_device {
+	int useCount;
+	u16 cylinders;
+	u16 tracks;
+	u16 sectors;
+	u16 bytesPerSector;
+	u64 size;
+	int readOnly;
+} *viodasd_devices;
+
+/* When we get a disk I/O request we take it off the general request queue
+ * and put it here.
+ */
+static LIST_HEAD(reqlist);
+
+/* Handle reads from the proc file system
+ */
+static int show_viodasd(struct seq_file *m, void *v)
+{
+	int i;
+	int j;
+
+#if defined(MODULE)
+	    seq_printf(m,
+		    "viod Module opened %d times.  Major number %d\n",
+		    MOD_IN_USE, major_table[0]);
+#endif
+	seq_printf(m, "viod %d possible devices\n", MAX_DISKNO);
+
+	for (i = 0; i <= viodasd_max_disk && i < MAX_DISKNO; i++) {
+		if (viod_stats[i][0].tot || viod_stats[i][1].tot) {
+			seq_printf(m,
+				    "DISK %2.2d: rd %-10.10ld wr %-10.10ld (no buffer list rd %-10.10ld wr %-10.10ld\n",
+				    i, viod_stats[i][0].tot,
+				    viod_stats[i][1].tot,
+				    viod_stats[i][0].nobh,
+				    viod_stats[i][1].nobh);
+
+			seq_printf(m, "rd DMA: ");
+
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				seq_printf(m, " [%2.2d] %ld",
+					       j,
+					       viod_stats[i][0].ntce[j]);
+
+			seq_printf(m, "\nwr DMA: ");
+
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				seq_printf(m, " [%2.2d] %ld",
+					       j,
+					       viod_stats[i][1].ntce[j]);
+			seq_printf(m, "\n");
+		}
+	}
+
+	return 0;
+}
+
+/* associate proc_viodasd_sops with this seq_file */
+static int viodasd_proc_open(struct inode *inode, struct file *file)
+{
+	size_t size = PAGE_SIZE;
+	char *buf = kmalloc(size, GFP_KERNEL);
+	struct seq_file *m;
+	int status;
+
+	if (!buf)
+		return -ENOMEM;
+	status = single_open(file, show_viodasd, NULL);
+	if (!status) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		kfree(buf);
+	return status;
+}
+
+static struct file_operations proc_viodasd_fops = {
+        .open = viodasd_proc_open,
+        .read = seq_read,
+        .llseek = seq_lseek,
+        .release = single_release,
+};
+
+/* setup our proc file system entries
+ */
+void viodasd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viodasd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->owner = THIS_MODULE;
+	ent->proc_fops = &proc_viodasd_fops;
+}
+
+/* clean up our proc file system entries
+ */
+void viodasd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viodasd", iSeries_proc);
+}
+
+/* Strip a single buffer-head off of a request; if it's the last
+ * one, also dequeue it and clean it up
+ */
+static int viodasd_end_request(struct request *req, int uptodate)
+{
+	if (end_that_request_first(req, uptodate, VIOD_DEVICE_NAME))
+		return 1;
+
+	add_blkdev_randomness(MAJOR(req->rq_dev));
+
+	list_del(&req->queue);
+	end_that_request_last(req);
+	return 0;
+}
+
+static void viodasd_end_request_with_error(struct request *req)
+{
+	while(viodasd_end_request(req, 0))
+		;
+}
+
+/* This rebuilds the partition information for a single disk device
+ */
+static int viodasd_revalidate(kdev_t dev)
+{
+	int i;
+	int device_no = DEVICE_NR(dev);
+	int dev_within_major = device_no % DEV_PER_MAJOR;
+	int part0 = (dev_within_major << PARTITION_SHIFT);
+	int npart = (1 << PARTITION_SHIFT);
+	int major = MAJOR(dev);
+	struct gendisk *gendisk = major_to_gendisk(major);
+
+	if (viodasd_devices[device_no].size == 0)
+		return 0;
+
+	for (i = npart - 1; i >= 0; i--) {
+		int minor = part0 + i;
+		struct hd_struct *partition = &gendisk->part[minor];
+
+		if (partition->nr_sects != 0) {
+			kdev_t devp = MKDEV(major, minor);
+			struct super_block *sb;
+			fsync_dev(devp);
+
+			sb = get_super(devp);
+			if (sb)
+				invalidate_inodes(sb);
+
+			invalidate_buffers(devp);
+		}
+
+		partition->start_sect = 0;
+		partition->nr_sects = 0;
+	}
+
+	grok_partitions(gendisk, dev_within_major, npart,
+			viodasd_devices[device_no].size >> 9);
+
+	return 0;
+}
+
+
+static inline u16 access_flags(mode_t mode)
+{
+	u16 flags = 0;
+	if (!(mode & FMODE_WRITE))
+		flags |= vioblockflags_ro;
+	return flags;
+}
+
+static void internal_register_disk(int diskno);
+
+/* This is the actual open code.  It gets called from the external
+ * open entry point, as well as from the init code when we're figuring
+ * out what disks we have
+ */
+static int internal_open(int device_no, u16 flags)
+{
+	int i;
+	const int dev_within_major = device_no % DEV_PER_MAJOR;
+	struct gendisk *gendisk =
+	    major_to_gendisk(diskno_to_major(device_no));
+	HvLpEvent_Rc hvrc;
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viodasd_waitevent we = { sem:&Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -ENXIO;
+	}
+
+	/* Send the open event to OS/400                                         */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_blockio |
+					     vioblockopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) &we,
+					     VIOVERSION << 16,
+					     ((u64) device_no << 48) |
+					     ((u64) flags << 32), 0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+		       (int) hvrc);
+		return -ENXIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code                                                 */
+	if (we.rc != 0) {
+		const struct vio_error_entry *err =
+		    vio_lookup_rc(viodasd_err_table, we.data.subRC);
+		/* Temporary patch to quiet down the viodasd when drivers are probing    */
+		/* for drives, especially lvm.  Collin is aware and is working on this.  */
+		/* printk(KERN_WARNING_VIO                                               */
+		/*       "bad rc opening disk: %d:0x%04x (%s)\n",                        */
+		/*       (int) we.rc, we.data.subRC, err->msg);                          */
+		return -err->errno;
+	}
+	
+	/* If this is the first open of this device, update the device information */
+	/* If this is NOT the first open, assume that it isn't changing            */
+	if (viodasd_devices[device_no].useCount == 0) {
+		if (viodasd_devices[device_no].size > 0) {
+			/* divide by 512 */
+			u64 tmpint = viodasd_devices[device_no].size >> 9;
+			gendisk->part[dev_within_major << PARTITION_SHIFT].nr_sects = tmpint;
+			/* Now the value divided by 1024 */
+			tmpint = tmpint >> 1;
+			gendisk->sizes[dev_within_major << PARTITION_SHIFT] = tmpint;
+
+			for (i = dev_within_major << PARTITION_SHIFT;
+			     i < ((dev_within_major + 1) << PARTITION_SHIFT);
+			     i++)
+			{
+				hardsect_size[diskno_to_major(device_no)][i] =
+				    viodasd_devices[device_no].bytesPerSector;
+			}
+		}
+	} else {
+		/* If the size of the device changed, weird things are happening!     */
+		if (gendisk->sizes[dev_within_major << PARTITION_SHIFT] !=
+		    viodasd_devices[device_no].size >> 10) {
+			printk(KERN_WARNING_VIO
+			       "disk size change (%dK to %dK) for device %d\n",
+			       gendisk->sizes[dev_within_major << PARTITION_SHIFT],
+			       (int) viodasd_devices[device_no].size >> 10, device_no);
+		}
+	}
+
+	internal_register_disk(device_no);
+
+	/* Bump the use count                                                      */
+	viodasd_devices[device_no].useCount++;
+	return 0;
+}
+
+/* This is the actual release code.  It gets called from the external
+ * release entry point, as well as from the init code when we're figuring
+ * out what disks we have
+ */
+static int internal_release(int device_no, u16 flags)
+{
+	/* Send the event to OS/400.  We DON'T expect a response                 */
+	HvLpEvent_Rc hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+							  HvLpEvent_Type_VirtualIo,
+							  viomajorsubtype_blockio
+							  | vioblockclose,
+							  HvLpEvent_AckInd_NoAck,
+							  HvLpEvent_AckType_ImmediateAck,
+							  viopath_sourceinst
+							  (viopath_hostLp),
+							  viopath_targetinst
+							  (viopath_hostLp),
+							  0,
+							  VIOVERSION << 16,
+							  ((u64) device_no
+							   << 48) | ((u64)
+								     flags
+								     <<
+								     32),
+							  0, 0, 0);
+
+	viodasd_devices[device_no].useCount--;
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO
+		       "bad rc sending event to OS/400 %d\n", (int) hvrc);
+		return -EIO;
+	}
+	return 0;
+}
+
+
+/* External open entry point.
+ */
+static int viodasd_open(struct inode *ino, struct file *fil)
+{
+	int device_no;
+	int old_max_disk = viodasd_max_disk;
+
+	/* Do a bunch of sanity checks                                           */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in open\n");
+		return -ENXIO;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on open\n");
+		return -ENXIO;
+	}
+
+	device_no = DEVICE_NR(ino->i_rdev);
+	if (device_no > MAX_DISKNO || device_no < 0) {
+		printk(KERN_WARNING_VIO
+		       "Invalid device number %d in open\n", device_no);
+		return -ENXIO;
+	}
+
+	/* Call the actual open code                                             */
+	if (internal_open(device_no, access_flags(fil ? fil->f_mode : 0)) == 0) {
+		int i;
+		/* For each new disk: */
+		/* update the disk's geometry via internal_open and register it */
+		for (i = old_max_disk + 1; i <= viodasd_max_disk; ++i) {
+			internal_open(i, vioblockflags_ro);
+			internal_release(i, vioblockflags_ro);
+		}
+		if(devt_to_partition(ino->i_rdev)->nr_sects == 0) {
+			internal_release(device_no, access_flags(fil ? fil->f_mode : 0));
+			return -ENXIO;
+		}
+		return 0;
+	} else {
+		return -ENXIO;
+	}
+}
+
+/* External release entry point.
+ */
+static int viodasd_release(struct inode *ino, struct file *fil)
+{
+	int device_no;
+
+	/* Do a bunch of sanity checks                                           */
+	if (!ino)
+		BUG();
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0)
+		BUG();
+
+	device_no = DEVICE_NR(ino->i_rdev);
+
+	if (device_no > MAX_DISKNO || device_no < 0)
+		BUG();
+
+	/* Call the actual release code                                          */
+	internal_release(device_no, access_flags(fil ? fil->f_mode : 0));
+
+	return 0;
+}
+
+/* External ioctl entry point.
+ */
+static int viodasd_ioctl(struct inode *ino, struct file *fil,
+			 unsigned int cmd, unsigned long arg)
+{
+	int device_no;
+	int err;
+	HvLpEvent_Rc hvrc;
+	struct hd_struct *partition;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Sanity checks                                                        */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in ioctl\n");
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on ioctl\n");
+		return -ENODEV;
+	}
+
+	partition = devt_to_partition(ino->i_rdev);
+
+	device_no = DEVICE_NR(ino->i_rdev);
+	if (device_no > viodasd_max_disk) {
+		printk(KERN_WARNING_VIO
+		       "Invalid device number %d in ioctl\n", device_no);
+		return -ENODEV;
+	}
+
+	switch (cmd) {
+	case BLKPG:
+		return blk_ioctl(ino->i_rdev, cmd, arg);
+	case BLKGETSIZE:
+		/* return the device size in sectors */
+		if (!arg)
+			return -EINVAL;
+		err =
+		    verify_area(VERIFY_WRITE, (long *) arg, sizeof(long));
+		if (err)
+			return err;
+
+		put_user(partition->nr_sects, (long *) arg);
+		return 0;
+
+	case FDFLUSH:
+	case BLKFLSBUF:
+		if (!capable(CAP_SYS_ADMIN))
+			return -EACCES;
+		fsync_dev(ino->i_rdev);
+		invalidate_buffers(ino->i_rdev);
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_blockio
+						     | vioblockflush,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     &Semaphore,
+						     VIOVERSION << 16,
+						     ((u64) device_no <<
+						      48), 0, 0, 0);
+
+
+		if (hvrc != 0) {
+			printk(KERN_WARNING_VIO
+			       "bad rc on sync signalLpEvent %d\n",
+			       (int) hvrc);
+			return -EIO;
+		}
+
+		down(&Semaphore);
+
+		return 0;
+
+	case BLKRAGET:
+		if (!arg)
+			return -EINVAL;
+		err = put_user(read_ahead[MAJOR(ino->i_rdev)], (long *) arg);
+		return err;
+
+	case BLKRASET:
+		if (!suser())
+			return -EACCES;
+		if (arg > 0x00ff)
+			return -EINVAL;
+		read_ahead[MAJOR(ino->i_rdev)] = arg;
+		return 0;
+
+	case BLKRRPART:
+		viodasd_revalidate(ino->i_rdev);
+		return 0;
+
+	case HDIO_GETGEO:
+		{
+			unsigned char sectors;
+			unsigned char heads;
+			unsigned short cylinders;
+
+			struct hd_geometry *geo =
+			    (struct hd_geometry *) arg;
+			if (geo == NULL)
+				return -EINVAL;
+
+			err = verify_area(VERIFY_WRITE, geo, sizeof(*geo));
+			if (err)
+				return err;
+
+			sectors = viodasd_devices[device_no].sectors;
+			if (sectors == 0)
+				sectors = 32;
+
+			heads = viodasd_devices[device_no].tracks;
+			if (heads == 0)
+				heads = 64;
+
+			cylinders = viodasd_devices[device_no].cylinders;
+			if (cylinders == 0)
+				cylinders =
+				    partition->nr_sects / (sectors *
+							   heads);
+
+			put_user(sectors, &geo->sectors);
+			put_user(heads, &geo->heads);
+			put_user(cylinders, &geo->cylinders);
+
+			put_user(partition->start_sect,
+				 (long *) &geo->start);
+
+			return 0;
+		}
+
+	case HDIO_GETGEO_BIG:
+		{
+			unsigned char sectors;
+			unsigned char heads;
+			unsigned int cylinders;
+
+			struct hd_big_geometry *geo =
+			    (struct hd_big_geometry *) arg;
+			if (geo == NULL)
+				return -EINVAL;
+
+			err = verify_area(VERIFY_WRITE, geo, sizeof(*geo));
+			if (err)
+				return err;
+
+			sectors = viodasd_devices[device_no].sectors;
+			if (sectors == 0)
+				sectors = 32;
+
+			heads = viodasd_devices[device_no].tracks;
+			if (heads == 0)
+				heads = 64;
+
+			cylinders = viodasd_devices[device_no].cylinders;
+			if (cylinders == 0)
+				cylinders =
+				    partition->nr_sects / (sectors *
+							   heads);
+
+			put_user(sectors, &geo->sectors);
+			put_user(heads, &geo->heads);
+			put_user(cylinders, &geo->cylinders);
+
+			put_user(partition->start_sect,
+				 (long *) &geo->start);
+
+			return 0;
+		}
+
+#define PRTIOC(x) case x: printk(KERN_WARNING_VIO "got unsupported FD ioctl " #x "\n"); \
+                          return -EINVAL;
+
+		PRTIOC(FDCLRPRM);
+		PRTIOC(FDSETPRM);
+		PRTIOC(FDDEFPRM);
+		PRTIOC(FDGETPRM);
+		PRTIOC(FDMSGON);
+		PRTIOC(FDMSGOFF);
+		PRTIOC(FDFMTBEG);
+		PRTIOC(FDFMTTRK);
+		PRTIOC(FDFMTEND);
+		PRTIOC(FDSETEMSGTRESH);
+		PRTIOC(FDSETMAXERRS);
+		PRTIOC(FDGETMAXERRS);
+		PRTIOC(FDGETDRVTYP);
+		PRTIOC(FDSETDRVPRM);
+		PRTIOC(FDGETDRVPRM);
+		PRTIOC(FDGETDRVSTAT);
+		PRTIOC(FDPOLLDRVSTAT);
+		PRTIOC(FDRESET);
+		PRTIOC(FDGETFDCSTAT);
+		PRTIOC(FDWERRORCLR);
+		PRTIOC(FDWERRORGET);
+		PRTIOC(FDRAWCMD);
+		PRTIOC(FDEJECT);
+		PRTIOC(FDTWADDLE);
+
+	}
+
+	return -EINVAL;
+}
+
+/* Send an actual I/O request to OS/400
+ */
+static int send_request(struct request *req)
+{
+	u64 sect_size;
+	u64 start;
+	u64 len;
+	int direction;
+	int nsg;
+	u16 viocmd;
+	HvLpEvent_Rc hvrc;
+	struct vioblocklpevent *bevent;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	struct buffer_head *bh;
+	int sgindex;
+	int device_no = DEVICE_NR(req->rq_dev);
+	int dev_within_major = device_no % DEV_PER_MAJOR;
+	int statindex;
+	struct hd_struct *partition = devt_to_partition(req->rq_dev);
+
+	if (device_no > viodasd_max_disk || device_no < 0)
+		BUG();
+	
+	/* Note that this SHOULD always be 512...but lets be architecturally correct */
+	sect_size = hardsect_size[MAJOR(req->rq_dev)][dev_within_major];
+
+	/* Figure out the starting sector and length                                 */
+	start = (req->sector + partition->start_sect) * sect_size;
+	len = req->nr_sectors * sect_size;
+
+	/* More paranoia checks                                                      */
+	if ((req->sector + req->nr_sectors) >
+	    (partition->start_sect + partition->nr_sects)) {
+		printk(KERN_WARNING_VIO
+		       "Invalid request offset & length\n");
+		printk(KERN_WARNING_VIO
+		       "req->sector: %ld, req->nr_sectors: %ld\n",
+		       req->sector, req->nr_sectors);
+		printk(KERN_WARNING_VIO "major: %d, minor: %d\n",
+		       MAJOR(req->rq_dev), MINOR(req->rq_dev));
+		return -1;
+	}
+
+	if (req->cmd == READ || req->cmd == READA) {
+		direction = PCI_DMA_FROMDEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockread;
+		statindex = 0;
+	} else {
+		direction = PCI_DMA_TODEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockwrite;
+		statindex = 1;
+	}
+
+	/* Update totals */
+	viod_stats[device_no][statindex].tot++;
+
+	/* Now build the scatter-gather list                                        */
+	memset(&sg, 0x00, sizeof(sg));
+	sgindex = 0;
+
+	/* See if this is a swap I/O (without a bh pointer) or a regular I/O        */
+	if (req->bh) {
+		/* OK...this loop takes buffers from the request and adds them to the SG
+		   until we're done, or until we hit a maximum.  If we hit a maximum we'll
+		   just finish this request later                                       */
+		bh = req->bh;
+		while ((bh) && (sgindex < VIOMAXBLOCKDMA)) {
+			sg[sgindex].address = bh->b_data;
+			sg[sgindex].length = bh->b_size;
+
+			sgindex++;
+			bh = bh->b_reqnext;
+		}
+		nsg = pci_map_sg(iSeries_vio_dev, sg, sgindex, direction);
+		if ((nsg == 0) || (sg[0].dma_length == 0)
+		    || (sg[0].dma_address == 0xFFFFFFFF)) {
+			printk(KERN_WARNING_VIO "error getting sg tces\n");
+			return -1;
+		}
+
+	} else {
+		/* Update stats */
+		viod_stats[device_no][statindex].nobh++;
+
+		sg[0].dma_address =
+		    pci_map_single(iSeries_vio_dev, req->buffer, len,
+				   direction);
+		sg[0].dma_length = len;
+		nsg = 1;
+	}
+
+	/* Update stats */
+	viod_stats[device_no][statindex].ntce[sgindex]++;
+
+	/* This optimization handles a single DMA block                          */
+	if (sgindex == 1) {
+		/* Send the open event to OS/400                                         */
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_blockio
+						     | viocmd,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     req,
+						     VIOVERSION << 16,
+						     ((u64) device_no <<
+						      48), start,
+						     ((u64) sg[0].
+						      dma_address) << 32,
+						     sg[0].dma_length);
+	} else {
+		bevent =
+		    (struct vioblocklpevent *)
+		    vio_get_event_buffer(viomajorsubtype_blockio);
+		if (bevent == NULL) {
+			printk(KERN_WARNING_VIO
+			       "error allocating disk event buffer\n");
+			/* Free up DMA resources */
+			pci_unmap_sg(iSeries_vio_dev, sg, nsg, direction);
+			return -1;
+		}
+
+		/* Now build up the actual request.  Note that we store the pointer      */
+		/* to the request buffer in the correlation token so we can match        */
+		/* this response up later                                                */
+		memset(bevent, 0x00, sizeof(struct vioblocklpevent));
+		bevent->event.xFlags.xValid = 1;
+		bevent->event.xFlags.xFunction = HvLpEvent_Function_Int;
+		bevent->event.xFlags.xAckInd = HvLpEvent_AckInd_DoAck;
+		bevent->event.xFlags.xAckType =
+		    HvLpEvent_AckType_ImmediateAck;
+		bevent->event.xType = HvLpEvent_Type_VirtualIo;
+		bevent->event.xSubtype = viocmd;
+		bevent->event.xSourceLp = HvLpConfig_getLpIndex();
+		bevent->event.xTargetLp = viopath_hostLp;
+		bevent->event.xSizeMinus1 =
+		    offsetof(struct vioblocklpevent,
+			     u.rwData.dmaInfo) +
+		    (sizeof(bevent->u.rwData.dmaInfo[0]) * (sgindex)) - 1;
+		bevent->event.xSourceInstanceId =
+		    viopath_sourceinst(viopath_hostLp);
+		bevent->event.xTargetInstanceId =
+		    viopath_targetinst(viopath_hostLp);
+		bevent->event.xCorrelationToken =
+		    (u64) (unsigned long) req;
+		bevent->mVersion = VIOVERSION;
+		bevent->mDisk = device_no;
+		bevent->u.rwData.mOffset = start;
+
+		/* Copy just the dma information from the sg list into the request */
+		for (sgindex = 0; sgindex < nsg; sgindex++) {
+			bevent->u.rwData.dmaInfo[sgindex].mToken =
+			    sg[sgindex].dma_address;
+			bevent->u.rwData.dmaInfo[sgindex].mLen =
+			    sg[sgindex].dma_length;
+		}
+
+		/* Send the request                                               */
+		hvrc = HvCallEvent_signalLpEvent(&bevent->event);
+		vio_free_event_buffer(viomajorsubtype_blockio, bevent);
+	}
+
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO
+		       "error sending disk event to OS/400 (rc %d)\n",
+		       (int) hvrc);
+		/* Free up DMA resources */
+		pci_unmap_sg(iSeries_vio_dev, sg, nsg, direction);
+		return -1;
+	} else {
+		/* If the request was successful, bump the number of outstanding */
+		num_req_outstanding++;
+	}
+	return 0;
+}
+
+/* This is the external request processing routine
+ */
+static void do_viodasd_request(request_queue_t * q)
+{
+	int device_no;
+	for (;;) {
+		struct request *req;
+		struct gendisk *gendisk;
+
+		/* inlined INIT_REQUEST here because we don't define MAJOR_NR before blk.h */
+		if (list_empty(&q->queue_head))
+			return;
+		req = blkdev_entry_next_request(&q->queue_head);
+		if (major_to_index(MAJOR(req->rq_dev)) < 0)
+			panic(VIOD_DEVICE_NAME ": request list destroyed");
+		if (req->bh) {
+			if (!buffer_locked(req->bh))
+				panic(VIOD_DEVICE_NAME
+				      ": block not locked");
+		}
+
+		gendisk = major_to_gendisk(MAJOR(req->rq_dev));
+
+		device_no = DEVICE_NR(req->rq_dev);
+		if (device_no > MAX_DISKNO || device_no < 0) {
+			printk(KERN_WARNING_VIO "Invalid device # %d\n", device_no);
+			viodasd_end_request_with_error(req);
+			continue;
+		}
+		
+		if (gendisk->sizes == NULL) {
+			printk(KERN_WARNING_VIO "Ouch! gendisk->sizes is NULL\n");
+			viodasd_end_request_with_error(req);
+			continue;
+		}
+
+		/* If the queue is plugged, don't dequeue anything right now */
+		if ((q) && (q->plugged)) {
+			return;
+		}
+
+		/* If we already have the maximum number of requests outstanding to OS/400
+		   just bail out. We'll come back later                              */
+		if (num_req_outstanding >= VIOMAXREQ) {
+			return;
+		}
+
+		/* Try sending the request                                           */
+		if (send_request(req) == 0) {
+			// it worked--transfer it to our internal queue
+			blkdev_dequeue_request(req);
+			list_add_tail(&req->queue, &reqlist);
+		} else {
+			// strip off one bh and try again
+			viodasd_end_request(req, 0);
+		}
+	}
+}
+
+/* Check for changed disks
+ */
+static int viodasd_check_change(kdev_t dev)
+{
+	struct viodasd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = DEVICE_NR(dev);
+
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400                                         */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_blockio |
+					     vioblockcheck,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) &we,
+					     VIOVERSION << 16,
+					     ((u64) device_no << 48), 0, 0,
+					     0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc != 0) {
+		printk(KERN_WARNING_VIO
+		       "bad rc %d on check_change. Assuming no change\n",
+		       (int) we.rc);
+		return 0;
+	}
+
+	return we.data.changed;
+}
+
+/* Our file operations table
+ */
+static struct block_device_operations viodasd_fops = {
+	owner:THIS_MODULE,
+	open:viodasd_open,
+	release:viodasd_release,
+	ioctl:viodasd_ioctl,
+	check_media_change:viodasd_check_change,
+	revalidate:viodasd_revalidate
+};
+
+/* returns the total number of scatterlist elements converted */
+static int block_event_to_scatterlist(const struct vioblocklpevent *bevent,
+				      struct scatterlist *sg,
+				      int *total_len)
+{
+	int i, numsg;
+	const struct rwData *rwData = &bevent->u.rwData;
+	static const int offset =
+	    offsetof(struct vioblocklpevent, u.rwData.dmaInfo);
+	static const int element_size = sizeof(rwData->dmaInfo[0]);
+
+	numsg = ((bevent->event.xSizeMinus1 + 1) - offset) / element_size;
+	if (numsg > VIOMAXBLOCKDMA)
+		panic("[viodasd] I/O completion too large "
+		      "(numsg %d, bevent %p)\n", numsg, bevent);
+
+	*total_len = 0;
+	memset(sg, 0x00, sizeof(sg[0]) * VIOMAXBLOCKDMA);
+
+	for (i = 0; (i < numsg) && (rwData->dmaInfo[i].mLen > 0); ++i) {
+		sg[i].dma_address = rwData->dmaInfo[i].mToken;
+		sg[i].dma_length = rwData->dmaInfo[i].mLen;
+		*total_len += rwData->dmaInfo[i].mLen;
+	}
+	return i;
+}
+
+static struct request *find_request_with_token(u64 token)
+{
+	struct request *req = blkdev_entry_to_request(reqlist.next);
+	while ((&req->queue != &reqlist) &&
+	       ((u64) (unsigned long) req != token))
+		req = blkdev_entry_to_request(req->queue.next);
+	if (&req->queue == &reqlist) {
+		return NULL;
+	}
+	return req;
+}
+
+/* Restart all queues, starting with the one _after_ the major given, */
+/* thus reducing the chance of starvation of disks with late majors. */
+static void viodasd_restart_all_queues_starting_from(int first_major)
+{
+	int i, first_index = major_to_index(first_major);
+	for(i = first_index + 1; i < NUM_MAJORS; ++i)
+		do_viodasd_request(BLK_DEFAULT_QUEUE(major_table[i]));
+	for(i = 0; i <= first_index; ++i)
+		do_viodasd_request(BLK_DEFAULT_QUEUE(major_table[i]));
+}
+
+/* For read and write requests, decrement the number of outstanding requests,
+ * Free the DMA buffers we allocated, and find the matching request by
+ * using the buffer pointer we stored in the correlation token.
+ */
+static int viodasd_handleReadWrite(struct vioblocklpevent *bevent)
+{
+	int num_sg, num_sect, pci_direction, total_len, major;
+	struct request *req;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	struct HvLpEvent *event = &bevent->event;
+	unsigned long irq_flags;
+
+	num_sg = block_event_to_scatterlist(bevent, sg, &total_len);
+	num_sect = total_len >> 9;
+	if (event->xSubtype == (viomajorsubtype_blockio | vioblockread))
+		pci_direction = PCI_DMA_FROMDEVICE;
+	else
+		pci_direction = PCI_DMA_TODEVICE;
+	pci_unmap_sg(iSeries_vio_dev, sg, num_sg, pci_direction);
+
+
+	/* Since this is running in interrupt mode, we need to make sure we're not
+	 * stepping on any global I/O operations
+	 */
+	spin_lock_irqsave(&io_request_lock, irq_flags);
+
+	num_req_outstanding--;
+
+	/* Now find the matching request in OUR list (remember we moved the request
+	 * from the global list to our list when we got it)
+	 */
+	req = find_request_with_token(bevent->event.xCorrelationToken);
+	if (req == NULL) {
+		printk(KERN_WARNING_VIO
+		       "[viodasd] No request found matching token 0x%lx\n",
+		       bevent->event.xCorrelationToken);
+		spin_unlock_irqrestore(&io_request_lock, irq_flags);
+		return -1;
+	}
+
+	/* Record this event's major number so we can check that queue again */
+	major = MAJOR(req->rq_dev);
+
+	if (!req->bh) {
+		if (event->xRc != HvLpEvent_Rc_Good) {
+			const struct vio_error_entry *err =
+			    vio_lookup_rc(viodasd_err_table,
+					  bevent->mSubTypeRc);
+			printk(KERN_WARNING_VIO
+			       "read/write error %d:0x%04x (%s)\n",
+			       event->xRc, bevent->mSubTypeRc, err->msg);
+			viodasd_end_request(req, 0);
+		} else {
+			if (num_sect != req->current_nr_sectors) {
+				printk(KERN_WARNING_VIO
+				       "Yikes...non bh i/o # sect doesn't match!!!\n");
+			}
+			viodasd_end_request(req, 1);
+		}
+	} else {
+		int success = (event->xRc == HvLpEvent_Rc_Good);
+		if (!success) {
+			const struct vio_error_entry *err =
+			    vio_lookup_rc(viodasd_err_table,
+					  bevent->mSubTypeRc);
+			printk(KERN_WARNING_VIO
+			       "read/write error %d:0x%04x (%s)\n",
+			       event->xRc, bevent->mSubTypeRc, err->msg);
+		}
+		/* record having received the answers we did */
+		while ((num_sect > 0) && (req->bh)) {
+			num_sect -= req->current_nr_sectors;
+			viodasd_end_request(req, success);
+		}
+		/* if they somehow answered _more_ than we asked for,
+		 * data corruption has occurred */
+		if (num_sect)
+			panic("[viodasd] %d sectors left over on a request "
+			       "(bevent %p, starting num_sect %d)\n", 
+			       num_sect, bevent, total_len >> 9);
+
+		/* if they didn't answer the whole request this time, re-submit the request */
+		if (req->bh) {
+			if (send_request(req) != 0) {
+				// we'll never know to resubmit this one
+				// we _could_ try to put it out on the request queue for the
+				// related device...
+				viodasd_end_request_with_error(req);
+			}
+		}
+	}
+
+	/* Finally, try to get more requests off of this device's queue */
+	viodasd_restart_all_queues_starting_from(major);
+
+	spin_unlock_irqrestore(&io_request_lock, irq_flags);
+
+	return 0;
+}
+
+/* This routine handles incoming block LP events */
+static void vioHandleBlockEvent(struct HvLpEvent *event)
+{
+	struct vioblocklpevent *bevent = (struct vioblocklpevent *) event;
+	struct viodasd_waitevent *pwe;
+
+	if (event == NULL) {
+		/* Notification that a partition went away! */
+		return;
+	}
+	// First, we should NEVER get an int here...only acks
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "Yikes! got an int in viodasd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+
+		/* Handle a response to an open request.  We get all the disk information
+		 * in the response, so update it.  The correlation token contains a pointer to
+		 * a waitevent structure that has a semaphore in it.  update the return code
+		 * in the waitevent structure and post the semaphore to wake up the guy who
+		 * sent the request */
+	case vioblockopen:
+		pwe =
+		    (struct viodasd_waitevent *) (unsigned long) event->
+		    xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.subRC = bevent->mSubTypeRc;
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			const struct openData *data = &bevent->u.openData;
+			struct viodasd_device *device =
+			    &viodasd_devices[bevent->mDisk];
+			device->readOnly =
+			    bevent->mFlags & vioblockflags_ro;
+			device->size = data->mDiskLen;
+			device->cylinders = data->mCylinders;
+			device->tracks = data->mTracks;
+			device->sectors = data->mSectors;
+			device->bytesPerSector = data->mBytesPerSector;
+			viodasd_max_disk = data->mMaxDisks;
+			if (viodasd_max_disk > MAX_DISKNO - 1)
+				viodasd_max_disk = MAX_DISKNO - 1;
+		}
+		up(pwe->sem);
+		break;
+	case vioblockclose:
+		break;
+	case vioblockcheck:
+		pwe =
+		    (struct viodasd_waitevent *) (unsigned long) event->
+		    xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.changed = bevent->u.check.changed;
+		up(pwe->sem);
+		break;
+	case vioblockflush:
+		up((void *) (unsigned long) event->xCorrelationToken);
+		break;
+	case vioblockread:
+	case vioblockwrite:
+		viodasd_handleReadWrite(bevent);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO "invalid subtype!");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+static const char *major_name(int major)
+{
+	static char major_names[NUM_MAJORS][MAX_MAJOR_NAME];
+	int index = major_to_index(major);
+
+	if(index < 0)
+		return NULL;
+	if(major_names[index][0] == '\0') {
+		if(index == 0)
+			strcpy(major_names[index], VIOD_GENHD_NAME);
+		else
+			sprintf(major_names[index], VIOD_GENHD_NAME"%d", index);
+	}
+	return major_names[index];
+}
+
+static const char *device_name(int major)
+{
+	static char device_names[NUM_MAJORS][MAX_MAJOR_NAME];
+	int index = major_to_index(major);
+
+	if(index < 0)
+		return NULL;
+	if(device_names[index][0] == '\0') {
+		strcpy(device_names[index], VIOD_DEVICE_NAME);
+	}
+	return device_names[index];
+}
+
+/* This routine tries to clean up anything we allocated/registered
+ */
+static void viodasd_cleanup_major(int major)
+{
+	const int num_partitions = DEV_PER_MAJOR << PARTITION_SHIFT;
+	int minor;
+
+	for (minor = 0; minor < num_partitions; minor++)
+		fsync_dev(MKDEV(major, minor));
+
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(major));
+
+	read_ahead[major] = 0;
+
+	kfree(blk_size[major]);
+	kfree(blksize_size[major]);
+	kfree(hardsect_size[major]);
+	kfree(max_sectors[major]);
+	kfree(major_to_gendisk(major)->part);
+
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(major));
+
+	devfs_unregister_blkdev(major, device_name(major));
+}
+
+/* in case of bad return code, caller must viodasd_cleanup_major() for this major */
+static int viodasd_init_major(int major)
+{
+	int i;
+	const int numpart = DEV_PER_MAJOR << PARTITION_SHIFT;
+	int *sizes, *sectsizes, *blksizes, *maxsectors;
+	struct hd_struct *partitions;
+	struct gendisk *gendisk = major_to_gendisk(major);
+
+	/*
+	 * Do the devfs_register.  This works even if devfs is not
+	 * configured
+	 */
+	if (devfs_register_blkdev(major, device_name(major), &viodasd_fops)) {
+		printk(KERN_WARNING_VIO
+		       "%s: can't register major number %d\n",
+		       device_name(major), major);
+		return -1;
+	}
+
+	blk_init_queue(BLK_DEFAULT_QUEUE(major), do_viodasd_request);
+	blk_queue_headactive(BLK_DEFAULT_QUEUE(major), 0);
+
+	read_ahead[major] = 8;	/* 8 sector (4kB) read ahead */
+
+	/* initialize the struct */
+	gendisk->major = major;
+	gendisk->major_name = major_name(major);
+	gendisk->minor_shift = PARTITION_SHIFT;
+	gendisk->max_p = 1 << PARTITION_SHIFT;
+	gendisk->nr_real = DEV_PER_MAJOR;
+	gendisk->fops = &viodasd_fops;
+
+	/* to be assigned later */
+	gendisk->next = NULL;
+	gendisk->part = NULL;
+	gendisk->sizes = NULL;
+	gendisk->de_arr = NULL;
+	gendisk->flags = NULL;
+
+	/* register us in the global list */
+	add_gendisk(gendisk);
+
+	/*
+	 * Now fill in all the device driver info     
+	 */
+	sizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!sizes)
+		return -ENOMEM;
+	memset(sizes, 0x00, numpart * sizeof(int));
+	blk_size[major] = gendisk->sizes = sizes;
+
+	partitions =
+	    kmalloc(numpart * sizeof(struct hd_struct), GFP_KERNEL);
+	if (!partitions)
+		return -ENOMEM;
+	memset(partitions, 0x00, numpart * sizeof(struct hd_struct));
+	gendisk->part = partitions;
+
+	blksizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!blksizes)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		blksizes[i] = blksize;
+	blksize_size[major] = blksizes;
+
+	sectsizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!sectsizes)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		sectsizes[i] = 0;
+	hardsect_size[major] = sectsizes;
+
+	maxsectors = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!maxsectors)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		maxsectors[i] = VIODASD_MAXSECTORS;
+	max_sectors[major] = maxsectors;
+
+	return 0;
+}
+
+static void internal_register_disk(int diskno)
+{
+	static int registered[MAX_DISKNO] = { 0, };
+	int major = diskno_to_major(diskno);
+	int dev_within_major = diskno % DEV_PER_MAJOR;
+	struct gendisk *gendisk = major_to_gendisk(major);
+	int i;
+
+	if(registered[diskno])
+		return;
+	registered[diskno] = 1;
+
+	if (diskno == 0) {
+		printk(KERN_INFO_VIO
+		       "%s: Currently %d disks connected\n",
+		       VIOD_DEVICE_NAME, (int) viodasd_max_disk + 1);
+		if (viodasd_max_disk > MAX_DISKNO - 1)
+			printk(KERN_INFO_VIO
+			       "Only examining the first %d\n",
+			       MAX_DISKNO);
+	}
+
+	register_disk(gendisk,
+		      MKDEV(major,
+			    dev_within_major <<
+			    PARTITION_SHIFT),
+		      1 << PARTITION_SHIFT, &viodasd_fops,
+		      gendisk->
+		      part[dev_within_major << PARTITION_SHIFT].nr_sects);
+
+	printk(KERN_INFO_VIO
+	       "%s: Disk %2.2d size %dM, sectors %d, heads %d, cylinders %d, sectsize %d\n",
+	       VIOD_DEVICE_NAME,
+	       diskno,
+	       (int) (viodasd_devices[diskno].size /
+		      (1024 * 1024)),
+	       (int) viodasd_devices[diskno].sectors,
+	       (int) viodasd_devices[diskno].tracks,
+	       (int) viodasd_devices[diskno].cylinders,
+	       (int) hardsect_size[major][dev_within_major <<
+					  PARTITION_SHIFT]);
+
+	for (i = 1; i < (1 << PARTITION_SHIFT); ++i) {
+		int minor = (dev_within_major << PARTITION_SHIFT) + i;
+		struct hd_struct *partition = &gendisk->part[minor];
+		if (partition->nr_sects)
+			printk(KERN_INFO_VIO
+			       "%s: Disk %2.2d partition %2.2d start sector %ld, # sector %ld\n",
+			       VIOD_DEVICE_NAME, diskno, i,
+			       partition->start_sect, partition->nr_sects);
+	}
+}
+
+/* Initialize the whole device driver.  Handle module and non-module
+ * versions
+ */
+int __init viodasd_init(void)
+{
+	int i, j;
+	int rc;
+
+	/* Try to open to our host lp
+	 */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		vio_set_hostlp();
+	}
+
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "%s: invalid hosting partition\n",
+		       VIOD_DEVICE_NAME);
+		rc = -EIO;
+		goto no_hosting_partition;
+	}
+
+	printk(KERN_INFO_VIO
+	       "%s: Disk vers %s, major %d, max disks %d, hosting partition %d\n",
+	       VIOD_DEVICE_NAME, VIODASD_VERS, major_table[0], MAX_DISKNO,
+	       viopath_hostLp);
+
+	if (ROOT_DEV == NODEV) {
+		/* first disk, first partition */
+		ROOT_DEV = diskno_to_devt(0, 3);
+
+		printk(KERN_INFO_VIO
+		       "Claiming root file system as first partition of first virtual disk");
+	}
+
+	/* Actually open the path to the hosting partition           */
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_blockio,
+			  VIOMAXREQ + 2);
+	if (rc) {
+		printk(KERN_WARNING_VIO
+		       "error opening path to host partition %d\n",
+		       viopath_hostLp);
+		goto viopath_open_failed;
+	} else {
+		printk(KERN_INFO_VIO "%s: opened path to hosting partition %d\n",
+		       VIOD_DEVICE_NAME, viopath_hostLp);
+	}
+
+	viodasd_devices =
+	    kmalloc(MAX_DISKNO * sizeof(struct viodasd_device),
+		    GFP_KERNEL);
+	if (!viodasd_devices)
+	{
+		printk(KERN_WARNING_VIO "couldn't allocate viodasd_devices\n");
+		rc = -ENOMEM;
+		goto viodasd_devices_failed;
+	}
+	memset(viodasd_devices, 0x00,
+	       MAX_DISKNO * sizeof(struct viodasd_device));
+
+	/*
+	 * Initialize our request handler
+	 */
+	vio_setHandler(viomajorsubtype_blockio, vioHandleBlockEvent);
+
+	for (i = 0; i < NUM_MAJORS; ++i) {
+		int init_rc = viodasd_init_major(major_table[i]);
+		if (init_rc < 0) {
+			for (j = 0; j <= i; ++j)
+				viodasd_cleanup_major(major_table[j]);
+			return init_rc;
+		}
+	}
+
+	viodasd_max_disk = MAX_DISKNO - 1;
+	for (i = 0; i <= viodasd_max_disk && i < MAX_DISKNO; i++) {
+		// Note that internal_open has side effects:
+		//  a) it updates the size of the disk
+		//  b) it updates viodasd_max_disk
+		//  c) it registers the disk if it has not done so already
+		if (internal_open(i, vioblockflags_ro) == 0)
+			internal_release(i, vioblockflags_ro);
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viodasd_proc_init);
+
+	return 0;
+viodasd_devices_failed:
+	viopath_close(viopath_hostLp, viomajorsubtype_blockio, VIOMAXREQ + 2);
+viopath_open_failed:
+no_hosting_partition:
+	return rc;
+}
+
+void __exit viodasd_exit(void)
+{
+	int i;
+	for(i = 0; i < NUM_MAJORS; ++i)
+		viodasd_cleanup_major(major_table[i]);
+
+	kfree(viodasd_devices);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_blockio, VIOMAXREQ + 2);
+	iSeries_proc_callback(&viodasd_proc_delete);
+
+}
+
+module_init(viodasd_init);
+module_exit(viodasd_exit);
diff -urNp linux-341/drivers/iseries/viopath.c linux-342/drivers/iseries/viopath.c
--- linux-341/drivers/iseries/viopath.c
+++ linux-342/drivers/iseries/viopath.c
@@ -0,0 +1,813 @@
+/* -*- linux-c -*-
+ *  arch/ppc64/viopath.c
+ *
+ *  iSeries Virtual I/O Message Path code
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ *
+ * This code is used by the iSeries virtual disk, cd,
+ * tape, and console to communicate with OS/400 in another
+ * partition.
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/config.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/proc_fs.h>
+#include <linux/pci.h>
+#include <linux/wait.h>
+
+#include <asm/iSeries/LparData.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvCallCfg.h>
+#include <asm/iSeries/mf.h>
+#include <asm/iSeries/iSeries_proc.h>
+
+#include "vio.h"
+
+EXPORT_SYMBOL(viopath_hostLp);
+EXPORT_SYMBOL(viopath_ourLp);
+EXPORT_SYMBOL(vio_set_hostlp);
+EXPORT_SYMBOL(vio_lookup_rc);
+EXPORT_SYMBOL(viopath_open);
+EXPORT_SYMBOL(viopath_close);
+EXPORT_SYMBOL(viopath_isactive);
+EXPORT_SYMBOL(viopath_sourceinst);
+EXPORT_SYMBOL(viopath_targetinst);
+EXPORT_SYMBOL(vio_setHandler);
+EXPORT_SYMBOL(vio_clearHandler);
+EXPORT_SYMBOL(vio_get_event_buffer);
+EXPORT_SYMBOL(vio_free_event_buffer);
+
+extern struct pci_dev *iSeries_vio_dev;
+
+/* Status of the path to each other partition in the system.
+ * This is overkill, since we will only ever establish connections
+ * to our hosting partition and the primary partition on the system.
+ * But this allows for other support in the future.
+ */
+static struct viopathStatus {
+	int isOpen:1;		/* Did we open the path?            */
+	int isActive:1;		/* Do we have a mon msg outstanding */
+	int users[VIO_MAX_SUBTYPES];
+	HvLpInstanceId mSourceInst;
+	HvLpInstanceId mTargetInst;
+	int numberAllocated;
+} viopathStatus[HVMAXARCHITECTEDLPS];
+
+static spinlock_t statuslock = SPIN_LOCK_UNLOCKED;
+
+/*
+ * For each kind of event we allocate a buffer that is
+ * guaranteed not to cross a page boundary
+ */
+static void *event_buffer[VIO_MAX_SUBTYPES] = { };
+static atomic_t event_buffer_available[VIO_MAX_SUBTYPES] = { };
+
+static void handleMonitorEvent(struct HvLpEvent *event);
+
+/* We use this structure to handle asynchronous responses.  The caller
+ * blocks on the semaphore and the handler posts the semaphore.
+ */
+struct doneAllocParms_t {
+	struct semaphore *sem;
+	int number;
+};
+
+/* Put a sequence number in each mon msg.  The value is not
+ * important.  Start at something other than 0 just for
+ * readability.  wrapping this is ok.
+ */
+static u8 viomonseq = 22;
+
+/* Our hosting logical partition.  We get this at startup
+ * time, and different modules access this variable directly.
+ */
+HvLpIndex viopath_hostLp = 0xff;	/* HvLpIndexInvalid */
+HvLpIndex viopath_ourLp = 0xff;
+
+/* For each kind of incoming event we set a pointer to a
+ * routine to call.
+ */
+static vio_event_handler_t *vio_handler[VIO_MAX_SUBTYPES];
+
+static unsigned char e2a(unsigned char x)
+{
+	switch (x) {
+	case 0xF0:
+		return '0';
+	case 0xF1:
+		return '1';
+	case 0xF2:
+		return '2';
+	case 0xF3:
+		return '3';
+	case 0xF4:
+		return '4';
+	case 0xF5:
+		return '5';
+	case 0xF6:
+		return '6';
+	case 0xF7:
+		return '7';
+	case 0xF8:
+		return '8';
+	case 0xF9:
+		return '9';
+	case 0xC1:
+		return 'A';
+	case 0xC2:
+		return 'B';
+	case 0xC3:
+		return 'C';
+	case 0xC4:
+		return 'D';
+	case 0xC5:
+		return 'E';
+	case 0xC6:
+		return 'F';
+	case 0xC7:
+		return 'G';
+	case 0xC8:
+		return 'H';
+	case 0xC9:
+		return 'I';
+	case 0xD1:
+		return 'J';
+	case 0xD2:
+		return 'K';
+	case 0xD3:
+		return 'L';
+	case 0xD4:
+		return 'M';
+	case 0xD5:
+		return 'N';
+	case 0xD6:
+		return 'O';
+	case 0xD7:
+		return 'P';
+	case 0xD8:
+		return 'Q';
+	case 0xD9:
+		return 'R';
+	case 0xE2:
+		return 'S';
+	case 0xE3:
+		return 'T';
+	case 0xE4:
+		return 'U';
+	case 0xE5:
+		return 'V';
+	case 0xE6:
+		return 'W';
+	case 0xE7:
+		return 'X';
+	case 0xE8:
+		return 'Y';
+	case 0xE9:
+		return 'Z';
+	}
+	return ' ';
+}
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	HvLpEvent_Rc hvrc;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	dma_addr_t dmaa =
+	    pci_map_single(iSeries_vio_dev, buf, PAGE_SIZE,
+			   PCI_DMA_FROMDEVICE);
+	int len = PAGE_SIZE;
+	int bufoff;
+
+	if (len > blen)
+		len = blen;
+
+	memset(buf, 0x00, len);
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_config |
+					     vioconfigget,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long)
+					     &Semaphore, VIOVERSION << 16,
+					     ((u64) dmaa) << 32, len, 0,
+					     0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk("viopath hv error on op %d\n", (int) hvrc);
+	}
+
+	down(&Semaphore);
+
+	pci_unmap_single(iSeries_vio_dev, dmaa, PAGE_SIZE,
+			 PCI_DMA_FROMDEVICE);
+
+	bufoff = strlen(buf);
+	snprintf(buf + bufoff, len - bufoff, "SRLNBR=%c%c%c%c%c%c%c\n",
+		 e2a(xItExtVpdPanel.mfgID[2]),
+		 e2a(xItExtVpdPanel.mfgID[3]),
+		 e2a(xItExtVpdPanel.systemSerial[1]),
+		 e2a(xItExtVpdPanel.systemSerial[2]),
+		 e2a(xItExtVpdPanel.systemSerial[3]),
+		 e2a(xItExtVpdPanel.systemSerial[4]),
+		 e2a(xItExtVpdPanel.systemSerial[5]));
+	*eof = 1;
+	return strlen(buf);
+}
+
+/* Handle writes to our proc file system
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	/* Doesn't do anything today!!!
+	 */
+	return count;
+}
+
+/* setup our proc file system entries
+ */
+static void vio_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("config", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/* See if a given LP is active.  Allow for invalid lps to be passed in
+ * and just return invalid
+ */
+int viopath_isactive(HvLpIndex lp)
+{
+	if (lp == HvLpIndexInvalid)
+		return 0;
+	if (lp < HVMAXARCHITECTEDLPS)
+		return viopathStatus[lp].isActive;
+	else
+		return 0;
+}
+
+/* We cache the source and target instance ids for each
+ * partition.  
+ */
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mSourceInst;
+}
+
+HvLpInstanceId viopath_targetinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mTargetInst;
+}
+
+/* Send a monitor message.  This is a message with the acknowledge
+ * bit on that the other side will NOT explicitly acknowledge.  When
+ * the other side goes down, the hypervisor will acknowledge any
+ * outstanding messages....so we will know when the other side dies.
+ */
+static void sendMonMsg(HvLpIndex remoteLp)
+{
+	HvLpEvent_Rc hvrc;
+
+	viopathStatus[remoteLp].mSourceInst =
+	    HvCallEvent_getSourceLpInstanceId(remoteLp,
+					      HvLpEvent_Type_VirtualIo);
+	viopathStatus[remoteLp].mTargetInst =
+	    HvCallEvent_getTargetLpInstanceId(remoteLp,
+					      HvLpEvent_Type_VirtualIo);
+
+	/* Deliberately ignore the return code here.  if we call this
+	 * more than once, we don't care.
+	 */
+	vio_setHandler(viomajorsubtype_monitor, handleMonitorEvent);
+
+	hvrc = HvCallEvent_signalLpEventFast(remoteLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_monitor,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_DeferredAck,
+					     viopathStatus[remoteLp].
+					     mSourceInst,
+					     viopathStatus[remoteLp].
+					     mTargetInst, viomonseq++,
+					     0, 0, 0, 0, 0);
+
+	if (hvrc == HvLpEvent_Rc_Good) {
+		viopathStatus[remoteLp].isActive = 1;
+	} else {
+		printk(KERN_WARNING_VIO
+		       "could not connect to partition %d\n", remoteLp);
+		viopathStatus[remoteLp].isActive = 0;
+	}
+}
+
+static void handleMonitorEvent(struct HvLpEvent *event)
+{
+	HvLpIndex remoteLp;
+	int i;
+
+	/* This handler is _also_ called as part of the loop
+	 * at the end of this routine, so it must be able to
+	 * ignore NULL events...
+	 */
+	if (!event)
+		return;
+
+	/* First see if this is just a normal monitor message from the
+	 * other partition
+	 */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		if (!viopathStatus[remoteLp].isActive)
+			sendMonMsg(remoteLp);
+		return;
+	}
+
+	/* This path is for an acknowledgement; the other partition
+	 * died
+	 */
+	remoteLp = event->xTargetLp;
+	if ((event->xSourceInstanceId !=
+	     viopathStatus[remoteLp].mSourceInst)
+	    || (event->xTargetInstanceId !=
+		viopathStatus[remoteLp].mTargetInst)) {
+		printk(KERN_WARNING_VIO
+		       "ignoring ack....mismatched instances\n");
+		return;
+	}
+
+	printk(KERN_WARNING_VIO "partition %d ended\n", remoteLp);
+
+	viopathStatus[remoteLp].isActive = 0;
+
+	/* For each active handler, pass them a NULL
+	 * message to indicate that the other partition
+	 * died
+	 */
+	for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+		if (vio_handler[i] != NULL)
+			(*vio_handler[i]) (NULL);
+	}
+}
+
+int vio_setHandler(int subtype, vio_event_handler_t * beh)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	if (vio_handler[subtype] != NULL)
+		return -EBUSY;
+
+	vio_handler[subtype] = beh;
+	return 0;
+}
+
+int vio_clearHandler(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	if (vio_handler[subtype] == NULL)
+		return -EAGAIN;
+
+	vio_handler[subtype] = NULL;
+	return 0;
+}
+
+static void handleConfig(struct HvLpEvent *event)
+{
+	if (!event)
+		return;
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "unexpected config request from partition %d",
+		       event->xSourceLp);
+
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	up((struct semaphore *) event->xCorrelationToken);
+}
+
+/* Initialization of the hosting partition
+ */
+void vio_set_hostlp(void)
+{
+	/* If this has already been set then we DON'T want to either change
+	 * it or re-register the proc file system
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid)
+		return;
+
+	/* Figure out our hosting partition.  This isn't allowed to change
+	 * while we're active
+	 */
+	viopath_ourLp = HvLpConfig_getLpIndex();
+	viopath_hostLp = HvCallCfg_getHostingLpIndex(viopath_ourLp);
+
+	/* If we have a valid hosting LP, create a proc file system entry
+	 * for config information
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid) {
+		iSeries_proc_callback(&vio_proc_init);
+		vio_setHandler(viomajorsubtype_config, handleConfig);
+	}
+}
+
+static void vio_handleEvent(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	HvLpIndex remoteLp;
+	int subtype =
+	    (event->
+	     xSubtype & VIOMAJOR_SUBTYPE_MASK) >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		/* The isActive is checked because if the hosting partition
+		 * went down and came back up it would not be active but it would have
+		 * different source and target instances, in which case we'd want to
+		 * reset them.  This case really protects against an unauthorized
+		 * active partition sending interrupts or acks to this linux partition.
+		 */
+		if (viopathStatus[remoteLp].isActive
+		    && (event->xSourceInstanceId !=
+			viopathStatus[remoteLp].mTargetInst)) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (viopathStatus[remoteLp].isActive
+		    && (event->xTargetInstanceId !=
+			viopathStatus[remoteLp].mSourceInst)) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	} else {
+		remoteLp = event->xTargetLp;
+		if (event->xSourceInstanceId !=
+		    viopathStatus[remoteLp].mSourceInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "ack msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (event->xTargetInstanceId !=
+		    viopathStatus[remoteLp].mTargetInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "viopath: ack msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	}
+
+	if (vio_handler[subtype] == NULL) {
+		printk(KERN_WARNING_VIO
+		       "unexpected virtual io event subtype %d from partition %d\n",
+		       event->xSubtype, remoteLp);
+		/* No handler.  Ack if necessary
+		 */
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	/* This innocuous little line is where all the real work happens
+	 */
+	(*vio_handler[subtype]) (event);
+}
+
+static void viopath_donealloc(void *parm, int number)
+{
+	struct doneAllocParms_t *doneAllocParmsp =
+	    (struct doneAllocParms_t *) parm;
+	doneAllocParmsp->number = number;
+	up(doneAllocParmsp->sem);
+}
+
+static int allocateEvents(HvLpIndex remoteLp, int numEvents)
+{
+	struct doneAllocParms_t doneAllocParms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	doneAllocParms.sem = &Semaphore;
+
+	mf_allocateLpEvents(remoteLp, HvLpEvent_Type_VirtualIo, 250,	/* It would be nice to put a real number here! */
+			    numEvents,
+			    &viopath_donealloc, &doneAllocParms);
+
+	down(&Semaphore);
+
+	return doneAllocParms.number;
+}
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	int i;
+	unsigned long flags;
+	void *tempEventBuffer = NULL;
+	int tempNumAllocated;
+
+	if ((remoteLp >= HvMaxArchitectedLps)
+	    || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	/*
+	 * NOTE: If VIO_MAX_SUBTYPES exceeds 16 then we need
+	 * to allocate more than one page for the event_buffer.
+	 */
+	if (event_buffer[0] == NULL) {
+		if (VIO_MAX_SUBTYPES <= 16) {
+			tempEventBuffer =
+			    (void *) get_free_page(GFP_KERNEL);
+			if (tempEventBuffer == NULL)
+				return -ENOMEM;
+		} else {
+			printk(KERN_INFO_VIO
+			       "VIO_MAX_SUBTYPES > 16. Need more space.");
+			return -ENOMEM;
+		}
+	}
+
+	spin_lock_irqsave(&statuslock, flags);
+
+	/*
+	 * OK...we can fit 16 maximum-sized events (256 bytes) in
+	 * each page (4096).
+	 */
+	if (event_buffer[0] == NULL) {
+		event_buffer[0] = tempEventBuffer;
+		atomic_set(&event_buffer_available[0], 1);
+		/*
+		 * Start at the second element because we've already
+		 * set the pointer for the first element and set the
+		 * pointers for every 256 bytes in the page we
+		 * allocated earlier.
+		 */
+		for (i = 1; i < VIO_MAX_SUBTYPES; i++) {
+			event_buffer[i] = event_buffer[i - 1] + 256;
+			atomic_set(&event_buffer_available[i], 1);
+		}
+	} else {
+		/*
+		 * While we were fetching the pages, which shouldn't
+		 * be done in a spin lock, another call to viopath_open
+		 * decided to do the same thing and allocated storage
+		 * and set the event_buffer before we could so we'll
+		 * free the one that we allocated and continue with our
+		 * viopath_open operation.
+		 */
+		free_page((unsigned long) tempEventBuffer);
+	}
+
+	viopathStatus[remoteLp].users[subtype]++;
+
+	if (!viopathStatus[remoteLp].isOpen) {
+		viopathStatus[remoteLp].isOpen = 1;
+		HvCallEvent_openLpEventPath(remoteLp,
+					    HvLpEvent_Type_VirtualIo);
+
+		spin_unlock_irqrestore(&statuslock, flags);
+		/*
+		 * Don't hold the spinlock during an operation that
+		 * can sleep.
+		 */
+		tempNumAllocated = allocateEvents(remoteLp, 1);
+		spin_lock_irqsave(&statuslock, flags);
+
+		viopathStatus[remoteLp].numberAllocated +=
+		    tempNumAllocated;
+
+		if (viopathStatus[remoteLp].numberAllocated == 0) {
+			HvCallEvent_closeLpEventPath(remoteLp,
+						     HvLpEvent_Type_VirtualIo);
+
+			spin_unlock_irqrestore(&statuslock, flags);
+			return -ENOMEM;
+		}
+
+		viopathStatus[remoteLp].mSourceInst =
+		    HvCallEvent_getSourceLpInstanceId(remoteLp,
+						      HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].mTargetInst =
+		    HvCallEvent_getTargetLpInstanceId(remoteLp,
+						      HvLpEvent_Type_VirtualIo);
+
+		HvLpEvent_registerHandler(HvLpEvent_Type_VirtualIo,
+					  &vio_handleEvent);
+
+		sendMonMsg(remoteLp);
+
+		printk(KERN_INFO_VIO
+		       "Opening connection to partition %d, setting sinst %d, tinst %d\n",
+		       remoteLp,
+		       viopathStatus[remoteLp].mSourceInst,
+		       viopathStatus[remoteLp].mTargetInst);
+	}
+
+	spin_unlock_irqrestore(&statuslock, flags);
+	tempNumAllocated = allocateEvents(remoteLp, numReq);
+	spin_lock_irqsave(&statuslock, flags);
+	viopathStatus[remoteLp].numberAllocated += tempNumAllocated;
+	spin_unlock_irqrestore(&statuslock, flags);
+
+	return 0;
+}
+
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	unsigned long flags;
+	int i;
+	int numOpen;
+	struct doneAllocParms_t doneAllocParms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	doneAllocParms.sem = &Semaphore;
+
+	if ((remoteLp >= HvMaxArchitectedLps)
+	    || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	spin_lock_irqsave(&statuslock, flags);
+	/*
+	 * If the viopath_close somehow gets called before a
+	 * viopath_open it could decrement to -1 which is a non
+	 * recoverable state so we'll prevent this from
+	 * happening.
+	 */
+	if (viopathStatus[remoteLp].users[subtype] > 0) {
+		viopathStatus[remoteLp].users[subtype]--;
+	}
+	spin_unlock_irqrestore(&statuslock, flags);
+
+	mf_deallocateLpEvents(remoteLp, HvLpEvent_Type_VirtualIo,
+			      numReq, &viopath_donealloc, &doneAllocParms);
+	down(&Semaphore);
+
+	spin_lock_irqsave(&statuslock, flags);
+	for (i = 0, numOpen = 0; i < VIO_MAX_SUBTYPES; i++) {
+		numOpen += viopathStatus[remoteLp].users[i];
+	}
+
+	if ((viopathStatus[remoteLp].isOpen) && (numOpen == 0)) {
+		printk(KERN_INFO_VIO
+		       "Closing connection to partition %d", remoteLp);
+
+		HvCallEvent_closeLpEventPath(remoteLp,
+					     HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].isOpen = 0;
+		viopathStatus[remoteLp].isActive = 0;
+
+		for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+			atomic_set(&event_buffer_available[i], 0);
+		}
+
+		/*
+		 * Precautionary check to make sure we don't
+		 * erroneously try to free a page that wasn't
+		 * allocated.
+		 */
+		if (event_buffer[0] != NULL) {
+			free_page((unsigned long) event_buffer[0]);
+			for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+				event_buffer[i] = NULL;
+			}
+		}
+
+	}
+	spin_unlock_irqrestore(&statuslock, flags);
+	return 0;
+}
+
+void *vio_get_event_buffer(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return NULL;
+
+	if (atomic_dec_if_positive(&event_buffer_available[subtype]) == 0)
+		return event_buffer[subtype];
+	else
+		return NULL;
+}
+
+void vio_free_event_buffer(int subtype, void *buffer)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES)) {
+		printk(KERN_WARNING_VIO
+		       "unexpected subtype %d freeing event buffer\n",
+		       subtype);
+		return;
+	}
+
+	if (atomic_read(&event_buffer_available[subtype]) != 0) {
+		printk(KERN_WARNING_VIO
+		       "freeing unallocated event buffer, subtype %d\n",
+		       subtype);
+		return;
+	}
+
+	if (buffer != event_buffer[subtype]) {
+		printk(KERN_WARNING_VIO
+		       "freeing invalid event buffer, subtype %d\n",
+		       subtype);
+	}
+
+	atomic_set(&event_buffer_available[subtype], 1);
+}
+
+static const struct vio_error_entry vio_no_error =
+    { 0, 0, "Non-VIO Error" };
+static const struct vio_error_entry vio_unknown_error =
+    { 0, EIO, "Unknown Error" };
+
+static const struct vio_error_entry vio_default_errors[] = {
+	{0x0001, EIO, "No Connection"},
+	{0x0002, EIO, "No Receiver"},
+	{0x0003, EIO, "No Buffer Available"},
+	{0x0004, EBADRQC, "Invalid Message Type"},
+	{0x0000, 0, NULL},
+};
+
+const struct vio_error_entry *vio_lookup_rc(const struct vio_error_entry
+					    *local_table, u16 rc)
+{
+	const struct vio_error_entry *cur;
+	if (!rc)
+		return &vio_no_error;
+	if (local_table)
+		for (cur = local_table; cur->rc; ++cur)
+			if (cur->rc == rc)
+				return cur;
+	for (cur = vio_default_errors; cur->rc; ++cur)
+		if (cur->rc == rc)
+			return cur;
+	return &vio_unknown_error;
+}
diff -urNp linux-341/drivers/iseries/viotape.c linux-342/drivers/iseries/viotape.c
--- linux-341/drivers/iseries/viotape.c
+++ linux-342/drivers/iseries/viotape.c
@@ -0,0 +1,1222 @@
+/* -*- linux-c -*-
+ *  drivers/char/viotape.c
+ *
+ *  iSeries Virtual Tape
+ ***************************************************************************
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ ***************************************************************************
+ * This routine provides access to tape drives owned and managed by an OS/400 
+ * partition running on the same box as this Linux partition.
+ *
+ * All tape operations are performed by sending messages back and forth to 
+ * the OS/400 partition.  The format of the messages is defined in
+ * iSeries/vio.h
+ * 
+ */
+
+
+#undef VIOT_DEBUG
+
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <asm/ioctls.h>
+#include <linux/mtio.h>
+#include <linux/pci.h>
+#include <linux/devfs_fs.h>
+#include <linux/devfs_fs_kernel.h>
+#include <asm/uaccess.h>
+
+#include "vio.h"
+#include <asm/iSeries/HvLpEvent.h>
+#include "asm/iSeries/HvCallEvent.h"
+#include "asm/iSeries/HvLpConfig.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+extern struct pci_dev * iSeries_vio_dev;
+
+static int viotape_major = 230;
+static int viotape_numdev;
+
+#define VIOTAPE_MAXREQ 1
+
+/* version number for viotape driver */
+static unsigned int version_major = 1;
+static unsigned int version_minor;
+
+static u64 sndMsgSeq;
+static u64 sndMsgAck;
+static u64 rcvMsgSeq;
+static u64 rcvMsgAck;
+
+/***************************************************************************
+ * The minor number follows the conventions of the SCSI tape drives.  The
+ * rewind and mode are encoded in the minor #.  We use this struct to break
+ * them out
+ ***************************************************************************/
+struct viot_devinfo_struct {
+	int major;
+	int minor;
+	int devno;
+	int mode;
+	int rewind;
+};
+
+#define VIOTAPOP_RESET          0
+#define VIOTAPOP_FSF	        1
+#define VIOTAPOP_BSF	        2
+#define VIOTAPOP_FSR	        3
+#define VIOTAPOP_BSR	        4
+#define VIOTAPOP_WEOF	        5
+#define VIOTAPOP_REW	        6
+#define VIOTAPOP_NOP	        7
+#define VIOTAPOP_EOM	        8
+#define VIOTAPOP_ERASE          9
+#define VIOTAPOP_SETBLK        10
+#define VIOTAPOP_SETDENSITY    11
+#define VIOTAPOP_SETPOS	       12
+#define VIOTAPOP_GETPOS	       13
+#define VIOTAPOP_SETPART       14
+#define VIOTAPOP_UNLOAD        15
+
+struct viotapelpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mTape;
+	u16 mFlags;
+	u32 mToken;
+	u64 mLen;
+	union {
+		struct {
+			u32 mTapeOp;
+			u32 mCount;
+		} tapeOp;
+		struct {
+			u32 mType;
+			u32 mResid;
+			u32 mDsreg;
+			u32 mGstat;
+			u32 mErreg;
+			u32 mFileNo;
+			u32 mBlkNo;
+		} getStatus;
+		struct {
+			u32 mBlkNo;
+		} getPos;
+	} u;
+};
+enum viotapesubtype {
+	viotapeopen = 0x0001,
+	viotapeclose = 0x0002,
+	viotaperead = 0x0003,
+	viotapewrite = 0x0004,
+	viotapegetinfo = 0x0005,
+	viotapeop = 0x0006,
+	viotapegetpos = 0x0007,
+	viotapesetpos = 0x0008,
+	viotapegetstatus = 0x0009
+};
+
+enum viotapeRc {
+	viotape_InvalidRange = 0x0601,
+	viotape_InvalidToken = 0x0602,
+	viotape_DMAError = 0x0603,
+	viotape_UseError = 0x0604,
+	viotape_ReleaseError = 0x0605,
+	viotape_InvalidTape = 0x0606,
+	viotape_InvalidOp = 0x0607,
+	viotape_TapeErr = 0x0608,
+
+	viotape_AllocTimedOut = 0x0640,
+	viotape_BOTEnc = 0x0641,
+	viotape_BlankTape = 0x0642,
+	viotape_BufferEmpty = 0x0643,
+	viotape_CleanCartFound = 0x0644,
+	viotape_CmdNotAllowed = 0x0645,
+	viotape_CmdNotSupported = 0x0646,
+	viotape_DataCheck = 0x0647,
+	viotape_DecompressErr = 0x0648,
+	viotape_DeviceTimeout = 0x0649,
+	viotape_DeviceUnavail = 0x064a,
+	viotape_DeviceBusy = 0x064b,
+	viotape_EndOfMedia = 0x064c,
+	viotape_EndOfTape = 0x064d,
+	viotape_EquipCheck = 0x064e,
+	viotape_InsufficientRs = 0x064f,
+	viotape_InvalidLogBlk = 0x0650,
+	viotape_LengthError = 0x0651,
+	viotape_LibDoorOpen = 0x0652,
+	viotape_LoadFailure = 0x0653,
+	viotape_NotCapable = 0x0654,
+	viotape_NotOperational = 0x0655,
+	viotape_NotReady = 0x0656,
+	viotape_OpCancelled = 0x0657,
+	viotape_PhyLinkErr = 0x0658,
+	viotape_RdyNotBOT = 0x0659,
+	viotape_TapeMark = 0x065a,
+	viotape_WriteProt = 0x065b
+};
+
+static const struct vio_error_entry viotape_err_table[] = {
+	{viotape_InvalidRange, EIO, "Internal error"},
+	{viotape_InvalidToken, EIO, "Internal error"},
+	{viotape_DMAError, EIO, "DMA error"},
+	{viotape_UseError, EIO, "Internal error"},
+	{viotape_ReleaseError, EIO, "Internal error"},
+	{viotape_InvalidTape, EIO, "Invalid tape device"},
+	{viotape_InvalidOp, EIO, "Invalid operation"},
+	{viotape_TapeErr, EIO, "Tape error"},
+	{viotape_AllocTimedOut, EBUSY, "Allocate timed out"},
+	{viotape_BOTEnc, EIO, "Beginning of tape encountered"},
+	{viotape_BlankTape, EIO, "Blank tape"},
+	{viotape_BufferEmpty, EIO, "Buffer empty"},
+	{viotape_CleanCartFound, ENOMEDIUM, "Cleaning cartridge found"},
+	{viotape_CmdNotAllowed, EIO, "Command not allowed"},
+	{viotape_CmdNotSupported, EIO, "Command not supported"},
+	{viotape_DataCheck, EIO, "Data check"},
+	{viotape_DecompressErr, EIO, "Decompression error"},
+	{viotape_DeviceTimeout, EBUSY, "Device timeout"},
+	{viotape_DeviceUnavail, EIO, "Device unavailable"},
+	{viotape_DeviceBusy, EBUSY, "Device busy"},
+	{viotape_EndOfMedia, ENOSPC, "End of media"},
+	{viotape_EndOfTape, ENOSPC, "End of tape"},
+	{viotape_EquipCheck, EIO, "Equipment check"},
+	{viotape_InsufficientRs, EOVERFLOW, "Insufficient tape resources"},
+	{viotape_InvalidLogBlk, EIO, "Invalid logical block location"},
+	{viotape_LengthError, EOVERFLOW, "Length error"},
+	{viotape_LibDoorOpen, EBUSY, "Door open"},
+	{viotape_LoadFailure, ENOMEDIUM, "Load failure"},
+	{viotape_NotCapable, EIO, "Not capable"},
+	{viotape_NotOperational, EIO, "Not operational"},
+	{viotape_NotReady, EIO, "Not ready"},
+	{viotape_OpCancelled, EIO, "Operation cancelled"},
+	{viotape_PhyLinkErr, EIO, "Physical link error"},
+	{viotape_RdyNotBOT, EIO, "Ready but not beginning of tape"},
+	{viotape_TapeMark, EIO, "Tape mark"},
+	{viotape_WriteProt, EROFS, "Write protection error"},
+	{0, 0, NULL},
+};
+
+/* Maximum # tapes we support
+ */
+#define VIOTAPE_MAX_TAPE 8
+#define MAX_PARTITIONS 4
+
+/* defines for current tape state */
+#define VIOT_IDLE 0
+#define VIOT_READING 1
+#define VIOT_WRITING 2
+
+/* Our info on the tapes
+ */
+struct tape_descr {
+	char rsrcname[10];
+	char type[4];
+	char model[3];
+};
+
+static struct tape_descr *viotape_unitinfo;
+static dma_addr_t viotape_unitinfo_token;
+
+static const char *lasterr[VIOTAPE_MAX_TAPE];
+
+static struct mtget viomtget[VIOTAPE_MAX_TAPE];
+
+/* maintain the current state of each tape (and partition)
+   so that we know when to write EOF marks.
+*/
+static struct {
+	unsigned char cur_part;
+	devfs_handle_t dev_handle;
+	struct {
+		unsigned char rwi;
+	} part_stat[MAX_PARTITIONS];
+} state[VIOTAPE_MAX_TAPE];
+
+/* We single-thread
+ */
+static struct semaphore reqSem;
+
+/* When we send a request, we use this struct to get the response back
+ * from the interrupt handler
+ */
+struct opStruct {
+	void *buffer;
+	dma_addr_t dmaaddr;
+	size_t count;
+	int rc;
+	struct semaphore *sem;
+	struct opStruct *next;
+};
+
+static spinlock_t opStructListLock;
+static struct opStruct *opStructList;
+
+/* forward declaration to resolve interdependence */
+static int chg_state(int index, unsigned char new_state,
+		     struct file *file);
+static void clearOpStructPool(void);
+
+/* Decode the kdev_t into its parts
+ */
+void getDevInfo(kdev_t dev, struct viot_devinfo_struct *devi)
+{
+	devi->major = MAJOR(dev);
+	devi->minor = MINOR(dev);
+	devi->devno = devi->minor & 0x1F;
+	devi->mode = (devi->minor & 0x60) >> 5;
+	/* if bit is set in the minor, do _not_ rewind automatically */
+	devi->rewind = !(devi->minor & 0x80);
+}
+
+/* This is called only from init, so no need for locking */
+static int addOpStructs(int structs)
+{
+	int i;
+	for(i = 0; i < structs; ++i) {
+		struct opStruct *newStruct = kmalloc(sizeof(*newStruct), GFP_KERNEL);
+		if(!newStruct) {
+			clearOpStructPool();
+			return -ENOMEM;
+		}
+		memset(newStruct, 0x00, sizeof(*newStruct));
+		newStruct->next = opStructList;
+		opStructList = newStruct;
+	}
+	return 0;
+}
+
+/* Likewise, this is only called from the exit and init paths */
+static void clearOpStructPool(void)
+{
+	while(opStructList) {
+		struct opStruct *toFree = opStructList;
+		opStructList = opStructList->next;
+		kfree(toFree);
+	}
+}
+
+/* Allocate an op structure from our pool
+ */
+static struct opStruct *getOpStruct(void)
+{
+	struct opStruct *retval;
+	unsigned long flags;
+
+	spin_lock_irqsave(&opStructListLock, flags);
+	retval = opStructList;
+	if(retval) {
+		opStructList = retval->next;
+		memset(retval, 0x00, sizeof(*retval));
+	}
+	spin_unlock_irqrestore(&opStructListLock, flags);
+
+	return retval;
+}
+
+/* Return an op structure to our pool
+ */
+static void freeOpStruct(struct opStruct *opStruct)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&opStructListLock, flags);
+	opStruct->next = opStructList;
+	opStructList = opStruct;
+	spin_unlock_irqrestore(&opStructListLock, flags);
+}
+
+/* Map our tape return codes to errno values
+ */
+int tapeRcToErrno(int tapeRc, char *operation, int tapeno)
+{
+	const struct vio_error_entry *err;
+	if(tapeRc == 0)
+		return 0;
+	err = vio_lookup_rc(viotape_err_table, tapeRc);
+
+	printk(KERN_WARNING_VIO "tape error 0x%04x on Device %d (%-10s): %s\n",
+	       tapeRc, tapeno, viotape_unitinfo[tapeno].rsrcname, err->msg);
+
+	lasterr[tapeno] = err->msg;
+
+	return -err->errno;
+}
+
+/* Handle reads from the proc file system.  
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+
+	len += sprintf(buf + len, "viotape driver version %d.%d\n",
+		       version_major, version_minor);
+
+	for (i = 0; i < viotape_numdev; i++) {
+
+		len +=
+		    sprintf(buf + len,
+			    "viotape device %d is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			    i, viotape_unitinfo[i].rsrcname,
+			    viotape_unitinfo[i].type,
+			    viotape_unitinfo[i].model);
+		if (lasterr[i])
+			len += sprintf(buf + len, "   last error: %s\n", lasterr[i]);
+	}
+
+	*eof = 1;
+	return len;
+}
+
+/* setup our proc file system entries
+ */
+void viotape_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viotape", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->owner = THIS_MODULE;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+}
+
+/* clean up our proc file system entries
+ */
+void viotape_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viotape", iSeries_proc);
+}
+
+/* Get info on all tapes from OS/400
+ */
+static int get_viotape_info(void)
+{
+	HvLpEvent_Rc hvrc;
+	int i;
+	struct opStruct *op = getOpStruct();
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	if (op == NULL)
+		return -ENOMEM;
+
+	viotape_unitinfo = pci_alloc_consistent(
+			iSeries_vio_dev,
+			sizeof(viotape_unitinfo[0]) * VIOTAPE_MAX_TAPE,
+			&viotape_unitinfo_token);
+	if (viotape_unitinfo == NULL) {
+		freeOpStruct(op);
+		return -ENOMEM;
+	}
+
+	memset(viotape_unitinfo, 0x00, sizeof(viotape_unitinfo[0]) * VIOTAPE_MAX_TAPE);
+	memset(lasterr, 0x00, sizeof(lasterr));
+
+	op->sem = &Semaphore;
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapegetinfo,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     viotape_unitinfo_token,
+					     sizeof(viotape_unitinfo[0]) * VIOTAPE_MAX_TAPE,
+					     0,
+					     0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "viotape hv error on op %d\n", (int) hvrc);
+		freeOpStruct(op);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+	freeOpStruct(op);
+
+	for (i = 0;
+	     ((i < VIOTAPE_MAX_TAPE) && (viotape_unitinfo[i].rsrcname[0]));
+	     i++) {
+		printk(KERN_INFO_VIO "found tape %10.10s\n",
+		       viotape_unitinfo[i].rsrcname);
+		viotape_numdev++;
+	}
+	return 0;
+}
+
+
+/* Write
+ */
+static ssize_t viotap_write(struct file *file, const char *buf,
+			    size_t count, loff_t * ppos)
+{
+	HvLpEvent_Rc hvrc;
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	unsigned short flags = file->f_flags;
+	struct opStruct *op = getOpStruct();
+	int noblock = ((flags & O_NONBLOCK) != 0);
+	int err;
+	struct viot_devinfo_struct devi;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	/* We need to make sure we can send a request.  We use
+	 * a semaphore to keep track of # requests in use.  If
+	 * we are non-blocking, make sure we don't block on the 
+	 * semaphore
+	 */
+	if (noblock) {
+		if (down_trylock(&reqSem)) {
+			freeOpStruct(op);
+			return -EWOULDBLOCK;
+		}
+	} else {
+		down(&reqSem);
+	}
+
+	/* Allocate a DMA buffer */
+	op->buffer = pci_alloc_consistent(iSeries_vio_dev, count, &op->dmaaddr);
+
+	if (op->buffer == NULL) {
+		printk(KERN_WARNING_VIO "tape error allocating dma buffer for len %ld\n", count);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	op->count = count;
+
+	/* Copy the data into the buffer */
+	err = copy_from_user(op->buffer, (const void *) buf, count);
+	if (err) {
+		printk(KERN_WARNING_VIO "tape: error on copy from user\n");
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	if (noblock) {
+		op->sem = NULL;
+	} else {
+		op->sem = &Semaphore;
+	}
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapewrite,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.
+					      devno << 48) | op->dmaaddr,
+					     count, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "viotape hv error on op %d\n", (int) hvrc);
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EIO;
+	}
+
+	if (noblock)
+		return count;
+
+	down(&Semaphore);
+
+	err = op->rc;
+
+	/* Free the buffer */
+	pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+
+	count = op->count;
+
+	freeOpStruct(op);
+	up(&reqSem);
+	if (err)
+		return tapeRcToErrno(err, "write", devi.devno);
+	else {
+		chg_state(devi.devno, VIOT_WRITING, file);
+		return count;
+	}
+}
+
+/* read
+ */
+static ssize_t viotap_read(struct file *file, char *buf, size_t count,
+			   loff_t * ptr)
+{
+	HvLpEvent_Rc hvrc;
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	unsigned short flags = file->f_flags;
+	struct opStruct *op = getOpStruct();
+	int noblock = ((flags & O_NONBLOCK) != 0);
+	int err;
+	struct viot_devinfo_struct devi;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	/* We need to make sure we can send a request.  We use
+	 * a semaphore to keep track of # requests in use.  If
+	 * we are non-blocking, make sure we don't block on the 
+	 * semaphore
+	 */
+	if (noblock) {
+		if (down_trylock(&reqSem)) {
+			freeOpStruct(op);
+			return -EWOULDBLOCK;
+		}
+	} else {
+		down(&reqSem);
+	}
+
+	chg_state(devi.devno, VIOT_READING, file);
+
+	/* Allocate a DMA buffer */
+	op->buffer = pci_alloc_consistent(iSeries_vio_dev, count, &op->dmaaddr);
+
+	if (op->buffer == NULL) {
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	op->count = count;
+
+	op->sem = &Semaphore;
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotaperead,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.
+					      devno << 48) | op->dmaaddr,
+					     count, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "tape hv error on op %d\n", (int) hvrc);
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (op->rc == 0) {
+		/* If we got data back        */
+		if (op->count) {
+			/* Copy the data into the buffer */
+			err = copy_to_user(buf, op->buffer, count);
+			if (err) {
+				printk(KERN_WARNING_VIO "error on copy_to_user\n");
+				pci_free_consistent(iSeries_vio_dev, count,
+						    op->buffer,
+						    op->dmaaddr);
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EFAULT;
+			}
+		}
+	}
+
+	err = op->rc;
+
+	/* Free the buffer */
+	pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+	count = op->count;
+
+	freeOpStruct(op);
+	up(&reqSem);
+	if (err)
+		return tapeRcToErrno(err, "read", devi.devno);
+	else
+		return count;
+}
+
+/* read
+ */
+static int viotap_ioctl(struct inode *inode, struct file *file,
+			unsigned int cmd, unsigned long arg)
+{
+	HvLpEvent_Rc hvrc;
+	int err;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	struct opStruct *op = getOpStruct();
+	struct viot_devinfo_struct devi;
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	down(&reqSem);
+
+	switch (cmd) {
+	case MTIOCTOP:{
+			struct mtop mtc;
+			u32 myOp;
+
+			/* inode is null if and only if we (the kernel) made the request */
+			if (inode == NULL)
+				memcpy(&mtc, (void *) arg,
+				       sizeof(struct mtop));
+			else if (copy_from_user
+				 ((char *) &mtc, (char *) arg,
+				  sizeof(struct mtop))) {
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EFAULT;
+			}
+
+			switch (mtc.mt_op) {
+			case MTRESET:
+				myOp = VIOTAPOP_RESET;
+				break;
+			case MTFSF:
+				myOp = VIOTAPOP_FSF;
+				break;
+			case MTBSF:
+				myOp = VIOTAPOP_BSF;
+				break;
+			case MTFSR:
+				myOp = VIOTAPOP_FSR;
+				break;
+			case MTBSR:
+				myOp = VIOTAPOP_BSR;
+				break;
+			case MTWEOF:
+				myOp = VIOTAPOP_WEOF;
+				break;
+			case MTREW:
+				myOp = VIOTAPOP_REW;
+				break;
+			case MTNOP:
+				myOp = VIOTAPOP_NOP;
+				break;
+			case MTEOM:
+				myOp = VIOTAPOP_EOM;
+				break;
+			case MTERASE:
+				myOp = VIOTAPOP_ERASE;
+				break;
+			case MTSETBLK:
+				myOp = VIOTAPOP_SETBLK;
+				break;
+			case MTSETDENSITY:
+				myOp = VIOTAPOP_SETDENSITY;
+				break;
+			case MTTELL:
+				myOp = VIOTAPOP_GETPOS;
+				break;
+			case MTSEEK:
+				myOp = VIOTAPOP_SETPOS;
+				break;
+			case MTSETPART:
+				myOp = VIOTAPOP_SETPART;
+				break;
+			case MTOFFL:
+				myOp = VIOTAPOP_UNLOAD;
+				break;
+			default:
+				printk(KERN_WARNING_VIO "viotape: MTIOCTOP called with invalid op 0x%x\n", mtc.mt_op);
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EIO;
+			}
+
+/* if we moved the head, we are no longer reading or writing */
+			switch (mtc.mt_op) {
+			case MTFSF:
+			case MTBSF:
+			case MTFSR:
+			case MTBSR:
+			case MTTELL:
+			case MTSEEK:
+			case MTREW:
+				chg_state(devi.devno, VIOT_IDLE, file);
+			}
+
+			op->sem = &Semaphore;
+			hvrc =
+			    HvCallEvent_signalLpEventFast(viopath_hostLp,
+							  HvLpEvent_Type_VirtualIo,
+							  viomajorsubtype_tape
+							  | viotapeop,
+							  HvLpEvent_AckInd_DoAck,
+							  HvLpEvent_AckType_ImmediateAck,
+							  viopath_sourceinst
+							  (viopath_hostLp),
+							  viopath_targetinst
+							  (viopath_hostLp),
+							  (u64) (unsigned
+								 long) op,
+							  VIOVERSION << 16,
+							  ((u64) devi.
+							   devno << 48), 0,
+							  (((u64) myOp) <<
+							   32) | mtc.
+							  mt_count, 0);
+			if (hvrc != HvLpEvent_Rc_Good) {
+				printk(KERN_WARNING_VIO "viotape hv error on op %d\n", (int) hvrc);
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EIO;
+			}
+			down(&Semaphore);
+			if (op->rc) {
+				freeOpStruct(op);
+				up(&reqSem);
+				return tapeRcToErrno(op->rc,
+						     "tape operation",
+						     devi.devno);
+			} else {
+				freeOpStruct(op);
+				up(&reqSem);
+				return 0;
+			}
+			break;
+		}
+
+	case MTIOCGET:
+		op->sem = &Semaphore;
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_tape |
+						     viotapegetstatus,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     op, VIOVERSION << 16,
+						     ((u64) devi.
+						      devno << 48), 0, 0,
+						     0);
+		if (hvrc != HvLpEvent_Rc_Good) {
+			printk(KERN_WARNING_VIO "viotape hv error on op %d\n", (int) hvrc);
+			freeOpStruct(op);
+			up(&reqSem);
+			return -EIO;
+		}
+		down(&Semaphore);
+
+		/* Operation is complete - grab the error code */
+		err = op->rc;
+		freeOpStruct(op);
+		up(&reqSem);
+
+		if (err) 
+			return tapeRcToErrno(err, "get status", devi.devno);
+
+		if (copy_to_user((void *) arg, &viomtget[dev], sizeof(viomtget[0])))
+			return -EFAULT;
+		break;
+	case MTIOCPOS:
+		printk(KERN_WARNING_VIO "Got an (unsupported) MTIOCPOS\n");
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EINVAL;
+	default:
+		printk(KERN_WARNING_VIO "viotape: got an unsupported ioctl 0x%0x\n", cmd);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/* Open
+ */
+static int viotap_open(struct inode *inode, struct file *file)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	HvLpEvent_Rc hvrc;
+	struct opStruct *op = getOpStruct();
+	struct viot_devinfo_struct devi;
+	int err;
+
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+// Note: We currently only support one mode!
+	if ((devi.devno >= viotape_numdev) || (devi.mode)) {
+		freeOpStruct(op);
+		return -ENODEV;
+	}
+
+	op->sem = &Semaphore;
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapeopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.devno << 48), 0,
+					     0, 0);
+
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "viotape bad rc on signalLpEvent %d\n", (int) hvrc);
+		freeOpStruct(op);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+	err = op->rc;
+	freeOpStruct(op);
+	if (err)
+		return tapeRcToErrno(err, "open", devi.devno);
+	else 
+		return 0;
+}
+
+
+/* Release
+ */
+static int viotap_release(struct inode *inode, struct file *file)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	HvLpEvent_Rc hvrc;
+	struct viot_devinfo_struct devi;
+	struct opStruct *op = getOpStruct();
+
+	if (op == NULL)
+		return -ENOMEM;
+	op->sem = &Semaphore;
+
+	getDevInfo(dev, &devi);
+
+	if (devi.devno >= viotape_numdev) {
+		freeOpStruct(op);
+		return -ENODEV;
+	}
+
+	chg_state(devi.devno, VIOT_IDLE, file);
+
+	if (devi.rewind) {
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_tape |
+						     viotapeop,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     op, VIOVERSION << 16,
+						     ((u64) devi.
+						      devno << 48), 0,
+						     ((u64) VIOTAPOP_REW)
+						     << 32, 0);
+		down(&Semaphore);
+
+		if (op->rc) {
+			tapeRcToErrno(op->rc, "rewind", devi.devno);
+		}
+	}
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapeclose,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.devno << 48), 0,
+					     0, 0);
+
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "viotape: bad rc on signalLpEvent %d\n", (int) hvrc);
+		freeOpStruct(op);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (op->rc) {
+		printk(KERN_WARNING_VIO "viotape: close failed\n");
+	}
+	freeOpStruct(op);
+	return 0;
+}
+
+struct file_operations viotap_fops = {
+	owner:THIS_MODULE,
+	read:viotap_read,
+	write:viotap_write,
+	ioctl:viotap_ioctl,
+	open:viotap_open,
+	release:viotap_release,
+};
+
+/* Handle interrupt events for tape
+ */
+static void vioHandleTapeEvent(struct HvLpEvent *event)
+{
+	int tapeminor;
+	struct opStruct *op;
+	struct viotapelpevent *tevent = (struct viotapelpevent *) event;
+
+	if (event == NULL) {
+	  /* Notification that a partition went away! */
+	  if (!viopath_isactive(viopath_hostLp)) {
+	    /* TODO! Clean up */
+	  }
+	  return;
+	}
+
+	tapeminor = event->xSubtype & VIOMINOR_SUBTYPE_MASK;
+	switch (tapeminor) {
+	case viotapegetinfo:
+	case viotapeopen:
+	case viotapeclose:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		op->rc = tevent->mSubTypeRc;
+		up(op->sem);
+		break;
+	case viotaperead:
+	case viotapewrite:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		op->rc = tevent->mSubTypeRc;
+		op->count = tevent->mLen;
+
+		if (op->sem) {
+			up(op->sem);
+		} else {
+			pci_free_consistent(iSeries_vio_dev, op->count, op->buffer, op->dmaaddr);
+			freeOpStruct(op);
+			up(&reqSem);
+		}
+		break;
+	case viotapeop:
+	case viotapegetpos:
+	case viotapesetpos:
+	case viotapegetstatus:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		if (op) {
+			op->count = tevent->u.tapeOp.mCount;
+			op->rc = tevent->mSubTypeRc;
+
+			if (op->sem) {
+				up(op->sem);
+			}
+		}
+		break;
+	default:
+		printk(KERN_WARNING_VIO "viotape: wierd ack\n");
+	}
+}
+
+
+/* Do initialization
+ */
+int __init viotap_init(void)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int rc;
+	char tapename[32];
+	int i;
+
+	printk(KERN_INFO_VIO "viotape driver version %d.%d\n", version_major, version_minor);
+
+	sndMsgSeq = sndMsgAck = 0;
+	rcvMsgSeq = rcvMsgAck = 0;
+	opStructList = NULL;
+	if((rc = addOpStructs(VIOTAPE_MAXREQ)) < 0) {
+		printk(KERN_WARNING_VIO "viotape: couldn't allocate op structs\n");
+		return rc;
+	}
+	spin_lock_init(&opStructListLock);
+
+	sema_init(&reqSem, VIOTAPE_MAXREQ);
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/*
+	 * Open to our hosting lp
+	 */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		clearOpStructPool();
+		return -ENODEV;
+	}
+
+	printk(KERN_INFO_VIO "viotape: init - open path to hosting (%d)\n", viopath_hostLp);
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+	if (rc) {
+		printk(KERN_WARNING_VIO "viotape: error on viopath_open to hostlp %d\n", rc);
+		clearOpStructPool();
+		return -EIO;
+	}
+
+	vio_setHandler(viomajorsubtype_tape, vioHandleTapeEvent);
+
+	printk(KERN_INFO_VIO "viotape major is %d\n", viotape_major);
+
+	if ((rc = devfs_register_chrdev(viotape_major, "viotape", &viotap_fops)) < 0) {
+		printk(KERN_WARNING_VIO "Error registering viotape device\n");
+		viopath_close(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+		vio_clearHandler(viomajorsubtype_tape);
+		clearOpStructPool();
+		return rc;
+	}
+
+	if((rc = get_viotape_info()) < 0) {
+		printk(KERN_WARNING_VIO "Unable to obtain virtual device information");
+		devfs_unregister_chrdev(viotape_major, "viotape");
+		viopath_close(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+		vio_clearHandler(viomajorsubtype_tape);
+		clearOpStructPool();
+		return rc;
+	}
+
+	for (i = 0; i < viotape_numdev; i++) {
+		int j;
+		state[i].cur_part = 0;
+		for (j = 0; j < MAX_PARTITIONS; ++j)
+			state[i].part_stat[j].rwi = VIOT_IDLE;
+		sprintf(tapename, "viotape%d", i);
+		state[i].dev_handle =
+		    devfs_register(NULL, tapename, DEVFS_FL_DEFAULT,
+				   viotape_major, i,
+				   S_IFCHR | S_IRUSR | S_IWUSR | S_IRGRP |
+				   S_IWGRP, &viotap_fops, NULL);
+		printk
+		    (KERN_INFO_VIO "viotape device %s is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+		     tapename, viotape_unitinfo[i].rsrcname,
+		     viotape_unitinfo[i].type, viotape_unitinfo[i].model);
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viotape_proc_init);
+
+	return 0;
+}
+
+/* Give a new state to the tape object
+ */
+static int chg_state(int index, unsigned char new_state, struct file *file)
+{
+	unsigned char *cur_state =
+	    &state[index].part_stat[state[index].cur_part].rwi;
+	int rc = 0;
+
+	/* if the same state, don't bother */
+	if (*cur_state == new_state)
+		return 0;
+
+	/* write an EOF if changing from writing to some other state */
+	if (*cur_state == VIOT_WRITING) {
+		struct mtop write_eof = { MTWEOF, 1 };
+		rc = viotap_ioctl(NULL, file, MTIOCTOP,
+				  (unsigned long) &write_eof);
+	}
+	*cur_state = new_state;
+	return rc;
+}
+
+/* Cleanup
+ */
+static void __exit viotap_exit(void)
+{
+	int i, ret;
+	for (i = 0; i < viotape_numdev; ++i)
+		devfs_unregister(state[i].dev_handle);
+	ret = devfs_unregister_chrdev(viotape_major, "viotape");
+	if (ret < 0)
+		printk(KERN_WARNING_VIO "Error unregistering device: %d\n", ret);
+	iSeries_proc_callback(&viotape_proc_delete);
+	if (viotape_unitinfo)
+		pci_free_consistent(iSeries_vio_dev, sizeof(viotape_unitinfo[0]) * VIOTAPE_MAX_TAPE, viotape_unitinfo, viotape_unitinfo_token);
+	viopath_close(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+	vio_clearHandler(viomajorsubtype_tape);
+	clearOpStructPool();
+}
+
+MODULE_LICENSE("GPL");
+module_init(viotap_init);
+module_exit(viotap_exit);
diff -urNp linux-341/drivers/net/Makefile linux-342/drivers/net/Makefile
--- linux-341/drivers/net/Makefile
+++ linux-342/drivers/net/Makefile
@@ -80,7 +80,6 @@ obj-$(CONFIG_SIS900) += sis900.o mii.o
 obj-$(CONFIG_DM9102) += dmfe.o
 obj-$(CONFIG_YELLOWFIN) += yellowfin.o
 obj-$(CONFIG_ACENIC) += acenic.o
-obj-$(CONFIG_VETH) += veth.o
 obj-$(CONFIG_NATSEMI) += natsemi.o
 obj-$(CONFIG_NS83820) += ns83820.o
 obj-$(CONFIG_STNIC) += stnic.o 8390.o
diff -urNp linux-341/fs/partitions/check.c linux-342/fs/partitions/check.c
--- linux-341/fs/partitions/check.c
+++ linux-342/fs/partitions/check.c
@@ -194,6 +194,20 @@ char *disk_name (struct gendisk *hd, int
 			sprintf(buf, "%s/d%dp%d", maj, disk, part);
 		return buf;
 	}
+#if defined(CONFIG_VIODASD) || defined(CONFIG_VIODASD_MODULE)
+	if (hd->major == VIODASD_MAJOR) {
+		if ((unit + 'a') > 'z') {
+			unit -= 26;
+			if (part)
+				sprintf(buf, "%s%c%c%d", maj, 'a' + unit / 26,
+						'a' + unit % 26, part);
+			else
+				sprintf(buf, "%s%c%c", maj, 'a' + unit / 26,
+						'a' + unit % 26);
+			return buf;
+		}
+	}
+#endif
 	if (part)
 		sprintf(buf, "%s%c%d", maj, unit+'a', part);
 	else
diff -urNp linux-341/include/linux/blk.h linux-342/include/linux/blk.h
--- linux-341/include/linux/blk.h
+++ linux-342/include/linux/blk.h
@@ -321,6 +321,22 @@ static void floppy_off(unsigned int nr);
 #define DEVICE_REQUEST i2ob_request
 #define DEVICE_NR(device) (MINOR(device)>>4)
 
+#elif (MAJOR_NR == VIODASD_MAJOR)
+ 
+#define DEVICE_NAME "viod"
+#define TIMEOUT_VALUE (25*HZ)
+#define DEVICE_REQUEST do_viodasd_request
+#define DEVICE_NR(device) (MINOR(device) >> 3)
+ 
+#elif (MAJOR_NR == VIOCD_MAJOR)
+
+#define DEVICE_NAME "viocd"
+#define TIMEOUT_VALUE (25*HZ)
+#define DEVICE_REQUEST do_viocd_request
+#define DEVICE_NR(device) (MINOR(device))
+#define DEVICE_ON(device)
+#define DEVICE_OFF(device)
+
 #elif (MAJOR_NR == COMPAQ_SMART2_MAJOR)
 
 #define DEVICE_NAME "ida"
diff -urNp linux-341/include/linux/major.h linux-342/include/linux/major.h
--- linux-341/include/linux/major.h
+++ linux-342/include/linux/major.h
@@ -118,6 +118,9 @@
 #define COMPAQ_CISS_MAJOR6      110
 #define COMPAQ_CISS_MAJOR7      111
 
+#define VIODASD_MAJOR           112
+#define VIOCD_MAJOR             113
+
 #define ATARAID_MAJOR		114
 
 #define DASD_MAJOR      94	/* Official assignations from Peter */
diff -urNp linux-341/init/do_mounts.c linux-342/init/do_mounts.c
--- linux-341/init/do_mounts.c
+++ linux-342/init/do_mounts.c
@@ -211,6 +211,16 @@ static struct dev_name_struct {
 	{ "cciss/c5d0p",0x6D00 },
 	{ "cciss/c6d0p",0x6E00 },
 	{ "cciss/c7d0p",0x6F00 },
+#if defined(CONFIG_PPC_ISERIES)
+        { "iseries/vda",0x7000 },
+        { "iseries/vdb",0x7008 },
+        { "iseries/vdc",0x7010 },
+        { "iseries/vdd",0x7018 },
+        { "iseries/vde",0x7020 },
+        { "iseries/vdf",0x7028 },
+        { "iseries/vdg",0x7030 },
+        { "iseries/vdh",0x7038 },
+#endif
 	{ "ataraid/d0p",0x7200 },
 	{ "ataraid/d1p",0x7210 },
 	{ "ataraid/d2p",0x7220 },
