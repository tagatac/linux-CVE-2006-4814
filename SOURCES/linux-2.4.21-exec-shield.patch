diff -urNp linux-1090/arch/i386/kernel/process.c linux-1100/arch/i386/kernel/process.c
--- linux-1090/arch/i386/kernel/process.c
+++ linux-1100/arch/i386/kernel/process.c
@@ -34,6 +34,8 @@
 #include <linux/reboot.h>
 #include <linux/init.h>
 #include <linux/mc146818rtc.h>
+#include <linux/mman.h>
+#include <linux/random.h>
 #include <linux/elfcore.h>
 #include <linux/version.h>
 
@@ -724,6 +726,8 @@ struct task_struct *  __switch_to(struct
 
 	unlazy_fpu(prev_p);
 
+	if (next_p->mm)
+		load_user_cs_desc(cpu, next_p->mm);
 
 #if CONFIG_X86_HIGH_ENTRY
 	/*
@@ -1004,3 +1008,167 @@ asmlinkage int sys_get_thread_area(struc
 	return 0;
 }
 
+/*
+ * Get a random word:
+ */
+static inline unsigned int get_random_int(void)
+{
+	unsigned int val = 0;
+
+	if (!exec_shield_randomize)
+		return 0;
+
+#ifdef CONFIG_X86_HAS_TSC
+	rdtscl(val);
+#endif
+	val += current->pid + jiffies + (int)&val;
+
+	/*
+	 * Use IP's RNG. It suits our purpose perfectly: it re-keys itself
+	 * every second, from the entropy pool (and thus creates a limited
+	 * drain on it), and uses halfMD4Transform within the second. We
+	 * also spice it with the TSC (if available), jiffies, PID and the
+	 * stack address:
+	 */
+	return secure_ip_id(val);
+}
+
+unsigned long arch_align_stack(unsigned long sp)
+{
+	if (current->flags & PF_RELOCEXEC)
+		sp -= ((get_random_int() % 1024) << 4);
+	return sp & ~0xf;
+}
+
+#if SHLIB_BASE >= 0x01000000
+# error SHLIB_BASE must be under 16MB!
+#endif
+
+static unsigned long randomize_range(unsigned long start, unsigned long end, unsigned long len)
+{
+	unsigned long range = end - len - start;
+	if (end <= start + len)
+		return 0;
+	return PAGE_ALIGN(get_random_int() % range + start);
+}
+
+unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags,
+		unsigned long prot)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	int ascii_shield = 0;
+	unsigned long tmp;
+
+	if (len > TASK_SIZE)
+		return -ENOMEM;
+
+	/*
+	 * Default to the old behavior if executable relocation
+	 * is disabled:
+	 */
+	if (!(current->flags & PF_RELOCEXEC))
+		prot &= ~PROT_EXEC;
+
+	if (!addr && (prot & PROT_EXEC) && !(flags & MAP_FIXED))
+		addr = randomize_range(SHLIB_BASE, 0x01000000, len);
+
+	if (addr) {
+		addr = PAGE_ALIGN(addr);
+		vma = find_vma(mm, addr);
+		if (TASK_SIZE - len >= addr &&
+		    (!vma || addr + len <= vma->vm_start))
+			return addr;
+	}
+	if (prot & PROT_EXEC) {
+		ascii_shield = 1;
+		addr = SHLIB_BASE;
+	} else
+search_upper:
+		addr = TASK_UNMAPPED_BASE;
+
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (TASK_SIZE - len < addr)
+			return -ENOMEM;
+		if (!vma || addr + len <= vma->vm_start) {
+			/*
+			 * Must not let a PROT_EXEC mapping get into the
+			 * brk area:
+			 */
+			if (ascii_shield && (addr + len > mm->brk)) {
+				ascii_shield = 0;
+				goto search_upper;
+			}
+			/*
+			 * Up until the brk area we randomize addresses
+			 * as much as possible:
+			 */
+			if (ascii_shield && (addr >= 0x01000000)) {
+				tmp = randomize_range(0x01000000, PAGE_ALIGN(max(mm->start_brk, 0x08000000UL)), len);
+				vma = find_vma(mm, tmp);
+				if (TASK_SIZE - len >= tmp &&
+				    (!vma || tmp + len <= vma->vm_start))
+					return tmp;
+			}
+			/*
+			 * Ok, randomization didnt work out - return
+			 * the result of the linear search:
+			 */
+			return addr;
+		}
+		addr = vma->vm_end;
+	}
+}
+
+void arch_add_exec_range(struct mm_struct *mm, unsigned long limit)
+{
+	if (limit > mm->context.exec_limit) {
+		mm->context.exec_limit = limit;
+		set_user_cs(&mm->context.user_cs, limit);
+		if (mm == current->mm)
+			load_user_cs_desc(smp_processor_id(), mm);
+	}
+}
+
+void arch_remove_exec_range(struct mm_struct *mm, unsigned long old_end)
+{
+	struct vm_area_struct *vma;
+	unsigned long limit = 0;
+
+	if (old_end == mm->context.exec_limit) {
+		for (vma = mm->mmap; vma; vma = vma->vm_next)
+			if ((vma->vm_flags & VM_EXEC) && (vma->vm_end > limit))
+				limit = vma->vm_end;
+
+		mm->context.exec_limit = limit;
+		set_user_cs(&mm->context.user_cs, limit);
+		if (mm == current->mm)
+			load_user_cs_desc(smp_processor_id(), mm);
+	}
+}
+
+void arch_flush_exec_range(struct mm_struct *mm)
+{
+	mm->context.exec_limit = 0;
+	set_user_cs(&mm->context.user_cs, 0);
+}
+
+/*
+ * Generate random brk address between 128MB and 196MB. (if the layout
+ * allows it.)
+ */
+void randomize_brk(unsigned long old_brk)
+{
+	unsigned long new_brk, range_start, range_end;
+
+	range_start = 0x08000000;
+	if (current->mm->brk >= range_start)
+		range_start = current->mm->brk;
+	range_end = range_start + 0x02000000;
+	new_brk = randomize_range(range_start, range_end, 0);
+	if (new_brk)
+		current->mm->brk = new_brk;
+}
+
diff -urNp linux-1090/arch/i386/kernel/setup.c linux-1100/arch/i386/kernel/setup.c
--- linux-1090/arch/i386/kernel/setup.c
+++ linux-1100/arch/i386/kernel/setup.c
@@ -2888,6 +2888,14 @@ static int __init x86_fxsr_setup(char * 
 __setup("nofxsr", x86_fxsr_setup);
 
 
+static int __init setup_exec_shield(char *str)
+{
+	get_option (&str, &exec_shield);
+
+	return 1;
+}
+__setup("exec-shield=", setup_exec_shield);
+
 /* Standard macro to see if a specific flag is changeable */
 static inline int flag_is_changeable_p(u32 flag)
 {
diff -urNp linux-1090/arch/i386/kernel/signal.c linux-1100/arch/i386/kernel/signal.c
--- linux-1090/arch/i386/kernel/signal.c
+++ linux-1100/arch/i386/kernel/signal.c
@@ -573,8 +573,6 @@ handle_signal(unsigned long sig, siginfo
 	}
 }
 
-int print_fatal_signals;
-
 /*
  * Note that 'init' is a special process: it doesn't get signals it doesn't
  * want to handle. Thus you cannot kill init even with a SIGKILL even by
diff -urNp linux-1090/arch/i386/kernel/traps.c linux-1100/arch/i386/kernel/traps.c
--- linux-1090/arch/i386/kernel/traps.c
+++ linux-1100/arch/i386/kernel/traps.c
@@ -444,6 +444,10 @@ DO_ERROR(11, SIGBUS,  "segment not prese
 DO_ERROR(12, SIGBUS,  "stack segment", stack_segment)
 DO_ERROR_INFO(17, SIGBUS, "alignment check", alignment_check, BUS_ADRALN, get_cr2())
 
+/*
+ * the original non-exec stack patch was written by
+ * Solar Designer <solar at openwall.com>. Thanks!
+ */
 asmlinkage void do_general_protection(struct pt_regs * regs, long error_code)
 {
 	if (regs->eflags & VM_MASK)
@@ -452,6 +456,46 @@ asmlinkage void do_general_protection(st
 	if (!(regs->xcs & 3))
 		goto gp_in_kernel;
 
+	/*
+	 * lazy-check for CS validity on exec-shield binaries:
+	 */
+	if (current->mm) {
+		int cpu = smp_processor_id();
+		struct desc_struct *desc1, *desc2;
+		struct vm_area_struct *vma;
+		unsigned long limit = 0;
+		
+		spin_lock(&current->mm->page_table_lock);
+		for (vma = current->mm->mmap; vma; vma = vma->vm_next)
+			if ((vma->vm_flags & VM_EXEC) && (vma->vm_end > limit))
+				limit = vma->vm_end;
+		spin_unlock(&current->mm->page_table_lock);
+
+		current->mm->context.exec_limit = limit;
+		set_user_cs(&current->mm->context.user_cs, limit);
+
+		desc1 = &current->mm->context.user_cs;
+		desc2 = cpu_gdt_table[cpu] + GDT_ENTRY_DEFAULT_USER_CS;
+
+		/*
+		 * The CS was not in sync - reload it and retry the
+		 * instruction. If the instruction still faults then
+		 * we wont hit this branch next time around.
+		 */
+		if (desc1->a != desc2->a || desc1->b != desc2->b) {
+			if (print_fatal_signals >= 3) {
+				printk("#GPF fixup (%ld[seg:%lx]) at %08lx, CPU#%d.\n", error_code, error_code/8, regs->eip, smp_processor_id());
+				printk(" exec_limit: %08lx, user_cs: %08lx/%08lx, CPU_cs: %08lx/%08lx.\n", current->mm->context.exec_limit, desc1->a, desc1->b, desc2->a, desc2->b);
+			}
+			load_user_cs_desc(cpu, current->mm);
+			return;
+		}
+	}
+	if (print_fatal_signals) {
+		printk("#GPF(%ld[seg:%lx]) at %08lx, CPU#%d.\n", error_code, error_code/8, regs->eip, smp_processor_id());
+		printk(" exec_limit: %08lx, user_cs: %08lx/%08lx.\n", current->mm->context.exec_limit, current->mm->context.user_cs.a, current->mm->context.user_cs.b);
+	}
+
 	current->thread.error_code = error_code;
 	current->thread.trap_no = 13;
 	force_sig(SIGSEGV, current);
diff -urNp linux-1090/arch/ia64/ia32/binfmt_elf32.c linux-1100/arch/ia64/ia32/binfmt_elf32.c
--- linux-1090/arch/ia64/ia32/binfmt_elf32.c
+++ linux-1100/arch/ia64/ia32/binfmt_elf32.c
@@ -47,7 +47,7 @@ extern void put_dirty_page (struct task_
 static void elf32_set_personality (void);
 
 #define ELF_PLAT_INIT(_r, load_addr)		ia64_elf32_init(_r)
-#define setup_arg_pages(bprm)		ia32_setup_arg_pages(bprm)
+#define setup_arg_pages(bprm, executable_stack)	ia32_setup_arg_pages(bprm, executable_stack)
 #define elf_map				elf32_map
 
 #undef SET_PERSONALITY
@@ -160,7 +160,7 @@ ia64_elf32_init (struct pt_regs *regs)
 }
 
 int
-ia32_setup_arg_pages (struct linux_binprm *bprm)
+ia32_setup_arg_pages (struct linux_binprm *bprm, int executable_stack)
 {
 	unsigned long stack_base;
 	struct vm_area_struct *mpnt;
@@ -183,7 +183,10 @@ ia32_setup_arg_pages (struct linux_binpr
 		mpnt->vm_start = PAGE_MASK & (unsigned long) bprm->p;
 		mpnt->vm_end = IA32_STACK_TOP;
 		mpnt->vm_page_prot = PAGE_COPY;
-		mpnt->vm_flags = VM_STACK_FLAGS;
+		if (executable_stack)
+			mpnt->vm_flags = VM_STACK_FLAGS | VM_EXEC;
+		else
+			mpnt->vm_flags = VM_STACK_FLAGS & ~VM_EXEC;
 		mpnt->vm_ops = NULL;
 		mpnt->vm_pgoff = 0;
 		mpnt->vm_file = NULL;
@@ -216,11 +219,15 @@ elf32_set_personality (void)
 }
 
 static unsigned long
-elf32_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type)
+elf32_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type, unsigned long total_size)
 {
 	unsigned long pgoff = (eppnt->p_vaddr) & ~IA32_PAGE_MASK;
+	unsigned long size = eppnt->p_filesz + pgoff;
 
-	return ia32_do_mmap(filep, (addr & IA32_PAGE_MASK), eppnt->p_filesz + pgoff, prot, type,
+	if (total_size)
+		size = total_size;
+
+	return ia32_do_mmap(filep, (addr & IA32_PAGE_MASK), size, prot, type,
 			    eppnt->p_offset - pgoff);
 }
 
diff -urNp linux-1090/arch/ia64/kernel/signal.c linux-1100/arch/ia64/kernel/signal.c
--- linux-1090/arch/ia64/kernel/signal.c
+++ linux-1100/arch/ia64/kernel/signal.c
@@ -51,7 +51,6 @@ register double f26 asm ("f26"); registe
 register double f28 asm ("f28"); register double f29 asm ("f29");
 register double f30 asm ("f30"); register double f31 asm ("f31");
 
-int print_fatal_signals;
 
 long
 ia64_rt_sigsuspend (sigset_t *uset, size_t sigsetsize, struct sigscratch *scr)
diff -urNp linux-1090/arch/ia64/kernel/sys_ia64.c linux-1100/arch/ia64/kernel/sys_ia64.c
--- linux-1090/arch/ia64/kernel/sys_ia64.c
+++ linux-1100/arch/ia64/kernel/sys_ia64.c
@@ -21,7 +21,7 @@
 
 unsigned long
 arch_get_unmapped_area (struct file *filp, unsigned long addr, unsigned long len,
-			unsigned long pgoff, unsigned long flags)
+			unsigned long pgoff, unsigned long flags, unsigned long executable)
 {
 	long map_shared = (flags & MAP_SHARED);
 	unsigned long align_mask = PAGE_SIZE - 1;
diff -urNp linux-1090/arch/ppc64/kernel/signal.c linux-1100/arch/ppc64/kernel/signal.c
--- linux-1090/arch/ppc64/kernel/signal.c
+++ linux-1100/arch/ppc64/kernel/signal.c
@@ -582,7 +582,6 @@ syscall_restart(struct pt_regs *regs, st
  */
 extern int do_signal32(sigset_t *oldset, struct pt_regs *regs);
 
-int print_fatal_signals;
 
 int
 do_signal(sigset_t *oldset, struct pt_regs *regs)
diff -urNp linux-1090/arch/ppc64/kernel/sys_ppc32.c linux-1100/arch/ppc64/kernel/sys_ppc32.c
--- linux-1090/arch/ppc64/kernel/sys_ppc32.c
+++ linux-1100/arch/ppc64/kernel/sys_ppc32.c
@@ -1154,10 +1154,12 @@ asmlinkage unsigned long sys32_mremap(un
 		if (new_addr > 0xf0000000UL - new_len)
 			goto out_sem;
 	} else if (addr > 0xf0000000UL - new_len) {
+		struct vm_area_struct *vma;
 		ret = -ENOMEM;
 		if (!(flags & MREMAP_MAYMOVE))
 			goto out_sem;
-		new_addr = get_unmapped_area (NULL, addr, new_len, 0, 0);
+		vma = find_vma(current->mm, addr);
+		new_addr = get_unmapped_area (NULL, addr, new_len, 0, 0, vma->vm_flags & VM_EXEC);
 		if (!new_addr)
 			goto out_sem;
 		flags |= MREMAP_FIXED;
diff -urNp linux-1090/arch/s390/kernel/signal.c linux-1100/arch/s390/kernel/signal.c
--- linux-1090/arch/s390/kernel/signal.c
+++ linux-1100/arch/s390/kernel/signal.c
@@ -460,7 +460,6 @@ handle_signal(unsigned long sig,
 	}
 }
 
-int print_fatal_signals;
 
 /*
  * Note that 'init' is a special process: it doesn't get signals it doesn't
diff -urNp linux-1090/arch/s390x/kernel/binfmt_elf32.c linux-1100/arch/s390x/kernel/binfmt_elf32.c
--- linux-1090/arch/s390x/kernel/binfmt_elf32.c
+++ linux-1100/arch/s390x/kernel/binfmt_elf32.c
@@ -180,7 +180,7 @@ struct elf_prpsinfo32
 
 #undef start_thread
 #define start_thread                    start_thread31 
-#define setup_arg_pages(bprm)           setup_arg_pages32(bprm)
+#define setup_arg_pages(bprm, exec_stk) setup_arg_pages32(bprm)
 #define elf_map				elf_map32
 
 MODULE_DESCRIPTION("Binary format loader for compatibility with 32bit Linux for S390 binaries,"
@@ -193,16 +193,19 @@ MODULE_AUTHOR("Gerhard Tonn <ton@de.ibm.
 #include "../../../fs/binfmt_elf.c"
 
 static unsigned long
-elf_map32 (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type)
+elf_map32 (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type, unsigned long total_size)
 {
 	unsigned long map_addr;
+	unsigned long size = eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr);
+
+	if(total_size)
+		size = total_size;
 
 	if(!addr)
 		addr = 0x40000000;
 
 	down_write(&current->mm->mmap_sem);
-	map_addr = do_mmap(filep, ELF_PAGESTART(addr),
-			   eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr), prot, type,
+	map_addr = do_mmap(filep, ELF_PAGESTART(addr), size, prot, type,
 			   eppnt->p_offset - ELF_PAGEOFFSET(eppnt->p_vaddr));
 	up_write(&current->mm->mmap_sem);
 	return(map_addr);
diff -urNp linux-1090/arch/s390x/kernel/signal.c linux-1100/arch/s390x/kernel/signal.c
--- linux-1090/arch/s390x/kernel/signal.c
+++ linux-1100/arch/s390x/kernel/signal.c
@@ -458,7 +458,6 @@ handle_signal(unsigned long sig,
 	}
 }
 
-int print_fatal_signals;
 
 /*
  * Note that 'init' is a special process: it doesn't get signals it doesn't
diff -urNp linux-1090/arch/x86_64/ia32/ia32_binfmt.c linux-1100/arch/x86_64/ia32/ia32_binfmt.c
--- linux-1090/arch/x86_64/ia32/ia32_binfmt.c
+++ linux-1100/arch/x86_64/ia32/ia32_binfmt.c
@@ -206,6 +206,9 @@ do {							\
 #define exit_elf_binfmt			exit_elf32_binfmt
 
 #define load_elf_binary load_elf32_binary
+#define exec_shield exec_shield32
+
+extern int exec_shield32;
 
 #undef CONFIG_BINFMT_ELF
 #ifdef CONFIG_BINFMT_ELF32
@@ -218,7 +221,7 @@ do {							\
 #endif
 
 #define ELF_PLAT_INIT(r, load_addr)		elf32_init(r)
-#define setup_arg_pages(bprm)		ia32_setup_arg_pages(bprm)
+#define setup_arg_pages(bprm, exec_stack)		ia32_setup_arg_pages(bprm, exec_stack)
 
 extern void load_gs_index(unsigned);
 
@@ -248,7 +251,7 @@ MODULE_AUTHOR("Eric Youngdale, Andi Klee
 #define elf_caddr_t __u32
 
 static void elf32_init(struct pt_regs *);
-int ia32_setup_arg_pages(struct linux_binprm *bprm);
+int ia32_setup_arg_pages(struct linux_binprm *bprm, int executable_stack);
 
 #include "../../../fs/binfmt_elf.c" 
 
@@ -276,7 +279,7 @@ static void elf32_init(struct pt_regs *r
 extern void put_dirty_page(struct task_struct * tsk, struct page *page, unsigned long address);
  
 
-int ia32_setup_arg_pages(struct linux_binprm *bprm)
+int ia32_setup_arg_pages(struct linux_binprm *bprm, int executable_stack)
 {
 	unsigned long stack_base;
 	struct vm_area_struct *mpnt;
@@ -298,7 +301,10 @@ int ia32_setup_arg_pages(struct linux_bi
 		mpnt->vm_mm = current->mm;
 		mpnt->vm_start = PAGE_MASK & (unsigned long) bprm->p;
 		mpnt->vm_end = IA32_STACK_TOP;		
-		mpnt->vm_flags = vm_stack_flags32; 
+		if (executable_stack) 
+			mpnt->vm_flags = vm_stack_flags32; 
+		else
+			mpnt->vm_flags = vm_stack_flags32 & ~VM_EXEC; 
 		mpnt->vm_page_prot = (mpnt->vm_flags & VM_EXEC) ? 
 			PAGE_COPY_EXEC : PAGE_COPY;
 		mpnt->vm_ops = NULL;
@@ -322,18 +328,22 @@ int ia32_setup_arg_pages(struct linux_bi
 	
 	return 0;
 }
+
 static unsigned long
-elf32_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type)
+elf32_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type, unsigned long total_size)
 {
 	unsigned long map_addr;
 	struct task_struct *me = current; 
+	unsigned long size = eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr);
 
 	if (prot & PROT_READ) 
 		prot |= PROT_EXEC; 
 
+	if (total_size)
+		size = total_size;
+
 	down_write(&me->mm->mmap_sem);
-	map_addr = do_mmap(filep, ELF_PAGESTART(addr),
-			   eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr), prot, 
+	map_addr = do_mmap(filep, ELF_PAGESTART(addr), size, prot, 
 			   type|MAP_32BIT,
 			   eppnt->p_offset - ELF_PAGEOFFSET(eppnt->p_vaddr));
 	up_write(&me->mm->mmap_sem);
diff -urNp linux-1090/arch/x86_64/kernel/process.c linux-1100/arch/x86_64/kernel/process.c
--- linux-1090/arch/x86_64/kernel/process.c
+++ linux-1100/arch/x86_64/kernel/process.c
@@ -816,7 +816,9 @@ static inline unsigned int get_random_in
 
 unsigned long arch_align_stack(unsigned long sp)
 {
-	return sp - ((get_random_int() % 65536) << 4);
+	if (current->flags & PF_RELOCEXEC)
+		sp -= ((get_random_int() % 1024) << 4);
+	return sp & ~0xf;
 }
 
 /*
diff -urNp linux-1090/arch/x86_64/kernel/setup.c linux-1100/arch/x86_64/kernel/setup.c
--- linux-1090/arch/x86_64/kernel/setup.c
+++ linux-1100/arch/x86_64/kernel/setup.c
@@ -428,6 +428,9 @@ void __init setup_arch(char **cmdline_p)
 #endif
 
 	num_mappedpages = end_pfn;
+
+	exec_shield = 1;	// exec_shield forced ON by default,
+				// PT_GNU_STACK can override
 }
 
 static int __init get_model_name(struct cpuinfo_x86 *c)
diff -urNp linux-1090/arch/x86_64/kernel/setup64.c linux-1100/arch/x86_64/kernel/setup64.c
--- linux-1090/arch/x86_64/kernel/setup64.c
+++ linux-1100/arch/x86_64/kernel/setup64.c
@@ -60,12 +60,14 @@ static int __init nonx_setup(char *str)
 		do_not_nx = 0; 
 		vm_data_default_flags &= ~VM_EXEC; 
 		vm_stack_flags &= ~VM_EXEC;  
+		exec_shield = 2;
 	} else if (!strncmp(str, "noforce",7) || !strncmp(str,"off",3)) { 
 		do_not_nx = (str[0] == 'o');
-		if (do_not_nx) 
+		if (do_not_nx)
 			__supported_pte_mask &= ~_PAGE_NX; 
 		vm_data_default_flags |= VM_EXEC; 
 		vm_stack_flags |= VM_EXEC;
+		exec_shield = do_not_nx ? 0 : 1;
 	}
 	return 1;
 } 
@@ -84,6 +86,9 @@ Valid options: 
    compat    (default) Imply PROT_EXEC for PROT_READ
 
 */
+
+int exec_shield32 = 1;
+
 static int __init nonx32_setup(char *str)
 {
 	char *s;
@@ -91,16 +96,21 @@ static int __init nonx32_setup(char *str
 		if (!strcmp(s, "all") || !strcmp(s,"on")) {
 			vm_data_default_flags32 &= ~VM_EXEC; 
 			vm_stack_flags32 &= ~VM_EXEC;  
+			exec_shield32 = 2;
 		} else if (!strcmp(s, "off")) { 
 			vm_data_default_flags32 |= VM_EXEC; 
 			vm_stack_flags32 |= VM_EXEC;  
+			exec_shield32 = 0;
 		} else if (!strcmp(s, "stack")) { 
 			vm_data_default_flags32 |= VM_EXEC; 
 			vm_stack_flags32 &= ~VM_EXEC;  		
+			exec_shield32 = 1;
 		} else if (!strcmp(s, "force")) { 
 			vm_force_exec32 = 0; 
+			exec_shield32 = 1;
 		} else if (!strcmp(s, "compat")) { 
 			vm_force_exec32 = PROT_EXEC;
+			exec_shield32 = 1;
 		} 
   	} 
   	return 1;
diff -urNp linux-1090/arch/x86_64/kernel/signal.c linux-1100/arch/x86_64/kernel/signal.c
--- linux-1090/arch/x86_64/kernel/signal.c
+++ linux-1100/arch/x86_64/kernel/signal.c
@@ -35,7 +35,6 @@
 
 #define _BLOCKABLE (~(sigmask(SIGKILL) | sigmask(SIGSTOP)))
 
-int print_fatal_signals;
 
 
 asmlinkage int do_signal(struct pt_regs *regs, sigset_t *oldset);
diff -urNp linux-1090/fs/binfmt_aout.c linux-1100/fs/binfmt_aout.c
--- linux-1090/fs/binfmt_aout.c
+++ linux-1100/fs/binfmt_aout.c
@@ -411,7 +411,7 @@ beyond_if:
 
 	set_brk(current->mm->start_brk, current->mm->brk);
 
-	retval = setup_arg_pages(bprm); 
+	retval = setup_arg_pages(bprm, 1); 
 	if (retval < 0) { 
 		/* Someone check-me: is this error path enough? */ 
 		send_sig(SIGKILL, current, 0); 
diff -urNp linux-1090/fs/binfmt_elf.c linux-1100/fs/binfmt_elf.c
--- linux-1090/fs/binfmt_elf.c
+++ linux-1100/fs/binfmt_elf.c
@@ -45,7 +45,7 @@
 
 static int load_elf_binary(struct linux_binprm * bprm, struct pt_regs * regs);
 static int load_elf_library(struct file*);
-static unsigned long elf_map (struct file *, unsigned long, struct elf_phdr *, int, int);
+static unsigned long elf_map (struct file *, unsigned long, struct elf_phdr *, int, int, unsigned long);
 extern int dump_fpu (struct pt_regs *, elf_fpregset_t *);
 extern void dump_thread(struct pt_regs *, struct user *);
 
@@ -116,7 +116,8 @@ create_elf_tables(char *p, int argc, int
 		  struct elfhdr * exec,
 		  unsigned long load_addr,
 		  unsigned long load_bias,
-		  unsigned long interp_load_addr, int ibcs)
+		  unsigned long interp_load_addr, int ibcs,
+		  int exec_stack)
 {
 	elf_caddr_t *argv;
 	elf_caddr_t *envp;
@@ -153,13 +154,16 @@ create_elf_tables(char *p, int argc, int
 	 * processors. This keeps Mr Marcelo Person happier but should be
 	 * removed for 2.5
 	 */
-	 
-	sp = (void *) (u_platform - (((current->pid+jiffies) % 64) << 7));
+	sp = (void *)(u_platform - (((current->pid+jiffies) % 64) << 7));
+#ifdef __HAVE_ARCH_ALIGN_STACK
+	sp = (void *)arch_align_stack((unsigned long)sp);
+#endif
 
 	/*
 	 * Force 16 byte _final_ alignment here for generality.
 	 */
-	sp = (elf_addr_t *)(~15UL & (unsigned long)(sp));
+	sp = (elf_addr_t *)(~15UL & (unsigned long)sp);
+
 	csp = sp;
 	csp -= (1+DLINFO_ITEMS)*2 + (k_platform ? 2 : 0);
 #ifdef DLINFO_ARCH_ITEMS
@@ -244,14 +248,18 @@ create_elf_tables(char *p, int argc, int
 
 #ifndef elf_map
 
-static inline unsigned long
-elf_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type)
+static unsigned long
+elf_map (struct file *filep, unsigned long addr, struct elf_phdr *eppnt, int prot, int type, unsigned long total_size)
 {
 	unsigned long map_addr;
+	unsigned long size = eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr);
 
+	if (total_size && !(type & MAP_FIXED))
+		size = total_size;
+
+	addr = ELF_PAGESTART(addr);
 	down_write(&current->mm->mmap_sem);
-	map_addr = do_mmap(filep, ELF_PAGESTART(addr),
-			   eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr), prot, type,
+	map_addr = do_mmap(filep, ELF_PAGESTART(addr), size, prot, type,
 			   eppnt->p_offset - ELF_PAGEOFFSET(eppnt->p_vaddr));
 	up_write(&current->mm->mmap_sem);
 	return(map_addr);
@@ -259,6 +267,24 @@ elf_map (struct file *filep, unsigned lo
 
 #endif /* !elf_map */
 
+static inline unsigned long total_mapping_size(struct elf_phdr *cmds, int nr)
+{
+	int i, first_idx = -1, last_idx = -1;
+
+	for (i = 0; i < nr; i++)
+		if (cmds[i].p_type == PT_LOAD) {
+			last_idx = i;
+			if (first_idx == -1)
+				first_idx = i;
+		}
+
+	if (first_idx == -1)
+		return 0;
+
+	return cmds[last_idx].p_vaddr + cmds[last_idx].p_memsz -
+				ELF_PAGESTART(cmds[first_idx].p_vaddr);
+}
+
 /* This is much more generalized than the library routine read function,
    so we keep this separate.  Technically the library read function
    is only provided so that we can read a.out libraries that have
@@ -274,6 +300,7 @@ static unsigned long load_elf_interp(str
 	int load_addr_set = 0;
 	unsigned long last_bss = 0, elf_bss = 0;
 	unsigned long error = ~0UL;
+	unsigned long total_size;
 	int retval, i, size;
 
 	/* First of all, some simple consistency checks */
@@ -311,6 +338,10 @@ static unsigned long load_elf_interp(str
 		goto out_close;
 	}
 
+	total_size = total_mapping_size(elf_phdata, interp_elf_ex->e_phnum);
+	if (!total_size)
+		goto out_close;
+
 	eppnt = elf_phdata;
 	for (i=0; i<interp_elf_ex->e_phnum; i++, eppnt++) {
 	  if (eppnt->p_type == PT_LOAD) {
@@ -326,7 +357,9 @@ static unsigned long load_elf_interp(str
 	    if (interp_elf_ex->e_type == ET_EXEC || load_addr_set)
 	    	elf_type |= MAP_FIXED;
 
-	    map_addr = elf_map(interpreter, load_addr + vaddr, eppnt, elf_prot, elf_type);
+	    map_addr = elf_map(interpreter, load_addr + vaddr, eppnt, elf_prot,
+				elf_type, total_size);
+	    total_size = 0UL;
 	    if (BAD_ADDR(map_addr)) {
 	        error = map_addr;
 	    	goto out_close;
@@ -458,7 +491,7 @@ static int load_elf_binary(struct linux_
 	struct elf_phdr * elf_ppnt, *elf_phdata;
 	unsigned long elf_bss, k, elf_brk;
 	int elf_exec_fileno;
-	int retval, i;
+	int retval, i, dumpable = 1;
 	unsigned int size;
 	unsigned long elf_entry, interp_load_addr = 0;
 	unsigned long start_code, end_code, start_data, end_data;
@@ -467,6 +500,8 @@ static int load_elf_binary(struct linux_
 	struct elfhdr interp_elf_ex;
   	struct exec interp_ex;
 	char passed_fileno[6];
+	int exec_stack, relocexec, old_relocexec = current->flags & PF_RELOCEXEC;
+	unsigned long total_size;
 	
 	/* Get the exec-header */
 	elf_ex = *((struct elfhdr *) bprm->buf);
@@ -563,6 +598,8 @@ static int load_elf_binary(struct linux_
 			retval = PTR_ERR(interpreter);
 			if (IS_ERR(interpreter))
 				goto out_free_interp;
+			if (permission(interpreter->f_dentry->d_inode, MAY_READ) != 0)
+				dumpable = 0;
 			retval = kernel_read(interpreter, 0, bprm->buf, BINPRM_BUF_SIZE);
 			if (retval != BINPRM_BUF_SIZE) {
 				if (retval >= 0)
@@ -578,6 +615,30 @@ static int load_elf_binary(struct linux_
 		elf_ppnt++;
 	}
 
+	relocexec = 0;
+	exec_stack = 1;
+
+	if (current->personality == PER_LINUX)
+	switch (exec_shield) {
+	case 1:
+		elf_ppnt = elf_phdata;
+		for (i = 0; i < elf_ex.e_phnum; i++, elf_ppnt++)
+			if (elf_ppnt->p_type == PT_GNU_STACK) {
+				if (!(elf_ppnt->p_flags & PF_X))
+					exec_stack = 0;
+				current->flags |= PF_RELOCEXEC;
+				relocexec = PF_RELOCEXEC;
+				break;
+			}
+		break;
+
+	case 2:
+		exec_stack = 0;
+		current->flags |= PF_RELOCEXEC;
+		relocexec = PF_RELOCEXEC;
+		break;
+	}
+
 	/* Some simple consistency checks for the interpreter */
 	if (elf_interpreter) {
 		interpreter_type = INTERPRETER_ELF | INTERPRETER_AOUT;
@@ -636,6 +697,18 @@ static int load_elf_binary(struct linux_
 	retval = flush_old_exec(bprm);
 	if (retval)
 		goto out_free_dentry;
+	if (!dumpable)
+		current->mm->dumpable = 0;
+	current->flags |= relocexec;
+
+#ifdef __i386__
+	/*
+	 * Turn off the CS limit completely if exec-shield disabled or
+	 * NX active:
+	 */
+	if (!exec_shield || use_nx)
+		arch_add_exec_range(current->mm, -1);
+#endif
 
 	/* OK, This is the point of no return */
 	current->mm->start_data = 0;
@@ -649,13 +722,22 @@ static int load_elf_binary(struct linux_
 	   change some of these later */
 	current->mm->rss = 0;
 	current->mm->free_area_cache = TASK_UNMAPPED_BASE;
-	setup_arg_pages(bprm);
+
+	retval = setup_arg_pages(bprm, exec_stack);
 	current->mm->start_stack = bprm->p;
 
 	/* Now we do a little grungy work by mmaping the ELF image into
-	   the correct location in memory.  At this point, we assume that
-	   the image should be loaded at fixed address, not at a variable
-	   address. */
+	   the correct location in memory.
+	 
+	   We first calculate the total mapping size, which the first
+	   mmap request uses - after that point we use MAP_FIXED to
+	   overmap the existing range. This ensures that a continuous
+	   VM region is allocated - even if VM addresses are randomized.
+	 */
+
+	total_size = total_mapping_size(elf_phdata, elf_ex.e_phnum);
+	if (!total_size)
+		goto out_free_dentry;
 
 	for(i = 0, elf_ppnt = elf_phdata; i < elf_ex.e_phnum; i++, elf_ppnt++) {
 		int elf_prot = 0, elf_flags;
@@ -692,16 +774,13 @@ static int load_elf_binary(struct linux_
 		elf_flags = MAP_PRIVATE|MAP_DENYWRITE|MAP_EXECUTABLE;
 
 		vaddr = elf_ppnt->p_vaddr;
-		if (elf_ex.e_type == ET_EXEC || load_addr_set) {
+		if (elf_ex.e_type == ET_EXEC || load_addr_set)
 			elf_flags |= MAP_FIXED;
-		} else if (elf_ex.e_type == ET_DYN) {
-			/* Try and get dynamic programs out of the way of the default mmap
-			   base, as well as whatever program they might try to exec.  This
-		           is because the brk will follow the loader, and is not movable.  */
-			load_bias = ELF_PAGESTART(ELF_ET_DYN_BASE - vaddr);
-		}
+		else if (elf_ex.e_type == ET_DYN)
+			load_bias = 0;
 
-		error = elf_map(bprm->file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags);
+		error = elf_map(bprm->file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size);
+		total_size = 0UL;
 		if (BAD_ADDR(error)) {
 			send_sig(SIGKILL, current, 0);
 			goto out_free_dentry;
@@ -811,7 +890,8 @@ static int load_elf_binary(struct linux_
 			&elf_ex,
 			load_addr, load_bias,
 			interp_load_addr,
-			(interpreter_type == INTERPRETER_AOUT ? 0 : 1));
+			(interpreter_type == INTERPRETER_AOUT ? 0 : 1),
+			exec_stack);
 	/* N.B. passed_fileno might not be initialized? */
 	if (interpreter_type == INTERPRETER_AOUT)
 		current->mm->arg_start += strlen(passed_fileno) + 1;
@@ -821,6 +901,11 @@ static int load_elf_binary(struct linux_
 	current->mm->end_data = end_data;
 	current->mm->start_stack = bprm->p;
 
+#ifdef __HAVE_ARCH_RANDOMIZE_BRK
+	if (current->flags & PF_RELOCEXEC)
+		randomize_brk(elf_brk);
+#endif
+
 #if 0
 	printk("(start_brk) %lx\n" , (long) current->mm->start_brk);
 	printk("(end_code) %lx\n" , (long) current->mm->end_code);
@@ -880,6 +965,8 @@ out_free_file:
 	sys_close(elf_exec_fileno);
 out_free_ph:
 	kfree(elf_phdata);
+	current->flags &= ~PF_RELOCEXEC;
+	current->flags |= old_relocexec;
 	goto out;
 }
 
diff -urNp linux-1090/fs/binfmt_som.c linux-1100/fs/binfmt_som.c
--- linux-1090/fs/binfmt_som.c
+++ linux-1100/fs/binfmt_som.c
@@ -250,7 +250,7 @@ do_load_som_binary(struct linux_binprm *
 
 	set_binfmt(&som_format);
 	compute_creds(bprm);
-	setup_arg_pages(bprm);
+	setup_arg_pages(bprm, 1);
 
 	create_som_tables(bprm);
 
diff -urNp linux-1090/fs/exec.c linux-1100/fs/exec.c
--- linux-1090/fs/exec.c
+++ linux-1100/fs/exec.c
@@ -314,7 +314,7 @@ void put_dirty_page(struct task_struct *
 	lru_cache_add(page);
 	flush_dcache_page(page);
 	flush_page_to_ram(page);
-	set_pte(pte, pte_mkdirty(pte_mkwrite(mk_pte(page, PAGE_COPY))));
+	set_pte(pte, pte_mkdirty(pte_mkwrite(mk_pte(page, vma->vm_page_prot))));
 	pte_chain = page_add_rmap(page, pte, pte_chain);
 	tsk->mm->rss++;
 	pte_unmap(pte);
@@ -332,14 +332,18 @@ out_nounlock:
 	return;
 }
 
-int setup_arg_pages(struct linux_binprm *bprm)
+int setup_arg_pages(struct linux_binprm *bprm, int executable_stack)
 {
 	unsigned long stack_base;
 	struct vm_area_struct *mpnt;
 	int i;
 
+#ifdef __HAVE_ARCH_ALIGN_STACK
+	stack_base = arch_align_stack(STACK_TOP - MAX_ARG_PAGES*PAGE_SIZE);
+	stack_base = PAGE_ALIGN(stack_base);
+#else
 	stack_base = STACK_TOP - MAX_ARG_PAGES*PAGE_SIZE;
-
+#endif
 	bprm->p += stack_base;
 	if (bprm->loader)
 		bprm->loader += stack_base;
@@ -360,8 +364,15 @@ int setup_arg_pages(struct linux_binprm 
 		mpnt->vm_mm = current->mm;
 		mpnt->vm_start = PAGE_MASK & (unsigned long) bprm->p;
 		mpnt->vm_end = STACK_TOP;
+#ifdef PAGE_COPY_EXEC
+		mpnt->vm_page_prot = executable_stack ? PAGE_COPY_EXEC : PAGE_COPY;
+#else
 		mpnt->vm_page_prot = PAGE_COPY;
-		mpnt->vm_flags = VM_STACK_FLAGS;
+#endif
+		if (executable_stack)
+			mpnt->vm_flags = VM_STACK_FLAGS | VM_MAYEXEC | VM_EXEC;
+		else
+			mpnt->vm_flags = (VM_STACK_FLAGS | VM_MAYEXEC) & ~VM_EXEC;
 		mpnt->vm_ops = NULL;
 		mpnt->vm_pgoff = 0;
 		mpnt->vm_file = NULL;
@@ -734,6 +745,7 @@ int flush_old_exec(struct linux_binprm *
 	}
 	current->comm[i] = '\0';
 
+	current->flags &= ~PF_RELOCEXEC;
 	flush_thread();
 
 	if (bprm->e_uid != current->euid || bprm->e_gid != current->egid || 
@@ -787,8 +799,13 @@ int prepare_binprm(struct linux_binprm *
 
 	if(!(bprm->file->f_vfsmnt->mnt_flags & MNT_NOSUID)) {
 		/* Set-uid? */
-		if (mode & S_ISUID)
+		if (mode & S_ISUID) {
 			bprm->e_uid = inode->i_uid;
+#ifdef __i386__
+			/* reset personality */
+			current->personality = PER_LINUX;
+#endif
+		}
 
 		/* Set-gid? */
 		/*
@@ -796,8 +813,13 @@ int prepare_binprm(struct linux_binprm *
 		 * is a candidate for mandatory locking, not a setgid
 		 * executable.
 		 */
-		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP))
+		if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
 			bprm->e_gid = inode->i_gid;
+#ifdef __i386__
+			/* reset personality */
+			current->personality = PER_LINUX;
+#endif
+		}
 	}
 
 	/* We don't have VFS support for capabilities yet */
diff -urNp linux-1090/fs/proc/array.c linux-1100/fs/proc/array.c
--- linux-1090/fs/proc/array.c
+++ linux-1100/fs/proc/array.c
@@ -343,7 +343,10 @@ int proc_pid_stat(struct task_struct *ta
 		up_read(&mm->mmap_sem);
 	}
 
-	wchan = get_wchan(task);
+	wchan = 0;
+	if (current->uid == task->uid || current->euid == task->uid ||
+							capable(CAP_SYS_NICE))
+		wchan = get_wchan(task);
 
 	collect_sigign_sigcatch(task, &sigign, &sigcatch);
 
diff -urNp linux-1090/fs/proc/base.c linux-1100/fs/proc/base.c
--- linux-1090/fs/proc/base.c
+++ linux-1100/fs/proc/base.c
@@ -561,7 +561,7 @@ static struct pid_entry base_stuff[] = {
 #ifdef CONFIG_SMP
   E(PROC_PID_CPU,	"cpu",		S_IFREG|S_IRUGO),
 #endif
-  E(PROC_PID_MAPS,	"maps",		S_IFREG|S_IRUGO),
+  E(PROC_PID_MAPS,	"maps",		S_IFREG|S_IRUSR),
   E(PROC_PID_MEM,	"mem",		S_IFREG|S_IRUSR|S_IWUSR),
   E(PROC_PID_CWD,	"cwd",		S_IFLNK|S_IRWXUGO),
   E(PROC_PID_ROOT,	"root",		S_IFLNK|S_IRWXUGO),
diff -urNp linux-1090/include/asm-i386/elf.h linux-1100/include/asm-i386/elf.h
--- linux-1090/include/asm-i386/elf.h
+++ linux-1100/include/asm-i386/elf.h
@@ -118,4 +118,7 @@ extern void dump_smp_unlazy_fpu(void);
 
 #endif
 
+#define __HAVE_ARCH_RANDOMIZE_BRK
+extern void randomize_brk(unsigned long old_brk);
+
 #endif
diff -urNp linux-1090/include/asm-i386/mmu.h linux-1100/include/asm-i386/mmu.h
--- linux-1090/include/asm-i386/mmu.h
+++ linux-1100/include/asm-i386/mmu.h
@@ -6,6 +6,9 @@
  * we put the segment information here.
  *
  * cpu_vm_mask is used to optimize ldt flushing.
+ *
+ * exec_limit is used to track the range PROT_EXEC
+ * mappings span.
  */
 
 #define MAX_LDT_PAGES 16
@@ -14,6 +17,8 @@ typedef struct { 
 	int size;
 	struct semaphore sem;
 	struct page *ldt_pages[MAX_LDT_PAGES];
+	struct desc_struct user_cs;
+	unsigned long exec_limit;
 } mm_context_t;
 
 #endif
diff -urNp linux-1090/include/asm-i386/page.h linux-1100/include/asm-i386/page.h
--- linux-1090/include/asm-i386/page.h
+++ linux-1100/include/asm-i386/page.h
@@ -176,8 +176,10 @@ static __inline__ int get_order(unsigned
 #define virt_to_page(kaddr)     pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define VALID_PAGE(page)	((page - mem_map) < max_mapnr)
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS \
+		(VM_READ | VM_WRITE | \
+			((current->flags & PF_RELOCEXEC) ? 0 : VM_EXEC) | \
+				VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
 #endif /* __KERNEL__ */
 
diff -urNp linux-1090/include/asm-i386/pgalloc.h linux-1100/include/asm-i386/pgalloc.h
--- linux-1090/include/asm-i386/pgalloc.h
+++ linux-1100/include/asm-i386/pgalloc.h
@@ -4,6 +4,7 @@
 #include <linux/config.h>
 #include <asm/processor.h>
 #include <asm/fixmap.h>
+#include <asm/desc.h>
 #include <linux/threads.h>
 #include <linux/mm.h>		/* for struct page */
 
@@ -142,4 +143,20 @@ static inline void flush_tlb_pgtables(st
 	flush_tlb_mm(mm);
 }
 
+static inline void set_user_cs(struct desc_struct *desc, unsigned long limit)
+{
+	limit = (limit - 1) / PAGE_SIZE;
+	desc->a = limit & 0xffff;
+	desc->b = (limit & 0xf0000) | 0x00c0fb00;
+}
+
+#define load_user_cs_desc(cpu, mm) \
+    	cpu_gdt_table[(cpu)][GDT_ENTRY_DEFAULT_USER_CS] = (mm)->context.user_cs
+
+extern void arch_add_exec_range(struct mm_struct *mm, unsigned long limit);
+extern void arch_remove_exec_range(struct mm_struct *mm, unsigned long limit);
+extern void arch_flush_exec_range(struct mm_struct *mm);
+
+#define HAVE_ARCH_UNMAPPED_AREA 1
+
 #endif /* _I386_PGALLOC_H */
diff -urNp linux-1090/include/asm-i386/processor.h linux-1100/include/asm-i386/processor.h
--- linux-1090/include/asm-i386/processor.h
+++ linux-1100/include/asm-i386/processor.h
@@ -286,7 +286,12 @@ extern unsigned int mca_pentium_flag;
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
  */
-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 3))
+#define TASK_UNMAPPED_BASE	PAGE_ALIGN(TASK_SIZE/3)
+
+#define SHLIB_BASE		0x00111000
+ 
+#define __HAVE_ARCH_ALIGN_STACK
+extern unsigned long arch_align_stack(unsigned long);
 
 /*
  * Size of io_bitmap in longwords: 32 is ports 0-0x3ff.
@@ -444,6 +449,7 @@ extern int use_nx;
 	regs->xcs = __USER_CS;					\
 	regs->eip = new_eip;					\
 	regs->esp = new_esp;					\
+	load_user_cs_desc(smp_processor_id(), current->mm);	\
 } while (0)
 
 /* Forward declaration, a strange C thing */
diff -urNp linux-1090/include/asm-i386/system.h linux-1100/include/asm-i386/system.h
--- linux-1090/include/asm-i386/system.h
+++ linux-1100/include/asm-i386/system.h
@@ -53,6 +53,10 @@ __asm__ __volatile__ ("movw %%dx,%1\n\t"
 	 "0" (limit) \
         ); } while(0)
 
+struct desc_struct;
+
+extern void set_desc_limit(struct desc_struct *desc, unsigned long limit);
+
 #define set_base(ldt,base) _set_base( ((char *)&(ldt)) , (base) )
 #define set_limit(ldt,limit) _set_limit( ((char *)&(ldt)) , ((limit)-1)>>12 )
 
diff -urNp linux-1090/include/asm-ia64/ia32.h linux-1100/include/asm-ia64/ia32.h
--- linux-1090/include/asm-ia64/ia32.h
+++ linux-1100/include/asm-ia64/ia32.h
@@ -570,7 +570,7 @@ extern void ia32_gdt_init (void);
 extern int ia32_setup_frame1 (int sig, struct k_sigaction *ka, siginfo_t *info,
 			       sigset_t *set, struct pt_regs *regs);
 extern void ia32_init_addr_space (struct pt_regs *regs);
-extern int ia32_setup_arg_pages (struct linux_binprm *bprm);
+extern int ia32_setup_arg_pages (struct linux_binprm *bprm, int executable_stack);
 extern int ia32_exception (struct pt_regs *regs, unsigned long isr);
 extern int ia32_intercept (struct pt_regs *regs, unsigned long isr);
 extern unsigned long ia32_do_mmap (struct file *, unsigned long, unsigned long, int, int, loff_t);
diff -urNp linux-1090/include/asm-ia64/page.h linux-1100/include/asm-ia64/page.h
--- linux-1090/include/asm-ia64/page.h
+++ linux-1100/include/asm-ia64/page.h
@@ -68,6 +68,8 @@ extern void copy_page (void *to, void *f
 # define page_to_phys(page)	((page - mem_map) << PAGE_SHIFT)
 #endif
 
+#define pfn_to_page(pfn) 	(mem_map + (pfn))
+
 struct page;
 extern int ia64_page_valid (struct page *);
 #define VALID_PAGE(page)	(((page - mem_map) < max_mapnr) && ia64_page_valid(page))
diff -urNp linux-1090/include/asm-ia64/pgalloc.h linux-1100/include/asm-ia64/pgalloc.h
--- linux-1090/include/asm-ia64/pgalloc.h
+++ linux-1100/include/asm-ia64/pgalloc.h
@@ -327,4 +327,7 @@ update_mmu_cache (struct vm_area_struct 
 	set_bit(PG_arch_1, &page->flags);	/* mark page as clean */
 }
 
+#define arch_add_exec_range(mm, limit)  do { ; } while (0)
+#define arch_flush_exec_range(mm)       do { ; } while (0)
+
 #endif /* _ASM_IA64_PGALLOC_H */
diff -urNp linux-1090/include/asm-ia64/pgtable.h linux-1100/include/asm-ia64/pgtable.h
--- linux-1090/include/asm-ia64/pgtable.h
+++ linux-1100/include/asm-ia64/pgtable.h
@@ -269,6 +269,8 @@ extern unsigned long vmalloc_end;
 #define pte_mkclean(pte)	(__pte(pte_val(pte) & ~_PAGE_D))
 #define pte_mkdirty(pte)	(__pte(pte_val(pte) | _PAGE_D))
 
+#define pte_pfn(x) 		(pte_val(x) >> PAGE_SHIFT)
+
 /*
  * Macro to make mark a page protection value as "uncacheable".  Note
  * that "protection" is really a misnomer here as the protection value
diff -urNp linux-1090/include/asm-ppc64/pgalloc.h linux-1100/include/asm-ppc64/pgalloc.h
--- linux-1090/include/asm-ppc64/pgalloc.h
+++ linux-1100/include/asm-ppc64/pgalloc.h
@@ -116,4 +116,7 @@ pte_alloc_one(struct mm_struct *mm, unsi
 
 extern int do_check_pgt_cache(int, int);
 
+#define arch_add_exec_range(mm, limit) do { ; } while (0)
+#define arch_flush_exec_range(mm)      do { ; } while (0)
+
 #endif /* _PPC64_PGALLOC_H */
diff -urNp linux-1090/include/asm-s390/pgalloc.h linux-1100/include/asm-s390/pgalloc.h
--- linux-1090/include/asm-s390/pgalloc.h
+++ linux-1100/include/asm-s390/pgalloc.h
@@ -302,4 +302,7 @@ static inline void ptep_establish(struct
 	set_pte(ptep, entry);
 }
 
+#define arch_add_exec_range(mm, limit)	do { ; } while (0)
+#define arch_flush_exec_range(mm)	do { ; } while (0)
+
 #endif /* _S390_PGALLOC_H */
diff -urNp linux-1090/include/asm-s390x/pgalloc.h linux-1100/include/asm-s390x/pgalloc.h
--- linux-1090/include/asm-s390x/pgalloc.h
+++ linux-1100/include/asm-s390x/pgalloc.h
@@ -291,4 +291,7 @@ static inline void ptep_establish(struct
 	set_pte(ptep, entry);
 }
 
+#define arch_add_exec_range(mm, limit)	do { ; } while (0)
+#define arch_flush_exec_range(mm)	do { ; } while (0)
+
 #endif /* _S390_PGALLOC_H */
diff -urNp linux-1090/include/asm-x86_64/page.h linux-1100/include/asm-x86_64/page.h
--- linux-1090/include/asm-x86_64/page.h
+++ linux-1100/include/asm-x86_64/page.h
@@ -148,7 +148,8 @@ extern __inline__ int get_order(unsigned
 				 VM_ACCOUNT)
 
 #define VM_DATA_DEFAULT_FLAGS \
-	((current->thread.flags & THREAD_IA32) ? vm_data_default_flags32 : \
+	((current->thread.flags & THREAD_IA32) ? \
+	 (vm_data_default_flags32 & ~((current->flags & PF_RELOCEXEC) ? VM_EXEC : 0)): \
 	  vm_data_default_flags) 
 #define VM_STACK_FLAGS	vm_stack_flags
 
diff -urNp linux-1090/include/asm-x86_64/pgalloc.h linux-1100/include/asm-x86_64/pgalloc.h
--- linux-1090/include/asm-x86_64/pgalloc.h
+++ linux-1100/include/asm-x86_64/pgalloc.h
@@ -158,4 +158,7 @@ extern inline void flush_tlb_pgtables(st
 	flush_tlb_mm(mm);
 }
 
+#define arch_add_exec_range(mm, limit) do { } while (0)
+#define arch_flush_exec_range(mm)      do { } while (0)
+
 #endif /* _X86_64_PGALLOC_H */
diff -urNp linux-1090/include/asm-x86_64/pgtable.h linux-1100/include/asm-x86_64/pgtable.h
--- linux-1090/include/asm-x86_64/pgtable.h
+++ linux-1100/include/asm-x86_64/pgtable.h
@@ -317,6 +317,8 @@ extern void __handle_bad_pmd_kernel(pmd_
 #ifndef CONFIG_DISCONTIGMEM
 #define pte_page(x) (pfn_to_page((pte_val(x) & PHYSICAL_PAGE_MASK) >> PAGE_SHIFT))
 #endif
+#define pte_pfn(x) ((pte_val(x) & PHYSICAL_PAGE_MASK) >> PAGE_SHIFT)
+
 /*
  * The following only work if pte_present() is true.
  * Undefined behaviour if not..
diff -urNp linux-1090/include/linux/binfmts.h linux-1100/include/linux/binfmts.h
--- linux-1090/include/linux/binfmts.h
+++ linux-1100/include/linux/binfmts.h
@@ -56,7 +56,7 @@ extern int prepare_binprm(struct linux_b
 extern void remove_arg_zero(struct linux_binprm *);
 extern int search_binary_handler(struct linux_binprm *,struct pt_regs *);
 extern int flush_old_exec(struct linux_binprm * bprm);
-extern int setup_arg_pages(struct linux_binprm * bprm);
+extern int setup_arg_pages(struct linux_binprm * bprm, int exec_stack);
 extern int copy_strings(int argc,char ** argv,struct linux_binprm *bprm); 
 extern int copy_strings_kernel(int argc,char ** argv,struct linux_binprm *bprm);
 extern void compute_creds(struct linux_binprm *binprm);
diff -urNp linux-1090/include/linux/elf.h linux-1100/include/linux/elf.h
--- linux-1090/include/linux/elf.h
+++ linux-1100/include/linux/elf.h
@@ -35,6 +35,7 @@ typedef __s64	Elf64_Sxword;
 #define PT_LOPROC  0x70000000
 #define PT_HIPROC  0x7fffffff
 #define PT_GNU_EH_FRAME		0x6474e550
+#define PT_GNU_STACK		0x6474e551
 #define PT_MIPS_REGINFO		0x70000000
 
 /* Flags in the e_flags field of the header */
diff -urNp linux-1090/include/linux/mm.h linux-1100/include/linux/mm.h
--- linux-1090/include/linux/mm.h
+++ linux-1100/include/linux/mm.h
@@ -121,11 +121,14 @@ struct vm_area_struct {
 
 #define VM_ACCOUNT	0x00100000	/* Memory is a vm accounted object */
 
+/* arches may define VM_STACK_FLAGS for their own purposes */
+#ifndef VM_STACK_FLAGS
 #ifdef ARCH_STACK_GROWSUP
 #define VM_STACK_FLAGS	(VM_DATA_DEFAULT_FLAGS|VM_GROWSUP|VM_ACCOUNT)
 #else
 #define VM_STACK_FLAGS	(VM_DATA_DEFAULT_FLAGS|VM_GROWSDOWN|VM_ACCOUNT)
 #endif
+#endif
 
 #define VM_READHINTMASK			(VM_SEQ_READ | VM_RAND_READ)
 #define VM_ClearReadHint(v)		(v)->vm_flags &= ~VM_READHINTMASK
@@ -666,7 +669,7 @@ extern void __insert_vm_struct(struct mm
 extern void build_mmap_rb(struct mm_struct *);
 extern void exit_mmap(struct mm_struct *);
 
-extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
+extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);
 
 extern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot,
@@ -689,12 +692,20 @@ extern int do_munmap(struct mm_struct *,
 
 extern unsigned long do_brk(unsigned long, unsigned long);
 
+#ifdef __i386__
+extern void arch_remove_exec_range(struct mm_struct *mm, unsigned long limit);
+#else
+#define arch_remove_exec_range(mm, limit)  do { } while (0)
+#endif
+
+
 static inline void __vma_unlink(struct mm_struct * mm, struct vm_area_struct * vma, struct vm_area_struct * prev)
 {
 	prev->vm_next = vma->vm_next;
 	rb_erase(&vma->vm_rb, &mm->mm_rb);
 	if (mm->mmap_cache == vma)
 		mm->mmap_cache = prev;
+	arch_remove_exec_range(mm, vma->vm_end);
 }
 
 #define VM_SPECIAL (VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_RESERVED)
diff -urNp linux-1090/include/linux/sched.h linux-1100/include/linux/sched.h
--- linux-1090/include/linux/sched.h
+++ linux-1100/include/linux/sched.h
@@ -30,7 +30,10 @@ extern unsigned long event;
 #include <linux/pid.h>
 
 struct exec_domain;
+extern int exec_shield;
+extern int exec_shield_randomize;
 extern int panic_timeout;
+extern int print_fatal_signals;
 
 /*
  * cloning flags:
@@ -563,6 +566,7 @@ struct task_struct {
 #define PF_NOIO		0x00004000	/* avoid generating further I/O */
 #define PF_USEDFPU	0x00100000	/* task used FPU this quantum (SMP) */
 #define PF_FSTRANS	0x00200000	/* inside a filesystem transaction */
+#define PF_RELOCEXEC	0x00400000	/* relocate shared libraries */
 
 /*
  * Ptrace flags
@@ -584,9 +588,9 @@ struct task_struct {
 
 /*
  * Limit the stack by to some sane default: root can always
- * increase this limit if needed..  8MB seems reasonable.
+ * increase this limit if needed..  10MB seems reasonable.
  */
-#define _STK_LIM	(8*1024*1024)
+#define _STK_LIM	(10*1024*1024)
 
 #if CONFIG_SMP
 extern void set_cpus_allowed(task_t *p, unsigned long new_mask);
diff -urNp linux-1090/kernel/signal.c linux-1100/kernel/signal.c
--- linux-1090/kernel/signal.c
+++ linux-1100/kernel/signal.c
@@ -22,6 +22,9 @@
 #include <asm/uaccess.h>
 #include <asm/siginfo.h>
 
+static void print_maps(void);
+static void print_ulimits(void);
+
 /*
  * SLAB caches for signal bits.
  */
@@ -1462,6 +1465,38 @@ do_signal_stop(int signr)
 	finish_stop(stop_count);
 }
 
+int print_fatal_signals = 0;
+
+static void print_fatal_signal(struct pt_regs *regs, int signr)
+{
+	int i;
+	unsigned char insn;
+
+	printk("%s/%d: potentially unexpected fatal signal %d.\n",
+		current->comm, current->pid, signr);
+	if (user_mode(regs)) {
+		printk("userspace code at %08lx: ", instruction_pointer(regs));
+		for (i = 0; i < 16; i++) {
+			__get_user(insn, (unsigned char *)(instruction_pointer(regs) + i));
+			printk("%02x ", insn);
+		}
+		printk("\n");
+	}
+	show_regs(regs);
+	if (print_fatal_signals >= 2) {
+		print_maps();
+		print_ulimits();
+	}
+}
+
+static int __init setup_print_fatal_signals(char *str)
+{
+	get_option (&str, &print_fatal_signals);
+
+	return 1;
+}
+
+__setup("print-fatal-signals=", setup_print_fatal_signals);
 
 #ifndef HAVE_ARCH_GET_SIGNAL_TO_DELIVER
 
@@ -1607,6 +1642,8 @@ relock:
 		 * Anything else is fatal, maybe with a core dump.
 		 */
 		current->flags |= PF_SIGNALED;
+		if (print_fatal_signals)
+			print_fatal_signal(regs, signr);
  		if (sig_kernel_coredump(signr)) {
 			/*
  			 * If it was able to dump core, this kills all
@@ -2234,3 +2271,109 @@ void __init signals_init(void)
 		panic("signals_init(): cannot create sigqueue SLAB cache");
 }
 
+
+/* for systems with sizeof(void*) == 4: */
+#define MAPS_LINE_FORMAT4	  "%08lx-%08lx %s %08lx %s %lu"
+#define MAPS_LINE_MAX4	49 /* sum of 8  1  8  1 4 1 8 1 5 1 10 1 */
+
+/* for systems with sizeof(void*) == 8: */
+#define MAPS_LINE_FORMAT8	  "%016lx-%016lx %s %016lx %s %lu"
+#define MAPS_LINE_MAX8	73 /* sum of 16  1  16  1 4 1 16 1 5 1 10 1 */
+
+#define MAPS_LINE_FORMAT	(sizeof(void *) == 4 ? \
+					MAPS_LINE_FORMAT4 : MAPS_LINE_FORMAT8)
+#define MAPS_LINE_MAX		(sizeof(void *) == 4 ? \
+					MAPS_LINE_MAX4 : MAPS_LINE_MAX8)
+
+static DECLARE_MUTEX(maps_sem);
+
+static char line_buf[PAGE_SIZE+1];
+
+static void maps_print_line(struct vm_area_struct *map)
+{
+	/* produce the next line */
+	char *line, *buf = line_buf;
+	char str[5];
+	int flags;
+	kdev_t dev;
+	unsigned long ino;
+	int len;
+
+	flags = map->vm_flags;
+
+	str[0] = flags & VM_READ ? 'r' : '-';
+	str[1] = flags & VM_WRITE ? 'w' : '-';
+	str[2] = flags & VM_EXEC ? 'x' : '-';
+	str[3] = flags & VM_MAYSHARE ? 's' : 'p';
+	str[4] = 0;
+
+	dev = 0;
+	ino = 0;
+	if (map->vm_file != NULL) {
+		dev = map->vm_file->f_dentry->d_inode->i_dev;
+		ino = map->vm_file->f_dentry->d_inode->i_ino;
+		line = d_path(map->vm_file->f_dentry,
+			      map->vm_file->f_vfsmnt,
+			      buf, PAGE_SIZE);
+		if (IS_ERR(line))
+			return;
+		buf[PAGE_SIZE-1] = '\n';
+		line -= MAPS_LINE_MAX;
+		if (line < buf)
+			line = buf;
+	} else
+		line = buf;
+
+	len = sprintf(line,
+		      MAPS_LINE_FORMAT,
+		      map->vm_start, map->vm_end, str,
+		      map->vm_pgoff << PAGE_SHIFT, kdevname(dev), ino);
+
+	if (map->vm_file) {
+		int i;
+		for (i = len; i < MAPS_LINE_MAX; i++)
+			line[i] = ' ';
+		len = buf + PAGE_SIZE - line;
+		memmove(buf, line, len);
+	} else
+		line[len++] = '\n';
+
+	buf[len] = 0;
+
+	printk("%s", buf);
+}
+
+static void print_maps(void)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *map;
+
+	if (!mm)
+		return;
+
+	printk("\nmemory maps:\n");
+	down(&maps_sem);
+	down_read(&mm->mmap_sem);
+	map = mm->mmap;
+	while (map) {
+		maps_print_line(map);
+		map = map->vm_next;
+	}
+	up_read(&mm->mmap_sem);
+	up(&maps_sem);
+}
+
+static const char *rlim_names[RLIM_NLIMITS] =
+	{ "     CPU", "   FSIZE", "    DATA", "   STACK",
+	  "    CORE", "     RSS", "   NPROC", "  NRFILE",
+	  " MEMLOCK", "      AS","   LOCKS" };
+
+static void print_ulimits(void)
+{
+	int i;
+
+	printk("\nresource limits:\n");
+	for (i = 0; i < RLIM_NLIMITS; i++)
+		printk("%s: cur: %08lx  [max: %08lx]\n", rlim_names[i],
+			current->rlim[i].rlim_cur, current->rlim[i].rlim_max);
+}
diff -urNp linux-1090/kernel/sysctl.c linux-1100/kernel/sysctl.c
--- linux-1090/kernel/sysctl.c
+++ linux-1100/kernel/sysctl.c
@@ -44,7 +44,6 @@ int sercons_escape_char = -1;
 
 /* External variables not in a header file. */
 extern int panic_timeout;
-extern int print_fatal_signals;
 extern int C_A_D;
 extern int bdf_prm[], bdflush_min[], bdflush_max[];
 extern int sysctl_overcommit_memory;
@@ -58,6 +57,12 @@ extern char core_pattern[];
 extern int cad_pid;
 extern int pid_max;
 
+int exec_shield = 1;
+#ifdef __x86_64__
+extern int exec_shield32;
+#endif
+int exec_shield_randomize = 1;
+
 /* this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */
 static int maxolduid = 65535;
 static int minolduid;
@@ -187,6 +192,14 @@ static ctl_table kern_table[] = {
 	 0644, NULL, &proc_dointvec},
 	{KERN_PANIC, "print_fatal_signals", &print_fatal_signals, sizeof(int),
 	 0644, NULL, &proc_dointvec},
+	{KERN_PANIC, "exec-shield", &exec_shield, sizeof(int),
+	 0644, NULL, &proc_dointvec},
+#ifdef __x86_64__
+	{KERN_PANIC, "exec-shield32", &exec_shield32, sizeof(int),
+	 0644, NULL, &proc_dointvec},
+#endif
+	{KERN_PANIC, "exec-shield-randomize", &exec_shield_randomize, sizeof(int),
+	 0644, NULL, &proc_dointvec},
 #ifdef __i386__
 	{KERN_PANIC, "use-nx", &use_nx, sizeof(int),
 	 0644, NULL, &proc_dointvec},
diff -urNp linux-1090/mm/mmap.c linux-1100/mm/mmap.c
--- linux-1090/mm/mmap.c
+++ linux-1100/mm/mmap.c
@@ -413,6 +413,8 @@ static inline void __vma_link_file(struc
 static void __vma_link(struct mm_struct * mm, struct vm_area_struct * vma,  struct vm_area_struct * prev,
 		       rb_node_t ** rb_link, rb_node_t * rb_parent)
 {
+	if (vma->vm_flags & VM_EXEC)
+		arch_add_exec_range(mm, vma->vm_end);
 	__vma_link_list(mm, vma, prev, rb_parent);
 	__vma_link_rb(mm, vma, rb_link, rb_parent);
 	__vma_link_file(vma);
@@ -468,6 +470,11 @@ static int vma_merge(struct mm_struct * 
 			return 1;
 		}
 		spin_unlock(lock);
+		/*
+		 * Just extended 'prev':
+		 */
+		if (prev->vm_flags & VM_EXEC)
+			arch_add_exec_range(mm, prev->vm_end);
 		if (need_unlock)
 			unlock_vma_mappings(next);
 		return 1;
@@ -524,7 +531,7 @@ unsigned long do_mmap_pgoff(struct file 
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
-	addr = get_unmapped_area(file, addr, len, pgoff, flags);
+	addr = get_unmapped_area(file, addr, len, pgoff, flags, prot & PROT_EXEC);
 	if (addr & ~PAGE_MASK)
 		return addr;
 
@@ -735,7 +742,7 @@ unacct_error:
 #ifndef HAVE_ARCH_UNMAPPED_AREA
 static inline unsigned long
 arch_get_unmapped_area(struct file *filp, unsigned long addr,
-		unsigned long len, unsigned long pgoff, unsigned long flags)
+		unsigned long len, unsigned long pgoff, unsigned long flags, unsigned long exec)
 {
 	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma;
@@ -770,10 +777,10 @@ arch_get_unmapped_area(struct file *filp
 	}
 }
 #else
-extern unsigned long arch_get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
+extern unsigned long arch_get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);
 #endif	
 
-unsigned long get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags)
+unsigned long get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags, unsigned long exec)
 {
 	if (flags & MAP_FIXED) {
 		if (addr > TASK_SIZE - len)
@@ -786,7 +793,7 @@ unsigned long get_unmapped_area(struct f
 	if (file && file->f_op && file->f_op->get_unmapped_area)
 		return file->f_op->get_unmapped_area(file, addr, len, pgoff, flags);
 
-	return arch_get_unmapped_area(file, addr, len, pgoff, flags);
+	return arch_get_unmapped_area(file, addr, len, pgoff, flags, exec);
 }
 
 /* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
@@ -995,17 +1002,21 @@ static struct vm_area_struct * unmap_fix
 			area->vm_ops->close(area);
 		if (area->vm_file)
 			fput(area->vm_file);
+		if (area->vm_flags & VM_EXEC)
+			arch_remove_exec_range(mm, area->vm_end);
 		kmem_cache_free(vm_area_cachep, area);
 		return extra;
 	}
 
 	/* Work out to one of the ends. */
 	if (end == area->vm_end) {
+		unsigned long old_end = area->vm_end;
 		/*
 		 * here area isn't visible to the semaphore-less readers
 		 * so we don't need to update it under the spinlock.
 		 */
 		area->vm_end = addr;
+		arch_remove_exec_range(mm, old_end);
 		lock_vma_mappings(area);
 		spin_lock(&mm->page_table_lock);
 	} else if (addr == area->vm_start) {
@@ -1387,8 +1398,8 @@ void exit_mmap(struct mm_struct * mm)
 
 	clear_page_tables(mm, FIRST_USER_PGD_NR, USER_PTRS_PER_PGD);
 	flush_tlb_mm(mm);
+	arch_flush_exec_range(mm);
 	vm_validate_enough("exiting exit_mmap");
-
 }
 
 /* Insert vm structure into process list sorted by address
diff -urNp linux-1090/mm/mprotect.c linux-1100/mm/mprotect.c
--- linux-1090/mm/mprotect.c
+++ linux-1100/mm/mprotect.c
@@ -107,7 +107,6 @@ static void change_protection(unsigned l
 	} while (start && (start < end));
 	spin_unlock(&current->mm->page_table_lock);
 	flush_tlb_range(current->mm, beg, end);
-	return;
 }
 
 static inline int mprotect_fixup_all(struct vm_area_struct * vma, struct vm_area_struct ** pprev,
@@ -115,6 +114,7 @@ static inline int mprotect_fixup_all(str
 {
 	struct vm_area_struct * prev = *pprev;
 	struct mm_struct * mm = vma->vm_mm;
+	int oldflags;
 
 	if (prev && prev->vm_end == vma->vm_start && can_vma_merge(prev, newflags) &&
 	    !vma->vm_file && !(vma->vm_flags & VM_SHARED)) {
@@ -130,8 +130,11 @@ static inline int mprotect_fixup_all(str
 	}
 
 	spin_lock(&mm->page_table_lock);
+	oldflags = vma->vm_flags;
 	vma->vm_flags = newflags;
 	vma->vm_page_prot = prot;
+	if (oldflags & VM_EXEC)
+		arch_remove_exec_range(current->mm, vma->vm_end);
 	spin_unlock(&mm->page_table_lock);
 
 	*pprev = vma;
@@ -144,13 +147,17 @@ static inline int mprotect_fixup_start(s
 	int newflags, pgprot_t prot)
 {
 	struct vm_area_struct * n, * prev = *pprev;
+	unsigned long prev_end;
 
 	*pprev = vma;
 
 	if (prev && prev->vm_end == vma->vm_start && can_vma_merge(prev, newflags) &&
 	    !vma->vm_file && !(vma->vm_flags & VM_SHARED)) {
 		spin_lock(&vma->vm_mm->page_table_lock);
+		prev_end = prev->vm_end;
 		prev->vm_end = end;
+		if (prev->vm_flags & VM_EXEC)
+			arch_remove_exec_range(current->mm, prev_end);
 		vma->vm_start = end;
 		spin_unlock(&vma->vm_mm->page_table_lock);
 
@@ -215,6 +222,8 @@ static inline int mprotect_fixup_middle(
 	int newflags, pgprot_t prot)
 {
 	struct vm_area_struct * left, * right;
+	unsigned long prev_end;
+	int oldflags;
 
 	left = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
 	if (!left)
@@ -242,9 +251,13 @@ static inline int mprotect_fixup_middle(
 	vma->vm_page_prot = prot;
 	lock_vma_mappings(vma);
 	spin_lock(&vma->vm_mm->page_table_lock);
+	oldflags = vma->vm_flags;
+	vma->vm_flags = newflags;
 	vma->vm_start = start;
+	prev_end = vma->vm_end;
 	vma->vm_end = end;
-	vma->vm_flags = newflags;
+	if (!(newflags & VM_EXEC))
+		arch_remove_exec_range(current->mm, prev_end);
 	__insert_vm_struct(current->mm, left);
 	__insert_vm_struct(current->mm, right);
 	spin_unlock(&vma->vm_mm->page_table_lock);
@@ -300,11 +313,20 @@ static int mprotect_fixup(struct vm_area
 	return 0;
 }
 
+#ifndef PROT_GROWSDOWN
+#define PROT_GROWSDOWN	0x01000000
+#define PROT_GROWSUP	0x02000000
+#endif
+
 asmlinkage long sys_mprotect(unsigned long start, size_t len, unsigned long prot)
 {
 	unsigned long nstart, end, tmp;
 	struct vm_area_struct * vma, * next, * prev;
 	int error = -EINVAL;
+	const int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);
+	prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
+	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can't be both */
+		return -EINVAL;
 
 	vm_validate_enough("entering mprotect");
 
@@ -323,8 +345,26 @@ asmlinkage long sys_mprotect(unsigned lo
 
 	vma = find_vma_prev(current->mm, start, &prev);
 	error = -ENOMEM;
-	if (!vma || vma->vm_start > start)
+	if (!vma)
 		goto out;
+	if (unlikely (grows & PROT_GROWSDOWN)) {
+		if (vma->vm_start >= end)
+			goto out;
+		start = vma->vm_start;
+		error = -EINVAL;
+		if (!(vma->vm_flags & VM_GROWSDOWN))
+			goto out;
+	}
+	else {
+		if (vma->vm_start > start)
+			goto out;
+		if (unlikely(grows & PROT_GROWSUP)) {
+			end = vma->vm_end;
+			error = -EINVAL;
+			if (!(vma->vm_flags & VM_GROWSUP))
+				goto out;
+		}
+	}
 
 	for (nstart = start ; ; ) {
 		unsigned int newflags;
diff -urNp linux-1090/mm/mremap.c linux-1100/mm/mremap.c
--- linux-1090/mm/mremap.c
+++ linux-1100/mm/mremap.c
@@ -402,7 +402,7 @@ unsigned long do_mremap(unsigned long ad
 			if (vma->vm_flags & VM_SHARED)
 				map_flags |= MAP_SHARED;
 
-			new_addr = get_unmapped_area(vma->vm_file, 0, new_len, vma->vm_pgoff, map_flags);
+			new_addr = get_unmapped_area(vma->vm_file, 0, new_len, vma->vm_pgoff, map_flags, vma->vm_flags & VM_EXEC);
 			ret = new_addr;
 			if (new_addr & ~PAGE_MASK)
 				goto out;
