diff -urNp linux-180/arch/x86_64/ia32/Makefile linux-181/arch/x86_64/ia32/Makefile
--- linux-180/arch/x86_64/ia32/Makefile
+++ linux-181/arch/x86_64/ia32/Makefile
@@ -13,7 +13,7 @@ all: ia32.o
 
 O_TARGET := ia32.o
 obj-$(CONFIG_IA32_EMULATION) := ia32entry.o sys_ia32.o ia32_ioctl.o ia32_signal.o \
-	ia32_binfmt.o fpu32.o socket32.o ptrace32.o ipc32.o
+	ia32_binfmt.o fpu32.o socket32.o ptrace32.o ipc32.o tls32.o
 
 clean::
 
diff -urNp linux-180/arch/x86_64/ia32/ia32_signal.c linux-181/arch/x86_64/ia32/ia32_signal.c
--- linux-180/arch/x86_64/ia32/ia32_signal.c
+++ linux-181/arch/x86_64/ia32/ia32_signal.c
@@ -90,18 +90,19 @@ sys32_sigsuspend(int history0, int histo
 	sigset_t saveset;
 
 	mask &= _BLOCKABLE;
-	spin_lock_irq(&current->sig->siglock);
+	spin_lock_irq(&current->sighand->siglock);
 	saveset = current->blocked;
 	siginitset(&current->blocked, mask);
 	recalc_sigpending();
-	spin_unlock_irq(&current->sig->siglock);
+	__set_current_state(TASK_INTERRUPTIBLE);
+	spin_unlock_irq(&current->sighand->siglock);
 
 	regs.rax = -EINTR;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
 		schedule();
 		if (do_signal(&regs, &saveset))
 			return -EINTR;
+		set_current_state(TASK_INTERRUPTIBLE);
 	}
 }
 
@@ -252,10 +253,10 @@ asmlinkage long sys32_sigreturn(struct p
 		goto badframe;
 
 	sigdelsetmask(&set, ~_BLOCKABLE);
-	spin_lock_irq(&current->sig->siglock);
+	spin_lock_irq(&current->sighand->siglock);
 	current->blocked = set;
 	recalc_sigpending();
-	spin_unlock_irq(&current->sig->siglock);
+	spin_unlock_irq(&current->sighand->siglock);
 	
 	if (ia32_restore_sigcontext(&regs, &frame->sc, &eax))
 		goto badframe;
@@ -279,10 +280,10 @@ asmlinkage long sys32_rt_sigreturn(struc
 		goto badframe;
 
 	sigdelsetmask(&set, ~_BLOCKABLE);
-	spin_lock_irq(&current->sig->siglock);
+	spin_lock_irq(&current->sighand->siglock);
 	current->blocked = set;
 	recalc_sigpending();
-	spin_unlock_irq(&current->sig->siglock);
+	spin_unlock_irq(&current->sighand->siglock);
 	
 	if (ia32_restore_sigcontext(&regs, &frame->uc.uc_mcontext, &eax))
 		goto badframe;
diff -urNp linux-180/arch/x86_64/ia32/sys_ia32.c linux-181/arch/x86_64/ia32/sys_ia32.c
--- linux-180/arch/x86_64/ia32/sys_ia32.c
+++ linux-181/arch/x86_64/ia32/sys_ia32.c
@@ -1934,11 +1934,11 @@ asmlinkage long sys32_modify_ldt(int fun
 {
         long ret;
         if (func == 0x1 || func == 0x11) { 
-				struct modify_ldt_ldt_s info;
+				struct user_desc info;
                 mm_segment_t old_fs = get_fs();
-                if (bytecount != sizeof(struct modify_ldt_ldt_s))
+                if (bytecount != sizeof(struct user_desc))
                         return -EINVAL;
-                if (copy_from_user(&info, ptr, sizeof(struct modify_ldt_ldt_s)))
+                if (copy_from_user(&info, ptr, sizeof(struct user_desc)))
                         return -EFAULT;
                 /* lm bit was undefined in the 32bit ABI and programs
                    give it random values. Force it to zero here. */
@@ -2212,14 +2212,14 @@ free:
 
 asmlinkage long sys32_fork(struct pt_regs regs)
 {
-	return do_fork(SIGCHLD, regs.rsp, &regs, 0);
+	return do_fork(SIGCHLD, regs.rsp, &regs, 0, NULL, NULL);
 }
 
 asmlinkage long sys32_clone(unsigned int clone_flags, unsigned int newsp, struct pt_regs regs)
 {
 	if (!newsp)
 		newsp = regs.rsp;
-	return do_fork(clone_flags, newsp, &regs, 0);
+	return do_fork(clone_flags, newsp, &regs, 0, NULL, NULL);
 }
 
 /*
@@ -2234,7 +2234,7 @@ asmlinkage long sys32_clone(unsigned int
  */
 asmlinkage long sys32_vfork(struct pt_regs regs)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.rsp, &regs, 0);
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.rsp, &regs, 0, NULL, NULL);
 }
 
 /*
diff -urNp linux-180/arch/x86_64/ia32/tls32.c linux-181/arch/x86_64/ia32/tls32.c
--- linux-180/arch/x86_64/ia32/tls32.c
+++ linux-181/arch/x86_64/ia32/tls32.c
@@ -0,0 +1,158 @@
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/user.h>
+                                                                                
+#include <asm/uaccess.h>
+#include <asm/desc.h>
+#include <asm/system.h>
+#include <asm/ldt.h>
+#include <asm/processor.h>
+#include <asm/proto.h>
+                                                                                
+
+/*
+ * sys_alloc_thread_area: get a yet unused TLS descriptor index.
+ */
+static int get_free_idx(void)
+{
+	struct thread_struct *t = &current->thread;
+	int idx;
+
+	for (idx = 0; idx < GDT_ENTRY_TLS_ENTRIES; idx++)
+		if (desc_empty((struct n_desc_struct *)(t->tls_array) + idx))
+			return idx + GDT_ENTRY_TLS_MIN;
+	return -ESRCH;
+}
+
+/*
+ * Set a given TLS descriptor:
+ * When you want addresses > 32bit use arch_prctl() 
+ */
+int do_set_thread_area(struct thread_struct *t, struct user_desc *u_info)
+{
+	struct user_desc info;
+	struct n_desc_struct *desc;
+	int cpu, idx;
+
+	if (copy_from_user(&info, u_info, sizeof(info)))
+		return -EFAULT;
+
+	idx = info.entry_number;
+
+	/*
+	 * index -1 means the kernel should try to find and
+	 * allocate an empty descriptor:
+	 */
+	if (idx == -1) {
+		idx = get_free_idx();
+		if (idx < 0)
+			return idx;
+		if (put_user(idx, &u_info->entry_number))
+			return -EFAULT;
+	}
+
+	if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+		return -EINVAL;
+
+	desc = ((struct n_desc_struct *)t->tls_array) + idx - GDT_ENTRY_TLS_MIN;
+
+	cpu = smp_processor_id();
+
+	if (LDT_empty(&info)) {
+		desc->a = 0;
+		desc->b = 0;
+	} else {
+		desc->a = LDT_entry_a(&info);
+		desc->b = LDT_entry_b(&info);
+	}
+	if (t == &current->thread)
+	load_TLS(t, cpu);
+
+	return 0;
+}
+
+asmlinkage int sys_set_thread_area(struct user_desc *u_info)
+{ 
+	return do_set_thread_area(&current->thread, u_info); 
+} 
+
+
+/*
+ * Get the current Thread-Local Storage area:
+ */
+
+#define GET_BASE(desc) ( \
+	(((desc)->a >> 16) & 0x0000ffff) | \
+	(((desc)->b << 16) & 0x00ff0000) | \
+	( (desc)->b        & 0xff000000)   )
+
+#define GET_LIMIT(desc) ( \
+	((desc)->a & 0x0ffff) | \
+	 ((desc)->b & 0xf0000) )
+	
+#define GET_32BIT(desc)		(((desc)->b >> 23) & 1)
+#define GET_CONTENTS(desc)	(((desc)->b >> 10) & 3)
+#define GET_WRITABLE(desc)	(((desc)->b >>  9) & 1)
+#define GET_LIMIT_PAGES(desc)	(((desc)->b >> 23) & 1)
+#define GET_PRESENT(desc)	(((desc)->b >> 15) & 1)
+#define GET_USEABLE(desc)	(((desc)->b >> 20) & 1)
+#define GET_LONGMODE(desc)	(((desc)->b >> 21) & 1)
+
+int do_get_thread_area(struct thread_struct *t, struct user_desc *u_info)
+{
+	struct user_desc info;
+	struct n_desc_struct *desc;
+	int idx;
+
+	if (get_user(idx, &u_info->entry_number))
+		return -EFAULT;
+	if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+		return -EINVAL;
+
+	desc = ((struct n_desc_struct *)t->tls_array) + idx - GDT_ENTRY_TLS_MIN;
+
+	memset(&info, 0, sizeof(struct user_desc));
+	info.entry_number = idx;
+	info.base_addr = GET_BASE(desc);
+	info.limit = GET_LIMIT(desc);
+	info.seg_32bit = GET_32BIT(desc);
+	info.contents = GET_CONTENTS(desc);
+	info.read_exec_only = !GET_WRITABLE(desc);
+	info.limit_in_pages = GET_LIMIT_PAGES(desc);
+	info.seg_not_present = !GET_PRESENT(desc);
+	info.useable = GET_USEABLE(desc);
+	info.lm = GET_LONGMODE(desc);
+
+	if (copy_to_user(u_info, &info, sizeof(info)))
+		return -EFAULT;
+	return 0;
+}
+
+asmlinkage int sys_get_thread_area(struct user_desc *u_info)
+{
+	return do_get_thread_area(&current->thread, u_info);
+} 
+
+int ia32_child_tls(struct task_struct *p, struct pt_regs *childregs)
+{
+	struct n_desc_struct *desc;
+	struct user_desc info, *cp;
+	int idx;
+	
+	cp = (void *)childregs->rsi;
+	if (copy_from_user(&info, cp, sizeof(info)))
+		return -EFAULT;
+	if (LDT_empty(&info))
+		return -EINVAL;
+	
+	idx = info.entry_number;
+	if (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)
+		return -EINVAL;
+	
+	desc = (struct n_desc_struct *)(p->thread.tls_array) + idx - GDT_ENTRY_TLS_MIN;
+	desc->a = LDT_entry_a(&info);
+	desc->b = LDT_entry_b(&info);
+
+	return 0;
+}
diff -urNp linux-180/arch/x86_64/kernel/entry.S linux-181/arch/x86_64/kernel/entry.S
--- linux-180/arch/x86_64/kernel/entry.S
+++ linux-181/arch/x86_64/kernel/entry.S
@@ -600,8 +600,14 @@ ENTRY(arch_kernel_thread)
 
 	movq %rsp, %rdx
 
+	# For new do_fork api, NULL out args 4, 5, 6
+	xorq	%rcx, %rcx
+	xorq	%r8, %r8
+	xorq	%r9, %r9
+
 	# clone now
 	call do_fork
+ 
 	# save retval on the stack so it's popped before `ret`
 	movq %rax, RAX(%rsp)
 
diff -urNp linux-180/arch/x86_64/kernel/head.S linux-181/arch/x86_64/kernel/head.S
--- linux-180/arch/x86_64/kernel/head.S
+++ linux-181/arch/x86_64/kernel/head.S
@@ -153,7 +153,7 @@ reach_long64:
 	 * addresses where we're currently running on. We have to do that here
 	 * because in 32bit we couldn't load a 64bit linear address.
 	 */
-	lgdt	pGDT64
+	lgdt	cpu_gdt_descr
 
 	/* 
 	 * Setup up a dummy PDA. this is just for some early bootup code
@@ -314,15 +314,18 @@ ENTRY(level3_physmem_pgt)
 .org 0xb000
 .data
 
-.globl SYMBOL_NAME(gdt)
-
-	.word 0
 	.align 16
-	.word 0
-pGDT64:
-	.word	gdt_end-gdt_table
-SYMBOL_NAME_LABEL(gdt)
-	.quad	gdt_table
+	.globl cpu_gdt_descr
+cpu_gdt_descr:
+	.word	gdt_end-cpu_gdt_table
+gdt:
+	.quad	cpu_gdt_table
+#ifdef CONFIG_SMP
+	.rept	NR_CPUS-1
+	.word	0
+	.quad	0
+	.endr
+#endif
 	
 
 .align 64 /* cacheline aligned */
@@ -338,7 +341,7 @@ gdt32_end:	
  */
 		 		
 .align 64 /* cacheline aligned, keep this synchronized with asm/desc.h */
-ENTRY(gdt_table)
+ENTRY(cpu_gdt_table)
 	.quad	0x0000000000000000	/* This one is magic */
 	.quad	0x00af9a000000ffff ^ (1<<21)	/* __KERNEL_COMPAT32_CS */	
 	.quad	0x00af9a000000ffff	/* __KERNEL_CS */
@@ -346,19 +349,23 @@ ENTRY(gdt_table)
 	.quad	0x00cffe000000ffff	/* __USER32_CS */
 	.quad	0x00cff2000000ffff	/* __USER_DS, __USER32_DS  */		
 	.quad	0x00affa000000ffff	/* __USER_CS */
-	.word	0xFFFF				# 4Gb - (0x100000*0x1000 = 4Gb)
-	.word	0				# base address = 0
-	.word	0x9A00				# code read/exec
-	.word	0x00CF				# granularity = 4096, 386
-						#  (+5th nibble of limit)
-					/* __KERNEL32_CS */
+	.quad	0x00cf9a000000ffff	/* __KERNEL32_CS */
+	.quad	0,0			/* TSS */
+	.quad	0			/* LDT */
+	.quad	0,0,0			/* Three TLS descriptors */
+	.quad	0			/* unused now */
+	.quad	0x00009a000000ffff	/* __KERNEL16_CS - 16bit PM for S3 wakeup */
 	/* when you add something here fix constant in desc.h */				
-	.globl gdt_cpu_table
-gdt_cpu_table:	
-	.fill NR_CPUS*PER_CPU_GDT_SIZE,1,0
 gdt_end:	
 	.globl gdt_end
 
+	/* GDTs of other CPUs */
+#ifdef CONFIG_SMP
+	.rept NR_CPUS-1
+	.quad 0,0,0,0,0,0,0,0,0,0,0
+	.endr
+#endif
+
 	.align  64
 ENTRY(idt_table)	
 	.rept   256
diff -urNp linux-180/arch/x86_64/kernel/init_task.c linux-181/arch/x86_64/kernel/init_task.c
--- linux-180/arch/x86_64/kernel/init_task.c
+++ linux-181/arch/x86_64/kernel/init_task.c
@@ -11,6 +11,7 @@ static struct fs_struct init_fs = INIT_F
 static struct files_struct init_files = INIT_FILES;
 struct mm_struct init_mm = INIT_MM(init_mm);
 static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
+static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
 
 /*
  * Initial task structure.
diff -urNp linux-180/arch/x86_64/kernel/ldt.c linux-181/arch/x86_64/kernel/ldt.c
--- linux-180/arch/x86_64/kernel/ldt.c
+++ linux-181/arch/x86_64/kernel/ldt.c
@@ -65,7 +65,7 @@ static int write_ldt(void * ptr, unsigne
 	struct mm_struct * mm = me->mm;
 	__u32 entry_1, entry_2, *lp;
 	int error;
-	struct modify_ldt_ldt_s ldt_info;
+	struct user_desc ldt_info;
 
 	error = -EINVAL;
 
@@ -106,34 +106,17 @@ static int write_ldt(void * ptr, unsigne
 
    	/* Allow LDTs to be cleared by the user. */
    	if (ldt_info.base_addr == 0 && ldt_info.limit == 0) {
-		if (oldmode ||
-		    (ldt_info.contents == 0		&&
-		     ldt_info.read_exec_only == 1	&&
-		     ldt_info.seg_32bit == 0		&&
-		     ldt_info.limit_in_pages == 0	&&
-		     ldt_info.seg_not_present == 1	&&
-		     ldt_info.useable == 0 && 
-		     ldt_info.lm == 0)) {
+		if (oldmode || LDT_empty(&ldt_info)) {
 			entry_1 = 0;
 			entry_2 = 0;
 			goto install;
 		}
 	}
 
-	entry_1 = ((ldt_info.base_addr & 0x0000ffff) << 16) |
-		  (ldt_info.limit & 0x0ffff);
-	entry_2 = (ldt_info.base_addr & 0xff000000) |
-		  ((ldt_info.base_addr & 0x00ff0000) >> 16) |
-		  (ldt_info.limit & 0xf0000) |
-		  ((ldt_info.read_exec_only ^ 1) << 9) |
-		  (ldt_info.contents << 10) |
-		  ((ldt_info.seg_not_present ^ 1) << 15) |
-		  (ldt_info.seg_32bit << 22) |
-		  (ldt_info.limit_in_pages << 23) |
-		  (ldt_info.lm << 21) |
-		  0x7000;
-	if (!oldmode)
-		entry_2 |= (ldt_info.useable << 20);
+	entry_1 = LDT_entry_a(&ldt_info);
+	entry_2 = LDT_entry_b(&ldt_info);
+	if (oldmode)
+		entry_2 &= ~(1 << 20);
 
 	/* Install the new entry ...  */
 install:
diff -urNp linux-180/arch/x86_64/kernel/process.c linux-181/arch/x86_64/kernel/process.c
--- linux-180/arch/x86_64/kernel/process.c
+++ linux-181/arch/x86_64/kernel/process.c
@@ -24,6 +24,8 @@
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
+#include <linux/elf.h>
+#include <linux/elfcore.h>
 #include <linux/smp.h>
 #include <linux/smp_lock.h>
 #include <linux/stddef.h>
@@ -59,6 +61,8 @@
 
 asmlinkage extern void ret_from_fork(void);
 
+extern int ia32_child_tls(struct task_struct *p, struct pt_regs *childregs);
+
 int hlt_counter;
 
 /*
@@ -89,7 +93,7 @@ static void default_idle(void)
 {
 	if (!hlt_counter) {
 		__cli();
-		if (!current->need_resched)
+		if (!need_resched())
 			safe_halt();
 		else
 			__sti();
@@ -131,14 +135,14 @@ static void poll_idle (void)
 void cpu_idle (void)
 {
 	/* endless idle loop with no priority at all */
-	init_idle();
-	current->nice = 20;
-	current->counter = -100;
 
 	while (1) {
 		void (*idle)(void) = pm_idle;
 		if (!idle)
 			idle = default_idle;
+		/* We use the last_run timestamp to measure the idleness of a CPU */
+		current->last_run = jiffies;
+
 		while (!current->need_resched)
 			idle();
 		schedule();
@@ -322,23 +326,6 @@ void show_regs(struct pt_regs * regs)
 	show_trace(&regs->rsp);
 }
 
-/*
- * No need to lock the MM as we are the last user
- */
-void release_segments(struct mm_struct *mm)
-{
-	void * ldt = mm->context.segments;
-
-	/*
-	 * free the LDT
-	 */
-	if (ldt) {
-		mm->context.segments = NULL;
-		clear_LDT();
-		vfree(ldt);
-	}
-}
-
 /* 
  * Free current thread data structures etc..
  */
@@ -358,6 +345,7 @@ void flush_thread(void)
 	struct task_struct *tsk = current;
 
 	memset(tsk->thread.debugreg, 0, sizeof(unsigned long)*8);
+	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
 	/*
 	 * Forget coprocessor state..
 	 */
@@ -368,47 +356,140 @@ void flush_thread(void)
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task->mm) {
-		void * ldt = dead_task->mm->context.segments;
-
-		// temporary debugging check
-		if (ldt) {
+		if (dead_task->mm->context.segments) {
 			printk("WARNING: dead process %8s still has LDT? <%p>\n",
-					dead_task->comm, ldt);
+					dead_task->comm, 
+					dead_task->mm->context.segments);
 			BUG();
 		}
 	}
 }
 
-/*
- * we do not have to muck with descriptors here, that is
- * done in switch_mm() as needed.
- */
-void copy_segments(struct task_struct *p, struct mm_struct *new_mm)
+
+static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)
 {
-	struct mm_struct * old_mm;
-	void *old_ldt, *ldt;
- 
-	ldt = NULL;
-	old_mm = current->mm;
-	if (old_mm && (old_ldt = old_mm->context.segments) != NULL) {
-		/*
-		 * Completely new LDT, we initialize it from the parent:
-		 */
-		ldt = vmalloc(LDT_ENTRIES*LDT_ENTRY_SIZE);
-		if (!ldt)
-			printk(KERN_WARNING "ldt allocation failed\n");
-		else
-			memcpy(ldt, old_ldt, LDT_ENTRIES*LDT_ENTRY_SIZE);
-	}
-	new_mm->context.segments = ldt;
-	new_mm->context.cpuvalid = 0UL;
-	return;
+	struct user_desc ud = { 
+		.base_addr = addr,
+		.limit = 0xfffff,
+		.seg_32bit = 1,
+		.limit_in_pages = 1,
+		.useable = 1,
+	};
+	struct n_desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	desc->a = LDT_entry_a(&ud); 
+	desc->b = LDT_entry_b(&ud); 
+}
+
+static inline u32 read_32bit_tls(struct task_struct *t, int tls)
+{
+	struct desc_struct *desc = (void *)t->thread.tls_array;
+	desc += tls;
+	return desc->base0 | 
+		(((u32)desc->base1) << 16) | 
+		(((u32)desc->base2) << 24);
 }
 
+asmlinkage long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)
+{ 
+	int ret = 0; 
+	int doit = (task == current);
+	unsigned long tmp; 
+	int cpu;
+
+	switch (code) { 
+	case ARCH_SET_GS:
+		if (addr >= TASK_SIZE) 
+			return -EPERM; 
+
+		cpu = smp_processor_id();
+		if (addr <= 0xffffffff) {
+			set_32bit_tls(task, GS_TLS, addr);
+			if (doit) {
+				load_TLS(&task->thread, cpu);
+				load_gs_index(GS_TLS_SEL);
+			}
+			task->thread.gsindex = GS_TLS_SEL;
+			task->thread.gs = 0;
+		} else {
+			task->thread.gsindex = 0;
+			task->thread.gs = addr;
+			if (doit) {
+				load_gs_index(0);
+				ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr);
+			}
+		}
+		break;
+	case ARCH_SET_FS:
+		/* Not strictly needed for fs, but do it for symmetry
+		   with gs. */
+		if (addr >= TASK_SIZE)
+			return -EPERM; 
+
+		cpu = smp_processor_id();
+		/* handle small bases via the GDT because that's faster to 
+		   switch. */
+		if (addr <= 0xffffffff) { 
+			set_32bit_tls(task, FS_TLS, addr);
+			if (doit) { 
+				load_TLS(&task->thread, cpu); 
+				asm volatile("movl %0,%%fs" :: "r" (FS_TLS_SEL));
+			}
+			task->thread.fsindex = FS_TLS_SEL;
+			task->thread.fs = 0;
+		} else { 
+			task->thread.fsindex = 0;
+			task->thread.fs = addr;
+			if (doit) {
+				/* set the selector to 0 to not confuse
+				   __switch_to */
+				asm volatile("movl %0,%%fs" :: "r" (0));
+				ret = checking_wrmsrl(MSR_FS_BASE, addr); 
+			}
+		}
+
+		break;
+
+		/* Returned value may not be correct when the user changed fs/gs */ 
+	case ARCH_GET_FS:
+		if (task->thread.fsindex == FS_TLS_SEL)
+			tmp = read_32bit_tls(task, FS_TLS);
+		else if (doit) {
+			rdmsrl(MSR_FS_BASE, tmp);
+		} else
+			tmp = task->thread.fs;
+		ret = put_user(tmp, (unsigned long *)addr); 
+		break; 
+
+	case ARCH_GET_GS: 
+		if (task->thread.gsindex == GS_TLS_SEL)
+			tmp = read_32bit_tls(task, GS_TLS);
+		else if (doit) {
+			rdmsrl(MSR_KERNEL_GS_BASE, tmp);
+		} else
+			tmp = task->thread.gs;
+		ret = put_user(tmp, (unsigned long *)addr); 
+		break;
+
+	default:
+		ret = -EINVAL;
+		break;
+	} 
+	return ret;	
+} 
+
+asmlinkage long sys_arch_prctl(int code, unsigned long addr)
+{
+	return do_arch_prctl(current, code, addr);
+}
+
+
+
 int copy_thread(int nr, unsigned long clone_flags, unsigned long rsp, 
 		unsigned long unused,
 	struct task_struct * p, struct pt_regs * regs)
 {
+	int err;
 	struct pt_regs * childregs;
 	struct task_struct *me = current;
 
@@ -421,6 +502,7 @@ int copy_thread(int nr, unsigned long cl
 	if (rsp == ~0) {
 		childregs->rsp = (unsigned long)childregs;
 	}
+	p->set_child_tid = p->clear_child_tid = NULL;
 
 	p->thread.rsp = (unsigned long) childregs;
 	p->thread.rsp0 = (unsigned long) (childregs+1);
@@ -447,7 +529,41 @@ int copy_thread(int nr, unsigned long cl
 		       (IO_BITMAP_SIZE+1)*4);
 	} 
 
-	return 0;
+
+	/*
+	 * Set a new TLS for the child thread?
+	 */
+	if (clone_flags & CLONE_SETTLS) {
+#ifdef CONFIG_IA32_EMULATION
+		if (current->thread.flags & THREAD_IA32)
+			err = ia32_child_tls(p, childregs);
+		else
+#endif
+			err = do_arch_prctl(p, ARCH_SET_FS, childregs->r8);
+
+		if (err)
+			goto out;
+	}
+	err = 0;
+
+out:
+	if (err && p->thread.io_bitmap_ptr)
+		kfree(p->thread.io_bitmap_ptr);
+	return err;
+}
+
+/* 
+ * Capture the user space registers if the task is not running (in user space)
+ */
+int dump_task_regs(struct task_struct *tsk, elf_gregset_t *regs)
+{
+	struct pt_regs ptregs;
+	
+	ptregs = *(struct pt_regs *)((unsigned long)tsk + THREAD_SIZE - sizeof(struct pt_regs));
+
+	elf_core_copy_regs(regs, &ptregs);
+
+	return 1;
 }
 
 /*
@@ -467,7 +583,10 @@ struct task_struct *__switch_to(struct t
 {
 	struct thread_struct *prev = &prev_p->thread,
 				 *next = &next_p->thread;
-	struct tss_struct *tss = init_tss + smp_processor_id();
+	int cpu = smp_processor_id();
+	struct tss_struct *tss = init_tss + cpu;
+
+	/* never put a printk in __switch_to... printk() calls wake_up*() indirectly */
 
 	unlazy_fpu(prev_p);
 
@@ -487,6 +606,8 @@ struct task_struct *__switch_to(struct t
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
+	load_TLS(next, cpu);
+
 	/* 
 	 * Switch FS and GS.
 	 */
@@ -499,12 +620,12 @@ struct task_struct *__switch_to(struct t
 		   to avoid an information leak. */
 		if (unlikely((fsindex | next->fsindex) || prev->fs)) {
 			loadsegment(fs, next->fsindex);
-			/* check if the user use a selector != 0
+			/* check if the user used a selector != 0
 			 * if yes clear 64bit base, since overloaded base
-			 * is allways mapped to the Null selector
+			 * is always mapped to the Null selector
 			 */
 			if (fsindex)
-			prev->fs = 0; 
+				prev->fs = 0;
 		}
 		/* when next process has a 64bit base use it */
 		if (next->fs) 
@@ -517,7 +638,7 @@ struct task_struct *__switch_to(struct t
 		if (unlikely((gsindex | next->gsindex) || prev->gs)) {
 			load_gs_index(next->gsindex);
 			if (gsindex)
-			prev->gs = 0;				
+				prev->gs = 0;				
 		}
 		if (next->gs)
 			wrmsrl(MSR_KERNEL_GS_BASE, next->gs); 
@@ -607,14 +728,15 @@ void set_personality_64bit(void)
 
 asmlinkage long sys_fork(struct pt_regs regs)
 {
-	return do_fork(SIGCHLD, regs.rsp, &regs, 0, NULL);
+	return do_fork(SIGCHLD, regs.rsp, &regs, 0, NULL, NULL);
 }
 
-asmlinkage long sys_clone(unsigned long clone_flags, unsigned long newsp, struct pt_regs regs)
+asmlinkage long sys_clone(unsigned long clone_flags, unsigned long newsp, void *parent_tid, void *child_tid, struct pt_regs regs)
 {
 	if (!newsp)
 		newsp = regs.rsp;
-	return do_fork(clone_flags, newsp, &regs, 0, NULL);
+	return do_fork(clone_flags & ~CLONE_IDLETASK, newsp, &regs, 0, 
+		       parent_tid, child_tid);
 }
 
 /*
@@ -629,7 +751,8 @@ asmlinkage long sys_clone(unsigned long 
  */
 asmlinkage long sys_vfork(struct pt_regs regs)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.rsp, &regs, 0, NULL);
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs.rsp, &regs, 0, 
+		       NULL, NULL);
 }
 
 /*
@@ -663,45 +786,3 @@ unsigned long get_wchan(struct task_stru
 #undef last_sched
 #undef first_sched
 
-asmlinkage long sys_arch_prctl(int code, unsigned long addr)
-{ 
-	int ret = 0; 
-	unsigned long tmp; 
-
-	switch (code) { 
-	case ARCH_SET_GS:
-		if (addr >= TASK_SIZE) 
-			return -EPERM; 
-		asm volatile("movl %0,%%gs" :: "r" (0)); 
-		current->thread.gsindex = 0;
-		current->thread.gs = addr;
-		ret = checking_wrmsrl(MSR_KERNEL_GS_BASE, addr); 
-		break;
-	case ARCH_SET_FS:
-		/* Not strictly needed for fs, but do it for symmetry
-		   with gs. */
-		if (addr >= TASK_SIZE)
-			return -EPERM; 
-		asm volatile("movl %0,%%fs" :: "r" (0)); 
-		current->thread.fsindex = 0;
-		current->thread.fs = addr;
-		ret = checking_wrmsrl(MSR_FS_BASE, addr); 
-		break;
-
-		/* Returned value may not be correct when the user changed fs/gs */ 
-	case ARCH_GET_FS:
-		rdmsrl(MSR_FS_BASE, tmp);
-		ret = put_user(tmp, (unsigned long *)addr); 
-		break; 
-
-	case ARCH_GET_GS: 
-		rdmsrl(MSR_KERNEL_GS_BASE, tmp); 
-		ret = put_user(tmp, (unsigned long *)addr); 
-		break;
-
-	default:
-		ret = -EINVAL;
-		break;
-	} 
-	return ret;	
-} 
diff -urNp linux-180/arch/x86_64/kernel/ptrace.c linux-181/arch/x86_64/kernel/ptrace.c
--- linux-180/arch/x86_64/kernel/ptrace.c
+++ linux-181/arch/x86_64/kernel/ptrace.c
@@ -22,6 +22,8 @@
 #include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/debugreg.h>
+#include <asm/ldt.h>
+#include <asm/desc.h>
 
 /*
  * does not yet catch signals sent when the child dies.
@@ -330,6 +332,34 @@ asmlinkage long sys_ptrace(long request,
 		ret = 0;
 		break;
 	}
+#ifdef CONFIG_IA32_EMULATION
+		/* This makes only sense with 32bit programs. Allow a
+		   64bit debugger to fully examine them too. Better
+		   don't use it against 64bit processes, use
+		   PTRACE_ARCH_PRCTL instead. */
+	case PTRACE_SET_THREAD_AREA: {
+		int old; 
+		get_user(old,  &((struct user_desc *)data)->entry_number); 
+		put_user(addr, &((struct user_desc *)data)->entry_number);
+		ret = do_set_thread_area(&child->thread, 
+					 (struct user_desc *)data);
+		put_user(old,  &((struct user_desc *)data)->entry_number); 
+		break;
+	case PTRACE_GET_THREAD_AREA:
+		get_user(old,  &((struct user_desc *)data)->entry_number); 
+		put_user(addr, &((struct user_desc *)data)->entry_number);
+		ret = do_get_thread_area(&child->thread, 
+					 (struct user_desc *)data);
+		put_user(old,  &((struct user_desc *)data)->entry_number); 
+		break;
+	} 
+#endif
+		/* normal 64bit interface to access TLS data.
+		   Works just like arch_prctl, except that the arguments
+		   are reversed. */
+	case PTRACE_ARCH_PRCTL:
+		ret = do_arch_prctl(child, data, addr);
+		break;
 
 /*
  * make the child exit.  Best I can do is send it a sigkill. 
@@ -426,17 +456,8 @@ asmlinkage long sys_ptrace(long request,
 		break;
 	}
 
-	case PTRACE_SETOPTIONS: {
-		if (data & PTRACE_O_TRACESYSGOOD)
-			child->ptrace |= PT_TRACESYSGOOD;
-		else
-			child->ptrace &= ~PT_TRACESYSGOOD;
-		ret = 0;
-		break;
-	}
-
 	default:
-		ret = -EIO;
+		ret = ptrace_request(child, request, addr, data);
 		break;
 	}
 out_tsk:
@@ -466,4 +487,5 @@ asmlinkage void syscall_trace(struct pt_
 		send_sig(current->exit_code, current, 1);
 		current->exit_code = 0;
 	}
+	recalc_sigpending();
 }
diff -urNp linux-180/arch/x86_64/kernel/setup64.c linux-181/arch/x86_64/kernel/setup64.c
--- linux-180/arch/x86_64/kernel/setup64.c
+++ linux-181/arch/x86_64/kernel/setup64.c
@@ -29,7 +29,7 @@ struct x8664_pda cpu_pda[NR_CPUS] __cach
 extern void system_call(void); 
 extern void ia32_cstar_target(void); 
 
-struct desc_ptr gdt_descr = { 0 /* filled in */, (unsigned long) gdt_table }; 
+extern struct desc_ptr cpu_gdt_descr[];
 struct desc_ptr idt_descr = { 256 * 16, (unsigned long) idt_table }; 
 
 /* When you change the default make sure the no EFER path below sets the 
@@ -115,7 +115,7 @@ void pda_init(int cpu)
 	
 	if (cpu == 0) {
 		/* others are initialized in smpboot.c */
-		cpu_pda[cpu].pcurrent = init_tasks[cpu];
+		cpu_pda[cpu].pcurrent = &init_task;
 		cpu_pda[cpu].irqstackptr = boot_cpu_stack; 
 		level4 = init_level4_pgt; 
 	} else {
@@ -143,7 +143,6 @@ void pda_init(int cpu)
 	wrmsrl(MSR_GS_BASE, cpu_pda + cpu);
 } 
 
-#define EXCEPTION_STK_ORDER 0 /* >= N_EXCEPTION_STACKS*EXCEPTION_STKSZ */
 char boot_exception_stacks[N_EXCEPTION_STACKS*EXCEPTION_STKSZ];
 
 /*
@@ -156,33 +155,36 @@ char boot_exception_stacks[N_EXCEPTION_S
 void __init cpu_init (void)
 {
 #ifdef CONFIG_SMP
-	int nr = stack_smp_processor_id();
+	int cpu = stack_smp_processor_id();
 #else
-	int nr = smp_processor_id();
+	int cpu = smp_processor_id();
 #endif
-	struct tss_struct * t = &init_tss[nr];
+	struct tss_struct * t = &init_tss[cpu];
 	unsigned long v, efer; 	
-	char *estacks; 
+	char *estack;
 
 	/* CPU 0 is initialised in head64.c */
-	if (nr != 0) {
-		pda_init(nr);
-		estacks = (char *)__get_free_pages(GFP_ATOMIC, EXCEPTION_STK_ORDER); 
-		if (!estacks)
-			panic("Can't allocate exception stacks for CPU %d\n",nr);
-	} else 
-		estacks = boot_exception_stacks; 
+	if (cpu != 0)
+		pda_init(cpu);
 
-	if (test_and_set_bit(nr, &cpu_initialized))
-		panic("CPU#%d already initialized!\n", nr);
+	if (test_and_set_bit(cpu, &cpu_initialized))
+		panic("CPU#%d already initialized!\n", cpu);
 
-	printk("Initializing CPU#%d\n", nr);
+	printk("Initializing CPU#%d\n", cpu);
 	
 	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
-	gdt_descr.size = NR_CPUS * sizeof(struct per_cpu_gdt) + __GDT_HEAD_SIZE; 
+	/*
+	 * Initialize the per-CPU GDT with the boot GDT,
+	 * and set up the GDT descriptor:
+	 */
+	if (cpu) {
+		memcpy(cpu_gdt_table[cpu], cpu_gdt_table[0], GDT_SIZE);
+		cpu_gdt_descr[cpu].size = GDT_SIZE-1;
+		cpu_gdt_descr[cpu].address = (unsigned long)cpu_gdt_table[cpu];
+	}	
 
-	__asm__ __volatile__("lgdt %0": "=m" (gdt_descr));
+	__asm__ __volatile__("lgdt %0": "=m" (cpu_gdt_descr[cpu]));
 	__asm__ __volatile__("lidt %0": "=m" (idt_descr));
 
 	/*
@@ -220,20 +222,27 @@ void __init cpu_init (void)
 	/*
 	 * set up and load the per-CPU TSS
 	 */
-	estacks += EXCEPTION_STKSZ;
+	estack = boot_exception_stacks + EXCEPTION_STKSZ;
 	for (v = 0; v < N_EXCEPTION_STACKS; v++) {
-		t->ist[v] = (unsigned long)estacks;
-		estacks += EXCEPTION_STKSZ;
+		if (cpu == 0) {
+			t->ist[v] = (u64)estack;
+			estack += EXCEPTION_STKSZ;
+		} else {
+			estack = (char *)__get_free_pages(GFP_ATOMIC, EXCEPTION_STK_ORDER);
+			if (!estack)
+				panic("Can't allocate exception stack %ld for CPU %d\n", v, cpu);
+			t->ist[v] = (u64)estack + EXCEPTION_STKSZ;		
+		}
 	}
 
 	atomic_inc(&init_mm.mm_count);
 	current->active_mm = &init_mm;
 	if(current->mm)
 		BUG();
-	enter_lazy_tlb(&init_mm, current, nr);
+	enter_lazy_tlb(&init_mm, current, cpu);
 
-	set_tss_desc(nr, t);
-	load_TR(nr);
+	set_tss_desc(cpu, t);
+	load_TR_desc();
 	load_LDT(&init_mm);
 
 	/*
diff -urNp linux-180/arch/x86_64/kernel/signal.c linux-181/arch/x86_64/kernel/signal.c
--- linux-180/arch/x86_64/kernel/signal.c
+++ linux-181/arch/x86_64/kernel/signal.c
@@ -93,21 +93,22 @@ sys_rt_sigsuspend(sigset_t *unewset, siz
 		return -EFAULT;
 	sigdelsetmask(&newset, ~_BLOCKABLE);
 
-	spin_lock_irq(&current->sig->siglock);
+	spin_lock_irq(&current->sighand->siglock);
 	saveset = current->blocked;
 	current->blocked = newset;
 	recalc_sigpending();
-	spin_unlock_irq(&current->sig->siglock);
+	__set_current_state(TASK_INTERRUPTIBLE);
+	spin_unlock_irq(&current->sighand->siglock);
 #if DEBUG_SIG
 	printk("rt_sigsuspend savset(%lx) newset(%lx) regs(%p) rip(%lx)\n",
 		saveset, newset, &regs, regs.rip);
 #endif 
 	regs.rax = -EINTR;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
 		schedule();
 		if (do_signal(&regs, &saveset))
 			return -EINTR;
+		set_current_state(TASK_INTERRUPTIBLE);
 	}
 }
 
@@ -192,10 +193,10 @@ asmlinkage long sys_rt_sigreturn(struct 
 		goto badframe;
 
 	sigdelsetmask(&set, ~_BLOCKABLE);
-	spin_lock_irq(&current->sig->siglock);
+	spin_lock_irq(&current->sighand->siglock);
 	current->blocked = set;
 	recalc_sigpending();
-	spin_unlock_irq(&current->sig->siglock);
+	spin_unlock_irq(&current->sighand->siglock);
 	
 	if (restore_sigcontext(&regs, &frame->uc.uc_mcontext, &eax))
 		goto badframe;
@@ -380,9 +381,10 @@ give_sigsegv:
  */	
 
 static void
-handle_signal(unsigned long sig, struct k_sigaction *ka,
-	      siginfo_t *info, sigset_t *oldset, struct pt_regs * regs)
+handle_signal(unsigned long sig, siginfo_t *info, sigset_t *oldset, struct pt_regs * regs)
 {
+	struct k_sigaction *ka = &current->sighand->action[sig-1];
+
 #if DEBUG_SIG
 	printk("handle_signal pid:%d sig:%lu rip:%lx rsp:%lx regs=%p\n", current->pid, sig, 
 		regs->rip, regs->rsp, regs);
@@ -423,11 +425,11 @@ handle_signal(unsigned long sig, struct 
 		ka->sa.sa_handler = SIG_DFL;
 
 	if (!(ka->sa.sa_flags & SA_NODEFER)) {
-		spin_lock_irq(&current->sig->siglock);
+		spin_lock_irq(&current->sighand->siglock);
 		sigorsets(&current->blocked,&current->blocked,&ka->sa.sa_mask);
 		sigaddset(&current->blocked,sig);
 		recalc_sigpending();
-		spin_unlock_irq(&current->sig->siglock);
+		spin_unlock_irq(&current->sighand->siglock);
 	}
 }
 
@@ -439,7 +441,7 @@ handle_signal(unsigned long sig, struct 
 int do_signal(struct pt_regs *regs, sigset_t *oldset)
 {
 	siginfo_t info;
-	struct k_sigaction *ka;
+	int signr;
 
 	/*
 	 * We want the common case to go fast, which
@@ -454,99 +456,8 @@ int do_signal(struct pt_regs *regs, sigs
 	if (!oldset)
 		oldset = &current->blocked;
 
-	for (;;) {
-		unsigned long signr;
-
-		spin_lock_irq(&current->sig->siglock);
-		signr = dequeue_signal(&current->blocked, &info);
-		spin_unlock_irq(&current->sig->siglock);
-
-		if (!signr) { 
-			break;
-		}
-
-		if ((current->ptrace & PT_PTRACED) && signr != SIGKILL) {
-			/* Let the debugger run.  */
-			current->exit_code = signr;
-			current->state = TASK_STOPPED;
-			notify_parent(current, SIGCHLD);
-			schedule();
-
-			/* We're back.  Did the debugger cancel the sig?  */
-			if (!(signr = current->exit_code))
-				continue;
-			current->exit_code = 0;
-
-			/* The debugger continued.  Ignore SIGSTOP.  */
-			if (signr == SIGSTOP)
-				continue;
-
-			/* Update the siginfo structure.  Is this good?  */
-			if (signr != info.si_signo) {
-				info.si_signo = signr;
-				info.si_errno = 0;
-				info.si_code = SI_USER;
-				info.si_pid = current->parent->pid;
-				info.si_uid = current->parent->uid;
-			}
-
-			/* If the (new) signal is now blocked, requeue it.  */
-			if (sigismember(&current->blocked, signr)) {
-				send_sig_info(signr, &info, current);
-				continue;
-			}
-		}
-
-		ka = &current->sig->action[signr-1];
-		if (ka->sa.sa_handler == SIG_IGN) {
-			if (signr != SIGCHLD)
-				continue;
-			/* Check for SIGCHLD: it's special.  */
-			while (sys_wait4(-1, NULL, WNOHANG, NULL) > 0)
-				/* nothing */;
-			continue;
-		}
-
-		if (ka->sa.sa_handler == SIG_DFL) {
-			int exit_code = signr;
-
-			/* Init gets no signals it doesn't want.  */
-			if (current->pid == 1)			      
-				continue;
-
-			switch (signr) {
-			case SIGCONT: case SIGCHLD: case SIGWINCH: case SIGURG:
-				continue;
-
-			case SIGTSTP: case SIGTTIN: case SIGTTOU:
-				if (is_orphaned_pgrp(current->pgrp))
-					continue;
-				/* FALLTHRU */
-
-			case SIGSTOP: {
-				struct signal_struct *sig;
-				current->state = TASK_STOPPED;
-				current->exit_code = signr;
-				sig = current->parent->sig;
-				if (sig && !(sig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))
-					notify_parent(current, SIGCHLD);
-				schedule();
-				continue;
-			}
-
-			case SIGQUIT: case SIGILL: case SIGTRAP:
-			case SIGABRT: case SIGFPE: case SIGSEGV:
-			case SIGBUS: case SIGSYS: case SIGXCPU: case SIGXFSZ:
-				if (do_coredump(signr, regs))
-					exit_code |= 0x80;
-				/* FALLTHRU */
-
-			default:
-				sig_exit(signr, exit_code, &info);
-				/* NOTREACHED */
-			}
-		}
-
+	signr = get_signal_to_deliver(&info, regs);
+	if (signr > 0) {
 		/* Reenable any watchpoints before delivering the
 		 * signal to user space. The processor register will
 		 * have been cleared if the watchpoint triggered
@@ -555,7 +466,7 @@ int do_signal(struct pt_regs *regs, sigs
 		if (current->thread.debugreg[7])
 			asm volatile("movq %0,%%db7" :: "r" (current->thread.debugreg[7]));
 		/* Whee!  Actually deliver the signal.  */
-		handle_signal(signr, ka, &info, oldset, regs);
+		handle_signal(signr, &info, oldset, regs);
 		return 1;
 	}
 
diff -urNp linux-180/arch/x86_64/kernel/smpboot.c linux-181/arch/x86_64/kernel/smpboot.c
--- linux-180/arch/x86_64/kernel/smpboot.c
+++ linux-181/arch/x86_64/kernel/smpboot.c
@@ -37,6 +37,7 @@
 #include <linux/init.h>
 
 #include <linux/mm.h>
+#include <linux/sched.h>
 #include <linux/kernel_stat.h>
 #include <linux/smp_lock.h>
 #include <linux/irq.h>
@@ -131,7 +132,7 @@ static unsigned char *trampoline_base;
 static unsigned long __init setup_trampoline(void)
 {
 	extern volatile __u32 tramp_gdt_ptr; 
-	tramp_gdt_ptr = __pa_symbol(&gdt_table); 
+	tramp_gdt_ptr = __pa_symbol(&cpu_gdt_table); 
 	memcpy(trampoline_base, trampoline_data, trampoline_end - trampoline_data);
 	return virt_to_phys(trampoline_base);
 }
@@ -340,7 +341,7 @@ void __init smp_callin(void)
 	 * (This works even if the APIC is not enabled.)
 	 */
 	phys_id = GET_APIC_ID(apic_read(APIC_ID));
-	cpuid = current->processor;
+	cpuid = current->cpu;
 	if (test_and_set_bit(cpuid, &cpu_online_map)) {
 		printk("huh, phys CPU#%d, CPU#%d already present??\n",
 					phys_id, cpuid);
@@ -473,14 +474,14 @@ void __init initialize_secondary(void)
 extern volatile void *init_rsp; 
 extern void (*initial_code)(void);
 
-static int __init fork_by_hand(void)
+static struct task_struct * __init fork_by_hand(void)
 {
 	struct pt_regs regs;
 	/*
 	 * don't care about the eip and regs settings since
 	 * we'll never reschedule the forked task.
 	 */
-	return do_fork(CLONE_VM|CLONE_PID, 0, &regs, 0);
+	return copy_process(CLONE_VM|CLONE_IDLETASK, 0, &regs, 0, NULL, NULL);
 }
 
 #if APIC_DEBUG
@@ -534,28 +535,20 @@ static int __init do_boot_cpu (int apici
 	 * We can't use kernel_thread since we must avoid to
 	 * reschedule the child.
 	 */
-	if (fork_by_hand() < 0)
+	idle = fork_by_hand();
+	if (IS_ERR(idle))
 		panic("failed fork for CPU %d", cpu);
-
-	/*
-	 * We remove it from the pidhash and the runqueue
-	 * once we got the process:
-	 */
-	idle = init_task.prev_task;
-	if (!idle)
-		panic("No idle process for CPU %d", cpu);
-
-	idle->processor = cpu;
+	wake_up_forked_process(idle);
 	x86_cpu_to_apicid[cpu] = apicid;
 	x86_apicid_to_cpu[apicid] = cpu;
-	idle->cpus_runnable = 1<<cpu; 
+	init_idle(idle, cpu);
+	idle->cpu = cpu;
 	idle->cpus_allowed = 1<<cpu;
-	idle->thread.rip = (unsigned long)start_secondary;
+	idle->thread.rip = (unsigned long) start_secondary;
 	idle->thread.rsp = (unsigned long)idle + THREAD_SIZE - 8;
 
-	del_from_runqueue(idle);
 	unhash_process(idle);
-	cpu_pda[cpu].pcurrent = init_tasks[cpu] = idle;
+	cpu_pda[cpu].pcurrent = idle;
 
 	/* start_eip had better be page-aligned! */
 	start_eip = setup_trampoline();
@@ -773,6 +766,7 @@ static int __init do_boot_cpu (int apici
 }
 
 cycles_t cacheflush_time;
+unsigned long cache_decay_ticks;
 
 static __init void smp_tune_scheduling (void)
 {
@@ -807,10 +801,14 @@ static __init void smp_tune_scheduling (
 	}
 
 	cacheflush_time *= 10;  /* Add an NUMA factor */
+	cache_decay_ticks = (long)cacheflush_time/cpu_khz * HZ / 1000;
 
 	printk("per-CPU timeslice cutoff: %ld.%02ld usecs.\n",
 		(long)cacheflush_time/(cpu_khz/1000),
 		((long)cacheflush_time*100/(cpu_khz/1000)) % 100);
+	printk("task migration cache decay timeout: %ld msecs.\n",
+		(cache_decay_ticks + 1) * 1000 / HZ);
+
 }
 
 /*
@@ -855,8 +853,7 @@ void __init smp_boot_cpus(void)
 	x86_apicid_to_cpu[boot_cpu_id] = 0;
 	x86_cpu_to_apicid[0] = boot_cpu_id;
 	global_irq_holder = 0;
-	current->processor = 0;
-	init_idle();
+	current->cpu = 0;
 	smp_tune_scheduling();
 
 	/*
diff -urNp linux-180/arch/x86_64/mm/fault.c linux-181/arch/x86_64/mm/fault.c
--- linux-180/arch/x86_64/mm/fault.c
+++ linux-181/arch/x86_64/mm/fault.c
@@ -95,7 +95,7 @@ bad:
 }
 
 int page_fault_trace; 
-int exception_trace = 1;
+int exception_trace = 0;
 
 /*
  * This routine handles page faults.  It determines the address,
@@ -302,8 +302,7 @@ no_context:
 out_of_memory:
 	up_read(&mm->mmap_sem);
 	if (current->pid == 1) { 
-		tsk->policy |= SCHED_YIELD;
-		schedule();
+		yield();
 		goto again;
 	}
 	printk("VM: killing process %s\n", tsk->comm);
diff -urNp linux-180/arch/x86_64/tools/offset.c linux-181/arch/x86_64/tools/offset.c
--- linux-180/arch/x86_64/tools/offset.c
+++ linux-181/arch/x86_64/tools/offset.c
@@ -32,9 +32,10 @@ int main(void) 
 	ENTRY(need_resched); 
 	ENTRY(exec_domain); 
 	ENTRY(ptrace); 
-	ENTRY(processor);
+	ENTRY(cpu);
 	ENTRY(need_resched); 
 	ENTRY(thread); 
+	ENTRY(pid);
 #undef ENTRY
 #define ENTRY(entry) outconst("#define pda_" #entry " %0", offsetof(struct x8664_pda, entry))
 	ENTRY(kernelstack); 
@@ -61,8 +62,8 @@ int main(void) 
 	outconst("#define thread_flags %0" , offsetof(struct thread_struct, flags));
 	outconst("#define thread_rsp0 %0" , offsetof(struct thread_struct, rsp0));
 	outconst("#define ASM_THREAD_IA32 %0", THREAD_IA32);
+	outconst("#define PER_CPU_GDT_SIZE %0", sizeof(struct desc_struct));
 
-	outconst("#define PER_CPU_GDT_SIZE %0", sizeof(struct per_cpu_gdt)); 
 	output("#endif"); 
 
 	output("#endif\n"); 
diff -urNp linux-180/include/asm-x86_64/bitops.h linux-181/include/asm-x86_64/bitops.h
--- linux-180/include/asm-x86_64/bitops.h
+++ linux-181/include/asm-x86_64/bitops.h
@@ -427,8 +427,38 @@ static __inline__ unsigned long ffz(unsi
 	return word;
 }
 
+/**
+ * __ffs - find first bit in word.
+ * @word: The word to search
+ *
+ * Undefined if no bit exists, so code should check against 0 first.
+ */
+static __inline__ unsigned long __ffs(unsigned long word)
+{
+	__asm__("bsfq %1,%0"
+		:"=r" (word)
+		:"rm" (word));
+	return word;
+}
+
 #ifdef __KERNEL__
 
+/*
+ * Every architecture must define this function.  It's the fastest
+ * way of searching a 140-bit bitmap where the first 100 bits are
+ * unlikely to be set.  It's guaranteed that at least one of the 140
+ * bits is cleared.
+ */
+static inline int sched_find_first_bit(unsigned long *b)
+{
+	if (b[0])
+		return __ffs(b[0]);
+	if (b[1])
+		return __ffs(b[1]) + 64;
+	if (b[2])
+		return __ffs(b[2]) + 128;
+}
+
 /**
  * ffs - find first bit set
  * @x: the word to search
diff -urNp linux-180/include/asm-x86_64/desc.h linux-181/include/asm-x86_64/desc.h
--- linux-180/include/asm-x86_64/desc.h
+++ linux-181/include/asm-x86_64/desc.h
@@ -10,9 +10,6 @@
 #define __TSS_INDEX(n)  ((n)*64)
 #define __LDT_INDEX(n)  ((n)*64)
 
-extern __u8 gdt_table[];
-extern __u8 gdt_end[];
-
 enum { 
 	GATE_INTERRUPT = 0xE, 
 	GATE_TRAP = 0xF, 	
@@ -37,6 +34,10 @@ struct desc_struct { 
 	unsigned limit : 4, avl : 1, l : 1, d : 1, g : 1, base2 : 8;
 } __attribute__((packed)); 
 
+struct n_desc_struct {
+	int a,b;
+};
+
 // LDT or TSS descriptor in the GDT. 16 bytes.
 struct ldttss_desc { 
 	u16 limit0;
@@ -48,12 +49,7 @@ struct ldttss_desc { 
 } __attribute__((packed)); 
 
 
-struct per_cpu_gdt {
-	struct ldttss_desc tss;
-	struct ldttss_desc ldt; 
-} ____cacheline_aligned; 
-
-extern struct per_cpu_gdt gdt_cpu_table[]; 
+extern struct desc_struct cpu_gdt_table[NR_CPUS][GDT_ENTRIES];
 
 #define PTR_LOW(x) ((unsigned long)(x) & 0xFFFF) 
 #define PTR_MIDDLE(x) (((unsigned long)(x) >> 16) & 0xFFFF)
@@ -73,9 +69,9 @@ struct desc_ptr {
 #define __CPU_DESC_INDEX(x,field) \
 	((x) * sizeof(struct per_cpu_gdt) + offsetof(struct per_cpu_gdt, field) + __GDT_HEAD_SIZE)
 
-#define load_TR(cpu) asm volatile("ltr %w0"::"r" (__CPU_DESC_INDEX(cpu, tss)));
-#define __load_LDT(cpu) asm volatile("lldt %w0"::"r" (__CPU_DESC_INDEX(cpu, ldt)));
-#define clear_LDT(n)  asm volatile("lldt %w0"::"r" (0))
+#define load_TR_desc() asm volatile("ltr %w0"::"r" (GDT_ENTRY_TSS*8))
+#define load_LDT_desc() asm volatile("lldt %w0"::"r" (GDT_ENTRY_LDT*8))
+#define clear_LDT()  asm volatile("lldt %w0"::"r" (0))
 
 extern struct gate_struct idt_table[]; 
 
@@ -121,19 +117,19 @@ static inline void set_tssldt_descriptor
 	dst->base1 = PTR_MIDDLE(ptr) & 0xFF; 
 	dst->type = type;
 	dst->p = 1; 
-	dst->limit1 = 0xF;
+	dst->limit1 = (size >> 16) & 0xF;
 	dst->base2 = (PTR_MIDDLE(ptr) >> 8) & 0xFF; 
 	dst->base3 = PTR_HIGH(ptr); 
 }
 
-static inline void set_tss_desc(unsigned n, void *addr)
+static inline void set_tss_desc(unsigned cpu, void *addr)
 { 
-	set_tssldt_descriptor((void *)&gdt_table + __CPU_DESC_INDEX(n,tss), (unsigned long)addr, DESC_TSS, sizeof(struct tss_struct)); 
+	set_tssldt_descriptor((void*)&cpu_gdt_table[cpu][GDT_ENTRY_TSS], (unsigned long)addr, DESC_TSS, IO_BITMAP_OFFSET + 4 * IO_BITMAP_SIZE + 3);
 } 
 
-static inline void set_ldt_desc(unsigned n, void *addr, int size)
+static inline void set_ldt_desc(unsigned cpu, void *addr, int size)
 { 
-	set_tssldt_descriptor((void *)&gdt_table + __CPU_DESC_INDEX(n,ldt), (unsigned long)addr, DESC_LDT, size); 
+	set_tssldt_descriptor((void*)&cpu_gdt_table[cpu][GDT_ENTRY_LDT], (unsigned long)addr, DESC_LDT, size*8-1);
 }	
 
 /*
@@ -145,12 +141,47 @@ static inline void load_LDT (struct mm_s
 	void *segments = mm->context.segments;
 
 	if (!segments) {
-		clear_LDT(cpu);
+		clear_LDT();
 		return;
 	}
 		
 	set_ldt_desc(cpu, segments, LDT_ENTRIES);
-	__load_LDT(cpu);
+	load_LDT_desc();
+}
+
+#define LDT_entry_a(info) \
+	((((info)->base_addr & 0x0000ffff) << 16) | ((info)->limit & 0x0ffff))
+#define LDT_entry_b(info) \
+	(((info)->base_addr & 0xff000000) | \
+	(((info)->base_addr & 0x00ff0000) >> 16) | \
+	((info)->limit & 0xf0000) | \
+	(((info)->read_exec_only ^ 1) << 9) | \
+	((info)->contents << 10) | \
+	(((info)->seg_not_present ^ 1) << 15) | \
+	((info)->seg_32bit << 22) | \
+	((info)->limit_in_pages << 23) | \
+	((info)->useable << 20) | \
+	((info)->lm << 21) | \
+	0x7000)
+
+#define LDT_empty(info) (\
+	(info)->base_addr	== 0	&& \
+	(info)->limit		== 0	&& \
+	(info)->contents	== 0	&& \
+	(info)->read_exec_only	== 1	&& \
+	(info)->seg_32bit	== 0	&& \
+	(info)->limit_in_pages	== 0	&& \
+	(info)->seg_not_present	== 1	&& \
+	(info)->useable		== 0	&& \
+	(info)->lm		== 0)
+
+
+static inline void load_TLS(struct thread_struct *t, unsigned int cpu)
+{
+	u64 *gdt = (u64 *)(cpu_gdt_table[cpu] + GDT_ENTRY_TLS_MIN);
+	gdt[0] = t->tls_array[0];
+	gdt[1] = t->tls_array[1];
+	gdt[2] = t->tls_array[2];
 }
 
 #endif /* !__ASSEMBLY__ */
diff -urNp linux-180/include/asm-x86_64/ldt.h linux-181/include/asm-x86_64/ldt.h
--- linux-180/include/asm-x86_64/ldt.h
+++ linux-181/include/asm-x86_64/ldt.h
@@ -15,7 +15,7 @@
 /* Note on 64bit base and limit is ignored and you cannot set
    DS/ES/CS not to the default values if you still want to do syscalls. This
    call is more for 32bit mode therefore. */
-struct modify_ldt_ldt_s {
+struct user_desc {
 	unsigned int  entry_number;
 	unsigned int  base_addr;
 	unsigned int  limit;
diff -urNp linux-180/include/asm-x86_64/processor.h linux-181/include/asm-x86_64/processor.h
--- linux-180/include/asm-x86_64/processor.h
+++ linux-181/include/asm-x86_64/processor.h
@@ -28,6 +28,15 @@
 #define VIP_MASK	0x00100000	/* virtual interrupt pending */
 #define ID_MASK		0x00200000
 
+typedef unsigned long desc_struct;
+
+#define desc_empty(desc) \
+		(!((desc)->a | (desc)->b))
+
+#define desc_equal(desc1, desc2) \
+		(((desc1)->a == (desc2)->a) && ((desc1)->b == (desc2)->b))
+
+
 /*
  * Default implementation of macro that returns current
  * instruction pointer ("program counter").
@@ -305,10 +314,16 @@ struct tss_struct {
 	u32 reserved4;
 	u16 reserved5;
 	u16 io_map_base;
-	u32 io_bitmap[IO_BITMAP_SIZE];
+	u32 io_bitmap[IO_BITMAP_SIZE+1];
+	/*
+	 * pads the TSS to be cacheline-aligned (size is 0x100)
+	 */
+	u32 __cacheline_filler[5];
 } __attribute__((packed)) ____cacheline_aligned;
 
 struct thread_struct {
+/* cached TLS descriptors. */
+	desc_struct tls_array[GDT_ENTRY_TLS_ENTRIES];
 	unsigned long	rsp0;
 	unsigned long	rip;
 	unsigned long	rsp;
@@ -338,7 +353,7 @@ struct thread_struct {
 #define DOUBLEFAULT_STACK 2 
 #define NMI_STACK 3 
 #define N_EXCEPTION_STACKS 3  /* hw limit: 7 */
-#define EXCEPTION_STKSZ 1024
+#define EXCEPTION_STKSZ PAGE_SIZE
 #define EXCEPTION_STK_ORDER 0
 
 extern void load_gs_index(unsigned);
@@ -366,8 +381,8 @@ extern void release_thread(struct task_s
 extern long kernel_thread(int (*fn)(void *), void * arg, unsigned long flags);
 
 /* Copy and release all segment info associated with a VM */
-extern void copy_segments(struct task_struct *p, struct mm_struct * mm);
-extern void release_segments(struct mm_struct * mm);
+static inline void copy_segments(struct task_struct *p, struct mm_struct * mm) { }
+static inline void release_segments(struct mm_struct * mm) { }
 
 /*
  * Return saved PC of a blocked thread.
diff -urNp linux-180/include/asm-x86_64/proto.h linux-181/include/asm-x86_64/proto.h
--- linux-180/include/asm-x86_64/proto.h
+++ linux-181/include/asm-x86_64/proto.h
@@ -39,6 +39,8 @@ extern unsigned long start_pfn, end_pfn,
 extern void show_stack(unsigned long * rsp);
 extern void show_trace(unsigned long *stack);
 
+long do_arch_prctl(struct task_struct *task, int code, unsigned long addr);
+
 #define round_up(x,y) (((x) + (y) - 1) & ~((y)-1))
 #define round_down(x,y) ((x) & ~((y)-1))
 
diff -urNp linux-180/include/asm-x86_64/ptrace.h linux-181/include/asm-x86_64/ptrace.h
--- linux-180/include/asm-x86_64/ptrace.h
+++ linux-181/include/asm-x86_64/ptrace.h
@@ -75,6 +75,12 @@ struct pt_regs {
 #define PTRACE_GETFPXREGS         18
 #define PTRACE_SETFPXREGS         19
 
+/* only useful for access 32bit programs */
+#define PTRACE_GET_THREAD_AREA    25
+#define PTRACE_SET_THREAD_AREA    26
+
+#define PTRACE_ARCH_PRCTL	30	/* arch_prctl for child */
+
 #if defined(__KERNEL__) && !defined(__ASSEMBLY__) 
 #define user_mode(regs) (!!((regs)->cs & 3))
 #define instruction_pointer(regs) ((regs)->rip)
diff -urNp linux-180/include/asm-x86_64/segment.h linux-181/include/asm-x86_64/segment.h
--- linux-180/include/asm-x86_64/segment.h
+++ linux-181/include/asm-x86_64/segment.h
@@ -16,5 +16,28 @@
 #define __USER_DS     0x2b   /* 5*8+3 */ 
 #define __USER_CS     0x33   /* 6*8+3 */ 
 #define __USER32_DS	__USER_DS 
+#define __KERNEL16_CS	(GDT_ENTRY_KERNELCS16 * 8)
+
+#define GDT_ENTRY_TLS 1
+#define GDT_ENTRY_TSS 8	/* needs two entries */
+#define GDT_ENTRY_LDT 10
+#define GDT_ENTRY_TLS_MIN 11
+#define GDT_ENTRY_TLS_MAX 13
+/* 14 free */
+#define GDT_ENTRY_KERNELCS16 15
+
+#define GDT_ENTRY_TLS_ENTRIES 3
+
+/* TLS indexes for 64bit - hardcoded in arch_prctl */
+#define FS_TLS 0
+#define GS_TLS 1
+
+#define GS_TLS_SEL ((GDT_ENTRY_TLS_MIN+GS_TLS)*8 + 3)
+#define FS_TLS_SEL ((GDT_ENTRY_TLS_MIN+FS_TLS)*8 + 3)
+
+#define IDT_ENTRIES 256
+#define GDT_ENTRIES 16
+#define GDT_SIZE (GDT_ENTRIES * 8)
+#define TLS_SIZE (GDT_ENTRY_TLS_ENTRIES * 8) 
 
 #endif
diff -urNp linux-180/include/asm-x86_64/smp.h linux-181/include/asm-x86_64/smp.h
--- linux-180/include/asm-x86_64/smp.h
+++ linux-181/include/asm-x86_64/smp.h
@@ -78,7 +78,7 @@ extern void smp_store_cpu_info(int id);	
 
 #define smp_processor_id() read_pda(cpunumber)
 
-#define stack_smp_processor_id() (stack_current()->processor)
+#define stack_smp_processor_id() (stack_current()->cpu)
 
 
 extern __inline int hard_smp_processor_id(void)
diff -urNp linux-180/include/asm-x86_64/system.h linux-181/include/asm-x86_64/system.h
--- linux-180/include/asm-x86_64/system.h
+++ linux-181/include/asm-x86_64/system.h
@@ -51,23 +51,28 @@ struct save_context_frame { 
 	__POP(r15) __POP(r14) __POP(r13) __POP(r12) \
 	__POP(rdi) __POP(rsi)
 
-#define switch_to(prev,next,last) do { void *l; \
-	asm volatile(SAVE_CONTEXT					\
-		     "movq %%rsp,%0\n\t"	/* save RSP */		\
-		     "movq %3,%%rsp\n\t"	/* restore RSP */	\
-		     "leaq thread_return(%%rip),%%rax\n\t"		\
-		     "movq %%rax,%1\n\t"	/* save RIP */		\
-		     "pushq %4\n\t"		/* setup new RIP */	\
-		     "jmp __switch_to\n\t"				\
-		     ".globl thread_return\n"				\
-		     "thread_return:\n\t"				\
-		     RESTORE_CONTEXT					\
-		     :"=m" (prev->thread.rsp),"=m" (prev->thread.rip), "=a" (l) \
-		     :"m" (next->thread.rsp),"m" (next->thread.rip),	\
-		      "S" (next), "D" (prev)				\
-		     :"memory","cc");					\
-	last = l; 							\
-} while(0)
+/* RED-PEN: pipeline stall on ret because it is not predicted */
+/* RED-PEN: the register saving could be optimized */
+/* frame pointer must be last for get_wchan */
+
+#define switch_to(prev,next,last) \
+	asm volatile(SAVE_CONTEXT					    \
+		     "movq %%rsp,%[prevrsp]\n\t"			    \
+		     "movq %[nextrsp],%%rsp\n\t"			    \
+		     "movq $thread_return,%[prevrip]\n\t"		    \
+		     "pushq %[nextrip]\n\t"				    \
+		     "jmp __switch_to\n\t"				    \
+		     ".globl thread_return\n"				    \
+		     "thread_return:\n\t"				    \
+		     RESTORE_CONTEXT					    \
+		     :[prevrsp] "=m" (prev->thread.rsp), 		    \
+		      [prevrip] "=m" (prev->thread.rip),	    	    \
+		      "=a" (last)					    \
+		     :[nextrsp] "m" (next->thread.rsp), 		    \
+		      [nextrip] "m" (next->thread.rip),			    \
+		      [next] "S" (next), [prev] "D" (prev)  		    \
+	             :"memory")
+    
 		     
 extern void load_gs_index(unsigned); 
 
