diff -urNp linux-7070/fs/buffer.c linux-7080/fs/buffer.c
--- linux-7070/fs/buffer.c
+++ linux-7080/fs/buffer.c
@@ -2337,6 +2337,7 @@ int prepare_direct_IO_iobuf(int rw, stru
 		bh.b_state = 0;
 		bh.b_dev = inode->i_dev;
 		bh.b_size = blocksize;
+		bh.b_page = NULL;
 
 		chunksize = sectors_per_block - s_offset;
 		if (chunksize > nr_sectors)
@@ -2507,7 +2508,7 @@ int brw_kiovec(int rw, int nr, struct ki
 	int		length;
 	int		transferred;
 	int		i;
-	int		bufind;
+	int		bufind, nr_bhs;
 	int		pageind;
 	int		bhind;
 	int		offset;
@@ -2515,14 +2516,19 @@ int brw_kiovec(int rw, int nr, struct ki
 	struct kiobuf *	iobuf = NULL;
 	struct page *	map;
 	struct buffer_head *tmp, **bhs = NULL, *bh_first, *bh_prev;
-	int		iosize = size;
+	int		tmpsize, iosize = size;
+	int		size_shift;
 
 	if (!nr)
 		return 0;
 
+	tmpsize = size;
+	for (size_shift = 9; tmpsize > 512; tmpsize >>= 1, size_shift++);
+	
 	/* 
 	 * First, do some alignment and validity checks 
 	 */
+	nr_bhs = 0;
 	for (i = 0; i < nr; i++) {
 		iobuf = iovec[i];
 		if ((iobuf->offset & (size-1)) ||
@@ -2530,8 +2536,24 @@ int brw_kiovec(int rw, int nr, struct ki
 			return -EINVAL;
 		if (!iobuf->nr_pages)
 			panic("brw_kiovec: iobuf not initialised");
+		nr_bhs += (iobuf->length >> size_shift);
 	}
 
+	if  (iobuf->varyio) {
+		/* If it's a varyio IO, we might end up needing fewer
+		 * bh's than we've got blocks, but only if the blocks
+		 * are all contiguous (which they won't necessarily be
+		 * for O_DIRECT.
+		 * 
+		 * Be optimistic here, we just have to expand the iobuf
+		 * later on if we don't allocate enough up-front (but
+		 * that should be the rare path.) */
+		nr_bhs >>= (PAGE_SHIFT - size_shift);
+		if (nr_bhs > KIO_MAX_SECTORS)
+			nr_bhs = KIO_MAX_SECTORS;
+	}
+	alloc_kiobuf_bhs(iobuf, nr_bhs, GFP_KERNEL);
+		
 	/* 
 	 * OK to walk down the iovec doing page IO on each page we find. 
 	 */
@@ -2567,19 +2589,22 @@ int brw_kiovec(int rw, int nr, struct ki
 					} else
 						BUG();
 				}
-				if (iobuf->varyio &&
-				    (!(offset & RAWIO_BLOCKMASK))) {
+				if (iobuf->varyio) {
 					int block_iter;
-					iosize = RAWIO_BLOCKSIZE; 
-					if (iosize > length)
-						iosize = length;
-					for (block_iter = 1; block_iter < iosize / size; block_iter++) {
+					iosize = (PAGE_SIZE-offset) < length ?
+						 (PAGE_SIZE-offset) : length;
+					for (block_iter = 1; block_iter < (iosize >> size_shift); block_iter++) {
 						if (blocknr + block_iter != b[bufind + block_iter]) {
 							iosize = size;
 							break;
 						}
 					}
 				}
+				if (bhind >= nr_bhs) {
+					alloc_kiobuf_bhs(iobuf, ++nr_bhs,
+							 GFP_KERNEL);
+					bhs = iobuf->bh;
+				}
 				tmp = bhs[bhind++];
 
 				tmp->b_size = iosize;
@@ -2592,16 +2617,10 @@ int brw_kiovec(int rw, int nr, struct ki
 				init_buffer(tmp, end_buffer_io_kiobuf, iobuf);
 				tmp->b_dev = dev;
 				tmp->b_blocknr = blocknr;
-				tmp->b_rsector = blocknr * (size >> 9);
-				tmp->b_state = (1 << BH_Mapped) | (1 << BH_Lock) | (1 << BH_Req);
+				tmp->b_rsector = blocknr << (size_shift - 9);
+				tmp->b_state = (1 << BH_Mapped) | (1 << BH_Lock) | (1 << BH_Req) | (1 << BH_Uptodate);
 				tmp->b_reqnext = NULL;
 
-				if (rw == WRITE) {
-					set_bit(BH_Uptodate, &tmp->b_state);
-					clear_bit(BH_Dirty, &tmp->b_state);
-				} else
-					set_bit(BH_Uptodate, &tmp->b_state);
-
 				atomic_inc(&iobuf->io_count);
 
 				if (bh_prev)
@@ -2626,7 +2645,7 @@ submit_io:
 				}
 
 			skip_block:
-				bufind += iosize / size;
+				bufind += iosize >> size_shift;
 				length -= iosize;
 				offset += iosize;
 
@@ -3301,7 +3320,7 @@ struct brw_cb {
 	kvec_cb_t		cb;
 	atomic_t		io_count;
 	int			nr;
-	struct buffer_head	*bh[1];
+	struct kiobuf		*bh_cache;
 };
 
 static inline void brw_cb_put(struct brw_cb *brw_cb)
@@ -3309,22 +3328,24 @@ static inline void brw_cb_put(struct brw
 	if (atomic_dec_and_test(&brw_cb->io_count)) {
 		ssize_t res = 0, err = 0;
 		int nr;
+		struct buffer_head **bhs = brw_cb->bh_cache->bh;
 
 		/* Walk the buffer heads associated with this kiobuf
 		 * checking for errors and freeing them as we go.
 		 */
 		for (nr=0; nr < brw_cb->nr; nr++) {
-			struct buffer_head *bh = brw_cb->bh[nr];
+			struct buffer_head *bh = bhs[nr];
 			if (!err && buffer_uptodate(bh))
 				res += bh->b_size;
 			else
 				err = -EIO;
-			kmem_cache_free(bh_cachep, bh);
 		}
 
 		if (!res)
 			res = err;
 
+		free_kiovec(1, &brw_cb->bh_cache);
+		
 		brw_cb->cb.fn(brw_cb->cb.data, brw_cb->cb.vec, res);
 
 		kfree(brw_cb);
@@ -3365,6 +3386,8 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 {
 	struct kvec	*vec = cb.vec;
 	struct kveclet	*veclet;
+	struct kiobuf	*bh_cache;
+	int		nr_bhs, nr_sectors, wanted;
 	int		err;
 	int		length;
 	unsigned	sector_size = 1 << sector_shift;
@@ -3396,22 +3419,31 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 	/* 
 	 * OK to walk down the iovec doing page IO on each page we find. 
 	 */
-	err = 0;
-
 	if (!blocks) {
 		printk("brw_kiovec_async: !i\n");
 		return -EINVAL;
 	}
 
 	/* FIXME: tie into userbeans here */
-	brw_cb = kmalloc(sizeof(*brw_cb) + (blocks * sizeof(struct buffer_head *)), GFP_KERNEL);
+	brw_cb = kmalloc(sizeof(*brw_cb), GFP_KERNEL);
 	if (!brw_cb)
 		return -ENOMEM;
 
 	brw_cb->cb = cb;
 	brw_cb->nr = 0;
 	bh_prev = NULL;
-	
+
+	err = alloc_kiovec(1, &bh_cache);
+	if (err)
+		goto error;
+	wanted = blocks;
+	if (can_do_varyio)
+		wanted >>= (PAGE_SHIFT - sector_shift);
+	alloc_kiobuf_bhs(bh_cache, wanted, GFP_NOIO);
+
+	nr_bhs = nr_sectors = 0;
+	brw_cb->bh_cache = bh_cache;
+
 	/* This is ugly.  FIXME. */
 	for (i=0, veclet=vec->veclet; i<vec->nr; i++,veclet++) {
 		struct page *page = veclet->page;
@@ -3425,19 +3457,22 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 			struct buffer_head *tmp;
 			int iosize;
 
-			if (can_do_varyio && ((offset & RAWIO_BLOCKMASK) == 0)) {
-				iosize = RAWIO_BLOCKSIZE;
-				if (iosize > length)
-					iosize = length;
-				if (offset+iosize > PAGE_SIZE)
-					iosize = PAGE_SIZE - offset;
+			if (can_do_varyio) {
+				iosize = length;
 			} else
 				iosize = sector_size;
 			
-			tmp = kmem_cache_alloc(bh_cachep, GFP_NOIO);
-			err = -ENOMEM;
-			if (!tmp)
-				goto error;
+			if (nr_bhs >= bh_cache->bh_len) {
+				err = alloc_kiobuf_bhs(bh_cache, nr_bhs + 1, 
+						       GFP_NOIO);
+				if (err)
+					goto error;
+			}
+			tmp = bh_cache->bh[nr_bhs++];
+			brw_cb->nr++;
+			nr_sectors += (iosize >> sector_shift);
+			if (nr_sectors < blocks && nr_bhs < bh_cache->bh_len)
+				prefetch(bh_cache->bh[nr_bhs]);
 
 			tmp->b_dev = B_FREE;
 			tmp->b_size = iosize;
@@ -3448,9 +3483,15 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 			tmp->b_dev = dev;
 			tmp->b_blocknr = blknr;
 			tmp->b_rsector = blknr << (sector_shift - 9);
-			blknr += (iosize / sector_size);
-			tmp->b_state = (1 << BH_Mapped) | (1 << BH_Lock)
-					| (1 << BH_Req);
+			blknr += (iosize >> sector_shift);
+			if (rw == WRITE)
+				tmp->b_state = (1 << BH_Mapped) |
+						(1 << BH_Lock) |
+					       	(1 << BH_Req) |
+						(1 << BH_Uptodate);
+			else
+				tmp->b_state = (1 << BH_Mapped) |
+				       	(1 << BH_Lock) | (1 << BH_Req);
 			tmp->b_private = brw_cb;
 
 			tmp->b_reqnext = NULL;
@@ -3458,12 +3499,6 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 				bh_prev->b_reqnext = tmp;
 			bh_prev = tmp;
 			
-			if (rw == WRITE) {
-				set_bit(BH_Uptodate, &tmp->b_state);
-				clear_bit(BH_Dirty, &tmp->b_state);
-			}
-
-			brw_cb->bh[brw_cb->nr++] = tmp;
 			length -= iosize;
 			offset += iosize;
 
@@ -3472,7 +3507,7 @@ int brw_kvec_async(int rw, kvec_cb_t cb,
 				break;
 			}
 
-			if (brw_cb->nr >= blocks)
+			if (nr_sectors >= blocks)
 				goto submit;
 		} /* End of block loop */
 	} /* End of page loop */		
@@ -3481,7 +3516,7 @@ submit:
 	atomic_set(&brw_cb->io_count, brw_cb->nr+1);
 	/* okay, we've setup all our io requests, now fire them off! */
 	if (brw_cb->nr)
-		submit_bh_linked(rw, brw_cb->bh[0]);
+		submit_bh_linked(rw, bh_cache->bh[0]);
 	brw_cb_put(brw_cb);
 	run_task_queue(&tq_disk);
 	return 0;
@@ -3491,8 +3526,7 @@ error:
 	if (brw_cb) {
 		/* We got an error allocating the bh'es.  Just free the current
 		   buffer_heads and exit. */
-		for (i=0; i<brw_cb->nr; i++)
-			kmem_cache_free(bh_cachep, brw_cb->bh[i]);
+		free_kiovec(1, &bh_cache);
 		kfree(brw_cb);
 	}
 
diff -urNp linux-7070/fs/iobuf.c linux-7080/fs/iobuf.c
--- linux-7070/fs/iobuf.c
+++ linux-7080/fs/iobuf.c
@@ -9,7 +9,8 @@
 #include <linux/slab.h>
 #include <linux/iobuf.h>
 #include <linux/vmalloc.h>
-
+#include <linux/locks.h>
+#include <linux/interrupt.h>
 
 kmem_cache_t *kiobuf_cachep;
 
@@ -31,6 +32,7 @@ static int kiobuf_init(struct kiobuf *io
 
 	init_waitqueue_head(&iobuf->wait_queue);
 	iobuf->array_len = 0;
+	iobuf->bh_len = 0;
 	iobuf->nr_pages = 0;
 	iobuf->locked = 0;
 	iobuf->varyio = 0;
@@ -39,47 +41,48 @@ static int kiobuf_init(struct kiobuf *io
 	atomic_set(&iobuf->io_count, 0);
 	iobuf->end_io = NULL;
 	iobuf->initialized = 0;
-	retval = expand_kiobuf(iobuf, KIO_STATIC_PAGES);
-	if (retval) return retval;
-	retval = alloc_kiobuf_bhs(iobuf);
-	if (retval) {
-		kfree(iobuf->maplist);
-		return retval;
+	if (!in_interrupt()) {
+		retval = expand_kiobuf(iobuf, KIO_STATIC_PAGES);
+		if (retval)
+			return retval;
+		iobuf->initialized = 1;
 	}
-	iobuf->initialized = 1;
 	return 0;
 }
 
-int alloc_kiobuf_bhs(struct kiobuf * kiobuf)
+int alloc_kiobuf_bhs(struct kiobuf * kiobuf, int nr, int gfp_mask)
 {
 	int i;
-
-	kiobuf->blocks =
-		kmalloc(sizeof(*kiobuf->blocks) * KIO_MAX_SECTORS, GFP_KERNEL);
-	if (unlikely(!kiobuf->blocks))
-		goto nomem;
-	kiobuf->bh =
-		kmalloc(sizeof(*kiobuf->bh) * KIO_MAX_SECTORS, GFP_KERNEL);
-	if (unlikely(!kiobuf->bh))
+	struct buffer_head **new_bh;
+	if (kiobuf->bh_len >= nr)
+		return 0;
+	
+	new_bh = kmalloc(sizeof(*new_bh) * nr, gfp_mask);
+	if (unlikely(!new_bh))
 		goto nomem;
 
-	for (i = 0; i < KIO_MAX_SECTORS; i++) {
-		kiobuf->bh[i] = kmem_cache_alloc(bh_cachep, GFP_KERNEL);
-		if (unlikely(!kiobuf->bh[i]))
+	/* Copy any existing bh'es into the new vector */
+	if (kiobuf->bh_len) {
+		memcpy(new_bh, kiobuf->bh, 
+		       kiobuf->bh_len * sizeof(*new_bh));
+	}
+
+	for (i = kiobuf->bh_len; i < nr; i++) {
+		new_bh[i] = kmem_cache_alloc(bh_cachep, gfp_mask);
+		if (unlikely(!new_bh[i]))
 			goto nomem2;
 	}
 
+	kfree(kiobuf->bh);
+	kiobuf->bh = new_bh;
+	kiobuf->bh_len = nr;
 	return 0;
 
 nomem2:
-	while (i--) {
-		kmem_cache_free(bh_cachep, kiobuf->bh[i]);
-		kiobuf->bh[i] = NULL;
-	}
-	memset(kiobuf->bh, 0, sizeof(*kiobuf->bh) * KIO_MAX_SECTORS);
-
+	while (i-- >= kiobuf->bh_len)
+		kmem_cache_free(bh_cachep, new_bh[i]);
+	kfree(new_bh);
 nomem:
-	free_kiobuf_bhs(kiobuf);
 	return -ENOMEM;
 }
 
@@ -88,16 +91,12 @@ void free_kiobuf_bhs(struct kiobuf * kio
 	int i;
 
 	if (kiobuf->bh) {
-		for (i = 0; i < KIO_MAX_SECTORS; i++)
+		for (i = 0; i < kiobuf->bh_len; i++)
 			if (kiobuf->bh[i])
 				kmem_cache_free(bh_cachep, kiobuf->bh[i]);
 		kfree(kiobuf->bh);
 		kiobuf->bh = NULL;
-	}
-
-	if (kiobuf->blocks) {
-		kfree(kiobuf->blocks);
-		kiobuf->blocks = NULL;
+		kiobuf->bh_len = 0;
 	}
 }
 
@@ -112,17 +111,35 @@ void kiobuf_dtor(void * objp, kmem_cache
 	struct kiobuf * iobuf = (struct kiobuf *) objp;
 	if (iobuf->initialized) {
 		kfree(iobuf->maplist);
+		kfree(iobuf->blocks);
 		free_kiobuf_bhs(iobuf);
 	}
 }
 
+static spinlock_t kiobuf_lock = SPIN_LOCK_UNLOCKED;
+
+struct kiobuf *kiobuf_cache_list = NULL;
+int kiobuf_cache = 0;
+int kiobuf_cache_max = 0;
+
 int alloc_kiovec(int nr, struct kiobuf **bufp)
 {
 	int i;
 	struct kiobuf *iobuf;
+	unsigned long flags;
 	
 	for (i = 0; i < nr; i++) {
-		iobuf = kmem_cache_alloc(kiobuf_cachep, GFP_KERNEL);
+		spin_lock_irqsave(&kiobuf_lock, flags);
+		if (kiobuf_cache) {
+			iobuf = kiobuf_cache_list;
+			kiobuf_cache_list = (struct kiobuf *)iobuf->end_io;
+			iobuf->end_io = NULL;
+			kiobuf_cache--;
+			spin_unlock_irqrestore(&kiobuf_lock, flags);
+		} else {
+			spin_unlock_irqrestore(&kiobuf_lock, flags);
+			iobuf = kmem_cache_alloc(kiobuf_cachep, GFP_KERNEL);
+		}
 		if (unlikely(!iobuf))
 			goto nomem;
 		if (unlikely(!iobuf->initialized)) {
@@ -146,19 +163,39 @@ void free_kiovec(int nr, struct kiobuf *
 {
 	int i;
 	struct kiobuf *iobuf;
+	unsigned long flags;
 	
 	for (i = 0; i < nr; i++) {
 		iobuf = bufp[i];
 		init_waitqueue_head(&iobuf->wait_queue);
 		iobuf->io_count.counter = 0;
 		iobuf->end_io = NULL;
-		kmem_cache_free(kiobuf_cachep, iobuf);
+		if (kiobuf_cache_max) {
+			spin_lock_irqsave(&kiobuf_lock, flags);
+			if (kiobuf_cache < kiobuf_cache_max) {
+				iobuf->end_io =
+				  (void (*)(struct kiobuf *))kiobuf_cache_list;
+				kiobuf_cache_list = iobuf;
+				kiobuf_cache++;
+				spin_unlock_irqrestore(&kiobuf_lock, flags);
+			} else {
+				spin_unlock_irqrestore(&kiobuf_lock, flags);
+				kiobuf_dtor(iobuf, kiobuf_cachep, 0);
+				kiobuf_ctor(iobuf, kiobuf_cachep, 0);
+				kmem_cache_free(kiobuf_cachep, iobuf);
+			}
+		} else
+			kmem_cache_free(kiobuf_cachep, iobuf);
 	}
 }
 
+#define SECTOR_SIZE 512
+#define SECTORS_PER_PAGE (PAGE_SIZE / SECTOR_SIZE)
+
 int expand_kiobuf(struct kiobuf *iobuf, int wanted)
 {
 	struct page ** maplist;
+	unsigned long *blocks;
 	
 	if (iobuf->array_len >= wanted)
 		return 0;
@@ -166,19 +203,31 @@ int expand_kiobuf(struct kiobuf *iobuf, 
 	maplist = kmalloc(wanted * sizeof(struct page **), GFP_KERNEL);
 	if (unlikely(!maplist))
 		return -ENOMEM;
-
+	blocks = kmalloc(wanted * SECTORS_PER_PAGE * sizeof(unsigned long), 
+			 GFP_KERNEL);
+	if (unlikely(!blocks)) {
+		kfree(maplist);
+		return -ENOMEM;
+	}
+	
 	/* Did it grow while we waited? */
 	if (unlikely(iobuf->array_len >= wanted)) {
 		kfree(maplist);
+		kfree(blocks);
 		return 0;
 	}
 
 	if (iobuf->array_len) {
-		memcpy(maplist, iobuf->maplist, iobuf->array_len * sizeof(*maplist));
+		memcpy(maplist, iobuf->maplist, 
+		       iobuf->array_len * sizeof(*maplist));
+		memcpy(blocks, iobuf->blocks,
+		       iobuf->array_len * sizeof(*blocks) * SECTORS_PER_PAGE);
 		kfree(iobuf->maplist);
+		kfree(iobuf->blocks);
 	}
 	
 	iobuf->maplist   = maplist;
+	iobuf->blocks    = blocks;
 	iobuf->array_len = wanted;
 	return 0;
 }
diff -urNp linux-7070/include/linux/iobuf.h linux-7080/include/linux/iobuf.h
--- linux-7070/include/linux/iobuf.h
+++ linux-7080/include/linux/iobuf.h
@@ -26,18 +26,16 @@
  */
 
 #define KIO_MAX_ATOMIC_IO	512 /* in kb */
-#define KIO_STATIC_PAGES	(KIO_MAX_ATOMIC_IO / (PAGE_SIZE >> 10) + 1)
+#define KIO_STATIC_PAGES	4
 #define KIO_MAX_SECTORS		(KIO_MAX_ATOMIC_IO * 2)
 
-#define RAWIO_BLOCKSIZE		4096
-#define RAWIO_BLOCKMASK		(RAWIO_BLOCKSIZE-1)
-
 /* The main kiobuf struct used for all our IO! */
 
 struct kiobuf 
 {
 	int		nr_pages;	/* Pages actually referenced */
-	int		array_len;	/* Space in the allocated lists */
+	int		array_len;	/* Space in the allocated map lists */
+	int		bh_len;		/* Nr of bhes currently allocated */
 	int		offset;		/* Offset to start of valid data */
 	int		length;		/* Number of valid bytes of data */
 
@@ -75,7 +73,7 @@ int	alloc_kiovec(int nr, struct kiobuf *
 void	free_kiovec(int nr, struct kiobuf **);
 int	expand_kiobuf(struct kiobuf *, int);
 void	kiobuf_wait_for_io(struct kiobuf *);
-extern int alloc_kiobuf_bhs(struct kiobuf *);
+extern int alloc_kiobuf_bhs(struct kiobuf *, int nr, int gfp_mask);
 extern void free_kiobuf_bhs(struct kiobuf *);
 extern kmem_cache_t *kiobuf_cachep;
 
diff -urNp linux-7070/include/linux/sysctl.h linux-7080/include/linux/sysctl.h
--- linux-7070/include/linux/sysctl.h
+++ linux-7080/include/linux/sysctl.h
@@ -637,6 +637,7 @@ enum
 	FS_AIO_MAX_SIZE=20,	/* int: max size of read/write chunks */
 	FS_AIO_MAX_PINNED=21,	/* int: max memory pinned (in pages) */
 	FS_AIO_PINNED=22,	/* int: cur memory pinned (in pages) */
+	FS_KIOBUF_CACHE_MAX=23,	/* int: max kiobuf cache size */
 };
 
 /* /proc/sys/fs/quota/ */
diff -urNp linux-7070/kernel/sysctl.c linux-7080/kernel/sysctl.c
--- linux-7070/kernel/sysctl.c
+++ linux-7080/kernel/sysctl.c
@@ -395,6 +395,7 @@ static ctl_table proc_table[] = {
 };
 
 extern int user_pinned_pages;
+extern int kiobuf_cache_max;
 
 static ctl_table fs_table[] = {
 	{FS_NRINODE, "inode-nr", &inodes_stat, 2*sizeof(int),
@@ -429,6 +430,8 @@ static ctl_table fs_table[] = {
 	 sizeof(aio_max_pinned), 0644, NULL, &proc_dointvec},
 	{FS_AIO_PINNED, "aio-pinned", &user_pinned_pages,
 	 sizeof(user_pinned_pages), 0644, NULL, &proc_dointvec},
+	{FS_KIOBUF_CACHE_MAX, "kiobuf-cache-max", &kiobuf_cache_max,
+	 sizeof(kiobuf_cache_max), 0644, NULL, &proc_dointvec},
 	{0}
 };
 
