diff -urNp linux-204/arch/i386/kernel/io_apic.c linux-210/arch/i386/kernel/io_apic.c
--- linux-204/arch/i386/kernel/io_apic.c
+++ linux-210/arch/i386/kernel/io_apic.c
@@ -1340,6 +1340,8 @@ static void end_level_ioapic_irq (unsign
 
 static void mask_and_ack_level_ioapic_irq (unsigned int irq) { /* nothing */ }
 
+extern int no_valid_irqaffinity;
+
 static void set_ioapic_affinity (unsigned int irq, unsigned long cpu_mask)
 {
 	unsigned int apicid_value = cpu_mask_to_apicid(cpu_mask) << 24;
@@ -1347,6 +1349,9 @@ static void set_ioapic_affinity (unsigne
 	unsigned long flags;
 	int pin;
 	
+	if (no_valid_irqaffinity)
+		return;
+	
 	spin_lock_irqsave(&ioapic_lock, flags);
 	for (;;) {
 		pin = entry->pin;
diff -urNp linux-204/arch/i386/kernel/irq.c linux-210/arch/i386/kernel/irq.c
--- linux-204/arch/i386/kernel/irq.c
+++ linux-210/arch/i386/kernel/irq.c
@@ -1104,6 +1104,15 @@ static int irq_affinity_write_proc (stru
 {
 	int irq = (long) data, full_count = count, err;
 	unsigned long new_value;
+	extern int no_valid_irqaffinity;
+
+	/* HACK: Since some proc entries get set up before
+	 * we determine (via pci-quirks) that we should never
+	 * modify IRQ affinities.  If so, disallow the write
+	 * even if the proc entry permissions allow...
+	 */
+	if (no_valid_irqaffinity)
+		return -EROFS;
 
 	if (!irq_desc[irq].handler->set_affinity)
 		return -EIO;
@@ -1168,9 +1177,15 @@ static void register_irq_proc (unsigned 
 #if CONFIG_SMP
 	{
 		struct proc_dir_entry *entry;
+		int mask;
+		extern int no_valid_irqaffinity;
+
+		mask = 0600;
+		if (no_valid_irqaffinity)
+			mask = 0400;
 
 		/* create /proc/irq/1234/smp_affinity */
-		entry = create_proc_entry("smp_affinity", 0600, irq_dir[irq]);
+		entry = create_proc_entry("smp_affinity", mask, irq_dir[irq]);
 
 		if (entry) {
 			entry->nlink = 1;
diff -urNp linux-204/arch/i386/kernel/microcode.c linux-210/arch/i386/kernel/microcode.c
--- linux-204/arch/i386/kernel/microcode.c
+++ linux-210/arch/i386/kernel/microcode.c
@@ -371,7 +371,13 @@ static void do_update_one (void * unused
 	spin_lock_irqsave(&microcode_update_lock, flags);          
 
 	/* write microcode via MSR 0x79 */
+#if !defined(CONFIG_X86_64)
 	wrmsr(MSR_IA32_UCODE_WRITE, (unsigned int)(uci->mc->bits), 0);
+#else
+	wrmsr(MSR_IA32_UCODE_WRITE, (unsigned long)(uci->mc->bits), 
+					((unsigned long)(uci->mc->bits)) >> 32);
+#endif
+
 	wrmsr(MSR_IA32_UCODE_REV, 0, 0);
 
 	__asm__ __volatile__ ("cpuid" : : : "ax", "bx", "cx", "dx");
diff -urNp linux-204/arch/i386/kernel/setup.c linux-210/arch/i386/kernel/setup.c
--- linux-204/arch/i386/kernel/setup.c
+++ linux-210/arch/i386/kernel/setup.c
@@ -74,6 +74,7 @@
  *  Provisions for empty E820 memory regions (reported by certain BIOSes).
  *  Alex Achenbach <xela@slit.de>, December 2002.
  *
+ *  Adding cache identification through cpuid(4)
  */
 
 /*
@@ -2268,6 +2269,8 @@ static struct _cache_table cache_table[]
 	{ 0x7A, LVL_2,      256 },
 	{ 0x7B, LVL_2,      512 },
 	{ 0x7C, LVL_2,      1024 },
+	{ 0x7D, LVL_2,      2048 },
+	{ 0x7F, LVL_2,      512 },
 	{ 0x82, LVL_2,      256 },
 	{ 0x83, LVL_2,      512 },
 	{ 0x84, LVL_2,      1024 },
@@ -2275,10 +2278,111 @@ static struct _cache_table cache_table[]
 	{ 0x00, 0, 0}
 };
 
+/* Some CPUID calls want 'count' to be placed in ecx */
+static inline void cpuid_count(int op, int count, int *eax, int *ebx, int *ecx,
+	       	int *edx)
+{
+	__asm__("cpuid"
+		: "=a" (*eax),
+		  "=b" (*ebx),
+		  "=c" (*ecx),
+		  "=d" (*edx)
+		: "0" (op), "c" (count));
+}
+
+enum _cache_type
+{
+	CACHE_TYPE_NULL	= 0,
+	CACHE_TYPE_DATA = 1,
+	CACHE_TYPE_INST = 2,
+	CACHE_TYPE_UNIFIED = 3
+};
+
+union _cpuid4_leaf_eax {
+	struct {
+		enum _cache_type	type:5;
+		unsigned int		level:3;
+		unsigned int		is_self_initializing:1;
+		unsigned int		is_fully_associative:1;
+		unsigned int		reserved:4;
+		unsigned int		num_threads_sharing:12;
+		unsigned int		num_cores_on_die:6;
+	} split;
+	u32 full;
+};
+
+union _cpuid4_leaf_ebx {
+	struct {
+		unsigned int		coherency_line_size:12;
+		unsigned int		physical_line_partition:10;
+		unsigned int		ways_of_associativity:10;
+	} split;
+	u32 full;
+};
+
+union _cpuid4_leaf_ecx {
+	struct {
+		unsigned int		number_of_sets:32;
+	} split;
+	u32 full;
+};
+
+struct _cpuid4_info {
+	union _cpuid4_leaf_eax eax;
+	union _cpuid4_leaf_ebx ebx;
+	union _cpuid4_leaf_ecx ecx;
+	unsigned long size;
+};
+
+#define MAX_CACHE_LEAVES		4
+
+static int __init cpuid4_cache_lookup(int index, struct _cpuid4_info *this_leaf)
+{
+	unsigned int		eax, ebx, ecx, edx;
+	union _cpuid4_leaf_eax	cache_eax;
+
+	cpuid_count(4, index, &eax, &ebx, &ecx, &edx);
+	cache_eax.full = eax;
+	if (cache_eax.split.type == CACHE_TYPE_NULL)
+		return -1;
+
+	this_leaf->eax.full = eax;
+	this_leaf->ebx.full = ebx;
+	this_leaf->ecx.full = ecx;
+	this_leaf->size = (this_leaf->ecx.split.number_of_sets + 1) *
+		(this_leaf->ebx.split.coherency_line_size + 1) *
+		(this_leaf->ebx.split.physical_line_partition + 1) *
+		(this_leaf->ebx.split.ways_of_associativity + 1);
+	return 0;
+}
+
+static int __init find_num_cache_leaves(void)
+{
+	unsigned int		eax, ebx, ecx, edx;
+	union _cpuid4_leaf_eax	cache_eax;
+	int 			i;
+	int 			retval;
+
+	retval = MAX_CACHE_LEAVES;
+	/* Do cpuid(4) loop to find out num_cache_leaves */
+	for (i = 0; i < MAX_CACHE_LEAVES; i++) {
+		cpuid_count(4, i, &eax, &ebx, &ecx, &edx);
+		cache_eax.full = eax;
+		if (cache_eax.split.type == CACHE_TYPE_NULL) {
+			retval = i;
+			break;
+		}
+	}
+	return retval;
+}
+
 static void __init init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int trace = 0, l1i = 0, l1d = 0, l2 = 0, l3 = 0; /* Cache sizes */
 	char *p = NULL;
+	unsigned int new_l1d = 0, new_l1i = 0; /* Cache sizes from cpuid(4) */
+	unsigned int new_l2 = 0, new_l3 = 0, i; /* Cache sizes from cpuid(4) */
+
 #ifndef CONFIG_X86_F00F_WORKS_OK
 	static int f00f_workaround_enabled = 0;
 
@@ -2298,6 +2402,47 @@ static void __init init_intel(struct cpu
 	}
 #endif /* CONFIG_X86_F00F_WORKS_OK */
 
+	if (c->cpuid_level > 4) {
+		static int is_initialized;
+		static int num_cache_leaves;
+
+		if (is_initialized == 0) {
+			/* Init num_cache_leaves from boot CPU */
+			num_cache_leaves = find_num_cache_leaves();
+			is_initialized++;
+		}
+ 
+		/*
+		 * Whenever possible use cpuid(4), deterministic cache 
+		 * parameters cpuid leaf to find the cache details
+		 */
+		for (i = 0; i < num_cache_leaves; i++) {
+			struct _cpuid4_info this_leaf;
+			int retval;
+
+			retval = cpuid4_cache_lookup(i, &this_leaf);
+			if (retval >= 0) {
+				switch(this_leaf.eax.split.level) {
+				    case 1:
+					if (this_leaf.eax.split.type == 
+							CACHE_TYPE_DATA)
+						new_l1d = this_leaf.size/1024;
+					else if (this_leaf.eax.split.type == 
+							CACHE_TYPE_INST)
+						new_l1i = this_leaf.size/1024;
+					break;
+				    case 2:
+					new_l2 = this_leaf.size/1024;
+					break;
+				    case 3:
+					new_l3 = this_leaf.size/1024;
+					break;
+				    default:
+					break;
+				}
+			}
+		}
+	}
 	if (c->cpuid_level > 1) {
 		/* supports eax=2  call */
 		int i, j, n;
@@ -2349,6 +2494,15 @@ static void __init init_intel(struct cpu
 			}
 		}
 
+		if (new_l1d)
+			l1d = new_l1d;
+		if (new_l1i)
+			l1i = new_l1i;
+		if (new_l2)
+			l2 = new_l2;
+		if (new_l3)
+			l3 = new_l3;
+
 		/* Intel PIII Tualatin. This comes in two flavours.
 		 * One has 256kb of cache, the other 512. We have no way
 		 * to determine which, so we use a boottime override
@@ -2360,16 +2514,15 @@ static void __init init_intel(struct cpu
 		if (cachesize_override != -1)
 			l2 = cachesize_override;
 
-		if ( trace )
-			printk (KERN_INFO "CPU: Trace cache: %dK uops", trace);
-		else if ( l1i )
-			printk (KERN_INFO "CPU: L1 I cache: %dK", l1i);
-		if ( l1d )
-			printk(", L1 D cache: %dK\n", l1d);
-
-		if ( l2 )
+		if (trace)
+			printk(KERN_INFO "CPU: Trace cache: %dK uops\n", trace);
+		if (l1i || l1d)
+			printk(KERN_INFO
+				"CPU: L1 I-cache: %dK, L1 D-cache: %dK\n",
+				l1i, l1d);
+		if (l2)
 			printk(KERN_INFO "CPU: L2 cache: %dK\n", l2);
-		if ( l3 )
+		if (l3)
 			printk(KERN_INFO "CPU: L3 cache: %dK\n", l3);
 
 		/*
diff -urNp linux-204/arch/i386/oprofile/nmi_int.c linux-210/arch/i386/oprofile/nmi_int.c
--- linux-204/arch/i386/oprofile/nmi_int.c
+++ linux-210/arch/i386/oprofile/nmi_int.c
@@ -226,7 +226,7 @@ struct oprofile_operations nmi_ops = {
 };
  
 
-#if !defined(CONFIG_X86_64)
+#if !defined(CONFIG_X86_64) || defined(CONFIG_IA32E)
 
 static int __init p4_init(void)
 {
@@ -278,8 +278,8 @@ static int __init ppro_init(void)
 	return 1;
 }
 
-#endif /* !CONFIG_X86_64 */
- 
+#endif /* !CONFIG_X86_64 || CONFIG_IA32E */
+
 int __init nmi_init(struct oprofile_operations ** ops)
 {
 	__u8 vendor = current_cpu_data.x86_vendor;
@@ -307,7 +307,7 @@ int __init nmi_init(struct oprofile_oper
 			}
 			break;
  
-#if !defined(CONFIG_X86_64)
+#if !defined(CONFIG_X86_64) || defined(CONFIG_IA32E)
 		case X86_VENDOR_INTEL:
 			switch (family) {
 				/* Pentium IV */
@@ -326,7 +326,7 @@ int __init nmi_init(struct oprofile_oper
 					return 0;
 			}
 			break;
-#endif /* !CONFIG_X86_64 */
+#endif /* !CONFIG_X86_64 || CONFIG_IA32E */
 
 		default:
 			return 0;
diff -urNp linux-204/arch/ia64/lib/swiotlb.c linux-210/arch/ia64/lib/swiotlb.c
--- linux-204/arch/ia64/lib/swiotlb.c
+++ linux-210/arch/ia64/lib/swiotlb.c
@@ -55,8 +55,9 @@ static char *io_tlb_start, *io_tlb_end;
 /*
  * The number of IO TLB blocks (in groups of 64) betweeen io_tlb_start and io_tlb_end.
  * This is command line adjustable via setup_io_tlb_npages.
+ * Default to 64MB.
  */
-static unsigned long io_tlb_nslabs = 1024;
+static unsigned long io_tlb_nslabs = 32768;
 
 /*
  * This is a free list describing the number of free entries available from each index
@@ -75,7 +76,7 @@ static unsigned char **io_tlb_orig_addr;
  */
 static spinlock_t io_tlb_lock = SPIN_LOCK_UNLOCKED;
 
-static int __init
+int __init
 setup_io_tlb_npages (char *str)
 {
 	io_tlb_nslabs = simple_strtoul(str, NULL, 0) << (PAGE_SHIFT - IO_TLB_SHIFT);
@@ -343,9 +344,10 @@ swiotlb_map_single (struct pci_dev *hwde
  * DMA can be marked as "clean" so that update_mmu_cache() doesn't have to
  * flush them when they get mapped into an executable vm-area.
  */
-static void
+static inline void
 mark_clean (void *addr, size_t size)
 {
+#ifdef __ia64__
 	unsigned long pg_addr, end;
 
 	pg_addr = PAGE_ALIGN((unsigned long) addr);
@@ -355,6 +357,7 @@ mark_clean (void *addr, size_t size)
 		set_bit(PG_arch_1, &page->flags);
 		pg_addr += PAGE_SIZE;
 	}
+#endif
 }
 
 /*
diff -urNp linux-204/arch/x86_64/boot/setup.S linux-210/arch/x86_64/boot/setup.S
--- linux-204/arch/x86_64/boot/setup.S
+++ linux-210/arch/x86_64/boot/setup.S
@@ -289,8 +289,9 @@ loader_ok:
 	/* minimum CPUID flags for x86-64 */
 	/* see http://www.x86-64.org/lists/discuss/msg02971.html */		
 #define SSE_MASK ((1<<25)|(1<<26))
-#define REQUIRED_MASK1 ((1<<0)|(1<<3)|(1<<4)|(1<<5)|(1<<6)|(1<<8)|(1<<11)| \
-					   (1<<13)|(1<<15)|(1<<24)|(1<<29))
+#define REQUIRED_MASK1 ((1<<0)|(1<<3)|(1<<4)|(1<<5)|(1<<6)|(1<<8)|\
+					   (1<<13)|(1<<15)|(1<<24))
+#define REQUIRED_MASK2 (1<<29)
 
 	pushfl				/* standard way to check for cpuid */
 	popl	%eax
@@ -302,10 +303,10 @@ loader_ok:
 	popl	%eax
 	cmpl	%eax,%ebx
 	jz	no_longmode		/* cpu has no cpuid */
-	movl	$0x80000000,%eax
+	movl	$0x0,%eax
 	cpuid
-	cmpl	$0x80000001,%eax
-	jb	no_longmode		/* no extended cpuid */
+	cmpl	$0x1,%eax
+	jb	no_longmode		/* no cpuid 1 */
 	xor	%di,%di
 	cmpl	$0x68747541,%ebx	/* AuthenticAMD */
 	jnz	noamd
@@ -314,12 +315,21 @@ loader_ok:
 	cmpl	$0x444d4163,%ecx
 	jnz	noamd
 	mov	$1,%di			/* cpu is from AMD */
-noamd:		
-	movl	$0x80000001,%eax		
+noamd:
+	movl    $0x1,%eax
 	cpuid
 	andl	$REQUIRED_MASK1,%edx
 	xorl	$REQUIRED_MASK1,%edx
 	jnz	no_longmode
+	movl    $0x80000000,%eax
+	cpuid
+	cmpl    $0x80000001,%eax
+	jb      no_longmode             /* no extended cpuid */
+	movl    $0x80000001,%eax
+	cpuid
+	andl    $REQUIRED_MASK2,%edx
+	xorl    $REQUIRED_MASK2,%edx
+	jnz     no_longmode
 sse_test:		
 	movl	$1,%eax
 	cpuid
diff -urNp linux-204/arch/x86_64/config.in linux-210/arch/x86_64/config.in
--- linux-204/arch/x86_64/config.in
+++ linux-210/arch/x86_64/config.in
@@ -34,6 +34,7 @@ mainmenu_option next_comment
 comment 'Processor type and features'
 choice 'Processor family' \
 	"AMD-Hammer			CONFIG_MK8 \
+	 Intel-ia32e			CONFIG_IA32E \
 	 Generic-x86-64			CONFIG_GENERIC_CPU" AMD-Hammer
 	
 #
@@ -44,6 +45,7 @@ define_int CONFIG_X86_L1_CACHE_SHIFT 6
 define_bool CONFIG_X86_TSC y
 define_bool CONFIG_X86_GOOD_APIC y
 
+tristate '/dev/cpu/microcode - Intel CPU microcode support' CONFIG_MICROCODE
 tristate '/dev/cpu/*/msr - Model-specific register support' CONFIG_X86_MSR
 tristate '/dev/cpu/*/cpuid - CPU information support' CONFIG_X86_CPUID
 
@@ -58,10 +60,23 @@ bool 'MTRR (Memory Type Range Register) 
 bool 'Symmetric multi-processing support' CONFIG_SMP
 bool 'HPET timers' CONFIG_HPET_TIMER
 bool 'IOMMU support' CONFIG_GART_IOMMU
+if [ "$CONFIG_GART_IOMMU" = "y" ]; then
+    bool 'SWIOTLB support' CONFIG_SWIOTLB
+fi
 
 if [ "$CONFIG_GART_IOMMU" != "y" ]; then
    define_bool CONFIG_DUMMY_IOMMU y
 fi
+if [ "$CONFIG_SMP" = "y" ]; then
+   choice 'Hyperthreading Support' \
+       "off           CONFIG_NR_SIBLINGS_0 \
+        2-siblings    CONFIG_NR_SIBLINGS_2" off
+fi
+
+if [ "$CONFIG_NR_SIBLINGS_2" = "y" ]; then
+   define_bool CONFIG_SHARE_RUNQUEUE y
+   define_int CONFIG_MAX_NR_SIBLINGS 2
+fi
 if [ "$CONFIG_SMP" != "y" ]; then
    define_bool CONFIG_X86_UP_IOAPIC y
 else
diff -urNp linux-204/arch/x86_64/kernel/Makefile linux-210/arch/x86_64/kernel/Makefile
--- linux-204/arch/x86_64/kernel/Makefile
+++ linux-210/arch/x86_64/kernel/Makefile
@@ -16,6 +16,12 @@ O_TARGET := kernel.o
 
 
 export-objs     := mtrr.o msr.o cpuid.o x8664_ksyms.o pci-gart.o
+ifdef CONFIG_SWIOTLB
+export-objs	+= swiotlb.o
+endif
+ifdef CONFIG_MICROCODE
+export-objs	+= microcode.o
+endif
 
 obj-y	:= process.o semaphore.o signal.o entry.o traps.o irq.o \
 		ptrace.o i8259.o ioport.o ldt.o setup.o time.o sys_x86_64.o \
@@ -38,6 +44,18 @@ obj-$(CONFIG_EARLY_PRINTK) +=  early_pri
 obj-$(CONFIG_GART_IOMMU) += pci-gart.o aperture.o
 obj-$(CONFIG_DUMMY_IOMMU) += pci-nommu.o
 obj-$(CONFIG_MCE) += bluesmoke.o
+obj-$(CONFIG_SWIOTLB)	+= swiotlb.o
+obj-$(CONFIG_MICROCODE)	+= microcode.o
+
+obj := .
+ifdef CONFIG_SWIOTLB
+$(obj)/swiotlb.c: ${INCL}
+	@ln -sf ../../../arch/ia64/lib/swiotlb.c $(obj)/swiotlb.c
+endif
+ifdef CONFIG_MICROCODE
+$(obj)/microcode.c: ${INCL}
+	@ln -sf ../../../arch/i386/kernel/microcode.c $(obj)/microcode.c
+endif
 
 include $(TOPDIR)/Rules.make
 
diff -urNp linux-204/arch/x86_64/kernel/acpi.c linux-210/arch/x86_64/kernel/acpi.c
--- linux-204/arch/x86_64/kernel/acpi.c
+++ linux-210/arch/x86_64/kernel/acpi.c
@@ -559,6 +559,9 @@ acpi_boot_init (char * cmdline)
 	 * --------
 	 */
 
+	if (acpi_noirq)
+		return 0;
+
 	result = acpi_table_parse_madt(ACPI_MADT_IOAPIC, acpi_parse_ioapic);
 	if (!result) { 
 		printk(KERN_ERR PREFIX "No IOAPIC entries present\n");
diff -urNp linux-204/arch/x86_64/kernel/aperture.c linux-210/arch/x86_64/kernel/aperture.c
--- linux-204/arch/x86_64/kernel/aperture.c
+++ linux-210/arch/x86_64/kernel/aperture.c
@@ -25,6 +25,8 @@
 int fallback_aper_order __initdata = 1; /* 64MB */
 int fallback_aper_force __initdata = 0; 
 
+int iommu_aperture;
+
 extern int no_iommu, force_mmu;
 
 /* This code runs before the PCI subsystem is initialized, so just 
@@ -82,6 +84,8 @@ void __init iommu_hole_init(void) 
 		if (read_pci_config(0, num, 3, 0x00) != NB_ID_3) 
 			continue;	
 
+		iommu_aperture = 1;
+
 		aper_order = (read_pci_config(0, num, 3, 0x90) >> 1) & 7; 
 		aper_size = (32 * 1024 * 1024) << aper_order; 
 		aper_base = read_pci_config(0, num, 3, 0x94) & 0x7fff;
diff -urNp linux-204/arch/x86_64/kernel/apic.c linux-210/arch/x86_64/kernel/apic.c
--- linux-204/arch/x86_64/kernel/apic.c
+++ linux-210/arch/x86_64/kernel/apic.c
@@ -590,15 +590,11 @@ int dont_enable_local_apic __initdata = 
 static int __init detect_init_APIC (void)
 {
 	u32 h, l, features;
-	extern void get_cpu_vendor(struct cpuinfo_x86*);
 
 	/* Disabled by DMI scan or kernel option? */
 	if (dont_enable_local_apic)
 		return -1;
 
-	/* Workaround for us being called before identify_cpu(). */
-	get_cpu_vendor(&boot_cpu_data);
-
 	switch (boot_cpu_data.x86_vendor) {
 	case X86_VENDOR_AMD:
 		if (boot_cpu_data.x86 == 6 && boot_cpu_data.x86_model > 1)
diff -urNp linux-204/arch/x86_64/kernel/e820.c linux-210/arch/x86_64/kernel/e820.c
--- linux-204/arch/x86_64/kernel/e820.c
+++ linux-210/arch/x86_64/kernel/e820.c
@@ -14,6 +14,7 @@
 #include <asm/e820.h>
 #include <asm/proto.h>
 #include <asm/bootsetup.h>
+#include <asm/smp.h>
 
 extern unsigned long table_start, table_end;
 extern char _end[];
@@ -138,6 +139,13 @@ unsigned long end_pfn_map; 
  */
 unsigned long end_user_pfn = MAXMEM>>PAGE_SHIFT;  
 
+#ifdef CONFIG_IA32E
+/*
+ * last DMA zone pfn
+ */
+unsigned long end_dma_pfn = 0;
+#endif
+
 /*
  * Find the highest page frame number we have available
  */
@@ -543,6 +551,16 @@ void __init parse_mem_cmdline (char ** c
 		else if (!memcmp(from, "acpi=off", 8)) {
 			acpi_disabled = 1;
 		}
+		else if (!memcmp(from, "acpi=noirq", 8)) {
+			acpi_noirq = 1;
+#ifdef CONFIG_IA32E
+#ifdef CONFIG_X86_LOCAL_APIC
+			/* revert nmi_watchdog default if acpi=noirq is used */
+			if (nmi_watchdog == NMI_IO_APIC)
+				nmi_watchdog = NMI_LOCAL_APIC;
+#endif
+#endif
+		}
 #endif
 #ifdef CONFIG_GART_IOMMU 
 		else if (!memcmp(from,"iommu=",6)) { 
@@ -560,6 +578,20 @@ void __init parse_mem_cmdline (char ** c
 		else if (!memcmp(from, "acpi_sci=low", 12))
 			acpi_sci_flags.polarity = 3;
 #endif
+#ifdef CONFIG_SWIOTLB
+		else if (!memcmp(from, "swiotlb=", 8)) {
+			if (to != command_line)
+				to--;
+			from+=8;
+			setup_io_tlb_npages(from);
+		}
+#ifdef CONFIG_IA32E
+		else if (!memcmp(from, "maxdma=", 7)) {
+			end_dma_pfn = memparse(from+7, &from);
+			end_dma_pfn >>= PAGE_SHIFT;
+		}
+#endif
+#endif
 #ifdef CONFIG_ACPI_PMTMR
 		else if (!memcmp(from, "pmtmr", 5)) {
 			extern int use_pmtmr;
diff -urNp linux-204/arch/x86_64/kernel/head.S linux-210/arch/x86_64/kernel/head.S
--- linux-204/arch/x86_64/kernel/head.S
+++ linux-210/arch/x86_64/kernel/head.S
@@ -37,7 +37,11 @@ startup_32:
  	 * There is no stack until we set one up.
 	 */
 
+
 	movl %ebx,%ebp	/* Save trampoline flag */
+
+	movl $__KERNEL_DS,%eax
+	movl %eax,%ds
 	
 	/* First check if extended functions are implemented */
 	movl	$0x80000000, %eax
@@ -119,25 +123,11 @@ reach_compatibility_mode:
 	movl	$0x100F00, %eax
 	lgdt	(%eax)
 
+second:
 	movl    $0x100F10, %eax
 	/* Finally jump in 64bit mode */
 	ljmp	*(%eax)
 
-second:
-	/* abuse syscall to get into 64bit mode. this way we don't need
-	   a working low identity mapping just for the short 32bit roundtrip. 
-	   XXX kludge. this should not be needed. */
-	movl  $MSR_STAR,%ecx
-	xorl  %eax,%eax
-	movl  $(__USER32_CS<<16)|__KERNEL_CS,%edx
-	wrmsr
-
-	movl  $MSR_CSTAR,%ecx
-	movl  $0xffffffff,%edx
-	movl  $0x80100100,%eax	# reach_long64 absolute
-	wrmsr
-	syscall
-
 	.code64
 	.org 0x100	
 reach_long64:
diff -urNp linux-204/arch/x86_64/kernel/head64.c linux-210/arch/x86_64/kernel/head64.c
--- linux-204/arch/x86_64/kernel/head64.c
+++ linux-210/arch/x86_64/kernel/head64.c
@@ -15,6 +15,7 @@
 
 #include <asm/processor.h>
 #include <asm/proto.h>
+#include <asm/smp.h>
 
 static void __init clear_bss(void)
 {
@@ -55,16 +56,26 @@ static void __init copy_bootdata(char *r
 
 static void __init setup_boot_cpu_data(void)
 {
-	int dummy, eax;
+	int dummy, eax, extra;
 
 	/* get vendor info */
 	cpuid(0, &boot_cpu_data.cpuid_level,
 	      (int *)&boot_cpu_data.x86_vendor_id[0],
 	      (int *)&boot_cpu_data.x86_vendor_id[8],
 	      (int *)&boot_cpu_data.x86_vendor_id[4]);
+	get_cpu_vendor(&boot_cpu_data);
+#ifdef CONFIG_IA32E
+#ifdef CONFIG_X86_LOCAL_APIC
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+		nmi_watchdog = NMI_IO_APIC;
+#endif
+#endif
 
 	/* get cpu type */
-	cpuid(1, &eax, &dummy, &dummy, (int *) &boot_cpu_data.x86_capability);
+	cpuid(1, &eax, &dummy, &extra, (int *)&boot_cpu_data.x86_capability[0]);
+#ifdef CONFIG_IA32E
+	boot_cpu_data.x86_capability[4] = extra;
+#endif
 	boot_cpu_data.x86 = (eax >> 8) & 0xf;
 	boot_cpu_data.x86_model = (eax >> 4) & 0xf;
 	boot_cpu_data.x86_mask = eax & 0xf;
diff -urNp linux-204/arch/x86_64/kernel/io_apic.c linux-210/arch/x86_64/kernel/io_apic.c
--- linux-204/arch/x86_64/kernel/io_apic.c
+++ linux-210/arch/x86_64/kernel/io_apic.c
@@ -1305,9 +1305,15 @@ static void end_level_ioapic_irq (unsign
 
 static void mask_and_ack_level_ioapic_irq (unsigned int irq) { /* nothing */ }
 
+extern int no_valid_irqaffinity;
+
 static void set_ioapic_affinity (unsigned int irq, unsigned long mask)
 {
 	unsigned long flags;
+
+	if (no_valid_irqaffinity)
+		return;
+
 	/*
 	 * Only the first 8 bits are valid.
 	 */
diff -urNp linux-204/arch/x86_64/kernel/irq.c linux-210/arch/x86_64/kernel/irq.c
--- linux-204/arch/x86_64/kernel/irq.c
+++ linux-210/arch/x86_64/kernel/irq.c
@@ -1120,6 +1120,15 @@ static int irq_affinity_write_proc (stru
 {
 	int irq = (long) data, full_count = count, err;
 	unsigned long new_value;
+	extern int no_valid_irqaffinity;
+
+	/* HACK: Since some proc entries get set up before
+	 * we determine (via pci-quirks) that we should never
+	 * modify IRQ affinities.  If so, disallow the write
+	 * even if the proc entry permissions allow.
+	 */
+	if (no_valid_irqaffinity)
+		return -EROFS;
 
 	if (!irq_desc[irq].handler->set_affinity)
 		return -EIO;
@@ -1184,9 +1193,15 @@ static void register_irq_proc (unsigned 
 #if CONFIG_SMP
 	{
 		struct proc_dir_entry *entry;
+		int mask;
+		extern int no_valid_irqaffinity;
+
+		mask = 0600;
+		if (no_valid_irqaffinity)
+			mask = 0400;
 
 		/* create /proc/irq/1234/smp_affinity */
-		entry = create_proc_entry("smp_affinity", 0600, irq_dir[irq]);
+		entry = create_proc_entry("smp_affinity", mask, irq_dir[irq]);
 
 		if (entry) {
 			entry->nlink = 1;
diff -urNp linux-204/arch/x86_64/kernel/mpparse.c linux-210/arch/x86_64/kernel/mpparse.c
--- linux-204/arch/x86_64/kernel/mpparse.c
+++ linux-210/arch/x86_64/kernel/mpparse.c
@@ -80,6 +80,9 @@ extern int acpi_parse_ioapic (acpi_table
 #endif /*CONFIG_X86_IO_APIC*/
 #endif /*CONFIG_ACPI_BOOT*/
 
+u8 bios_cpu_apicid[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
+
+
 /*
  * Intel MP BIOS table parsing routines:
  */
@@ -133,6 +136,7 @@ static void __init MP_processor_info (st
 		ver = 0x10;
 	}
 	apic_version[m->mpc_apicid] = ver;
+	bios_cpu_apicid[num_processors - 1] = m->mpc_apicid;
 }
 
 static void __init MP_bus_info (struct mpc_config_bus *m)
diff -urNp linux-204/arch/x86_64/kernel/mtrr.c linux-210/arch/x86_64/kernel/mtrr.c
--- linux-204/arch/x86_64/kernel/mtrr.c
+++ linux-210/arch/x86_64/kernel/mtrr.c
@@ -144,12 +144,17 @@ static void set_mtrr_prepare (struct set
 	write_cr0(cr0);
 	wbinvd();
 
-	/* Disable MTRRs, and set the default type to uncached */
 	rdmsr(MSR_MTRRdefType, ctxt->deftype_lo, ctxt->deftype_hi);
+}
+
+static void set_mtrr_disable(struct set_mtrr_context *ctxt)
+{
+	/* Disable MTRRs, and set the default type to uncached */
 	wrmsr(MSR_MTRRdefType, ctxt->deftype_lo & 0xf300UL, ctxt->deftype_hi);
 }
 
 
+
 /* Restore the processor after a set_mtrr_prepare */
 static void set_mtrr_done (struct set_mtrr_context *ctxt)
 {
@@ -195,7 +200,7 @@ static u64 size_or_mask, size_and_mask;
 
 static void get_mtrr (unsigned int reg, u64 *base, u32 *size, mtrr_type * type)
 {
-	u32 count, tmp, mask_lo, mask_hi;
+	u32 tmp, mask_lo, mask_hi;
 	int i;
 	u32 base_lo, base_hi;
 
@@ -210,16 +215,11 @@ static void get_mtrr (unsigned int reg, 
 
 	rdmsr (MSR_MTRRphysBase(reg), base_lo, base_hi);
 
-	count = 0;
-	tmp = mask_lo >> MTRR_BEG_BIT;
-	for (i=MTRR_BEG_BIT; i <= 31; i++, tmp = tmp >> 1)
-		count = (count << (~tmp & 1)) | (~tmp & 1);
-
-	tmp = mask_hi;
-	for (i=0; i <= MTRR_END_BIT; i++, tmp = tmp >> 1)
-		count = (count << (~tmp & 1)) | (~tmp & 1);
+	/* Stuff the relevant bits from mask_hi and mask_lo into tmp. */
+	tmp = (mask_hi << 32-PAGE_SHIFT) | (mask_lo >> PAGE_SHIFT);
 
-	*size = (count+1);
+	/* Calculate the size from the mask. */
+	*size = (~tmp + 1) & tmp;
 	*base = base_hi << (32 - PAGE_SHIFT) | base_lo >> PAGE_SHIFT;
 	*type = base_lo & 0xff;
 }
@@ -242,8 +242,10 @@ static void set_mtrr_up (unsigned int re
 	u64 base64;
 	u64 size64;
 
-	if (do_safe)
+	if (do_safe) {
 		set_mtrr_prepare (&ctxt);
+		set_mtrr_disable (&ctxt);
+	}
 
 	if (size == 0) {
 		/* The invalid bit is kept in the mask, so we simply clear the
@@ -430,6 +432,7 @@ static u64 __init set_mtrr_state (struct
 
 
 static atomic_t undone_count;
+static volatile int wait_barrier_mtrr_disable = FALSE;
 static volatile int wait_barrier_execute = FALSE;
 static volatile int wait_barrier_cache_enable = FALSE;
 
@@ -451,8 +454,16 @@ static void ipi_handler (void *info)
 	set_mtrr_prepare (&ctxt);
 	/* Notify master that I've flushed and disabled my cache  */
 	atomic_dec (&undone_count);
-	while (wait_barrier_execute)
-		barrier ();
+	while (wait_barrier_mtrr_disable) {
+		rep_nop();
+	}
+
+	set_mtrr_disable (&ctxt);
+	/* wait again for disable confirmation*/
+	atomic_dec (&undone_count);
+	while (wait_barrier_execute) {
+		rep_nop();
+	}
 
 	/* The master has cleared me to execute  */
 	set_mtrr_up (data->smp_reg, data->smp_base, data->smp_size,
@@ -462,8 +473,9 @@ static void ipi_handler (void *info)
 	atomic_dec (&undone_count);
 
 	/* Wait for master to clear me to enable cache and return  */
-	while (wait_barrier_cache_enable)
-		barrier ();
+	while (wait_barrier_cache_enable) {
+		rep_nop();
+	}
 	set_mtrr_done (&ctxt);
 }
 
@@ -479,6 +491,7 @@ static void set_mtrr_smp (unsigned int r
 	data.smp_type = type;
 	wait_barrier_execute = TRUE;
 	wait_barrier_cache_enable = TRUE;
+	wait_barrier_mtrr_disable = TRUE;
 	atomic_set (&undone_count, smp_num_cpus - 1);
 
 	/*  Start the ball rolling on other CPUs  */
@@ -488,9 +501,18 @@ static void set_mtrr_smp (unsigned int r
 	/* Flush and disable the local CPU's cache */
 	set_mtrr_prepare (&ctxt);
 
+	while (atomic_read (&undone_count) > 0) {
+		rep_nop();
+	}
+	/* Set up for completion wait and then release other CPUs to change MTRRs*/
+	atomic_set (&undone_count, smp_num_cpus - 1);
+	wait_barrier_mtrr_disable = FALSE;
+	set_mtrr_disable (&ctxt);
+
 	/*  Wait for all other CPUs to flush and disable their caches  */
-	while (atomic_read (&undone_count) > 0)
-		barrier ();
+	while (atomic_read (&undone_count) > 0) {
+		rep_nop ();
+	}
 
 	/* Set up for completion wait and then release other CPUs to change MTRRs */
 	atomic_set (&undone_count, smp_num_cpus - 1);
@@ -498,8 +520,9 @@ static void set_mtrr_smp (unsigned int r
 	set_mtrr_up (reg, base, size, type, FALSE);
 
 	/*  Now wait for other CPUs to complete the function  */
-	while (atomic_read (&undone_count) > 0)
-		barrier ();
+	while (atomic_read (&undone_count) > 0) {
+		rep_nop();
+	}
 
 	/*  Now all CPUs should have finished the function. Release the barrier to
 	   allow them to re-enable their caches and return from their interrupt,
@@ -1282,6 +1305,7 @@ void __init mtrr_init_secondary_cpu (voi
 	   for this CPU while the MTRRs are changed, but changing this requires
 	   more invasive changes to the way the kernel boots  */
 	set_mtrr_prepare (&ctxt);
+	set_mtrr_disable (&ctxt);
 	mask = set_mtrr_state (&smp_mtrr_state, &ctxt);
 	set_mtrr_done (&ctxt);
 
diff -urNp linux-204/arch/x86_64/kernel/nmi.c linux-210/arch/x86_64/kernel/nmi.c
--- linux-204/arch/x86_64/kernel/nmi.c
+++ linux-210/arch/x86_64/kernel/nmi.c
@@ -150,9 +150,15 @@ static void disable_apic_nmi_watchdog(vo
 	case X86_VENDOR_INTEL:
 		switch (boot_cpu_data.x86) {
 		case 6:
+			if (boot_cpu_data.x86_model > 0xd)
+				break;
+
 			wrmsr(MSR_P6_EVNTSEL0, 0, 0);
 			break;
 		case 15:
+			if (boot_cpu_data.x86_model > 0x3)
+				break;
+
 			wrmsr(MSR_P4_IQ_CCCR0, 0, 0);
 			wrmsr(MSR_P4_CRU_ESCR0, 0, 0);
 			break;
@@ -237,7 +243,7 @@ static void __pminit setup_k7_watchdog(v
 	wrmsr(MSR_K7_EVNTSEL0, evntsel, 0);
 }
 
-#ifndef CONFIG_MK8
+#ifdef CONFIG_IA32E
 static void __pminit setup_p6_watchdog(void)
 {
 	unsigned int evntsel;
@@ -301,13 +307,19 @@ void __pminit setup_apic_nmi_watchdog (v
 			return;	    
 		setup_k7_watchdog();
 		break;
-#ifndef CONFIG_MK8
+#ifdef CONFIG_IA32E
 	case X86_VENDOR_INTEL:
 		switch (boot_cpu_data.x86) {
 		case 6:
+			if (boot_cpu_data.x86_model > 0xd)
+				return;
+
 			setup_p6_watchdog();
 			break;
 		case 15:
+			if (boot_cpu_data.x86_model > 0x3)
+				return;
+
 			if (!setup_p4_watchdog())
 				return;
 			break;
@@ -403,7 +415,7 @@ void nmi_watchdog_tick (struct pt_regs *
 		alert_counter[cpu] = 0;
 	}
 	if (nmi_perfctr_msr) {
-#ifndef CONFIG_MK8
+#ifdef CONFIG_IA32E
 		if (nmi_perfctr_msr == MSR_P4_IQ_COUNTER0) {
 			/*
 			 * P4 quirks:
diff -urNp linux-204/arch/x86_64/kernel/pci-gart.c linux-210/arch/x86_64/kernel/pci-gart.c
--- linux-204/arch/x86_64/kernel/pci-gart.c
+++ linux-210/arch/x86_64/kernel/pci-gart.c
@@ -302,6 +302,11 @@ dma_addr_t __pci_map_single(struct pci_d
 
 	BUG_ON(dir == PCI_DMA_NONE);
 
+#ifdef CONFIG_SWIOTLB
+	if (swiotlb)
+		return swiotlb_map_single(dev,addr,size,dir);
+#endif
+
 	phys_mem = virt_to_phys(addr); 
 	if (!need_iommu(dev, phys_mem, size))
 		return phys_mem; 
@@ -350,6 +355,14 @@ void pci_unmap_single(struct pci_dev *hw
 {
 	unsigned long iommu_page; 
 	int i, npages;
+
+#ifdef CONFIG_SWIOTLB
+	if (swiotlb) {
+		swiotlb_unmap_single(hwdev,dma_addr,size,direction);
+		return;
+	}
+#endif
+
 	if (dma_addr < iommu_bus_base + EMERGENCY_PAGES*PAGE_SIZE || 
 	    dma_addr >= iommu_bus_base + iommu_size)
 		return;
@@ -487,7 +500,14 @@ void __init pci_iommu_init(void)
 	no_agp = no_agp || (agp_init() < 0) || (agp_copy_info(&info) < 0); 
 #endif	
 
-	if (no_iommu || (!force_mmu && end_pfn < 0xffffffff>>PAGE_SHIFT)) { 
+#ifdef CONFIG_SWIOTLB
+	if (swiotlb) {
+		no_iommu = 1;
+		printk(KERN_INFO "PCI-DMA: Using SWIOTLB\n");
+		return;
+	}
+#endif
+	if (no_iommu || (!force_mmu && end_pfn < 0xffffffff>>PAGE_SHIFT) || !iommu_aperture) {
 		printk(KERN_INFO "PCI-DMA: Disabling IOMMU.\n"); 
 		no_iommu = 1;
 		return;
diff -urNp linux-204/arch/x86_64/kernel/pci-pc.c linux-210/arch/x86_64/kernel/pci-pc.c
--- linux-204/arch/x86_64/kernel/pci-pc.c
+++ linux-210/arch/x86_64/kernel/pci-pc.c
@@ -35,6 +35,8 @@ struct pci_ops *pci_root_ops;
 int (*pci_config_read)(int seg, int bus, int dev, int fn, int reg, int len, u32 *value) = NULL;
 int (*pci_config_write)(int seg, int bus, int dev, int fn, int reg, int len, u32 value) = NULL;
 
+static int pci_using_acpi_prt = 0;
+
 static spinlock_t pci_config_lock = SPIN_LOCK_UNLOCKED;
 
 static unsigned int acpi_root_bridges;
@@ -658,11 +660,12 @@ void __devinit pcibios_init(void)
 		pcibios_config_init();
 
 	/* Only probe blindly if ACPI didn't tell us about root bridges */
-	if (acpi_root_bridges) {
+	if (!acpi_noirq && acpi_root_bridges) {
 #ifdef CONFIG_ACPI_PCI
-		if (!acpi_pci_irq_init())
+		if (!acpi_pci_irq_init()) {
+			pci_using_acpi_prt = 1;
 			printk(KERN_INFO "PCI: Using ACPI for IRQ routing\n");
-		else
+		} else
 			printk(KERN_WARNING "PCI: Invalid ACPI-PCI IRQ routing table\n");
 #else
 		printk(KERN_WARNING "PCI: acpi_root_bridges nonzero but ACPI not configured\n");
@@ -737,6 +740,12 @@ int pcibios_enable_device(struct pci_dev
 
 	if ((err = pcibios_enable_resources(dev, mask)) < 0)
 		return err;
+#ifdef CONFIG_ACPI_PCI
+	if (pci_using_acpi_prt) {
+		acpi_pci_irq_enable(dev);
+		return 0;
+	}
+#endif
 	pcibios_enable_irq(dev);
 	return 0;
 }
diff -urNp linux-204/arch/x86_64/kernel/process.c linux-210/arch/x86_64/kernel/process.c
--- linux-204/arch/x86_64/kernel/process.c
+++ linux-210/arch/x86_64/kernel/process.c
@@ -75,6 +75,9 @@ void (*pm_idle)(void);
  */
 void (*pm_power_off)(void);
 
+/* optional machine reset function */
+void (*machine_reset)(void);
+
 void disable_hlt(void)
 {
 	hlt_counter++;
@@ -150,11 +153,53 @@ void cpu_idle (void)
 	}
 }
 
+/*
+ * This is a kind of hybrid between poll and halt idle routines. This uses new
+ * Monitor/Mwait instructions on P4 processors with PNI. We Monitor 
+ * need_resched and go to optimized wait state through Mwait. 
+ * Whenever someone changes need_resched, we would be woken up from Mwait 
+ * (without an IPI).
+ */
+static void mwait_idle (void)
+{
+	int oldval;
+
+	__sti();
+	/* Setting need_resched to -1 skips sending IPI during idle resched */
+	oldval = xchg(&current->need_resched, -1);
+	if (!oldval) {
+		do {
+			__monitor((void *)&current->need_resched, 0, 0);
+			if (current->need_resched != -1)
+				break;
+			__mwait(0, 0);
+		} while (current->need_resched == -1);
+	}
+}
+
+int __init select_idle_routine(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_IA32E
+	if (cpu_has(c, X86_FEATURE_MWAIT)) {
+		printk("Monitor/Mwait feature present.\n");
+		if (!pm_idle) {
+			pm_idle = mwait_idle;
+		}
+		return 1;
+	}
+#endif
+	pm_idle = default_idle;
+	return 1;
+}
+
 static int __init idle_setup (char *str)
 {
 	if (!strncmp(str, "poll", 4)) {
 		printk("using polling idle threads.\n");
 		pm_idle = poll_idle;
+	} else if (!strncmp(str, "halt", 4)) {
+		printk("using halt in idle threads.\n");
+                pm_idle = default_idle;
 	}
 
 	return 1;
@@ -255,6 +300,10 @@ void machine_restart(char * __unused)
 	*((unsigned short *)__va(0x472)) = reboot_mode;
 	for (;;) {
 		int i;
+
+		if (machine_reset)
+			(*machine_reset)();
+
 		/* First fondle with the keyboard controller. */ 
 		for (i=0; i<100; i++) {
 			kb_wait();
@@ -755,6 +804,21 @@ asmlinkage long sys_vfork(struct pt_regs
 		       NULL, NULL);
 }
 
+static inline unsigned int get_random_int(void)
+{
+	unsigned int jitter;
+	/*
+	 * This is a pretty fast call, so no performance worries:
+	 */
+	get_random_bytes(&jitter, sizeof(jitter));
+	return jitter;
+}
+
+unsigned long arch_align_stack(unsigned long sp)
+{
+	return sp - ((get_random_int() % 65536) << 4);
+}
+
 /*
  * These bracket the sleeping functions..
  */
diff -urNp linux-204/arch/x86_64/kernel/setup.c linux-210/arch/x86_64/kernel/setup.c
--- linux-204/arch/x86_64/kernel/setup.c
+++ linux-210/arch/x86_64/kernel/setup.c
@@ -47,6 +47,8 @@
 #include <asm/bootsetup.h>
 #include <asm/proto.h>
 
+int swiotlb;
+
 /*
  * Machine setup..
  */
@@ -57,6 +59,10 @@ struct cpuinfo_x86 boot_cpu_data = { 
 
 unsigned long mmu_cr4_features;
 
+#ifdef CONFIG_ACPI_BOOT
+int acpi_noirq __devinitdata;  /* skip ACPI IRQ initialization */
+#endif
+
 int acpi_disabled = 0;
 static int __init acpioff(char *str)
 {
@@ -342,6 +348,12 @@ void __init setup_arch(char **cmdline_p)
 #ifdef CONFIG_GART_IOMMU
 	iommu_hole_init();
 #endif
+#ifdef CONFIG_SWIOTLB
+       if (!iommu_aperture && end_pfn >= 0xffffffff>>PAGE_SHIFT) {
+              swiotlb_init();
+              swiotlb = 1;
+       }
+#endif
 
 #ifdef CONFIG_VT
 #if defined(CONFIG_VGA_CONSOLE)
@@ -402,6 +414,349 @@ static void __init display_cacheinfo(str
 	}
 }
 
+#define LVL_1_INST      1
+#define LVL_1_DATA      2
+#define LVL_2           3
+#define LVL_3           4
+#define LVL_TRACE       5
+
+struct _cache_table
+{
+        unsigned char descriptor;
+        char cache_type;
+        short size;
+};
+
+/* all the cache descriptor types we care about (no TLB or trace cache entries) */
+static struct _cache_table cache_table[] __initdata =
+{
+	{ 0x06, LVL_1_INST, 8 },
+	{ 0x08, LVL_1_INST, 16 },
+	{ 0x0A, LVL_1_DATA, 8 },
+	{ 0x0C, LVL_1_DATA, 16 },
+	{ 0x22, LVL_3,      512 },
+	{ 0x23, LVL_3,      1024 },
+	{ 0x25, LVL_3,      2048 },
+	{ 0x29, LVL_3,      4096 },
+	{ 0x39, LVL_2,      128 },
+	{ 0x3C, LVL_2,      256 },
+	{ 0x41, LVL_2,      128 },
+	{ 0x42, LVL_2,      256 },
+	{ 0x43, LVL_2,      512 },
+	{ 0x44, LVL_2,      1024 },
+	{ 0x45, LVL_2,      2048 },
+	{ 0x66, LVL_1_DATA, 8 },
+	{ 0x67, LVL_1_DATA, 16 },
+	{ 0x68, LVL_1_DATA, 32 },
+	{ 0x70, LVL_TRACE,  12 },
+	{ 0x71, LVL_TRACE,  16 },
+	{ 0x72, LVL_TRACE,  32 },
+	{ 0x79, LVL_2,      128 },
+	{ 0x7A, LVL_2,      256 },
+	{ 0x7B, LVL_2,      512 },
+	{ 0x7C, LVL_2,      1024 },
+	{ 0x7D, LVL_2,      2048 },
+	{ 0x7F, LVL_2,      512 },
+	{ 0x82, LVL_2,      256 },
+	{ 0x83, LVL_2,      512 },
+	{ 0x84, LVL_2,      1024 },
+	{ 0x85, LVL_2,      2048 },
+	{ 0x00, 0, 0}
+};
+
+int select_idle_routine(struct cpuinfo_x86 *c);
+
+/* Some CPUID calls want 'count' to be placed in ecx */
+static inline void cpuid_count(int op, int count, int *eax, int *ebx, int *ecx,
+	       	int *edx)
+{
+	__asm__("cpuid"
+		: "=a" (*eax),
+		  "=b" (*ebx),
+		  "=c" (*ecx),
+		  "=d" (*edx)
+		: "0" (op), "c" (count));
+}
+
+enum _cache_type
+{
+	CACHE_TYPE_NULL	= 0,
+	CACHE_TYPE_DATA = 1,
+	CACHE_TYPE_INST = 2,
+	CACHE_TYPE_UNIFIED = 3
+};
+
+union _cpuid4_leaf_eax {
+	struct {
+		enum _cache_type	type:5;
+		unsigned int		level:3;
+		unsigned int		is_self_initializing:1;
+		unsigned int		is_fully_associative:1;
+		unsigned int		reserved:4;
+		unsigned int		num_threads_sharing:12;
+		unsigned int		num_cores_on_die:6;
+	} split;
+	u32 full;
+};
+
+union _cpuid4_leaf_ebx {
+	struct {
+		unsigned int		coherency_line_size:12;
+		unsigned int		physical_line_partition:10;
+		unsigned int		ways_of_associativity:10;
+	} split;
+	u32 full;
+};
+
+union _cpuid4_leaf_ecx {
+	struct {
+		unsigned int		number_of_sets:32;
+	} split;
+	u32 full;
+};
+
+struct _cpuid4_info {
+	union _cpuid4_leaf_eax eax;
+	union _cpuid4_leaf_ebx ebx;
+	union _cpuid4_leaf_ecx ecx;
+	unsigned long size;
+};
+
+#define MAX_CACHE_LEAVES		4
+
+static int __init cpuid4_cache_lookup(int index, struct _cpuid4_info *this_leaf)
+{
+	unsigned int		eax, ebx, ecx, edx;
+	union _cpuid4_leaf_eax	cache_eax;
+
+	cpuid_count(4, index, &eax, &ebx, &ecx, &edx);
+	cache_eax.full = eax;
+	if (cache_eax.split.type == CACHE_TYPE_NULL)
+		return -1;
+
+	this_leaf->eax.full = eax;
+	this_leaf->ebx.full = ebx;
+	this_leaf->ecx.full = ecx;
+	this_leaf->size = (this_leaf->ecx.split.number_of_sets + 1) *
+		(this_leaf->ebx.split.coherency_line_size + 1) *
+		(this_leaf->ebx.split.physical_line_partition + 1) *
+		(this_leaf->ebx.split.ways_of_associativity + 1);
+	return 0;
+}
+
+static int __init find_num_cache_leaves(void)
+{
+	unsigned int		eax, ebx, ecx, edx;
+	union _cpuid4_leaf_eax	cache_eax;
+	int 			i;
+	int 			retval;
+
+	retval = MAX_CACHE_LEAVES;
+	/* Do cpuid(4) loop to find out num_cache_leaves */
+	for (i = 0; i < MAX_CACHE_LEAVES; i++) {
+		cpuid_count(4, i, &eax, &ebx, &ecx, &edx);
+		cache_eax.full = eax;
+		if (cache_eax.split.type == CACHE_TYPE_NULL) {
+			retval = i;
+			break;
+		}
+	}
+	return retval;
+}
+
+static void __init init_intel(struct cpuinfo_x86 *c)
+{
+	unsigned int trace = 0, l1i = 0, l1d = 0, l2 = 0, l3 = 0; /* Cache sizes */
+	char *p = NULL;
+	u32 eax, dummy;
+	unsigned int new_l1d = 0, new_l1i = 0; /* Cache sizes from cpuid(4) */
+	unsigned int new_l2 = 0, new_l3 = 0, i; /* Cache sizes from cpuid(4) */
+	unsigned int n;
+
+
+	select_idle_routine(c);
+	if (c->cpuid_level > 4) {
+		static int is_initialized;
+		static int num_cache_leaves;
+
+		if (is_initialized == 0) {
+			/* Init num_cache_leaves from boot CPU */
+			num_cache_leaves = find_num_cache_leaves();
+			is_initialized++;
+		}
+ 
+		/*
+		 * Whenever possible use cpuid(4), deterministic cache 
+		 * parameters cpuid leaf to find the cache details
+		 */
+		for (i = 0; i < num_cache_leaves; i++) {
+			struct _cpuid4_info this_leaf;
+			int retval;
+
+			retval = cpuid4_cache_lookup(i, &this_leaf);
+			if (retval >= 0) {
+				switch(this_leaf.eax.split.level) {
+				    case 1:
+					if (this_leaf.eax.split.type == 
+							CACHE_TYPE_DATA)
+						new_l1d = this_leaf.size/1024;
+					else if (this_leaf.eax.split.type == 
+							CACHE_TYPE_INST)
+						new_l1i = this_leaf.size/1024;
+					break;
+				    case 2:
+					new_l2 = this_leaf.size/1024;
+					break;
+				    case 3:
+					new_l3 = this_leaf.size/1024;
+					break;
+				    default:
+					break;
+				}
+			}
+		}
+	}
+	if (c->cpuid_level > 1) {
+		/* supports eax=2  call */
+		int i, j, n;
+		int regs[4];
+		unsigned char *dp = (unsigned char *)regs;
+
+		/* Number of times to iterate */
+		n = cpuid_eax(2) & 0xFF;
+
+		for ( i = 0 ; i < n ; i++ ) {
+			cpuid(2, &regs[0], &regs[1], &regs[2], &regs[3]);
+			
+			/* If bit 31 is set, this is an unknown format */
+			for ( j = 0 ; j < 3 ; j++ ) {
+				if ( regs[j] < 0 ) regs[j] = 0;
+			}
+
+			/* Byte 0 is level count, not a descriptor */
+			for ( j = 1 ; j < 16 ; j++ ) {
+				unsigned char des = dp[j];
+				unsigned char k = 0;
+
+				/* look up this descriptor in the table */
+				while (cache_table[k].descriptor != 0)
+				{
+					if (cache_table[k].descriptor == des) {
+						switch (cache_table[k].cache_type) {
+						case LVL_1_INST:
+							l1i += cache_table[k].size;
+							break;
+						case LVL_1_DATA:
+							l1d += cache_table[k].size;
+							break;
+						case LVL_2:
+							l2 += cache_table[k].size;
+							break;
+						case LVL_3:
+							l3 += cache_table[k].size;
+							break;
+						case LVL_TRACE:
+							trace += cache_table[k].size;
+							break;
+						}
+						break;
+					}
+
+					k++;
+				}
+			}
+		}
+
+		if (new_l1d)
+			l1d = new_l1d;
+		if (new_l1i)
+			l1i = new_l1i;
+		if (new_l2)
+			l2 = new_l2;
+		if (new_l3)
+			l3 = new_l3;
+
+		if (trace)
+			printk(KERN_INFO "CPU: Trace cache: %dK uops\n", trace);
+		if (l1i || l1d)
+			printk(KERN_INFO
+				"CPU: L1 I-cache: %dK, L1 D-cache: %dK\n",
+				l1i, l1d);
+		if (l2)
+			printk(KERN_INFO "CPU: L2 cache: %dK\n", l2);
+		if (l3)
+			printk(KERN_INFO "CPU: L3 cache: %dK\n", l3);
+
+		/*
+		 * This assumes the L3 cache is shared; it typically lives in
+		 * the northbridge.  The L1 caches are included by the L2
+		 * cache, and so should not be included for the purpose of
+		 * SMP switching weights.
+		 */
+		c->x86_cache_size = l2 ? l2 : (l1i+l1d);
+	}
+
+	if ( p )
+		strcpy(c->x86_model_id, p);
+	
+#ifdef CONFIG_SMP
+	if (test_bit(X86_FEATURE_HT, &c->x86_capability)) {
+		extern	int phys_proc_id[NR_CPUS];
+		
+		int 	index_lsb, index_msb, tmp;
+		int	initial_apic_id;
+		int 	cpu = smp_processor_id();
+		u32 	ebx, ecx, edx;
+
+		cpuid(1, &eax, &ebx, &ecx, &edx);
+		smp_num_siblings = (ebx & 0xff0000) >> 16;
+
+		if (smp_num_siblings == 1) {
+			printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
+		} else if (smp_num_siblings > 1 ) {
+			index_lsb = 0;
+			index_msb = 31;
+			/*
+			 * At this point we only support two siblings per
+			 * processor package.
+			 */
+#define NR_SIBLINGS	2
+			if (smp_num_siblings != NR_SIBLINGS) {
+				printk(KERN_WARNING "CPU: Unsupported number of the siblings %d", smp_num_siblings);
+				smp_num_siblings = 1;
+				return;
+			}
+			tmp = smp_num_siblings;
+			while ((tmp & 1) == 0) {
+				tmp >>=1 ;
+				index_lsb++;
+			}
+			tmp = smp_num_siblings;
+			while ((tmp & 0x80000000 ) == 0) {
+				tmp <<=1 ;
+				index_msb--;
+			}
+			if (index_lsb != index_msb )
+				index_msb++;
+			initial_apic_id = ebx >> 24 & 0xff;
+			phys_proc_id[cpu] = initial_apic_id >> index_msb;
+
+			printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
+                               phys_proc_id[cpu]);
+		}
+
+	}
+#endif
+
+	n = cpuid_eax(0x80000000);
+	if (n >= 0x80000008) {
+		cpuid(0x80000008, &eax, &dummy, &dummy, &dummy); 
+		c->x86_virt_bits = (eax >> 8) & 0xff;
+		c->x86_phys_bits = eax & 0xff;
+	}
+	
+}
+
 static int __init init_amd(struct cpuinfo_x86 *c)
 {
 	int r;
@@ -431,6 +786,8 @@ void __init get_cpu_vendor(struct cpuinf
 
 	if (!strcmp(v, "AuthenticAMD"))
 		c->x86_vendor = X86_VENDOR_AMD;
+	else if (!strcmp(v, "GenuineIntel"))
+		c->x86_vendor = X86_VENDOR_INTEL;
 	else
 		c->x86_vendor = X86_VENDOR_UNKNOWN;
 }
@@ -446,7 +803,7 @@ struct cpu_model_info {
  */
 void __init identify_cpu(struct cpuinfo_x86 *c)
 {
-	int junk, i;
+	int i;
 	u32 xlvl, tfms;
 
 	c->loops_per_jiffy = loops_per_jiffy;
@@ -464,14 +821,23 @@ void __init identify_cpu(struct cpuinfo_
 	      (int *)&c->x86_vendor_id[4]);
 		
 	get_cpu_vendor(c);
+#ifndef CONFIG_IA32E
+	if (c->x86_vendor == X86_VENDOR_INTEL)
+		panic("no EM64T cpu support in this kernel"
+			" -- use an ia32e rpm instead");
+#endif
 	/* Initialize the standard set of capabilities */
 	/* Note that the vendor-specific code below might override */
 
 	/* Intel-defined flags: level 0x00000001 */
 	if ( c->cpuid_level >= 0x00000001 ) {	
-		__u32 misc;
-		cpuid(0x00000001, &tfms, &misc, &junk,
+		__u32 misc, extra;
+		cpuid(0x00000001, &tfms, &misc, &extra,
 		      &c->x86_capability[0]);
+#ifdef CONFIG_IA32E
+		/* Only IA32E has the fifth config word */
+		c->x86_capability[4] = extra;
+#endif
 		c->x86 = (tfms >> 8) & 15;
 		c->x86_model = (tfms >> 4) & 15;
 		if (c->x86 == 0xf) { /* extended */
@@ -519,6 +885,9 @@ void __init identify_cpu(struct cpuinfo_
 			init_amd(c);
 			break;
 
+		case X86_VENDOR_INTEL:
+			init_intel(c);
+			break;
 		case X86_VENDOR_UNKNOWN:
 		default:
 			/* Not much we can do here... */
@@ -565,6 +934,10 @@ void __init print_cpu_info(struct cpuinf
 static int show_cpuinfo(struct seq_file *m, void *v)
 {
 	struct cpuinfo_x86 *c = v;
+	int n = c - cpu_data;
+#ifdef CONFIG_SMP
+	extern	int phys_proc_id[NR_CPUS];
+#endif
 
 	/* 
 	 * These flag bits must match the definitions in <asm/cpufeature.h>.
@@ -579,7 +952,7 @@ static int show_cpuinfo(struct seq_file 
 	        "fpu", "vme", "de", "pse", "tsc", "msr", "pae", "mce",
 	        "cx8", "apic", NULL, "sep", "mtrr", "pge", "mca", "cmov",
 	        "pat", "pse36", "pn", "clflush", NULL, "dts", "acpi", "mmx",
-	        "fxsr", "sse", "sse2", "ss", NULL, "tm", "ia64", NULL,
+	        "fxsr", "sse", "sse2", "ss", "ht", "tm", "ia64", "ferr", 
 
 		/* AMD-defined */
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
@@ -598,6 +971,12 @@ static int show_cpuinfo(struct seq_file 
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
+
+		/* Intel Defined (cpuid 1 and ecx) */
+		"sse3", NULL, NULL, "monitor", "ds-cpl", NULL, NULL, "gv3",	
+		"tm2", NULL, "cnxt-id", NULL, NULL, NULL, NULL, NULL,
+		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
+		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 	};
 	static char *x86_power_flags[] = { 
 		"ts",	/* temperature sensor */
@@ -622,6 +1001,18 @@ static int show_cpuinfo(struct seq_file 
 		     (int)c->x86_model,
 		     c->x86_model_id[0] ? c->x86_model_id : "unknown");
 	
+#ifdef CONFIG_SMP
+	seq_printf(m, "physical id\t: %d\n",phys_proc_id[n]);
+	seq_printf(m, "siblings\t: %d\n",smp_num_siblings);
+#if CONFIG_SHARE_RUNQUEUE
+{
+	extern long __rq_idx[NR_CPUS];
+
+	seq_printf(m, "runqueue\t: %ld\n", __rq_idx[n]);
+}
+#endif
+#endif
+
 	if (c->x86_mask || c->cpuid_level >= 0)
 		seq_printf(m, "stepping\t: %d\n", c->x86_mask);
 	else
diff -urNp linux-204/arch/x86_64/kernel/signal.c linux-210/arch/x86_64/kernel/signal.c
--- linux-204/arch/x86_64/kernel/signal.c
+++ linux-210/arch/x86_64/kernel/signal.c
@@ -30,6 +30,7 @@
 #include <asm/uaccess.h>
 #include <asm/i387.h>
 #include <asm/proto.h>
+#include <asm/vsyscall.h>
 
 #define DEBUG_SIG 0
 
@@ -137,15 +138,15 @@ restore_sigcontext(struct pt_regs *regs,
 
 
 #define COPY(x)		err |= __get_user(regs->x, &sc->x)
-#define COPY_CANON(x)   \
-	COPY(x); \
-	if ((regs->x >> 48)  != 0 && (regs->x >> 48) != 0xffff) \
-				regs->x = 0; 
 
 	/* fs and gs are ignored because we cannot handle the 64bit base easily */ 
 
-	COPY(rdi); COPY(rsi); COPY(rbp); COPY_CANON(rsp); COPY(rbx);
-	COPY(rdx); COPY(rcx); COPY_CANON(rip);
+	COPY(rdi); COPY(rsi); COPY(rbp); COPY(rsp);
+	if (unlikely(regs->rsp >= TASK_SIZE))
+		regs->rsp = 0UL;
+	COPY(rbx); COPY(rdx); COPY(rcx); COPY(rip);
+	if (unlikely(regs->rip >= TASK_SIZE && regs->rip < VSYSCALL_START))
+		regs->rip = 0UL;
 	COPY(r8);
 	COPY(r9);
 	COPY(r10);
@@ -356,7 +357,10 @@ static void setup_rt_frame(int sig, stru
 	regs->rsi = (unsigned long)&frame->info; 
 	regs->rdx = (unsigned long)&frame->uc; 
 	regs->rsp = (unsigned long) frame;
-	regs->rip = (unsigned long) ka->sa.sa_handler;
+	if (unlikely((unsigned long)ka->sa.sa_handler >= TASK_SIZE))
+		regs->rip = 0UL;
+	else
+		regs->rip = (unsigned long)ka->sa.sa_handler;
 	regs->cs = __USER_CS;
 	regs->ss = __USER_DS; 
 
diff -urNp linux-204/arch/x86_64/kernel/smpboot.c linux-210/arch/x86_64/kernel/smpboot.c
--- linux-204/arch/x86_64/kernel/smpboot.c
+++ linux-210/arch/x86_64/kernel/smpboot.c
@@ -60,11 +60,16 @@ static int cpu_mask = -1; 
 /* Total count of live CPUs */
 int smp_num_cpus = 1;
 
+/* Number of siblings per CPU package */
+int smp_num_siblings = 1;
+int phys_proc_id[NR_CPUS]; /* Package ID of each logical CPU */
+int cpu_sibling_map[NR_CPUS] __cacheline_aligned;
+
+static int test_ht;
+
 /* Bitmask of currently online CPUs */
 unsigned long cpu_online_map;
 
-/* which CPU (physical APIC ID) maps to which logical CPU number */
-volatile int x86_apicid_to_cpu[NR_CPUS];
 /* which logical CPU number maps to which CPU (physical APIC ID) */
 volatile int x86_cpu_to_apicid[NR_CPUS];
 
@@ -79,6 +84,14 @@ int smp_threads_ready;
 
 extern void time_init_smp(void);
 
+static int __init ht_setup(char *str)
+{
+	test_ht = 1;
+	return 1;
+}
+
+__setup("test_ht", ht_setup);
+
 /*
  * Setup routine for controlling SMP activation
  *
@@ -540,7 +553,6 @@ static int __init do_boot_cpu (int apici
 		panic("failed fork for CPU %d", cpu);
 	wake_up_forked_process(idle);
 	x86_cpu_to_apicid[cpu] = apicid;
-	x86_apicid_to_cpu[apicid] = cpu;
 	init_idle(idle, cpu);
 	idle->cpu = cpu;
 	idle->cpus_allowed = 1<<cpu;
@@ -755,7 +767,6 @@ static int __init do_boot_cpu (int apici
 	}
 	if (send_status || accept_status || boot_status) {
 		x86_cpu_to_apicid[cpu] = -1;
-		x86_apicid_to_cpu[apicid] = -1;
 		cpucount--;
 	}
 
@@ -821,7 +832,7 @@ extern int prof_counter[NR_CPUS];
 
 void __init smp_boot_cpus(void)
 {
-	int apicid, cpu, maxcpu;
+	int apicid, cpu, maxcpu, bit, kicked;
 
 #ifdef CONFIG_MTRR
 	/*  Must be done before other processors booted  */
@@ -833,7 +844,6 @@ void __init smp_boot_cpus(void)
 	 */
 
 	for (apicid = 0; apicid < NR_CPUS; apicid++) {
-		x86_apicid_to_cpu[apicid] = -1;
 		prof_counter[apicid] = 1;
 		prof_old_multiplier[apicid] = 1;
 		prof_multiplier[apicid] = 1;
@@ -850,7 +860,6 @@ void __init smp_boot_cpus(void)
 	 * We have the boot CPU online for sure.
 	 */
 	set_bit(0, &cpu_online_map);
-	x86_apicid_to_cpu[boot_cpu_id] = 0;
 	x86_cpu_to_apicid[0] = boot_cpu_id;
 	global_irq_holder = 0;
 	current->cpu = 0;
@@ -921,11 +930,13 @@ void __init smp_boot_cpus(void)
 	Dprintk("CPU present map: %lx\n", phys_cpu_present_map);
 
 	maxcpu = 0;
-	for (apicid = 0; apicid < NR_CPUS; apicid++) {
+	kicked = 1;
+	for (bit = 0; kicked < NR_CPUS && bit < MAX_APICS; bit++) {
+		apicid = cpu_present_to_apicid(bit);
 		/*
 		 * Don't even attempt to start the boot CPU!
 		 */
-		if (apicid == boot_cpu_id)
+		if (apicid == boot_cpu_id || (apicid == BAD_APICID))
 			continue;
 
 		if (!(phys_cpu_present_map & (1 << apicid)))
@@ -940,11 +951,13 @@ void __init smp_boot_cpus(void)
 		/*
 		 * Make sure we unmap all failed CPUs
 		 */
-		if ((x86_apicid_to_cpu[apicid] == -1) &&
-				(phys_cpu_present_map & (1 << apicid)))
+		if ((x86_cpu_to_apicid[cpu] == -1) && 
+				(phys_cpu_present_map & (1 << apicid))) {
 			printk("phys CPU #%d not responding - cannot use it.\n",apicid);
-		else if (cpu > maxcpu) 
+			continue;
+		} else if (cpu > maxcpu) 
 			maxcpu = cpu; 
+		++kicked;
 	}
 
 	/*
@@ -988,6 +1001,70 @@ void __init smp_boot_cpus(void)
 	Dprintk("Boot done.\n");
 
 	/*
+	 * If Hyper-Threading is avaialble, construct cpu_sibling_map[], so
+	 * that we can tell the sibling CPU efficiently.
+	 */
+	if (test_bit(X86_FEATURE_HT, boot_cpu_data.x86_capability)
+	    && smp_num_siblings > 1) {
+		for (cpu = 0; cpu < NR_CPUS; cpu++)
+			cpu_sibling_map[cpu] = NO_PROC_ID;
+		
+		for (cpu = 0; cpu < smp_num_cpus; cpu++) {
+			int 	i;
+			
+			for (i = 0; i < smp_num_cpus; i++) {
+				if (i == cpu)
+					continue;
+				if (phys_proc_id[cpu] == phys_proc_id[i]) {
+					cpu_sibling_map[cpu] = i;
+					printk("cpu_sibling_map[%d] = %d\n", cpu, cpu_sibling_map[cpu]);
+					break;
+				}
+			}
+			if (cpu_sibling_map[cpu] == NO_PROC_ID) {
+				smp_num_siblings = 1;
+				printk(KERN_WARNING "WARNING: No sibling found for CPU %d.\n", cpu);
+			}
+		}
+#if CONFIG_SHARE_RUNQUEUE
+		/*
+		 * At this point APs would have synchronised TSC and
+		 * waiting for smp_commenced, with their APIC timer
+		 * disabled. So BP can go ahead do some initialization
+		 * for Hyper-Threading (if needed).
+		 */
+		for (cpu = 0; cpu < NR_CPUS; cpu++) {
+			int i;
+			i = cpu_sibling_map[cpu];
+			
+			/* no sibbling? nothing to do */
+			if (i == NO_PROC_ID)
+				continue;
+			
+			/* both cpus need to be online */
+			if (!test_bit(cpu, &cpu_online_map))
+				continue;
+			if (!test_bit(i, &cpu_online_map))
+				continue;
+				
+			/* set this up once for the pair of cpus */	
+			if (i <= cpu)
+				continue;
+  
+			/*
+			 * merge runqueues, resulting in one
+			 * runqueue per package:
+			 */
+			sched_map_runqueue(cpu, i);
+		}
+#endif
+	}
+#if CONFIG_SHARE_RUNQUEUE
+	if (smp_num_siblings == 1 && test_ht)
+		sched_map_runqueue(0, 1);
+#endif
+	     
+	/*
 	 * Here we can be sure that there is an IO-APIC in the system. Let's
 	 * go and set it up:
 	 */
diff -urNp linux-204/arch/x86_64/kernel/x8664_ksyms.c linux-210/arch/x86_64/kernel/x8664_ksyms.c
--- linux-204/arch/x86_64/kernel/x8664_ksyms.c
+++ linux-210/arch/x86_64/kernel/x8664_ksyms.c
@@ -107,6 +107,8 @@ EXPORT_SYMBOL(cpu_data);
 EXPORT_SYMBOL(kernel_flag_cacheline);
 EXPORT_SYMBOL(smp_num_cpus);
 EXPORT_SYMBOL(cpu_online_map);
+EXPORT_SYMBOL(smp_num_siblings);
+EXPORT_SYMBOL(cpu_sibling_map);
 extern void __read_lock_failed(void);
 extern void __write_lock_failed(void);
 EXPORT_SYMBOL_NOVERS(__write_lock_failed);
@@ -231,3 +233,6 @@ EXPORT_SYMBOL(fake_node);
 
 extern void int_ret_from_sys_call(void);
 EXPORT_SYMBOL(int_ret_from_sys_call); 
+
+extern int swiotlb;
+EXPORT_SYMBOL(swiotlb);
diff -urNp linux-204/arch/x86_64/lib/copy_page.S linux-210/arch/x86_64/lib/copy_page.S
--- linux-204/arch/x86_64/lib/copy_page.S
+++ linux-210/arch/x86_64/lib/copy_page.S
@@ -8,6 +8,7 @@
 	.globl copy_page
 	.p2align 4
 copy_page:
+#ifdef CONFIG_PREFETCH
 	prefetch (%rsi) 
 	prefetch 1*64(%rsi)
 	prefetch 2*64(%rsi)
@@ -18,7 +19,13 @@ copy_page:
 	prefetchw 2*64(%rdi) 
 	prefetchw 3*64(%rdi)
 	prefetchw 4*64(%rdi)
-
+#else
+	prefetchnta (%rsi)
+	prefetchnta 1*64(%rsi)
+	prefetchnta 2*64(%rsi)
+	prefetchnta 3*64(%rsi)
+	prefetchnta 4*64(%rsi)
+#endif
 	subq	$3*8,%rsp
 	movq	%rbx,(%rsp)
 	movq	%r12,1*8(%rsp)
@@ -37,9 +44,11 @@ copy_page:
 	movq     40 (%rsi), %r10
 	movq     48 (%rsi), %r11
 	movq     56 (%rsi), %r12
-
+#ifdef CONFIG_PREFETCH
 	prefetch 5*64(%rsi)
-
+#else
+	prefetchnta 5*64(%rsi)
+#endif
 	movq     %rax,    (%rdi)
 	movq     %rbx,  8 (%rdi)
 	movq     %rdx, 16 (%rdi)
@@ -48,9 +57,9 @@ copy_page:
 	movq     %r10, 40 (%rdi)
 	movq     %r11, 48 (%rdi)
 	movq     %r12, 56 (%rdi)
-
+#ifdef CONFIG_PREFETCH
 	prefetchw 5*64(%rdi)
-
+#endif
 	leaq    64 (%rsi), %rsi
 	leaq    64 (%rdi), %rdi
 
diff -urNp linux-204/arch/x86_64/lib/csum-copy.S linux-210/arch/x86_64/lib/csum-copy.S
--- linux-204/arch/x86_64/lib/csum-copy.S
+++ linux-210/arch/x86_64/lib/csum-copy.S
@@ -58,7 +58,7 @@
 csum_partial_copy_generic:
 	cmpl	 $3*64,%edx
 	jle	 .Lignore
-
+#ifdef CONFIG_PREFETCH
 	ignore
 	prefetch (%rdi)
 	ignore
@@ -79,7 +79,18 @@ csum_partial_copy_generic:
 	prefetchw 3*64(%rsi)
 	ignore
 	prefetchw 4*64(%rsi)
-
+#else
+	ignore
+	prefetchnta (%rdi)
+	ignore
+	prefetchnta 1*64(%rdi)
+	ignore
+	prefetchnta 2*64(%rdi)
+	ignore
+	prefetchnta 3*64(%rdi)
+	ignore
+	prefetchnta 4*64(%rdi)
+#endif
 .Lignore:		
 	subq  $7*8,%rsp
 	movq  %rbx,2*8(%rsp)
@@ -125,9 +136,13 @@ csum_partial_copy_generic:
 	movq  48(%rdi),%r14
 	source
 	movq  56(%rdi),%r13
-		
+#ifdef CONFIG_PREFETCH		
 	ignore 2f
 	prefetch 5*64(%rdi)
+#else
+	ignore 2f
+	prefetchnta 5*64(%rdi)
+#endif
 2:							
 	adcq  %rbx,%rax
 	adcq  %r8,%rax
@@ -157,11 +172,11 @@ csum_partial_copy_generic:
 	movq %r14,48(%rsi)
 	dest
 	movq %r13,56(%rsi)
-	
+#ifdef CONFIG_PREFETCH	
 	ignore 3f
 	prefetchw 5*64(%rsi)
 3:
-	
+#endif
 	leaq 64(%rdi),%rdi
 	leaq 64(%rsi),%rsi
 
diff -urNp linux-204/arch/x86_64/mm/init.c linux-210/arch/x86_64/mm/init.c
--- linux-204/arch/x86_64/mm/init.c
+++ linux-210/arch/x86_64/mm/init.c
@@ -242,6 +242,15 @@ void __init init_memory_mapping(void) 
 	unsigned long end;
 	unsigned long next; 
 	unsigned long pgds, pmds, tables; 
+	unsigned long efer;
+
+	/* we need to set correct supported_pte_mask
+	 * before we setup direct memory mappings
+	 */
+	rdmsrl(MSR_EFER, efer);
+	if (!(efer & EFER_NX)) {
+		__supported_pte_mask &= ~_PAGE_NX;
+	}
 
 	end = end_pfn_map << PAGE_SHIFT; 
 
diff -urNp linux-204/arch/x86_64/mm/numa.c linux-210/arch/x86_64/mm/numa.c
--- linux-204/arch/x86_64/mm/numa.c
+++ linux-210/arch/x86_64/mm/numa.c
@@ -79,6 +79,11 @@ void __init setup_node_bootmem(int nodei
 
 EXPORT_SYMBOL(maxnode);
 
+#ifdef CONFIG_IA32E
+extern unsigned long end_dma_pfn;
+unsigned long max_dma_address_pa = __pa(MAX_DMA_ADDRESS);
+#endif
+
 /* Initialize final allocator for a zone */
 void __init setup_node_zones(int nodeid)
 { 
@@ -93,9 +98,20 @@ void __init setup_node_zones(int nodeid)
 	end_pfn = PLAT_NODE_DATA(nodeid)->end_pfn; 
 
 	printk("setting up node %d %lx-%lx\n", nodeid, start_pfn, end_pfn); 
-	
-	/* All nodes > 0 have a zero length zone DMA */ 
-	dma_end_pfn = __pa(MAX_DMA_ADDRESS) >> PAGE_SHIFT; 
+#ifdef CONFIG_IA32E
+	/* bootline maxdma= option overrides MAX_DMA_ADDRESS */
+	if (end_dma_pfn) {
+		if ((end_dma_pfn << PAGE_SHIFT) > (1L << 32)) {
+			printk("Warning: maxdma exceeds 4GB, resetting to %ld\n", max_dma_address_pa);
+			dma_end_pfn = max_dma_address_pa >> PAGE_SHIFT;
+		} else {
+			dma_end_pfn = end_dma_pfn;
+			max_dma_address_pa = end_dma_pfn << PAGE_SHIFT;
+		}
+	} else
+#endif
+		dma_end_pfn = __pa(MAX_DMA_ADDRESS) >> PAGE_SHIFT;
+	/* All nodes > 0 have a zero length zone DMA */
 	if (start_pfn < dma_end_pfn) { 
 		zones[ZONE_DMA] = dma_end_pfn - start_pfn;
 		zones[ZONE_NORMAL] = end_pfn - dma_end_pfn; 
diff -urNp linux-204/arch/x86_64/oprofile/Makefile linux-210/arch/x86_64/oprofile/Makefile
--- linux-204/arch/x86_64/oprofile/Makefile
+++ linux-210/arch/x86_64/oprofile/Makefile
@@ -14,6 +14,9 @@ obj-y = init.o timer_int.o \
  
 ifeq ($(CONFIG_X86_LOCAL_APIC),y)
 obj-y += nmi_int.o op_model_athlon.o
+ifdef CONFIG_IA32E
+obj-y += op_model_p4.o op_model_ppro.o
+endif
 endif
  
 obj	:= .
@@ -23,6 +26,12 @@ $(obj)/nmi_int.c: ${INCL}
 	@ln -sf ../../i386/oprofile/nmi_int.c $(obj)/nmi_int.c
 $(obj)/op_model_athlon.c: ${INCL}
 	@ln -sf ../../i386/oprofile/op_model_athlon.c $(obj)/op_model_athlon.c
+ifdef CONFIG_IA32E
+$(obj)/op_model_p4.c: ${INCL}
+	@ln -sf ../../i386/oprofile/op_model_p4.c $(obj)/op_model_p4.c
+$(obj)/op_model_ppro.c: ${INCL}
+	@ln -sf ../../i386/oprofile/op_model_ppro.c $(obj)/op_model_ppro.c
+endif
 $(obj)/init.c: ${INCL}
 	@ln -sf ../../i386/oprofile/init.c $(obj)/init.c
 $(obj)/timer_int.c: ${INCL}
@@ -33,5 +42,8 @@ $(obj)/op_x86_model.h:
 	@ln -sf ../../i386/oprofile/op_x86_model.h $(obj)/op_x86_model.h	
 clean-files += op_x86_model.h op_counter.h timer_int.c init.c \
 	       op_model_athlon.c nmi_int.c
+ifdef CONFIG_IA32E
+clean-files += op_model_p4.c op_model_ppro.c
+endif
 
 include $(TOPDIR)/Rules.make
diff -urNp linux-204/drivers/pci/quirks.c linux-210/drivers/pci/quirks.c
--- linux-204/drivers/pci/quirks.c
+++ linux-210/drivers/pci/quirks.c
@@ -647,6 +647,39 @@ static void __init quirk_svwks_csb5ide(s
 	}
 }
 
+/* 
+ * Some chipsets don't allow changing the irq affinity settings
+ */
+
+int no_valid_irqaffinity;
+
+#ifdef CONFIG_X86_IO_APIC
+static void __init quirk_intel_irq_affinity(struct pci_dev *pdev)
+{
+	u8 rev, config;
+	int word;
+	pci_read_config_byte(pdev, PCI_REVISION_ID, &rev);
+	if (rev > 0x09)		/* Only 09 and earlier require this */
+		return;
+
+	pci_read_config_byte(pdev, 0xF4, &config);
+	config |= 0x2;
+	/* enable access to config space*/
+	pci_write_config_byte(pdev, 0xF4, config);
+
+	pci_config_read(0, 0, 8, 0, 0x4c, 2, &word);
+	if (!(word & (1 << 13))) {
+		no_valid_irqaffinity = 1;
+		printk("Disabling IRQ affinity setting\n");
+	}
+
+	config &= ~0x2;
+	/* disable access to config space*/
+	pci_write_config_byte(pdev, 0xF4, config);
+}
+
+#endif
+
 /*
  *  The main table of quirks.
  */
@@ -697,6 +730,9 @@ static struct pci_fixup pci_fixups[] __i
 
 #ifdef CONFIG_X86_IO_APIC 
 	{ PCI_FIXUP_FINAL,	PCI_VENDOR_ID_VIA,	PCI_DEVICE_ID_VIA_82C686,	quirk_via_ioapic },
+	{ PCI_FIXUP_HEADER,	PCI_VENDOR_ID_INTEL,	0x3590,	quirk_intel_irq_affinity },
+	{ PCI_FIXUP_HEADER,	PCI_VENDOR_ID_INTEL,	0x3592,	quirk_intel_irq_affinity },
+	{ PCI_FIXUP_HEADER,	PCI_VENDOR_ID_INTEL,	0x359e,	quirk_intel_irq_affinity },
 #endif
 	{ PCI_FIXUP_HEADER,	PCI_VENDOR_ID_VIA,	PCI_DEVICE_ID_VIA_82C586_3,	quirk_via_acpi },
 	{ PCI_FIXUP_HEADER,	PCI_VENDOR_ID_VIA,	PCI_DEVICE_ID_VIA_82C686_4,	quirk_via_acpi },
diff -urNp linux-204/include/asm-x86_64/acpi.h linux-210/include/asm-x86_64/acpi.h
--- linux-204/include/asm-x86_64/acpi.h
+++ linux-210/include/asm-x86_64/acpi.h
@@ -107,6 +107,7 @@
 #ifndef CONFIG_ACPI_BOOT
 #define acpi_lapic 0
 #define acpi_ioapic 0
+#define acpi_noirq 0
 #else
 #ifdef CONFIG_X86_LOCAL_APIC
 extern int acpi_lapic;
@@ -119,6 +120,8 @@ extern int acpi_ioapic;
 #define acpi_ioapic 0
 #endif
 
+extern int acpi_noirq;
+
 /* Fixmap pages to reserve for ACPI boot-time tables (see fixmap.h) */
 #define FIX_ACPI_PAGES 4
 
diff -urNp linux-204/include/asm-x86_64/apicdef.h linux-210/include/asm-x86_64/apicdef.h
--- linux-204/include/asm-x86_64/apicdef.h
+++ linux-210/include/asm-x86_64/apicdef.h
@@ -360,4 +360,6 @@ struct local_apic {
 
 #undef u32
 
+#define BAD_APICID 0xFFu
+
 #endif
diff -urNp linux-204/include/asm-x86_64/cpufeature.h linux-210/include/asm-x86_64/cpufeature.h
--- linux-204/include/asm-x86_64/cpufeature.h
+++ linux-210/include/asm-x86_64/cpufeature.h
@@ -10,7 +10,11 @@
 /* Sample usage: CPU_FEATURE_P(cpu.x86_capability, FPU) */
 #define CPU_FEATURE_P(CAP, FEATURE) test_bit(CAP, X86_FEATURE_##FEATURE ##_BIT)
 
+#ifdef CONFIG_IA32E 
+#define NCAPINTS	5	/* Currently we have 5 32-bit words worth of info */
+#else
 #define NCAPINTS	4	/* Currently we have 4 32-bit words worth of info */
+#endif
 
 /* Intel-defined CPU features, CPUID level 0x00000001, word 0 */
 #define X86_FEATURE_FPU		(0*32+ 0) /* Onboard FPU */
@@ -40,8 +44,10 @@
 #define X86_FEATURE_XMM		(0*32+25) /* Streaming SIMD Extensions */
 #define X86_FEATURE_XMM2	(0*32+26) /* Streaming SIMD Extensions-2 */
 #define X86_FEATURE_SELFSNOOP	(0*32+27) /* CPU self snoop */
+#define X86_FEATURE_HT 		(0*32+28) /* Hyper-Threading */
 #define X86_FEATURE_ACC		(0*32+29) /* Automatic clock control */
 #define X86_FEATURE_IA64	(0*32+30) /* IA-64 processor */
+#define X86_FEATURE_FERR	(0*32+31) /* FERR feedback */
 
 /* AMD-defined CPU features, CPUID level 0x80000001, word 1 */
 /* Don't duplicate feature flags which are redundant with Intel! */
@@ -62,6 +68,17 @@
 #define X86_FEATURE_K6_MTRR	(3*32+ 1) /* AMD K6 nonstandard MTRRs */
 #define X86_FEATURE_CYRIX_ARR	(3*32+ 2) /* Cyrix ARRs (= MTRRs) */
 #define X86_FEATURE_CENTAUR_MCR	(3*32+ 3) /* Centaur MCRs (= MTRRs) */
+#ifdef CONFIG_IA32E
+/* Intel-defined CPU features, CPUID level 0x00000001, ecx, word 4 */
+#define X86_FEATURE_SSE3	(4*32+ 0) /* Streaming SIMD Extensions-3 */
+#define X86_FEATURE_MWAIT	(4*32+ 3) /* Monitor-Mwait Support */
+#define X86_FEATURE_DS_CPL	(4*32+ 4) /* CPL qualified debug store */
+#define X86_FEATURE_GV3	(4*32+ 7) /* Geyserville 3 */
+#define X86_FEATURE_TM2	(4*32+ 8) /* Thermal monitor 2 */
+#define X86_FEATURE_CNXT_ID	(4*32+10) /* L1 Context ID*/
+#endif
+#define boot_cpu_has(bit)  test_bit(bit, boot_cpu_data.x86_capability)
+#define cpu_has(c, bit)            test_bit(bit, (c)->x86_capability)
 
 #endif /* __ASM_X8664_CPUFEATURE_H */
 
diff -urNp linux-204/include/asm-x86_64/msr.h linux-210/include/asm-x86_64/msr.h
--- linux-204/include/asm-x86_64/msr.h
+++ linux-210/include/asm-x86_64/msr.h
@@ -105,6 +105,11 @@
 #define MSR_IA32_EVNTSEL0      0x186
 #define MSR_IA32_EVNTSEL1      0x187
 
+#define MSR_IA32_THERM_CONTROL          0x19a
+#define MSR_IA32_THERM_INTERRUPT        0x19b
+#define MSR_IA32_THERM_STATUS           0x19c
+#define MSR_IA32_MISC_ENABLE            0x1a0
+
 #define MSR_IA32_DEBUGCTLMSR       0x1d9
 #define MSR_IA32_LASTBRANCHFROMIP  0x1db
 #define MSR_IA32_LASTBRANCHTOIP        0x1dc
@@ -161,6 +166,90 @@
 #define MSR_K6_PSOR			0xC0000087
 #define MSR_K6_PFIR			0xC0000088
 
+/* Pentium IV performance counter MSRs */
+#define MSR_P4_BPU_PERFCTR0 		0x300
+#define MSR_P4_BPU_PERFCTR1 		0x301
+#define MSR_P4_BPU_PERFCTR2 		0x302
+#define MSR_P4_BPU_PERFCTR3 		0x303
+#define MSR_P4_MS_PERFCTR0 		0x304
+#define MSR_P4_MS_PERFCTR1 		0x305
+#define MSR_P4_MS_PERFCTR2 		0x306
+#define MSR_P4_MS_PERFCTR3 		0x307
+#define MSR_P4_FLAME_PERFCTR0 		0x308
+#define MSR_P4_FLAME_PERFCTR1 		0x309
+#define MSR_P4_FLAME_PERFCTR2 		0x30a
+#define MSR_P4_FLAME_PERFCTR3 		0x30b
+#define MSR_P4_IQ_PERFCTR0 		0x30c
+#define MSR_P4_IQ_PERFCTR1 		0x30d
+#define MSR_P4_IQ_PERFCTR2 		0x30e
+#define MSR_P4_IQ_PERFCTR3 		0x30f
+#define MSR_P4_IQ_PERFCTR4 		0x310
+#define MSR_P4_IQ_PERFCTR5 		0x311
+#define MSR_P4_BPU_CCCR0 		0x360
+#define MSR_P4_BPU_CCCR1 		0x361
+#define MSR_P4_BPU_CCCR2 		0x362
+#define MSR_P4_BPU_CCCR3 		0x363
+#define MSR_P4_MS_CCCR0 		0x364
+#define MSR_P4_MS_CCCR1 		0x365
+#define MSR_P4_MS_CCCR2 		0x366
+#define MSR_P4_MS_CCCR3 		0x367
+#define MSR_P4_FLAME_CCCR0 		0x368
+#define MSR_P4_FLAME_CCCR1 		0x369
+#define MSR_P4_FLAME_CCCR2 		0x36a
+#define MSR_P4_FLAME_CCCR3 		0x36b
+#define MSR_P4_IQ_CCCR0 		0x36c
+#define MSR_P4_IQ_CCCR1 		0x36d
+#define MSR_P4_IQ_CCCR2 		0x36e
+#define MSR_P4_IQ_CCCR3 		0x36f
+#define MSR_P4_IQ_CCCR4 		0x370
+#define MSR_P4_IQ_CCCR5 		0x371
+#define MSR_P4_ALF_ESCR0 		0x3ca
+#define MSR_P4_ALF_ESCR1 		0x3cb
+#define MSR_P4_BPU_ESCR0 		0x3b2
+#define MSR_P4_BPU_ESCR1 		0x3b3
+#define MSR_P4_BSU_ESCR0 		0x3a0
+#define MSR_P4_BSU_ESCR1 		0x3a1
+#define MSR_P4_CRU_ESCR0 		0x3b8
+#define MSR_P4_CRU_ESCR1 		0x3b9
+#define MSR_P4_CRU_ESCR2 		0x3cc
+#define MSR_P4_CRU_ESCR3 		0x3cd
+#define MSR_P4_CRU_ESCR4 		0x3e0
+#define MSR_P4_CRU_ESCR5 		0x3e1
+#define MSR_P4_DAC_ESCR0 		0x3a8
+#define MSR_P4_DAC_ESCR1 		0x3a9
+#define MSR_P4_FIRM_ESCR0 		0x3a4
+#define MSR_P4_FIRM_ESCR1 		0x3a5
+#define MSR_P4_FLAME_ESCR0 		0x3a6
+#define MSR_P4_FLAME_ESCR1 		0x3a7
+#define MSR_P4_FSB_ESCR0 		0x3a2
+#define MSR_P4_FSB_ESCR1 		0x3a3
+#define MSR_P4_IQ_ESCR0 		0x3ba
+#define MSR_P4_IQ_ESCR1 		0x3bb
+#define MSR_P4_IS_ESCR0 		0x3b4
+#define MSR_P4_IS_ESCR1 		0x3b5
+#define MSR_P4_ITLB_ESCR0 		0x3b6
+#define MSR_P4_ITLB_ESCR1 		0x3b7
+#define MSR_P4_IX_ESCR0 		0x3c8
+#define MSR_P4_IX_ESCR1 		0x3c9
+#define MSR_P4_MOB_ESCR0 		0x3aa
+#define MSR_P4_MOB_ESCR1 		0x3ab
+#define MSR_P4_MS_ESCR0 		0x3c0
+#define MSR_P4_MS_ESCR1 		0x3c1
+#define MSR_P4_PMH_ESCR0 		0x3ac
+#define MSR_P4_PMH_ESCR1 		0x3ad
+#define MSR_P4_RAT_ESCR0 		0x3bc
+#define MSR_P4_RAT_ESCR1 		0x3bd
+#define MSR_P4_SAAT_ESCR0 		0x3ae
+#define MSR_P4_SAAT_ESCR1 		0x3af
+#define MSR_P4_SSU_ESCR0 		0x3be
+#define MSR_P4_SSU_ESCR1 		0x3bf    /* guess: not defined in manual */
+#define MSR_P4_TBPU_ESCR0 		0x3c2
+#define MSR_P4_TBPU_ESCR1 		0x3c3
+#define MSR_P4_TC_ESCR0 		0x3c4
+#define MSR_P4_TC_ESCR1 		0x3c5
+#define MSR_P4_U2L_ESCR0 		0x3b0
+#define MSR_P4_U2L_ESCR1 		0x3b1
+
 /* Centaur-Hauls/IDT defined MSRs. */
 #define MSR_IDT_FCR1			0x107
 #define MSR_IDT_FCR2			0x108
@@ -191,4 +280,7 @@
 #define MSR_IA32_APICBASE_ENABLE        (1<<11)
 #define MSR_IA32_APICBASE_BASE          (0xfffff<<12)
 
+#define MSR_IA32_UCODE_WRITE		0x79
+#define MSR_IA32_UCODE_REV		0x8b
+
 #endif
diff -urNp linux-204/include/asm-x86_64/pci.h linux-210/include/asm-x86_64/pci.h
--- linux-204/include/asm-x86_64/pci.h
+++ linux-210/include/asm-x86_64/pci.h
@@ -70,6 +70,18 @@ extern void *pci_alloc_consistent(struct
 extern void pci_free_consistent(struct pci_dev *hwdev, size_t size,
 				void *vaddr, dma_addr_t dma_handle);
 
+#ifdef CONFIG_SWIOTLB
+extern int swiotlb;
+extern dma_addr_t swiotlb_map_single (struct pci_dev *hwdev, void *ptr, size_t size,
+                                     int dir);
+extern void swiotlb_unmap_single (struct pci_dev *hwdev, dma_addr_t dev_addr,
+                                 size_t size, int dir);
+extern void swiotlb_sync_single (struct pci_dev *hwdev, dma_addr_t dev_addr,
+                                size_t size, int dir);
+extern void swiotlb_sync_sg (struct pci_dev *hwdev, struct scatterlist *sg, int nelems,
+                            int dir);
+#endif
+
 #ifdef CONFIG_GART_IOMMU
 
 /* Map a single buffer of the indicated size for DMA in streaming mode.
@@ -110,6 +122,10 @@ static inline void pci_dma_sync_single(s
 				       dma_addr_t dma_handle,
 				       size_t size, int direction)
 {
+#ifdef CONFIG_SWIOTLB
+	if (swiotlb)
+		return swiotlb_sync_single(hwdev,dma_handle,size,direction);
+#endif
 	BUG_ON(direction == PCI_DMA_NONE); 
 } 
 
@@ -117,6 +133,10 @@ static inline void pci_dma_sync_sg(struc
 				   struct scatterlist *sg,
 				   int nelems, int direction)
 { 
+#ifdef CONFIG_SWIOTLB
+	if (swiotlb)
+		return swiotlb_sync_sg(hwdev,sg,nelems,direction);
+#endif
 	BUG_ON(direction == PCI_DMA_NONE); 
 } 
 
diff -urNp linux-204/include/asm-x86_64/processor.h linux-210/include/asm-x86_64/processor.h
--- linux-204/include/asm-x86_64/processor.h
+++ linux-210/include/asm-x86_64/processor.h
@@ -280,6 +280,9 @@ static inline void clear_in_cr4 (unsigne
 #define TASK_UNMAPPED_BASE	\
 	((current->thread.flags & THREAD_IA32) ? TASK_UNMAPPED_32 : TASK_UNMAPPED_64)  
 
+#define __HAVE_ARCH_ALIGN_STACK
+extern unsigned long arch_align_stack(unsigned long);
+
 /*
  * Size of io_bitmap in longwords: 32 is ports 0-0x3ff.
  */
@@ -414,12 +417,67 @@ extern unsigned long get_wchan(struct ta
 #define init_task	(init_task_union.task)
 #define init_stack	(init_task_union.stack)
 
+struct microcode_header {
+	unsigned int hdrver;
+	unsigned int rev;
+	unsigned int date;
+	unsigned int sig;
+	unsigned int cksum;
+	unsigned int ldrver;
+	unsigned int pf;
+	unsigned int datasize;
+	unsigned int totalsize;
+	unsigned int reserved[3];
+};
+
+struct microcode {
+	struct microcode_header hdr;
+	unsigned int bits[0];
+};
+
+typedef struct microcode microcode_t;
+typedef struct microcode_header microcode_header_t;
+
+/* microcode format is extended from prescott processors */
+struct extended_signature {
+	unsigned int sig;
+	unsigned int pf;
+	unsigned int cksum;
+};
+
+struct extended_sigtable {
+	unsigned int count;
+	unsigned int cksum;
+	unsigned int reserved[3];
+	struct extended_signature sigs[0];
+};
+
+/* '6' because it used to be for P6 only (but now covers Pentium 4 as well) */
+#define MICROCODE_IOCFREE	_IO('6',0)
+
 /* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
 extern inline void rep_nop(void)
 {
 	__asm__ __volatile__("rep;nop");
 }
 
+static __inline__ void __monitor(const void *eax, unsigned long ecx,
+               unsigned long edx)
+{
+       /* "monitor %eax,%ecx,%edx;" */
+       asm volatile(
+               ".byte 0x0f,0x01,0xc8;"
+               : :"a" (eax), "c" (ecx), "d"(edx));
+}
+
+static __inline__ void __mwait(unsigned long eax, unsigned long ecx)
+{
+       /* "mwait %eax,%ecx;" */
+       asm volatile(
+               ".byte 0x0f,0x01,0xc9;"
+               : :"a" (eax), "c" (ecx));
+}
+
 /* Avoid speculative execution by the CPU */
 extern inline void sync_core(void)
 { 
diff -urNp linux-204/include/asm-x86_64/proto.h linux-210/include/asm-x86_64/proto.h
--- linux-204/include/asm-x86_64/proto.h
+++ linux-210/include/asm-x86_64/proto.h
@@ -23,6 +23,9 @@ extern void iommu_hole_init(void);
 
 extern void do_softirq_thunk(void);
 
+extern void swiotlb_init(void);
+extern int setup_io_tlb_npages(char *);
+
 extern int setup_early_printk(char *); 
 extern void early_printk(const char *fmt, ...) __attribute__((format(printf,1,2)));
 
@@ -39,6 +42,8 @@ extern unsigned long start_pfn, end_pfn,
 extern void show_stack(unsigned long * rsp);
 extern void show_trace(unsigned long *stack);
 
+extern int iommu_aperture;
+
 long do_arch_prctl(struct task_struct *task, int code, unsigned long addr);
 
 #define round_up(x,y) (((x) + (y) - 1) & ~((y)-1))
diff -urNp linux-204/include/asm-x86_64/scatterlist.h linux-210/include/asm-x86_64/scatterlist.h
--- linux-204/include/asm-x86_64/scatterlist.h
+++ linux-210/include/asm-x86_64/scatterlist.h
@@ -9,6 +9,9 @@ struct scatterlist {
 
     unsigned int length;
     dma_addr_t dma_address;
+#ifdef CONFIG_SWIOTLB
+    unsigned int dma_length;
+#endif
 };
 
 #define ISA_DMA_THRESHOLD (0x00ffffff)
diff -urNp linux-204/include/asm-x86_64/smp.h linux-210/include/asm-x86_64/smp.h
--- linux-204/include/asm-x86_64/smp.h
+++ linux-210/include/asm-x86_64/smp.h
@@ -36,6 +36,9 @@ extern unsigned long phys_cpu_present_ma
 extern unsigned long cpu_online_map;
 extern volatile unsigned long smp_invalidate_needed;
 extern int pic_mode;
+extern int smp_num_siblings;
+extern int cpu_sibling_map[];
+
 extern void smp_flush_tlb(void);
 extern void smp_message_irq(int cpl, void *dev_id, struct pt_regs *regs);
 extern void smp_send_reschedule(int cpu);
@@ -68,9 +71,30 @@ extern inline int cpu_number_map(int cpu
  * Some lowlevel functions might want to know about
  * the real APIC ID <-> CPU # mapping.
  */
-extern volatile int x86_apicid_to_cpu[NR_CPUS];
 extern volatile int x86_cpu_to_apicid[NR_CPUS];
 
+static inline char x86_apicid_to_cpu(char apicid)
+{
+	int i;
+
+	for (i = 0; i < NR_CPUS; ++i)
+		if (x86_cpu_to_apicid[i] == apicid)
+			return i;
+
+	return -1;
+}
+
+
+extern u8 bios_cpu_apicid[];
+
+static inline int cpu_present_to_apicid(int mps_cpu)
+{
+	if (mps_cpu < NR_CPUS)
+		return (int)bios_cpu_apicid[mps_cpu];
+	else
+		return BAD_APICID;
+}
+
 /*
  * General functions that each host system must provide.
  */
@@ -118,7 +142,7 @@ extern int slow_smp_processor_id(void);
 
 #endif
 #define INT_DELIVERY_MODE 1     /* logical delivery */
-#define TARGET_CPUS 1
+#define TARGET_CPUS cpu_online_map
 
 #ifndef CONFIG_SMP
 #define stack_smp_processor_id() 0
diff -urNp linux-204/include/linux/bootmem.h linux-210/include/linux/bootmem.h
--- linux-204/include/linux/bootmem.h
+++ linux-210/include/linux/bootmem.h
@@ -35,12 +35,20 @@ extern unsigned long __init init_bootmem
 extern void __init reserve_bootmem (unsigned long addr, unsigned long size);
 extern void __init free_bootmem (unsigned long addr, unsigned long size);
 extern void * __init __alloc_bootmem (unsigned long size, unsigned long align, unsigned long goal);
+
+#ifdef CONFIG_IA32E
+extern unsigned long max_dma_address_pa;
+#define MAX_DMA_ADDRESS_PA max_dma_address_pa
+#else
+#define MAX_DMA_ADDRESS_PA __pa(MAX_DMA_ADDRESS)
+#endif
+
 #define alloc_bootmem(x) \
-	__alloc_bootmem((x), SMP_CACHE_BYTES, __pa(MAX_DMA_ADDRESS))
+	__alloc_bootmem((x), SMP_CACHE_BYTES, MAX_DMA_ADDRESS_PA)
 #define alloc_bootmem_low(x) \
 	__alloc_bootmem((x), SMP_CACHE_BYTES, 0)
 #define alloc_bootmem_pages(x) \
-	__alloc_bootmem((x), PAGE_SIZE, __pa(MAX_DMA_ADDRESS))
+	__alloc_bootmem((x), PAGE_SIZE, MAX_DMA_ADDRESS_PA)
 #define alloc_bootmem_low_pages(x) \
 	__alloc_bootmem((x), PAGE_SIZE, 0)
 extern unsigned long __init free_all_bootmem (void);
@@ -51,9 +59,9 @@ extern void __init free_bootmem_node (pg
 extern unsigned long __init free_all_bootmem_node (pg_data_t *pgdat);
 extern void * __init __alloc_bootmem_node (pg_data_t *pgdat, unsigned long size, unsigned long align, unsigned long goal);
 #define alloc_bootmem_node(pgdat, x) \
-	__alloc_bootmem_node((pgdat), (x), SMP_CACHE_BYTES, __pa(MAX_DMA_ADDRESS))
+	__alloc_bootmem_node((pgdat), (x), SMP_CACHE_BYTES, MAX_DMA_ADDRESS_PA)
 #define alloc_bootmem_pages_node(pgdat, x) \
-	__alloc_bootmem_node((pgdat), (x), PAGE_SIZE, __pa(MAX_DMA_ADDRESS))
+	__alloc_bootmem_node((pgdat), (x), PAGE_SIZE, MAX_DMA_ADDRESS_PA)
 #define alloc_bootmem_low_pages_node(pgdat, x) \
 	__alloc_bootmem_node((pgdat), (x), PAGE_SIZE, 0)
 
diff -urNp linux-204/include/linux/reboot.h linux-210/include/linux/reboot.h
--- linux-204/include/linux/reboot.h
+++ linux-210/include/linux/reboot.h
@@ -45,6 +45,9 @@ extern int unregister_reboot_notifier(st
 extern void machine_restart(char *cmd);
 extern void machine_halt(void);
 extern void machine_power_off(void);
+#ifdef CONFIG_X86_64
+extern void (*machine_reset)(void);
+#endif
 
 #endif
 
