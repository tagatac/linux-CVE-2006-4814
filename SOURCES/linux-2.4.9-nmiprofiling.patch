diff -urNp linux-5090/arch/i386/kernel/apic.c linux-5100/arch/i386/kernel/apic.c
--- linux-5090/arch/i386/kernel/apic.c
+++ linux-5100/arch/i386/kernel/apic.c
@@ -951,6 +951,9 @@ void __init setup_APIC_clocks (void)
 
 	/* and update all other cpus */
 	smp_call_function(setup_APIC_timer, (void *)calibration_result, 1, 1);
+
+	if (nmi_watchdog == NMI_LOCAL_APIC)
+		check_nmi_watchdog();
 }
 
 void __init disable_APIC_timer(void)
@@ -990,13 +993,8 @@ int setup_profiling_timer(unsigned int m
 		return -EINVAL;
 
 	/* 
-	 * Set the new multiplier for each CPU. CPUs don't start using the
-	 * new values until the next timer interrupt in which they do process
-	 * accounting. At that time they also adjust their APIC timers
-	 * accordingly.
+	 * There is no longer support for prof_multiplier[] use.
 	 */
-	for (i = 0; i < NR_CPUS; ++i)
-		prof_multiplier[i] = multiplier;
 
 	return 0;
 }
@@ -1017,27 +1015,11 @@ inline void smp_local_timer_interrupt(st
 {
 	int cpu = smp_processor_id();
 
+#ifndef CONFIG_SMP
 	x86_do_profile(regs);
-
-	if (--prof_counter[cpu] <= 0) {
-		/*
-		 * The multiplier may have changed since the last time we got
-		 * to this point as a result of the user writing to
-		 * /proc/profile. In this case we need to adjust the APIC
-		 * timer accordingly.
-		 *
-		 * Interrupts are already masked off at this point.
-		 */
-		prof_counter[cpu] = prof_multiplier[cpu];
-		if (prof_counter[cpu] != prof_old_multiplier[cpu]) {
-			__setup_APIC_LVTT(calibration_result/prof_counter[cpu]);
-			prof_old_multiplier[cpu] = prof_counter[cpu];
-		}
-
-#ifdef CONFIG_SMP
-		update_process_times(user_mode(regs));
+#else
+	update_process_times(user_mode(regs));
 #endif
-	}
 
 	/*
 	 * We take the 'long' return path, and there every subsystem
@@ -1166,8 +1148,6 @@ int __init APIC_init_uniprocessor (void)
 
 	setup_local_APIC();
 
-	if (nmi_watchdog == NMI_LOCAL_APIC)
-		check_nmi_watchdog();
 #ifdef CONFIG_X86_IO_APIC
 	if (smp_found_config && !skip_ioapic_setup && nr_ioapics) {
 		setup_IO_APIC();
diff -urNp linux-5090/arch/i386/kernel/nmi.c linux-5100/arch/i386/kernel/nmi.c
--- linux-5090/arch/i386/kernel/nmi.c
+++ linux-5100/arch/i386/kernel/nmi.c
@@ -27,7 +27,8 @@
 
 unsigned int nmi_watchdog = NMI_NONE;
 static unsigned int nmi_hz = HZ;
-unsigned int nmi_perfctr_msr;	/* the MSR to reset in NMI handler */
+static unsigned int nmi_perfctr_msr; /* the MSR to reset in NMI handler */
+static long nmi_perfctr_val; /* the counter value to write into the MSR */
 extern void show_registers(struct pt_regs *regs);
 
 #define K7_EVNTSEL_ENABLE	(1 << 22)
@@ -62,10 +63,8 @@ extern void show_registers(struct pt_reg
 /* Set up IQ_COUNTER0 to behave like a clock, by having IQ_CCCR0 filter
    CRU_ESCR0 (with any non-null event selector) through a complemented
    max threshold. [IA32-Vol3, Section 14.9.9] */
-#define MSR_P4_IQ_COUNTER0	0x30C
-#define MSR_P4_IQ_CCCR0		0x36C
-#define MSR_P4_CRU_ESCR0	0x3B8
-#define P4_NMI_CRU_ESCR0	(P4_ESCR_EVENT_SELECT(0x3F)|P4_ESCR_OS|P4_ESCR_USR)
+#define P4_NMI_CRU_ESCR0 \
+	(P4_ESCR_EVENT_SELECT(0x3F)|P4_ESCR_OS|P4_ESCR_USR)
 #define P4_NMI_IQ_CCCR0	\
 	(P4_CCCR_OVF_PMI|P4_CCCR_THRESHOLD(15)|P4_CCCR_COMPLEMENT|	\
 	 P4_CCCR_COMPARE|P4_CCCR_REQUIRED|P4_CCCR_ESCR_SELECT(4)|P4_CCCR_ENABLE)
@@ -92,12 +91,6 @@ int __init check_nmi_watchdog (void)
 		}
 	}
 	printk("OK.\n");
-
-	/* now that we know it works we can reduce NMI frequency to
-	   something more reasonable; makes a difference in some configs */
-	if (nmi_watchdog == NMI_LOCAL_APIC)
-		nmi_hz = 1;
-
 	return 0;
 }
 
@@ -116,14 +109,16 @@ static int __init setup_nmi_watchdog(cha
 	 * please test the NMI stuff there and send me the
 	 * missing bits. Right now Intel P6/P4 and AMD K7 only.
 	 */
-	if ((nmi == NMI_LOCAL_APIC) &&
-			(boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &&
-			(boot_cpu_data.x86 == 6 || boot_cpu_data.x86 == 15))
-		nmi_watchdog = nmi;
-	if ((nmi == NMI_LOCAL_APIC) &&
-			(boot_cpu_data.x86_vendor == X86_VENDOR_AMD) &&
-	       	    ((boot_cpu_data.x86 == 6) || (boot_cpu_data.x86 == 15)))
-		nmi_watchdog = nmi;
+	if (nmi == NMI_LOCAL_APIC) {
+		if (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &&
+		     (boot_cpu_data.x86 == 6 || boot_cpu_data.x86 == 15)) ||
+		    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) &&
+	       	     (boot_cpu_data.x86 == 6 || boot_cpu_data.x86 == 15))) {
+			nmi_watchdog = nmi;
+			nmi_hz = 10000;
+		}
+	}
+
 	/*
 	 * We can enable the IO-APIC watchdog
 	 * unconditionally.
@@ -236,8 +231,8 @@ static void __pminit setup_k7_watchdog(v
 		| K7_NMI_EVENT;
 
 	wrmsr(MSR_K7_EVNTSEL0, evntsel, 0);
-	Dprintk("setting K7_PERFCTR0 to %08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_K7_PERFCTR0, -(cpu_khz/nmi_hz*1000), -1);
+	Dprintk("setting K7_PERFCTR0 to %08lx\n", nmi_perfctr_val);
+	wrmsr(MSR_K7_PERFCTR0, nmi_perfctr_val, -1);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	evntsel |= K7_EVNTSEL_ENABLE;
 	wrmsr(MSR_K7_EVNTSEL0, evntsel, 0);
@@ -258,8 +253,8 @@ static void __pminit setup_p6_watchdog(v
 		| P6_NMI_EVENT;
 
 	wrmsr(MSR_P6_EVNTSEL0, evntsel, 0);
-	Dprintk("setting P6_PERFCTR0 to %08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_P6_PERFCTR0, -(cpu_khz/nmi_hz*1000), 0);
+	Dprintk("setting P6_PERFCTR0 to %08lx\n", nmi_perfctr_val);
+	wrmsr(MSR_P6_PERFCTR0, nmi_perfctr_val, 0);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	evntsel |= P6_EVNTSEL0_ENABLE;
 	wrmsr(MSR_P6_EVNTSEL0, evntsel, 0);
@@ -273,7 +268,7 @@ static int __pminit setup_p4_watchdog(vo
 	if (!(misc_enable & MSR_P4_MISC_ENABLE_PERF_AVAIL))
 		return 0;
 
-	nmi_perfctr_msr = MSR_P4_IQ_COUNTER0;
+	nmi_perfctr_msr = MSR_P4_IQ_PERFCTR0;
 
 	if (!(misc_enable & MSR_P4_MISC_ENABLE_PEBS_UNAVAIL))
 		clear_msr_range(0x3F1, 2);
@@ -288,15 +283,22 @@ static int __pminit setup_p4_watchdog(vo
 
 	wrmsr(MSR_P4_CRU_ESCR0, P4_NMI_CRU_ESCR0, 0);
 	wrmsr(MSR_P4_IQ_CCCR0, P4_NMI_IQ_CCCR0 & ~P4_CCCR_ENABLE, 0);
-	Dprintk("setting P4_IQ_COUNTER0 to 0x%08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_P4_IQ_COUNTER0, -(cpu_khz/nmi_hz*1000), -1);
+	Dprintk("setting P4_IQ_COUNTER0 to 0x%08lx\n", nmi_perfctr_val);
+	wrmsr(MSR_P4_IQ_PERFCTR0, nmi_perfctr_val, -1);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	wrmsr(MSR_P4_IQ_CCCR0, P4_NMI_IQ_CCCR0, 0);
 	return 1;
 }
 
+void set_nmi_perfctr_val(void)
+{
+	/* must be recalculated after any modification to cpu_khz */
+	nmi_perfctr_val = -(long)(cpu_khz * 1000 / nmi_hz);
+}
+
 void __pminit setup_apic_nmi_watchdog (void)
 {
+	set_nmi_perfctr_val();
 	switch (boot_cpu_data.x86_vendor) {
 	case X86_VENDOR_AMD:
 		if (boot_cpu_data.x86 != 6 && boot_cpu_data.x86 != 15)
@@ -377,10 +379,10 @@ void nmi_watchdog_tick (struct pt_regs *
 	if (last_irq_sums[cpu] == sum) {
 		/*
 		 * Ayiee, looks like this CPU is stuck ...
-		 * wait a few IRQs (5 seconds) before doing the oops ...
+		 * wait a few IRQs (30 seconds) before doing the oops ...
 		 */
 		alert_counter[cpu]++;
-		if (alert_counter[cpu] == 5*nmi_hz) {
+		if (alert_counter[cpu] == 30*nmi_hz) {
 			spin_lock(&nmi_print_lock);
 			/*
 			 * We are in trouble anyway, lets at least try
@@ -400,7 +402,7 @@ void nmi_watchdog_tick (struct pt_regs *
 		alert_counter[cpu] = 0;
 	}
 	if (nmi_perfctr_msr) {
-		if (nmi_perfctr_msr == MSR_P4_IQ_COUNTER0) {
+		if (nmi_perfctr_msr == MSR_P4_IQ_PERFCTR0) {
 			/*
 			 * P4 quirks:
 			 * - An overflown perfctr will assert its interrupt
@@ -411,6 +413,6 @@ void nmi_watchdog_tick (struct pt_regs *
 			wrmsr(MSR_P4_IQ_CCCR0, P4_NMI_IQ_CCCR0, 0);
 			apic_write(APIC_LVTPC, APIC_DM_NMI);
 		}
-		wrmsr(nmi_perfctr_msr, -(cpu_khz/nmi_hz*1000), -1);
+		wrmsr(nmi_perfctr_msr, nmi_perfctr_val, -1);
 	}
 }
diff -urNp linux-5090/arch/i386/kernel/time.c linux-5100/arch/i386/kernel/time.c
--- linux-5090/arch/i386/kernel/time.c
+++ linux-5100/arch/i386/kernel/time.c
@@ -592,7 +592,9 @@ static inline void do_timer_interrupt(in
  * system, in that case we have to call the local interrupt handler.
  */
 #ifndef CONFIG_X86_LOCAL_APIC
+#ifndef CONFIG_SMP
 	x86_do_profile(regs);
+#endif
 #else
 	if (!using_apic_timer)
 		smp_local_timer_interrupt(regs);
@@ -838,6 +840,10 @@ static unsigned long fast_gettimeoffset_
 static unsigned long cpu_khz_ref = 0;
 #endif
 
+#ifdef CONFIG_X86_LOCAL_APIC
+void set_nmi_perfctr_val(void);
+#endif
+
 static int
 time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
 		       void *data)
@@ -860,6 +866,9 @@ time_cpufreq_notifier(struct notifier_bl
 		if (use_tsc) {
 			fast_gettimeoffset_quotient = cpufreq_scale(fast_gettimeoffset_ref, freq->new, ref_freq);
 			cpu_khz = cpufreq_scale(cpu_khz_ref, ref_freq, freq->new);
+#ifdef CONFIG_X86_LOCAL_APIC
+			set_nmi_perfctr_val();
+#endif
 		}
 #endif
 	}
diff -urNp linux-5090/arch/x86_64/kernel/nmi.c linux-5100/arch/x86_64/kernel/nmi.c
--- linux-5090/arch/x86_64/kernel/nmi.c
+++ linux-5100/arch/x86_64/kernel/nmi.c
@@ -66,8 +66,6 @@ int nmi_watchdog_disabled; 
    CRU_ESCR0 (with any non-null event selector) through a complemented
    max threshold. [IA32-Vol3, Section 14.9.9] */
 #define MSR_P4_IQ_COUNTER0	0x30C
-#define MSR_P4_IQ_CCCR0		0x36C
-#define MSR_P4_CRU_ESCR0	0x3B8
 #define P4_NMI_CRU_ESCR0	(P4_ESCR_EVENT_SELECT(0x3F)|P4_ESCR_OS|P4_ESCR_USR)
 #define P4_NMI_IQ_CCCR0	\
 	(P4_CCCR_OVF_PMI|P4_CCCR_THRESHOLD(15)|P4_CCCR_COMPLEMENT|	\
@@ -123,10 +121,9 @@ static int __init setup_nmi_watchdog(cha
 
 	get_option(&str, &nmi);
 
-	if (nmi >= NMI_INVALID)
+	if (nmi < NMI_NONE || nmi >= NMI_INVALID)
 		return 0;
-	if (nmi == NMI_NONE)
-		nmi_watchdog = nmi;
+
 	nmi_watchdog = nmi;
 	return 1;
 }
@@ -384,10 +381,10 @@ void nmi_watchdog_tick (struct pt_regs *
 	if (last_irq_sums[cpu] == sum) {
 		/*
 		 * Ayiee, looks like this CPU is stuck ...
-		 * wait a few IRQs (5 seconds) before doing the oops ...
+		 * wait a while (30 seconds) before doing the oops ...
 		 */
 		alert_counter[cpu]++;
-		if (alert_counter[cpu] == 5*nmi_hz) {
+		if (alert_counter[cpu] == 30*nmi_hz) {
 
 
 			if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_BAD) { 
diff -urNp linux-5090/fs/proc/proc_misc.c linux-5100/fs/proc/proc_misc.c
--- linux-5090/fs/proc/proc_misc.c
+++ linux-5100/fs/proc/proc_misc.c
@@ -654,7 +654,7 @@ void __init proc_misc_init(void)
 		proc_root_kcore->size =
 				(size_t)high_memory - PAGE_OFFSET + PAGE_SIZE;
 	}
-	if (prof_shift) {
+	if (prof_on) {
 		entry = create_proc_entry("profile", S_IWUSR | S_IRUGO, NULL);
 		if (entry) {
 			entry->proc_fops = &proc_profile_operations;
diff -urNp linux-5090/kernel/profile.c linux-5100/kernel/profile.c
--- linux-5090/kernel/profile.c
+++ linux-5100/kernel/profile.c
@@ -8,6 +8,12 @@
 #include <linux/profile.h>
 #include <linux/bootmem.h>
 #include <linux/notifier.h>
+#ifdef CONFIG_X86_LOCAL_APIC
+#include <asm/apic.h>
+#endif
+#ifdef CONFIG_APM
+#include <linux/apm_bios.h>
+#endif
 
 extern char _stext, _etext;
 
@@ -23,6 +29,13 @@ int __init profile_setup(char * str)
 		prof_shift = par;
 		prof_on = 1;
 		printk(KERN_INFO "kernel profiling enabled\n");
+#ifdef CONFIG_X86_LOCAL_APIC
+		if (nmi_watchdog == NMI_NONE)
+			nmi_watchdog = NMI_IO_APIC;
+#endif
+#ifdef CONFIG_APM
+		apm_info.forbid_idle = 1;
+#endif
 	}
 	return 1;
 }
