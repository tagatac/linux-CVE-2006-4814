diff -urNp linux-266/arch/ia64/ia32/sys_ia32.c linux-268/arch/ia64/ia32/sys_ia32.c
--- linux-266/arch/ia64/ia32/sys_ia32.c
+++ linux-268/arch/ia64/ia32/sys_ia32.c
@@ -48,6 +48,7 @@
 #include <linux/personality.h>
 #include <linux/stat.h>
 #include <linux/ipc.h>
+#include <linux/ptrace.h>
 
 #include <asm/types.h>
 #include <asm/uaccess.h>
@@ -165,9 +166,9 @@ sys32_execve (char *filename, unsigned i
 		current->thread.map_base  = old_map_base;
 		current->thread.task_size = old_task_size;
 		set_fs(USER_DS);	/* establish new task-size as the address-limit */
-	  out:
-		kfree(av);
 	}
+  out:
+	kfree(av);
 	return r;
 }
 
@@ -250,9 +251,8 @@ sys32_newfstat (unsigned int fd, struct 
 
 
 static int
-get_page_prot (unsigned long addr)
+get_page_prot (struct vm_area_struct *vma, unsigned long addr)
 {
-	struct vm_area_struct *vma = find_vma(current->mm, addr);
 	int prot = 0;
 
 	if (!vma || vma->vm_start > addr)
@@ -275,14 +275,28 @@ static unsigned long
 mmap_subpage (struct file *file, unsigned long start, unsigned long end, int prot, int flags,
 	      loff_t off)
 {
-	void *page = (void *) get_zeroed_page(GFP_KERNEL);
+	void *page = NULL;
 	struct inode *inode;
-	unsigned long ret;
-	int old_prot = get_page_prot(start);
+	unsigned long ret = 0;
+	struct vm_area_struct *vma = find_vma(current->mm, start);
+	int old_prot = get_page_prot(vma, start);
 
 	DBG("mmap_subpage(file=%p,start=0x%lx,end=0x%lx,prot=%x,flags=%x,off=0x%llx)\n",
 	    file, start, end, prot, flags, off);
 
+
+	/* Optimize the case where the old mmap and the new mmap are both
+	   anonymous */
+	if ((old_prot & PROT_WRITE) && (flags & MAP_ANONYMOUS)
+	    && (!vma->vm_file)) {
+		if (clear_user((void *) start, end - start)) {
+			ret = -EFAULT;
+			goto out;
+		}
+		goto skip_mmap;
+	}
+	
+	page = (void *) get_zeroed_page(GFP_KERNEL);
 	if (!page)
 		return -ENOMEM;
 
@@ -307,6 +321,7 @@ mmap_subpage (struct file *file, unsigne
 			copy_to_user((void *) end, page + PAGE_OFF(end),
 				     PAGE_SIZE - PAGE_OFF(end));
 	}
+
 	if (!(flags & MAP_ANONYMOUS)) {
 		/* read the file contents */
 		inode = file->f_dentry->d_inode;
@@ -317,10 +332,13 @@ mmap_subpage (struct file *file, unsigne
 			goto out;
 		}
 	}
+
+ skip_mmap:	
 	if (!(prot & PROT_WRITE))
 		ret = sys_mprotect(PAGE_START(start), PAGE_SIZE, prot | old_prot);
   out:
-	free_page((unsigned long) page);
+	if (page)
+		free_page((unsigned long) page);
 	return ret;
 }
 
@@ -576,11 +594,12 @@ static long
 mprotect_subpage (unsigned long address, int new_prot)
 {
 	int old_prot;
+	struct vm_area_struct *vma;
 
 	if (new_prot == PROT_NONE)
 		return 0;		/* optimize case where nothing changes... */
-
-	old_prot = get_page_prot(address);
+	vma = find_vma(current->mm, address);
+	old_prot = get_page_prot(vma, address);
 	return sys_mprotect(address, PAGE_SIZE, new_prot | old_prot);
 }
 
@@ -3110,7 +3129,7 @@ sys32_ptrace (int request, pid_t pid, un
 		if (request != PTRACE_KILL)
 			goto out;
 	}
-	if (child->p_pptr != current)
+	if (child->parent != current)
 		goto out;
 
 	switch (request) {
diff -urNp linux-266/arch/ia64/kernel/entry.S linux-268/arch/ia64/kernel/entry.S
--- linux-266/arch/ia64/kernel/entry.S
+++ linux-268/arch/ia64/kernel/entry.S
@@ -91,20 +91,30 @@ ENTRY(ia64_execve)
 	br.ret.sptk.many rp
 END(ia64_execve)
 
+/*
+ * sys_clone2(u64 flags, u64 ustack_base, u64 ustack_size, u64 child_tidptr, u64 parent_tidptr,
+ *	      u64 tls)
+ */
 GLOBAL_ENTRY(sys_clone2)
 	/*
 	 * Allocate 8 input registers since ptrace() may clobber them
 	 */
 	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
-	alloc r16=ar.pfs,8,2,4,0
+	alloc r16=ar.pfs,8,2,6,0
 	DO_SAVE_SWITCH_STACK
+	adds r2=PT(R16)+IA64_SWITCH_STACK_SIZE+16,sp
 	mov loc0=rp
 	mov loc1=r16				// save ar.pfs across do_fork
 	.body
 	mov out1=in1
 	mov out3=in2
+	tbit.nz p6,p0=in0,CLONE_SETTLS_BIT
+	mov out4=in3	// child_tidptr:  valid only w/CLONE_CHILD_SETTID or CLONE_CHILD_CLEARTID
+	;;
+(p6)	st8 [r2]=in5				// store TLS in r16 for copy_thread()
+	mov out5=in4	// parent_tidptr: valid only w/CLONE_PARENT_SETTID
 	adds out2=IA64_SWITCH_STACK_SIZE+16,sp	// out2 = &regs
-	mov out0=in0				// out0 = clone_flags
+	dep out0=0,in0,CLONE_IDLETASK_BIT,1	// out0 = clone_flags & ~CLONE_IDLETASK
 	br.call.sptk.many rp=do_fork
 .ret1:	.restore sp
 	adds sp=IA64_SWITCH_STACK_SIZE,sp	// pop the switch stack
@@ -113,20 +123,29 @@ GLOBAL_ENTRY(sys_clone2)
 	br.ret.sptk.many rp
 END(sys_clone2)
 
+/*
+ * sys_clone(u64 flags, u64 ustack_base, u64 user_tid, u64 tls)
+ *	Deprecated.  Use sys_clone2() instead.
+ */
 GLOBAL_ENTRY(sys_clone)
 	/*
 	 * Allocate 8 input registers since ptrace() may clobber them
 	 */
 	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
-	alloc r16=ar.pfs,8,2,4,0
+	alloc r16=ar.pfs,8,2,5,0
 	DO_SAVE_SWITCH_STACK
+	adds r2=PT(R16)+IA64_SWITCH_STACK_SIZE+16,sp
 	mov loc0=rp
 	mov loc1=r16				// save ar.pfs across do_fork
 	.body
 	mov out1=in1
 	mov out3=16				// stacksize (compensates for 16-byte scratch area)
+	tbit.nz p6,p0=in0,CLONE_SETTLS_BIT
+	mov out4=in2				// out4 = user_tid (optional)
+	;;
+(p6)	st8 [r2]=in3				// store TLS in r13 (tp)
 	adds out2=IA64_SWITCH_STACK_SIZE+16,sp	// out2 = &regs
-	mov out0=in0				// out0 = clone_flags
+	dep out0=0,in0,CLONE_IDLETASK_BIT,1	// out0 = clone_flags & ~CLONE_IDLETASK
 	br.call.sptk.many rp=do_fork
 .ret2:	.restore sp
 	adds sp=IA64_SWITCH_STACK_SIZE,sp	// pop the switch stack
@@ -170,7 +189,6 @@ GLOBAL_ENTRY(ia64_switch_to)
 	mov r8=r13			// return pointer to previously running task
 	mov r13=in0			// set "current" pointer
 	;;
-(p6)	ssm psr.i			// renable psr.i AFTER the ic bit is serialized
 	DO_LOAD_SWITCH_STACK
 
 #ifdef CONFIG_SMP
diff -urNp linux-266/arch/ia64/kernel/perfmon.c linux-268/arch/ia64/kernel/perfmon.c
--- linux-266/arch/ia64/kernel/perfmon.c
+++ linux-268/arch/ia64/kernel/perfmon.c
@@ -672,7 +672,7 @@ pfm_remove_smpl_mapping(struct task_stru
 
 	down_write(&task->mm->mmap_sem);
 
-	r = do_munmap(task->mm, ctx->ctx_smpl_vaddr, psb->psb_size);
+	r = do_munmap(task->mm, ctx->ctx_smpl_vaddr, psb->psb_size, 0);
 
 	up_write(&task->mm->mmap_sem);
 	if (r !=0) {
@@ -4203,14 +4203,14 @@ pfm_cleanup_smpl_buf(struct task_struct 
 void
 pfm_cleanup_owners(struct task_struct *task)
 {
-	struct task_struct *p;
+	struct task_struct *p, *q;
 	pfm_context_t *ctx;
 
 	DBprintk(("called by [%d] for [%d]\n", current->pid, task->pid));
 
 	read_lock(&tasklist_lock);
 
-	for_each_task(p) {
+	do_each_thread(q, p) {
 		/*
 		 * It is safe to do the 2-step test here, because thread.ctx
 		 * is cleaned up only in release_thread() and at that point
@@ -4248,7 +4248,7 @@ pfm_cleanup_owners(struct task_struct *t
 
 			DBprintk(("done for notifier [%d] in [%d]\n", task->pid, p->pid));
 		}
-	}
+	} while_each_thread(q, p);
 	read_unlock(&tasklist_lock);
 
 	atomic_set(&task->thread.pfm_owners_check, 0);
@@ -4262,14 +4262,14 @@ pfm_cleanup_owners(struct task_struct *t
 void
 pfm_cleanup_notifiers(struct task_struct *task)
 {
-	struct task_struct *p;
+	struct task_struct *p, *q;
 	pfm_context_t *ctx;
 
 	DBprintk(("called by [%d] for [%d]\n", current->pid, task->pid));
 
 	read_lock(&tasklist_lock);
 
-	for_each_task(p) {
+	do_each_thread(q, p) {
 		/*
 		 * It is safe to do the 2-step test here, because thread.ctx
 		 * is cleaned up only in release_thread() and at that point
@@ -4307,7 +4307,8 @@ pfm_cleanup_notifiers(struct task_struct
 
 			DBprintk(("done for notifier [%d] in [%d]\n", task->pid, p->pid));
 		}
-	}
+	} while_each_thread(q, p);
+
 	read_unlock(&tasklist_lock);
 
 	atomic_set(&task->thread.pfm_notifiers_check, 0);
diff -urNp linux-266/arch/ia64/kernel/process.c linux-268/arch/ia64/kernel/process.c
--- linux-266/arch/ia64/kernel/process.c
+++ linux-268/arch/ia64/kernel/process.c
@@ -292,6 +292,8 @@ copy_thread (int nr, unsigned long clone
 	memcpy((void *) child_rbs, (void *) rbs, rbs_size);
 
 	if (user_mode(child_ptregs)) {
+		if (clone_flags & CLONE_SETTLS)
+			child_ptregs->r13 = regs->r16;  /* see sys_clone2() in entry.S */
 		if (user_stack_base) {
 			child_ptregs->r12 = user_stack_base + user_stack_size - 16;
 			child_ptregs->ar_bspstore = user_stack_base;
@@ -367,7 +369,13 @@ copy_thread (int nr, unsigned long clone
 }
 
 void
-do_copy_regs (struct unw_frame_info *info, void *arg)
+ia64_do_copy_regs (struct unw_frame_info *info, void *arg)
+{
+	do_copy_task_regs(current, info, arg);
+}
+
+static void
+do_copy_task_regs (struct task_struct *task, struct unw_frame_info *info, void *arg)
 {
 	unsigned long mask, sp, nat_bits = 0, ip, ar_rnat, urbs_end, cfm;
 	elf_greg_t *dst = arg;
@@ -383,12 +391,12 @@ do_copy_regs (struct unw_frame_info *inf
 	unw_get_sp(info, &sp);
 	pt = (struct pt_regs *) (sp + 16);
 
-	urbs_end = ia64_get_user_rbs_end(current, pt, &cfm);
+	urbs_end = ia64_get_user_rbs_end(task, pt, &cfm);
 
-	if (ia64_sync_user_rbs(current, info->sw, pt->ar_bspstore, urbs_end) < 0)
+	if (ia64_sync_user_rbs(task, info->sw, pt->ar_bspstore, urbs_end) < 0)
 		return;
 
-	ia64_peek(current, info->sw, urbs_end, (long) ia64_rse_rnat_addr((long *) urbs_end),
+	ia64_peek(task, info->sw, urbs_end, (long) ia64_rse_rnat_addr((long *) urbs_end),
 		  &ar_rnat);
 
 	/*
@@ -434,11 +442,19 @@ do_copy_regs (struct unw_frame_info *inf
 	dst[52] = pt->ar_pfs;	/* UNW_AR_PFS is == to pt->cr_ifs for interrupt frames */
 	unw_get_ar(info, UNW_AR_LC, &dst[53]);
 	unw_get_ar(info, UNW_AR_EC, &dst[54]);
+	unw_get_ar(info, UNW_AR_CSD, &dst[55]);
+	unw_get_ar(info, UNW_AR_SSD, &dst[56]);
 }
 
 void
 do_dump_fpu (struct unw_frame_info *info, void *arg)
 {
+	do_dump_task_fpu(current, info, arg);
+}
+
+void
+do_dump_task_fpu (struct task_struct *task, struct unw_frame_info *info, void *arg)
+{
 	elf_fpreg_t *dst = arg;
 	int i;
 
@@ -452,15 +468,45 @@ do_dump_fpu (struct unw_frame_info *info
 	for (i = 2; i < 32; ++i)
 		unw_get_fr(info, i, dst + i);
 
-	ia64_flush_fph(current);
-	if ((current->thread.flags & IA64_THREAD_FPH_VALID) != 0)
-		memcpy(dst + 32, current->thread.fph, 96*16);
+	ia64_flush_fph(task);
+	if ((task->thread.flags & IA64_THREAD_FPH_VALID) != 0)
+		memcpy(dst + 32, task->thread.fph, 96*16);
+}
+
+int
+dump_task_regs(struct task_struct *task, elf_gregset_t *regs)
+{
+	struct unw_frame_info tcore_info;
+
+	if (current == task) {
+		unw_init_running(ia64_do_copy_regs, regs);
+	} else {
+		memset(&tcore_info, 0, sizeof(tcore_info));
+		unw_init_from_blocked_task(&tcore_info, task);
+		do_copy_task_regs(task, &tcore_info, regs);
+	}
+	return 1;
 }
 
 void
 ia64_elf_core_copy_regs (struct pt_regs *pt, elf_gregset_t dst)
 {
-	unw_init_running(do_copy_regs, dst);
+	unw_init_running(ia64_do_copy_regs, dst);
+}
+
+int
+dump_task_fpu (struct task_struct *task, elf_fpregset_t *dst)
+{
+	struct unw_frame_info tcore_info;
+
+	if (current == task) {
+		unw_init_running(do_dump_fpu, dst);
+	} else {
+		memset(&tcore_info, 0, sizeof(tcore_info));
+		unw_init_from_blocked_task(&tcore_info, task);
+		do_dump_task_fpu(task, &tcore_info, dst);
+	}
+	return 1;
 }
 
 int
diff -urNp linux-266/arch/ia64/kernel/ptrace.c linux-268/arch/ia64/kernel/ptrace.c
--- linux-266/arch/ia64/kernel/ptrace.c
+++ linux-268/arch/ia64/kernel/ptrace.c
@@ -208,20 +208,27 @@ ia64_decrement_ip (struct pt_regs *regs)
  *   rnat0/rnat1 gets its value from sw->ar_rnat.
  */
 static unsigned long
-get_rnat (struct pt_regs *pt, struct switch_stack *sw,
-	  unsigned long *krbs, unsigned long *urnat_addr)
+get_rnat (struct task_struct *task, struct switch_stack *sw,
+	  unsigned long *krbs, unsigned long *urnat_addr, unsigned long *urbs_end)
 {
-	unsigned long rnat0 = 0, rnat1 = 0, urnat = 0, *slot0_kaddr, kmask = ~0UL;
+	unsigned long rnat0 = 0, rnat1 = 0, urnat = 0, *slot0_kaddr, umask = 0, mask, m;
 	unsigned long *kbsp, *ubspstore, *rnat0_kaddr, *rnat1_kaddr, shift;
-	long num_regs;
+	long num_regs, nbits;
+	struct pt_regs *pt;
 
+	pt = ia64_task_regs(task);
 	kbsp = (unsigned long *) sw->ar_bspstore;
 	ubspstore = (unsigned long *) pt->ar_bspstore;
+
+	if (urbs_end < urnat_addr)
+		nbits = ia64_rse_num_regs(urnat_addr - 63, urbs_end);
+	else
+		nbits = 63;
+	mask = (1UL << nbits) - 1;
 	/*
-	 * First, figure out which bit number slot 0 in user-land maps
-	 * to in the kernel rnat.  Do this by figuring out how many
-	 * register slots we're beyond the user's backingstore and
-	 * then computing the equivalent address in kernel space.
+	 * First, figure out which bit number slot 0 in user-land maps to in the kernel
+	 * rnat.  Do this by figuring out how many register slots we're beyond the user's
+	 * backingstore and then computing the equivalent address in kernel space.
 	 */
 	num_regs = ia64_rse_num_regs(ubspstore, urnat_addr + 1);
 	slot0_kaddr = ia64_rse_skip_regs(krbs, num_regs);
@@ -231,20 +238,26 @@ get_rnat (struct pt_regs *pt, struct swi
 
 	if (ubspstore + 63 > urnat_addr) {
 		/* some bits need to be merged in from pt->ar_rnat */
-		kmask = ~((1UL << ia64_rse_slot_num(ubspstore)) - 1);
-		urnat = (pt->ar_rnat & ~kmask);
+		umask = ((1UL << ia64_rse_slot_num(ubspstore)) - 1) & mask;
+		urnat = (pt->ar_rnat & umask);
+		mask &= ~umask;
+		if (!mask)
+			return urnat;
 	}
-	if (rnat0_kaddr >= kbsp) {
+
+	m = mask << shift;
+	if (rnat0_kaddr >= kbsp)
 		rnat0 = sw->ar_rnat;
-	} else if (rnat0_kaddr > krbs) {
+	else if (rnat0_kaddr > krbs)
 		rnat0 = *rnat0_kaddr;
-	}
-	if (rnat1_kaddr >= kbsp) {
+	urnat |= (rnat0 & m) >> shift;
+
+	m = mask >> (63 - shift);
+	if (rnat1_kaddr >= kbsp)
 		rnat1 = sw->ar_rnat;
-	} else if (rnat1_kaddr > krbs) {
+	else if (rnat1_kaddr > krbs)
 		rnat1 = *rnat1_kaddr;
-	}
-	urnat |= ((rnat1 << (63 - shift)) | (rnat0 >> shift)) & kmask;
+	urnat |= (rnat1 & m) << (63 - shift);
 	return urnat;
 }
 
@@ -252,22 +265,49 @@ get_rnat (struct pt_regs *pt, struct swi
  * The reverse of get_rnat.
  */
 static void
-put_rnat (struct pt_regs *pt, struct switch_stack *sw,
-	  unsigned long *krbs, unsigned long *urnat_addr, unsigned long urnat)
+put_rnat (struct task_struct *task, struct switch_stack *sw,
+	  unsigned long *krbs, unsigned long *urnat_addr, unsigned long urnat,
+	  unsigned long *urbs_end)
 {
-	unsigned long rnat0 = 0, rnat1 = 0, rnat = 0, *slot0_kaddr, kmask = ~0UL, mask;
+	unsigned long rnat0 = 0, rnat1 = 0, *slot0_kaddr, umask = 0, mask, m;
 	unsigned long *kbsp, *ubspstore, *rnat0_kaddr, *rnat1_kaddr, shift;
-	long num_regs;
+	long num_regs, nbits;
+	struct pt_regs *pt;
+	unsigned long cfm, *urbs_kargs;
+	struct unw_frame_info info;
 
+	pt = ia64_task_regs(task);
 	kbsp = (unsigned long *) sw->ar_bspstore;
 	ubspstore = (unsigned long *) pt->ar_bspstore;
+
+	urbs_kargs = urbs_end;
+	if ((long)pt->cr_ifs >= 0) {
+		/*
+		 * If entered via syscall, don't allow user to set rnat bits
+		 * for syscall args.
+		 */
+		unw_init_from_blocked_task(&info,task);
+		if (unw_unwind_to_user(&info) == 0) {
+			unw_get_cfm(&info,&cfm);
+			urbs_kargs = ia64_rse_skip_regs(urbs_end,-(cfm & 0x7f));
+		}
+	}
+
+	if (urbs_kargs >= urnat_addr)
+		nbits = 63;
+	else {
+		if ((urnat_addr - 63) >= urbs_kargs)
+			return;
+		nbits = ia64_rse_num_regs(urnat_addr - 63, urbs_kargs);
+	}
+	mask = (1UL << nbits) - 1;
+
 	/*
-	 * First, figure out which bit number slot 0 in user-land maps
-	 * to in the kernel rnat.  Do this by figuring out how many
-	 * register slots we're beyond the user's backingstore and
-	 * then computing the equivalent address in kernel space.
+	 * First, figure out which bit number slot 0 in user-land maps to in the kernel
+	 * rnat.  Do this by figuring out how many register slots we're beyond the user's
+	 * backingstore and then computing the equivalent address in kernel space.
 	 */
-	num_regs = (long) ia64_rse_num_regs(ubspstore, urnat_addr + 1);
+	num_regs = ia64_rse_num_regs(ubspstore, urnat_addr + 1);
 	slot0_kaddr = ia64_rse_skip_regs(krbs, num_regs);
 	shift = ia64_rse_slot_num(slot0_kaddr);
 	rnat1_kaddr = ia64_rse_rnat_addr(slot0_kaddr);
@@ -275,28 +315,29 @@ put_rnat (struct pt_regs *pt, struct swi
 
 	if (ubspstore + 63 > urnat_addr) {
 		/* some bits need to be place in pt->ar_rnat: */
-		kmask = ~((1UL << ia64_rse_slot_num(ubspstore)) - 1);
-		pt->ar_rnat = (pt->ar_rnat & kmask) | (rnat & ~kmask);
+		umask = ((1UL << ia64_rse_slot_num(ubspstore)) - 1) & mask;
+		pt->ar_rnat = (pt->ar_rnat & ~umask) | (urnat & umask);
+		mask &= ~umask;
+		if (!mask)
+			return;
 	}
 	/*
 	 * Note: Section 11.1 of the EAS guarantees that bit 63 of an
 	 * rnat slot is ignored. so we don't have to clear it here.
 	 */
 	rnat0 = (urnat << shift);
-	mask = ~0UL << shift;
-	if (rnat0_kaddr >= kbsp) {
-		sw->ar_rnat = (sw->ar_rnat & ~mask) | (rnat0 & mask);
-	} else if (rnat0_kaddr > krbs) {
-		*rnat0_kaddr = ((*rnat0_kaddr & ~mask) | (rnat0 & mask));
-	}
+	m = mask << shift;
+	if (rnat0_kaddr >= kbsp)
+		sw->ar_rnat = (sw->ar_rnat & ~m) | (rnat0 & m);
+	else if (rnat0_kaddr > krbs)
+		*rnat0_kaddr = ((*rnat0_kaddr & ~m) | (rnat0 & m));
 
 	rnat1 = (urnat >> (63 - shift));
-	mask = ~0UL >> (63 - shift);
-	if (rnat1_kaddr >= kbsp) {
-		sw->ar_rnat = (sw->ar_rnat & ~mask) | (rnat1 & mask);
-	} else if (rnat1_kaddr > krbs) {
-		*rnat1_kaddr = ((*rnat1_kaddr & ~mask) | (rnat1 & mask));
-	}
+	m = mask >> (63 - shift);
+	if (rnat1_kaddr >= kbsp)
+		sw->ar_rnat = (sw->ar_rnat & ~m) | (rnat1 & m);
+	else if (rnat1_kaddr > krbs)
+		*rnat1_kaddr = ((*rnat1_kaddr & ~m) | (rnat1 & m));
 }
 
 /*
@@ -329,7 +370,7 @@ ia64_peek (struct task_struct *child, st
 		 * read the corresponding bits in the kernel RBS.
 		 */
 		rnat_addr = ia64_rse_rnat_addr(laddr);
-		ret = get_rnat(child_regs, child_stack, krbs, rnat_addr);
+		ret = get_rnat(child, child_stack, krbs, rnat_addr, urbs_end);
 
 		if (laddr == rnat_addr) {
 			/* return NaT collection word itself */
@@ -380,7 +421,7 @@ ia64_poke (struct task_struct *child, st
 		 * => write the corresponding bits in the kernel RBS.
 		 */
 		if (ia64_rse_is_rnat_slot(laddr))
-			put_rnat(child_regs, child_stack, krbs, laddr, val);
+			put_rnat(child, child_stack, krbs, laddr, val, urbs_end);
 		else {
 			if (laddr < urbs_end) {
 				regnum = ia64_rse_num_regs(bspstore, laddr);
@@ -514,7 +555,7 @@ finish_task (struct task_list *list, int
 	struct task_list *next = list->next;
 
 	sync_user_rbs_one_thread(list->task, make_writable);
-	free_task_struct(list->task);
+	put_task_struct(list->task);
 	kfree(list);
 	return next;
 }
@@ -537,7 +578,7 @@ static void
 threads_sync_user_rbs (struct task_struct *child, unsigned long child_urbs_end, int make_writable)
 {
 	struct switch_stack *sw;
-	struct task_struct *p;
+	struct task_struct *p, *q;
 	struct mm_struct *mm;
 	struct pt_regs *pt;
 	long multi_threaded;
@@ -568,10 +609,10 @@ threads_sync_user_rbs (struct task_struc
 
 		read_lock(&tasklist_lock);
 		{
-			for_each_task(p) {
+			do_each_thread(q, p) {
 				if (p->mm == mm && p->state != TASK_RUNNING)
 					collect_task(&list, p, make_writable);
-			}
+			} while_each_thread(q, p);
 		}
 		read_unlock(&tasklist_lock);
 
@@ -1316,7 +1357,7 @@ sys_ptrace (long request, pid_t pid, uns
 		goto out_tsk;
 	}
   out_tsk:
-	free_task_struct(child);
+	put_task_struct(child);
   out:
 	unlock_kernel();
 	return ret;
diff -urNp linux-266/arch/ia64/kernel/smpboot.c linux-268/arch/ia64/kernel/smpboot.c
--- linux-266/arch/ia64/kernel/smpboot.c
+++ linux-268/arch/ia64/kernel/smpboot.c
@@ -72,6 +72,9 @@ extern unsigned long ia64_iobase;
 
 int cpucount;
 
+/* needed on IA64, arch/ia64/kernel/head.S relies on it (EF) */
+struct task_struct * init_tasks[NR_CPUS] __initdata = {&init_task, };
+
 /* Setup configured maximum number of CPUs to activate */
 static int max_cpus = -1;
 
@@ -390,14 +393,14 @@ start_secondary (void *unused)
 	return cpu_idle();
 }
 
-static int __init
+static struct task_struct * __init
 fork_by_hand (void)
 {
 	/*
 	 * don't care about the eip and regs settings since
 	 * we'll never reschedule the forked task.
 	 */
-	return do_fork(CLONE_VM|CLONE_PID, 0, 0, 0);
+	return copy_process(CLONE_VM|CLONE_IDLETASK, 0, 0, 0, NULL, NULL);
 }
 
 static void __init
@@ -407,26 +410,21 @@ do_boot_cpu (int sapicid)
 	int timeout, cpu;
 
 	cpu = ++cpucount;
-	/*
-	 * We can't use kernel_thread since we must avoid to
-	 * reschedule the child.
-	 */
-	if (fork_by_hand() < 0)
-		panic("failed fork for CPU %d", cpu);
 
 	/*
 	 * We remove it from the pidhash and the runqueue
 	 * once we got the process:
 	 */
-	idle = init_task.prev_task;
+	idle = fork_by_hand();
 	if (!idle)
 		panic("No idle process for CPU %d", cpu);
 
-	task_set_cpu(idle, cpu);	/* we schedule the first task manually */
+	wake_up_forked_process(idle);
+
+	init_idle(idle, cpu);	/* we schedule the first task manually */
 
 	ia64_cpu_to_sapicid[cpu] = sapicid;
 
-	del_from_runqueue(idle);
 	unhash_process(idle);
 	init_tasks[cpu] = idle;
 
@@ -487,8 +485,7 @@ smp_boot_cpus (void)
 	printk("Boot processor id 0x%x/0x%x\n", 0, boot_cpu_id);
 
 	global_irq_holder = 0;
-	current->processor = 0;
-	init_idle();
+	current->cpu = 0;
 
 	/*
 	 * If SMP should be disabled, then really disable it!
@@ -575,3 +572,10 @@ init_smp_config(void)
 		smp_num_cpus = 1;
 	}
 }
+
+/* Number of ticks we consider an idle tasks still cache-hot.
+ * For Itanium: with 1GB/s bandwidth we need 4ms to fill up 4MB L3 cache...
+ * So let's try 10 ticks.
+ */
+unsigned long cache_decay_ticks=10;
+
diff -urNp linux-266/arch/ia64/mm/tlb.c linux-268/arch/ia64/mm/tlb.c
--- linux-266/arch/ia64/mm/tlb.c
+++ linux-268/arch/ia64/mm/tlb.c
@@ -47,7 +47,8 @@ void
 wrap_mmu_context (struct mm_struct *mm)
 {
 	unsigned long tsk_context, max_ctx = ia64_ctx.max_ctx;
-	struct task_struct *tsk;
+	struct task_struct *tsk, *g;
+	int i;
 
 	if (ia64_ctx.next > max_ctx)
 		ia64_ctx.next = 300;	/* skip daemons */
@@ -59,7 +60,7 @@ wrap_mmu_context (struct mm_struct *mm)
 
 	read_lock(&tasklist_lock);
   repeat:
-	for_each_task(tsk) {
+	do_each_thread(g, tsk) {
 		if (!tsk->mm)
 			continue;
 		tsk_context = tsk->mm->context;
@@ -74,9 +75,16 @@ wrap_mmu_context (struct mm_struct *mm)
 		}
 		if ((tsk_context > ia64_ctx.next) && (tsk_context < ia64_ctx.limit))
 			ia64_ctx.limit = tsk_context;
-	}
+	} while_each_thread(g, tsk);
 	read_unlock(&tasklist_lock);
-	flush_tlb_all();
+        /*
+         * Can't call flush_tlb_all() here because of race condition with the o1 scheduler
+         * and because interrupts are disabled during context switch.
+         */
+        for (i = 0; i < NR_CPUS; ++i)
+                if (i != smp_processor_id())
+                        cpu_data(i)->need_tlb_flush = 1;
+	local_flush_tlb_all();
 }
 
 void
diff -urNp linux-266/arch/ia64/tools/print_offsets.awk linux-268/arch/ia64/tools/print_offsets.awk
--- linux-266/arch/ia64/tools/print_offsets.awk
+++ linux-268/arch/ia64/tools/print_offsets.awk
@@ -13,6 +13,9 @@ BEGIN {
 	#
 	print "#define PT_PTRACED_BIT		0"
 	print "#define PT_TRACESYS_BIT		1"
+	print ""
+	print "#define CLONE_IDLETASK_BIT       12"
+	print "#define CLONE_SETTLS_BIT 19"
 }
 
 # look for .tab:
diff -urNp linux-266/arch/ia64/tools/print_offsets.c linux-268/arch/ia64/tools/print_offsets.c
--- linux-266/arch/ia64/tools/print_offsets.c
+++ linux-268/arch/ia64/tools/print_offsets.c
@@ -54,7 +54,7 @@ tab[] =
     { "IA64_TASK_PTRACE_OFFSET",	offsetof (struct task_struct, ptrace) },
     { "IA64_TASK_SIGPENDING_OFFSET",	offsetof (struct task_struct, sigpending) },
     { "IA64_TASK_NEED_RESCHED_OFFSET",	offsetof (struct task_struct, need_resched) },
-    { "IA64_TASK_PROCESSOR_OFFSET",	offsetof (struct task_struct, processor) },
+    { "IA64_TASK_PROCESSOR_OFFSET",	offsetof (struct task_struct, cpu) },
     { "IA64_TASK_THREAD_OFFSET",	offsetof (struct task_struct, thread) },
     { "IA64_TASK_THREAD_KSP_OFFSET",	offsetof (struct task_struct, thread.ksp) },
     { "IA64_TASK_THREAD_CSD_OFFSET",	offsetof (struct task_struct, thread.csd) },
@@ -222,6 +222,9 @@ main (int argc, char **argv)
 	}
     }
 
+  printf ("\n#define CLONE_IDLETASK_BIT        %ld\n", ia64_fls (CLONE_IDLETASK));
+  printf ("\n#define CLONE_SETTLS_BIT  %ld\n", ia64_fls (CLONE_SETTLS));
+
   printf ("\n#endif /* _ASM_IA64_OFFSETS_H */\n");
   return 0;
 }
diff -urNp linux-266/include/asm-ia64/elf.h linux-268/include/asm-ia64/elf.h
--- linux-266/include/asm-ia64/elf.h
+++ linux-268/include/asm-ia64/elf.h
@@ -89,6 +89,15 @@ extern void ia64_elf_core_copy_regs (str
 struct elf64_hdr;
 extern void ia64_set_personality (struct elf64_hdr *elf_ex, int ibcs2_interpreter);
 #define SET_PERSONALITY(ex, ibcs2)	ia64_set_personality(&(ex), ibcs2)
+
+struct task_struct;
+
+extern int dump_task_regs(struct task_struct *, elf_gregset_t *);
+extern int dump_task_fpu (struct task_struct *, elf_fpregset_t *);
+
+#define ELF_CORE_COPY_TASK_REGS(tsk, elf_gregs) dump_task_regs(tsk, elf_gregs)
+#define ELF_CORE_COPY_FPREGS(tsk, elf_fpregs) dump_task_fpu(tsk, elf_fpregs)
+
 #endif
 
 #endif /* _ASM_IA64_ELF_H */
diff -urNp linux-266/include/asm-ia64/mmu_context.h linux-268/include/asm-ia64/mmu_context.h
--- linux-266/include/asm-ia64/mmu_context.h
+++ linux-268/include/asm-ia64/mmu_context.h
@@ -44,6 +44,23 @@ enter_lazy_tlb (struct mm_struct *mm, st
 {
 }
 
+/*
+ * When the context counter wraps around all TLBs need to be flushed because an old
+ * context number might have been reused. This is signalled by the ia64_need_tlb_flush
+ * per-CPU variable, which is checked in the routine below. Called by activate_mm().
+ * <efocht@ess.nec.de>
+ */
+static inline void
+delayed_tlb_flush (void)
+{
+        extern void local_flush_tlb_all (void);
+
+        if (unlikely(local_cpu_data->need_tlb_flush)) {
+                local_flush_tlb_all();
+                local_cpu_data->need_tlb_flush = 0;
+        }
+}
+
 static inline mm_context_t
 get_mmu_context (struct mm_struct *mm)
 {
@@ -129,6 +146,9 @@ activate_context (struct mm_struct *mm)
 static inline void
 activate_mm (struct mm_struct *prev, struct mm_struct *next)
 {
+	/* flush local tlb if requested */
+	delayed_tlb_flush();
+
 	/*
 	 * We may get interrupts here, but that's OK because interrupt handlers cannot
 	 * touch user-space.
diff -urNp linux-266/include/asm-ia64/processor.h linux-268/include/asm-ia64/processor.h
--- linux-266/include/asm-ia64/processor.h
+++ linux-268/include/asm-ia64/processor.h
@@ -157,6 +157,7 @@ struct cpuinfo_ia64 {
 	__u8 family;
 	__u8 archrev;
 	char vendor[16];
+	__u8 need_tlb_flush;	/* do a local tlb flush on next mm activation */
 	__u64 itc_freq;		/* frequency of ITC counter */
 	__u64 proc_freq;	/* frequency of processor */
 	__u64 cyc_per_usec;	/* itc_freq/1000000 */
diff -urNp linux-266/include/asm-ia64/smp.h linux-268/include/asm-ia64/smp.h
--- linux-266/include/asm-ia64/smp.h
+++ linux-268/include/asm-ia64/smp.h
@@ -27,7 +27,7 @@
 #define SMP_IRQ_REDIRECTION	(1 << 0)
 #define SMP_IPI_REDIRECTION	(1 << 1)
 
-#define smp_processor_id()	(current->processor)
+#define smp_processor_id()	(current->cpu)
 
 extern struct smp_boot_data {
 	int cpu_count;
