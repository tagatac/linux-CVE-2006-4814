diff -urNp linux-770/drivers/block/DAC960.c linux-780/drivers/block/DAC960.c
--- linux-770/drivers/block/DAC960.c
+++ linux-780/drivers/block/DAC960.c
@@ -2052,6 +2052,8 @@ static boolean DAC960_RegisterBlockDevic
   Controller->GenericDiskInfo.sizes = Controller->PartitionSizes;
   blksize_size[MajorNumber] = Controller->BlockSizes;
   max_sectors[MajorNumber] = Controller->MaxSectorsPerRequest;
+  RequestQueue->max_segments = Controller->DriverScatterGatherLimit;
+  blk_queue_superbh(RequestQueue, Controller->MaxBlocksPerCommand);
   /*
     Initialize Read Ahead to 128 sectors.
   */
diff -urNp linux-770/drivers/block/cciss.c linux-780/drivers/block/cciss.c
--- linux-770/drivers/block/cciss.c
+++ linux-780/drivers/block/cciss.c
@@ -2963,7 +2963,8 @@ static int __init cciss_init_one(struct 
 	q->back_merge_fn = cpq_back_merge_fn;
         q->front_merge_fn = cpq_front_merge_fn;
         q->merge_requests_fn = cpq_merge_requests_fn;
-
+	q->max_segments = MAXSGENTRIES;
+	blk_queue_large_superbh(q, MAX_SECTORS, MAXSGENTRIES);
 
 	/* Fill in the gendisk data */ 	
 	hba[i]->gendisk.major = MAJOR_NR + i;
diff -urNp linux-770/drivers/block/cpqarray.c linux-780/drivers/block/cpqarray.c
--- linux-770/drivers/block/cpqarray.c
+++ linux-780/drivers/block/cpqarray.c
@@ -571,7 +571,9 @@ static int __init cpqarray_init_one( str
 	q->back_merge_fn = cpq_back_merge_fn;
 	q->front_merge_fn = cpq_front_merge_fn;
 	q->merge_requests_fn = cpq_merge_requests_fn;
-
+	q->max_segments = SG_MAX;
+	blk_queue_superbh(q, MAX_SECTORS);
+	
 	hba[i]->gendisk.major = MAJOR_NR + i;
 	hba[i]->gendisk.major_name = "ida";
 	hba[i]->gendisk.minor_shift = NWD_SHIFT;
diff -urNp linux-770/drivers/block/ll_rw_blk.c linux-780/drivers/block/ll_rw_blk.c
--- linux-770/drivers/block/ll_rw_blk.c
+++ linux-780/drivers/block/ll_rw_blk.c
@@ -193,6 +193,48 @@ void blk_cleanup_queue(request_queue_t *
 }
 
 /**
+ * blk_queue_superbh - indicate whether queue can accept superbhs or not
+ * @q:       The queue which this applies to.
+ * @superbh: Max size in sectors
+ *
+ **/
+void __blk_queue_superbh(request_queue_t *q, int superbh, int limit)
+{
+	int i = 0;
+
+	if ((superbh << 9) > limit)
+		superbh = limit >> 9;
+
+	/* Force it to be a power of 2 */
+	if (superbh)
+		for (i = 1; (i<<1) <= superbh; i <<= 1);
+
+	q->superbh_queue = i << 9;
+}
+
+void blk_queue_superbh(request_queue_t *q, int sectors)
+{
+	__blk_queue_superbh(q, sectors, MAX_SUPERBH);
+}
+
+/* Allow a driver to request a larger superbh limit: we only expect
+ * varyio-capable drivers to call this.
+ *
+ * Limit the larger superbh depending on the caller's scatter-gather
+ * segment limit.  We determine the maximum size that a scatter- gather
+ * list can be given the number of sg segments, and use that as an upper
+ * bound for the superbh.  In no cases do we force the superbh to be
+ * below the old limit of MAX_SUPERBH due to this new bound.
+ */
+void blk_queue_large_superbh(request_queue_t *q, int sectors, int segments)
+{
+	int seg_sectors = (PAGE_SIZE / 512) * segments + 1;
+	if (sectors > seg_sectors && seg_sectors > (MAX_SUPERBH >> 9))
+		sectors = seg_sectors;
+	__blk_queue_superbh(q, sectors, MAX_SUPERBH_NOBOUNCE);
+}
+
+/**
  * blk_queue_headactive - indicate whether head of request queue may be active
  * @q:       The queue which this applies to.
  * @active:  A flag indication where the head of the queue is active.
@@ -247,6 +289,12 @@ void blk_queue_headactive(request_queue_
 void blk_queue_make_request(request_queue_t * q, make_request_fn * mfn)
 {
 	q->make_request_fn = mfn;
+
+	/*
+	 * clear this, if queue can take a superbh it needs to enable this
+	 * manually
+	 */
+	blk_queue_superbh(q, 0);
 }
 
 /**
@@ -511,6 +559,8 @@ void blk_init_queue(request_queue_t * q,
 	 */
 	q->plug_device_fn 	= generic_plug_device;
 	q->head_active    	= 1;
+	q->superbh_queue	= 0;
+	q->max_segments		= MAX_SEGMENTS;
 
 	blk_queue_bounce_limit(q, BLK_BOUNCE_HIGH);
 }
@@ -915,8 +965,32 @@ static inline void attempt_front_merge(r
 	attempt_merge(q, blkdev_entry_to_request(prev), max_sectors, max_segments);
 }
 
-static int __make_request(request_queue_t * q, int rw,
-				  struct buffer_head * bh)
+static inline void req_add_bh(struct request *rq, struct buffer_head *bh)
+{
+	unsigned short tot_size = bh->b_size >> 9;
+	struct buffer_head *bhtail = bh;
+	int segments = 1;
+
+	if (buffer_superbh(bh)) {
+		segments = superbh_segments(bh);
+		bhtail = superbh_bhtail(bh);
+
+		/*
+		 * now bh points to the first real bh, superbh is used no more
+		 */
+		bh = superbh_bh(bh);
+	}
+
+	rq->hard_nr_sectors = rq->nr_sectors = tot_size;
+	rq->current_nr_sectors = rq->hard_cur_sectors = bh->b_size >> 9;
+	rq->nr_segments = segments;
+	rq->nr_hw_segments = segments;
+	rq->buffer = bh->b_data;
+	rq->bh = bh;
+	rq->bhtail = bhtail;
+}
+
+static int __make_request(request_queue_t *q, int rw, struct buffer_head *bh)
 {
 	unsigned int sector, count;
 	int max_segments = MAX_SEGMENTS;
@@ -959,7 +1033,6 @@ static int __make_request(request_queue_
 	 */
 	bh = blk_queue_bounce(q, rw, bh);
 
-/* look for a free request. */
 	/*
 	 * Try to coalesce the new request with old requests
 	 */
@@ -981,6 +1054,17 @@ again:
 	} else if (q->head_active && !q->plugged)
 		head = head->next;
 
+	/*
+	 * potentially we could easily allow merge with superbh, however
+	 * it requires extensive changes to merge functions etc, so I prefer
+	 * to only let merges happen the other way around (ie normal bh _can_
+	 * be merged into a request which originated from a superbh). look
+	 * into doing this, if MAX_GROUPED is not enough to pull full io
+	 * bandwidth from device.
+	 */
+	if (buffer_superbh(bh))
+		goto get_rq;
+
 	el_ret = elevator->elevator_merge_fn(q, &req, head, bh, rw,max_sectors);
 	switch (el_ret) {
 
@@ -1064,21 +1148,18 @@ get_rq:
 		}
 	}
 
-/* fill up the request-info, and add it to the queue */
+	/*
+	 * fill up the request-info, and add it to the queue
+	 */
 	req->elevator_sequence = latency;
 	req->cmd = rw;
 	req->errors = 0;
 	req->hard_sector = req->sector = sector;
-	req->hard_nr_sectors = req->nr_sectors = count;
-	req->current_nr_sectors = req->hard_cur_sectors = count;
-	req->nr_segments = 1; /* Always 1 for a new request. */
-	req->nr_hw_segments = 1; /* Always 1 for a new request. */
-	req->buffer = bh->b_data;
 	req->waiting = NULL;
-	req->bh = bh;
-	req->bhtail = bh;
 	req->rq_dev = bh->b_rdev;
 	req->start_time = jiffies;
+	req->seg_invalid = buffer_superbh(bh);
+	req_add_bh(req, bh);
 	req_new_io(req, 0, count);
 	blk_started_io(count);
 	add_request(q, req, insert_here);
@@ -1179,10 +1260,19 @@ void generic_make_request (int rw, struc
 			buffer_IO_error(bh);
 			break;
 		}
+
+		/*
+		 * superbh_end_io() will gracefully resubmit, this is a soft
+		 * error. if !grouped_queue, fail hard
+		 */
+		if (buffer_superbh(bh) && !q->superbh_queue) {
+			buffer_IO_error(bh);
+			break;
+		}
+
 	} while (q->make_request_fn(q, rw, bh));
 }
 
-
 /**
  * submit_bh: submit a buffer_head to the block device later for I/O
  * @rw: whether to %READ or %WRITE, or maybe to %READA (read ahead)
@@ -1196,7 +1286,7 @@ void generic_make_request (int rw, struc
  * This is is appropriate for IO requests that come from the buffer
  * cache and page cache which (currently) always use aligned blocks.
  */
-void __submit_bh(int rw, struct buffer_head * bh, unsigned long blocknr)
+void submit_bh_rsector(int rw, struct buffer_head * bh)
 {
 	int count = bh->b_size >> 9;
 
@@ -1211,7 +1301,6 @@ void __submit_bh(int rw, struct buffer_h
 	 * further remap this.
 	 */
 	bh->b_rdev = bh->b_dev;
-	bh->b_rsector = blocknr;
 
 	generic_make_request(rw, bh);
 
@@ -1225,6 +1314,167 @@ void __submit_bh(int rw, struct buffer_h
 	}
 }
 
+/*
+ * if this is called, it's prior to the super_bh being submitted. so
+ * we can fallback to normal IO submission here. uptodate bool means
+ * absolutely nothing here.
+ */
+void superbh_end_io(struct buffer_head *superbh, int uptodate)
+{
+	struct buffer_head *bh = superbh_bh(superbh);
+	const int rw = superbh_rw(superbh);
+	struct buffer_head *next_bh;
+
+	if (!buffer_superbh(superbh))
+		BUG();
+
+	/*
+	 * detach each bh and resubmit, or completely fail if its a grouped bh
+	 */
+	do {
+		next_bh = bh->b_reqnext;
+		bh->b_reqnext = NULL;
+
+		submit_bh_rsector(rw, bh);
+	} while ((bh = next_bh));
+}
+
+static inline int bounce_initialised(void)
+{
+#ifdef CONFIG_HIGHMEM
+	extern int emergency_bounce_initialised;
+	return emergency_bounce_initialised;
+#else
+	return 0;
+#endif
+}
+
+/**
+ * submit_bh_linked: submit a list of buffer_heads for I/O
+ * @rw: whether to %READ or %WRITE, or maybe to %READA (read ahead)
+ * @bh: The first &struct buffer_head
+ *
+ * submit_bh_linked() acts like submit_bh(), but it can submit more than
+ * one buffer_head in one go. It sets up a "superbh" describing the combined
+ * transfer of the linked buffer_heads, and submit these in one go. This
+ * heavily cuts down on time spent inside io scheduler.
+ *
+ * The buffer_head strings must be linked via b_reqnext. All
+ * buffer_heads must be valid and pre-setup like the caller would for
+ * submit_bh(), except that the caller is responsible for setting up
+ * b_rsector.
+ */
+int submit_bh_linked(int rw, struct buffer_head *bh)
+{
+	struct buffer_head superbh, *tmp_bh, *bhprev, *bhfirst;
+	request_queue_t *q = blk_get_queue(bh->b_dev);
+	int size, segments, max_size;
+	unsigned long next_sector;
+
+	/*
+	 * these must be the same for all buffer_heads
+	 */
+	superbh_rw(&superbh) = rw;
+	superbh.b_rdev = bh->b_dev;
+	superbh.b_state = bh->b_state | (1 << BH_Super);
+	superbh.b_end_io = superbh_end_io;
+
+	/*
+	 * not really needed...
+	 */
+	superbh.b_data = NULL;
+	superbh.b_page = NULL;
+
+	tmp_bh = bh;
+
+queue_next:
+	bhprev = bhfirst = tmp_bh;
+	segments = size = 0;
+	next_sector = -1;
+
+	superbh_bh(&superbh) = bhfirst;
+
+	if (!q)
+		goto punt;
+
+	max_size = q->superbh_queue;
+
+	/* If this queue may need bounce buffering, enforce a stricter
+	 * upper bound on the superbh size for bounce buffer deadlock
+	 * safety. */
+	if (q->bounce_pfn < blk_max_pfn && bounce_initialised() &&
+	    superbh_will_bounce(q->bounce_pfn, &superbh)) {
+		if (max_size > MAX_SUPERBH)
+			max_size = MAX_SUPERBH;
+	}
+
+	/*
+	 * doesn't support superbh queueing, punt
+	 */
+	if (!max_size || !q->max_segments)
+		goto punt;
+
+	do {
+		if (!buffer_locked(tmp_bh))
+			BUG();
+
+		/*
+		 * submit this bit if it would exceed the allowed size
+		 * or if it would result in a non-contiguous IO
+		 */
+		if ((size + tmp_bh->b_size > max_size)
+		    || (segments >= q->max_segments) 
+		    || (next_sector != -1 && tmp_bh->b_rsector != next_sector)
+		    ) {
+			bhprev->b_reqnext = NULL;
+			break;
+		}
+
+		/*
+		 * init each bh like submit_bh() does
+		 */
+		tmp_bh->b_rdev = tmp_bh->b_dev;
+		set_bit(BH_Req, &tmp_bh->b_state);
+		set_bit(BH_Launder, &tmp_bh->b_state);
+
+		size += tmp_bh->b_size;
+		bhprev = tmp_bh;
+		segments++;
+		next_sector = tmp_bh->b_rsector + (tmp_bh->b_size >> 9);
+	} while ((tmp_bh = tmp_bh->b_reqnext));
+
+	/*
+	 * this is a super bh, it's only valid for io submission. it describes
+	 * in size the entire bh list submitted.
+	 */
+	superbh.b_size = size;
+	superbh_segments(&superbh) = segments;
+	superbh_bhtail(&superbh) = bhprev;
+	superbh.b_rsector = bhfirst->b_rsector;
+
+	generic_make_request(rw, &superbh);
+
+	switch (rw) {
+		case WRITE:
+			kstat.pgpgout += size >> 9;
+			break;
+		default:
+			kstat.pgpgin += size >> 9;
+			break;
+	}
+
+	/*
+	 * not done
+	 */
+	if (tmp_bh)
+		goto queue_next;
+
+	return 0;
+punt:
+	superbh.b_end_io(&superbh, 0);
+	return 1;
+}
+
 /**
  * ll_rw_block: low-level access to block devices
  * @rw: whether to %READ or %WRITE or maybe %READA (readahead)
@@ -1526,6 +1776,8 @@ EXPORT_SYMBOL(blk_init_queue);
 EXPORT_SYMBOL(blk_get_queue);
 EXPORT_SYMBOL(blk_cleanup_queue);
 EXPORT_SYMBOL(blk_queue_headactive);
+EXPORT_SYMBOL(blk_queue_superbh);
+EXPORT_SYMBOL(blk_queue_large_superbh);
 EXPORT_SYMBOL(blk_queue_make_request);
 EXPORT_SYMBOL(generic_make_request);
 EXPORT_SYMBOL(blkdev_release_request);
@@ -1535,3 +1787,7 @@ EXPORT_SYMBOL(blk_max_low_pfn);
 EXPORT_SYMBOL(blk_max_pfn);
 EXPORT_SYMBOL(blk_seg_merge_ok);
 EXPORT_SYMBOL(blk_nohighio);
+
+EXPORT_SYMBOL(ll_rw_block);
+EXPORT_SYMBOL(submit_bh_rsector);
+EXPORT_SYMBOL(submit_bh_linked);
diff -urNp linux-770/drivers/ide/ide-probe.c linux-780/drivers/ide/ide-probe.c
--- linux-770/drivers/ide/ide-probe.c
+++ linux-780/drivers/ide/ide-probe.c
@@ -968,9 +968,15 @@ EXPORT_SYMBOL(save_match);
 static void ide_init_queue(ide_drive_t *drive)
 {
 	request_queue_t *q = &drive->queue;
+	int max_sectors = 128;
 
 	q->queuedata = HWGROUP(drive);
 	blk_init_queue(q, do_ide_request);
+
+	if (HWIF(drive)->chipset == ide_pdc4030)
+		max_sectors = 127;
+
+	blk_queue_superbh(q, max_sectors);
 }
 
 #undef __IRQ_HELL_SPIN
diff -urNp linux-770/drivers/scsi/scsi.c linux-780/drivers/scsi/scsi.c
--- linux-770/drivers/scsi/scsi.c
+++ linux-780/drivers/scsi/scsi.c
@@ -198,6 +198,13 @@ void  scsi_initialize_queue(Scsi_Device 
 	blk_init_queue(q, scsi_request_fn);
 	blk_queue_headactive(q, 0);
 	q->queuedata = (void *) SDpnt;
+
+	q->max_segments = SHpnt->sg_tablesize;
+
+        if (SHpnt->hostt->vary_io)
+                blk_queue_large_superbh(q, SHpnt->max_sectors, q->max_segments);
+        else
+                blk_queue_superbh(q, SHpnt->max_sectors);
 }
 
 #ifdef MODULE
diff -urNp linux-770/drivers/scsi/scsi_merge.c linux-780/drivers/scsi/scsi_merge.c
--- linux-770/drivers/scsi/scsi_merge.c
+++ linux-780/drivers/scsi/scsi_merge.c
@@ -810,11 +810,9 @@ __inline static int __init_io(Scsi_Cmnd 
 	/*
 	 * First we need to know how many scatter gather segments are needed.
 	 */
-	if (!sg_count_valid) {
+	count = req->nr_segments;
+	if (!sg_count_valid || req->seg_invalid)
 		count = __count_segments(req, use_clustering, dma_host, NULL);
-	} else {
-		count = req->nr_segments;
-	}
 
 	/*
 	 * If the dma pool is nearly empty, then queue a minimal request
diff -urNp linux-770/fs/buffer.c linux-780/fs/buffer.c
--- linux-770/fs/buffer.c
+++ linux-780/fs/buffer.c
@@ -2269,12 +2269,12 @@ int brw_kiovec(int rw, int nr, struct ki
 	unsigned long	blocknr;
 	struct kiobuf *	iobuf = NULL;
 	struct page *	map;
-	struct buffer_head *tmp, **bhs = NULL;
-	int iosize = size;
+	struct buffer_head *tmp, **bhs = NULL, *bh_first, *bh_prev;
+	int		iosize = size;
 
 	if (!nr)
 		return 0;
-	
+
 	/* 
 	 * First, do some alignment and validity checks 
 	 */
@@ -2291,6 +2291,7 @@ int brw_kiovec(int rw, int nr, struct ki
 	 * OK to walk down the iovec doing page IO on each page we find. 
 	 */
 	bufind = bhind = transferred = err = 0;
+	bh_first = bh_prev = NULL;
 	for (i = 0; i < nr; i++) {
 		iobuf = iovec[i];
 		offset = iobuf->offset;
@@ -2305,7 +2306,7 @@ int brw_kiovec(int rw, int nr, struct ki
 				err = -EFAULT;
 				goto finished;
 			}
-			
+
 			while (length > 0) {
 				blocknr = b[bufind];
 				if (blocknr == -1UL) {
@@ -2337,13 +2338,18 @@ int brw_kiovec(int rw, int nr, struct ki
 				tmp = bhs[bhind++];
 
 				tmp->b_size = iosize;
+				if (!bh_first)
+					bh_first = tmp;
+
 				set_bh_page(tmp, map, offset);
 				tmp->b_this_page = tmp;
 
 				init_buffer(tmp, end_buffer_io_kiobuf, iobuf);
 				tmp->b_dev = dev;
 				tmp->b_blocknr = blocknr;
+				tmp->b_rsector = blocknr * (size >> 9);
 				tmp->b_state = (1 << BH_Mapped) | (1 << BH_Lock) | (1 << BH_Req);
+				tmp->b_reqnext = NULL;
 
 				if (rw == WRITE) {
 					set_bit(BH_Uptodate, &tmp->b_state);
@@ -2352,15 +2358,18 @@ int brw_kiovec(int rw, int nr, struct ki
 					set_bit(BH_Uptodate, &tmp->b_state);
 
 				atomic_inc(&iobuf->io_count);
-				if (iobuf->varyio) {
-					tmp->b_blocknr *= size >> 9;
-					submit_bh_blknr(rw, tmp);
-				} else 
-					submit_bh(rw, tmp);
+
+				if (bh_prev)
+					bh_prev->b_reqnext = tmp;
+
+				bh_prev = tmp;
+
 				/* 
 				 * Wait for IO if we have got too much 
 				 */
 				if (bhind >= KIO_MAX_SECTORS) {
+submit_io:
+					submit_bh_linked(rw, bh_first);
 					kiobuf_wait_for_io(iobuf); /* wake-one */
 					err = wait_kio(rw, bhind, bhs, size);
 					if (err >= 0)
@@ -2368,6 +2377,7 @@ int brw_kiovec(int rw, int nr, struct ki
 					else
 						goto finished;
 					bhind = 0;
+					bh_prev = bh_first = NULL;
 				}
 
 			skip_block:
@@ -2379,18 +2389,24 @@ int brw_kiovec(int rw, int nr, struct ki
 					offset = 0;
 					break;
 				}
+
+				/*
+				 * see if we need to submit the previous bit
+				 */
+				if ((blocknr == -1) && bhind)
+					goto submit_io;
+
 			} /* End of block loop */
 		} /* End of page loop */		
 	} /* End of iovec loop */
 
 	/* Is there any IO still left to submit? */
 	if (bhind) {
+		submit_bh_linked(rw, bh_first);
 		kiobuf_wait_for_io(iobuf); /* wake-one */
 		err = wait_kio(rw, bhind, bhs, size);
 		if (err >= 0)
 			transferred += err;
-		else
-			goto finished;
 	}
 
  finished:
@@ -3264,8 +3280,11 @@ submit:
 	atomic_set(&brw_cb->io_count, brw_cb->nr+1);
 	/* okay, we've setup all our io requests, now fire them off! */
 	if (can_do_varyio)
-		for (i=0; i<brw_cb->nr; i++) 
-			submit_bh_blknr(rw, brw_cb->bh[i]);
+		for (i=0; i<brw_cb->nr; i++) {
+			struct buffer_head *tmp = brw_cb->bh[i];
+			tmp->b_rsector = tmp->b_blocknr << (sector_shift - 9);
+			submit_bh_rsector(rw, tmp);
+		}
 	else
 		for (i=0; i<brw_cb->nr; i++) 
 			submit_bh(rw, brw_cb->bh[i]);
diff -urNp linux-770/include/linux/blkdev.h linux-780/include/linux/blkdev.h
--- linux-770/include/linux/blkdev.h
+++ linux-780/include/linux/blkdev.h
@@ -46,6 +46,7 @@ struct request {
 	struct buffer_head * bh;
 	struct buffer_head * bhtail;
 	request_queue_t *q;
+	char seg_invalid;
 };
 
 #include <linux/elevator.h>
@@ -126,6 +127,16 @@ struct request_queue
 	 */
 	char			head_active;
 
+	/*
+	 * max segments
+	 */
+	int			max_segments;
+
+	/*
+	 * The biggest superbh we can generate for this queue
+	 */
+	int			superbh_queue;
+
 	unsigned long		bounce_pfn;
 
 	/*
@@ -166,21 +177,7 @@ extern unsigned long blk_max_low_pfn, bl
 extern void blk_queue_bounce_limit(request_queue_t *, u64);
 
 #ifdef CONFIG_HIGHMEM
-extern struct buffer_head *create_bounce(int, struct buffer_head *);
-extern inline struct buffer_head *blk_queue_bounce(request_queue_t *q, int rw,
-						   struct buffer_head *bh)
-{
-	struct page *page = bh->b_page;
-
-#ifndef CONFIG_DISCONTIGMEM
-	if (page - mem_map <= q->bounce_pfn)
-#else
-	if ((page - page_zone(page)->zone_mem_map) + (page_zone(page)->zone_start_paddr >> PAGE_SHIFT) <= q->bounce_pfn)
-#endif
-		return bh;
-
-	return create_bounce(rw, bh);
-}
+extern inline struct buffer_head *blk_queue_bounce(request_queue_t *, int, struct buffer_head *);
 #else
 #define blk_queue_bounce(q, rw, bh)	(bh)
 #endif
@@ -227,6 +224,8 @@ extern int blk_grow_request_list(request
 extern void blk_init_queue(request_queue_t *, request_fn_proc *);
 extern void blk_cleanup_queue(request_queue_t *);
 extern void blk_queue_headactive(request_queue_t *, int);
+extern void blk_queue_superbh(request_queue_t *, int);
+extern void blk_queue_large_superbh(request_queue_t *, int, int);
 extern void blk_queue_make_request(request_queue_t *, make_request_fn *);
 extern void generic_unplug_device(void *);
 extern inline int blk_seg_merge_ok(struct buffer_head *, struct buffer_head *);
@@ -247,8 +246,19 @@ extern char * blkdev_varyio[MAX_BLKDEV];
 
 #define MAX_SEGMENTS 128
 #define MAX_SECTORS 255
+/* General-case limit for superbh size: */
+#define MAX_SUPERBH 32768	/* must fit info ->b_size right now */
 
-#define PageAlignSize(size) (((size) + PAGE_SIZE -1) & PAGE_MASK)
+/* Limit for superbh when we're certain it cannot be bounce-buffered: */
+#define MAX_SUPERBH_NOBOUNCE (1024*1024) /* must fit info ->b_size right now */
+
+/*
+ * bh abuse :/
+ */
+#define superbh_segments(bh)	((bh)->b_blocknr)
+#define superbh_rw(bh)		((bh)->b_list)
+#define superbh_bhtail(bh)	((struct buffer_head *) (bh)->b_reqnext)
+#define superbh_bh(bh)		((struct buffer_head *) (bh)->b_private)
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queue)
 #define blkdev_entry_next_request(entry) blkdev_entry_to_request((entry)->next)
diff -urNp linux-770/include/linux/fs.h linux-780/include/linux/fs.h
--- linux-770/include/linux/fs.h
+++ linux-780/include/linux/fs.h
@@ -225,6 +225,7 @@ enum bh_state_bits {
 	BH_Attached,	/* 1 if b_inode_buffers is linked into a list */
 	BH_JBD,		/* 1 if it has an attached journal_head */
 	BH_Delay,	/* 1 if the buffer is delayed allocate */
+	BH_Super,	/* 1 if this is a superbh */
 
 	BH_PrivateStart,/* not a state bit, but the first bit available
 			 * for private allocation by other entities
@@ -288,6 +289,7 @@ void init_buffer(struct buffer_head *, b
 #define buffer_async(bh)	__buffer_state(bh,Async)
 #define buffer_launder(bh)	__buffer_state(bh,Launder)
 #define buffer_delay(bh)	__buffer_state(bh,Delay)
+#define buffer_superbh(bh)	__buffer_state(bh,Super)
 
 #define bh_offset(bh)		((unsigned long)(bh)->b_data & ~PAGE_MASK)
 
@@ -1461,15 +1463,23 @@ extern void file_move(struct file *f, st
 extern struct buffer_head * get_hash_table(kdev_t, int, int);
 extern struct buffer_head * getblk(kdev_t, int, int);
 extern void ll_rw_block(int, int, struct buffer_head * bh[]);
-extern void __submit_bh(int, struct buffer_head *, unsigned long);
+extern void submit_bh_rsector(int rw, struct buffer_head * bh);
 static inline void submit_bh(int rw, struct buffer_head * bh)
 {
-	__submit_bh(rw, bh, bh->b_blocknr * (bh->b_size >> 9));
+	bh->b_rsector = bh->b_blocknr * (bh->b_size >> 9);
+	submit_bh_rsector(rw, bh);
 }
 static inline void submit_bh_blknr(int rw, struct buffer_head * bh)
 {
-	__submit_bh(rw, bh, bh->b_blocknr);
+	bh->b_rsector = bh->b_blocknr;
+	submit_bh_rsector(rw, bh);
 }
+static inline void __submit_bh(int rw, struct buffer_head *bh, unsigned long blknr)
+{
+	bh->b_rsector = blknr;
+	submit_bh_rsector(rw, bh);
+}
+extern int submit_bh_linked(int, struct buffer_head *);
 extern int is_read_only(kdev_t);
 extern void __brelse(struct buffer_head *);
 static inline void brelse(struct buffer_head *buf)
diff -urNp linux-770/include/linux/highmem.h linux-780/include/linux/highmem.h
--- linux-770/include/linux/highmem.h
+++ linux-780/include/linux/highmem.h
@@ -14,6 +14,7 @@ extern struct page *highmem_start_page;
 unsigned int nr_free_highpages(void);
 
 extern struct buffer_head *create_bounce(int rw, struct buffer_head * bh_orig);
+extern int superbh_will_bounce(unsigned long pfn, struct buffer_head *bh);
 
 static inline char *bh_kmap(struct buffer_head *bh)
 {
@@ -63,6 +64,7 @@ static inline void bh_kunmap_irq(char *b
 
 #else /* CONFIG_HIGHMEM */
 
+static inline int superbh_will_bounce(unsigned long pfn, struct buffer_head *bh) { return 0; }
 static inline unsigned int nr_free_highpages(void) { return 0; }
 
 static inline void *kmap(struct page *page) { return page_address(page); }
diff -urNp linux-770/kernel/ksyms.c linux-780/kernel/ksyms.c
--- linux-770/kernel/ksyms.c
+++ linux-780/kernel/ksyms.c
@@ -144,6 +144,7 @@ EXPORT_SYMBOL(highmem_start_page);
 EXPORT_SYMBOL(create_bounce);
 EXPORT_SYMBOL(kmap_prot);
 EXPORT_SYMBOL(kmap_pte);
+EXPORT_SYMBOL(blk_queue_bounce);
 #endif
 
 /* filesystem internal functions */
@@ -223,8 +224,6 @@ EXPORT_SYMBOL(bdput);
 EXPORT_SYMBOL(bread);
 EXPORT_SYMBOL(__brelse);
 EXPORT_SYMBOL(__bforget);
-EXPORT_SYMBOL(ll_rw_block);
-EXPORT_SYMBOL(__submit_bh);
 EXPORT_SYMBOL(unlock_buffer);
 EXPORT_SYMBOL(__wait_on_buffer);
 EXPORT_SYMBOL(___wait_on_page);
diff -urNp linux-770/mm/highmem.c linux-780/mm/highmem.c
--- linux-770/mm/highmem.c
+++ linux-780/mm/highmem.c
@@ -21,6 +21,7 @@
 #include <linux/highmem.h>
 #include <linux/swap.h>
 #include <linux/slab.h>
+#include <linux/blkdev.h>
 
 /*
  * Virtual_count is not a pure "count".
@@ -279,6 +280,8 @@ static inline void bounce_end_io (struct
 	spin_unlock_irqrestore(&emergency_lock, flags);
 }
 
+int emergency_bounce_initialised;
+
 static __init int init_emergency_pool(void)
 {
 	struct sysinfo i;
@@ -287,6 +290,7 @@ static __init int init_emergency_pool(vo
         
         if (!i.totalhigh)
         	return 0;
+	emergency_bounce_initialised = 1;
 
 	spin_lock_irq(&emergency_lock);
 	while (nr_emergency_pages < POOL_SIZE) {
@@ -454,3 +458,114 @@ struct buffer_head * create_bounce(int r
 	return bh;
 }
 
+#ifndef CONFIG_DISCONTIGMEM
+#define bounce_page(page, pfn)	((page) - mem_map > (pfn))
+#else
+#define bounce_page(page, pfn) \
+	(((page) - page_zone((page))->zone_mem_map) + (page_zone((page))->zone_start_paddr >> PAGE_SHIFT) > (pfn))
+#endif
+
+/*
+ * FIXME: assuming PAGE_SIZE buffer_heads
+ */
+#define SUPERBH_MAX_USERS	(POOL_SIZE * PAGE_SIZE / MAX_SUPERBH)
+
+static int superbh_users;
+static DECLARE_WAIT_QUEUE_HEAD(superbh_wait);
+
+int superbh_will_bounce(unsigned long pfn, struct buffer_head *bh)
+{
+	bh = superbh_bh(bh);
+	do {
+		if (bounce_page(bh->b_page, pfn))
+			return 1;
+	} while ((bh = bh->b_reqnext) != NULL);
+	return 0;
+}
+
+
+static struct buffer_head *
+superbh_bounce(unsigned long pfn, int rw, struct buffer_head *bh)
+{
+	struct buffer_head *bhprev, *bhnext, *superbh = bh;
+	DECLARE_WAITQUEUE(wait, current);
+	int bounce_pages;
+
+	/*
+	 * first check how many pages we need to bounce (and thus how many
+	 * buffer_heads and pages we need to reserve)
+	 */
+	bounce_pages = 0;
+	bh = superbh_bh(superbh);
+	do {
+		if (bounce_page(bh->b_page, pfn))
+			bounce_pages++;
+	} while ((bh = bh->b_reqnext) != NULL);
+
+	if (!bounce_pages)
+		return superbh;
+
+	/*
+	 * to prevent resource deadlock, we must make sure that anyone who
+	 * enters the allocation path can get their share of the reserved
+	 * buffer_heads and pages.
+	 */
+	spin_lock_irq(&emergency_lock);
+	add_wait_queue(&superbh_wait, &wait);
+	for (;;) {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		if (superbh_users < SUPERBH_MAX_USERS)
+			break;
+		spin_unlock_irq(&emergency_lock);
+		schedule();
+		spin_lock_irq(&emergency_lock);
+	}
+	__set_current_state(TASK_RUNNING);
+	remove_wait_queue(&superbh_wait, &wait);
+
+	superbh_users++;
+	spin_unlock_irq(&emergency_lock);
+
+	/*
+	 * now bounce them
+	 */
+	bh = superbh_bh(superbh);
+	bhprev = NULL;
+	do {
+		bhnext = bh->b_reqnext;
+		bh->b_reqnext = NULL;
+
+		if (bounce_page(bh->b_page, pfn))
+			bh = create_bounce(rw, bh);
+
+		if (bhprev)
+			bhprev->b_reqnext = bh;
+		else
+			superbh->b_private = bh;
+
+		bhprev = bh;
+	} while ((bh = bhnext) != NULL);
+	superbh_bhtail(superbh) = bhprev;
+
+	spin_lock_irq(&emergency_lock);
+	superbh_users--;
+	spin_unlock_irq(&emergency_lock);
+	wake_up(&superbh_wait);
+
+	return superbh;
+}
+
+struct buffer_head *blk_queue_bounce(request_queue_t *q, int rw,
+				     struct buffer_head *bh)
+{
+	if (q->bounce_pfn >= blk_max_pfn)
+		return bh;
+
+	if (test_bit(BH_Super, &bh->b_state))
+		return superbh_bounce(q->bounce_pfn, rw, bh);
+
+	if (!bounce_page(bh->b_page, q->bounce_pfn))
+		return bh;
+
+	return create_bounce(rw, bh);
+}
