diff -urNp linux-7055/drivers/s390/net/fsm.c linux-7060/drivers/s390/net/fsm.c
--- linux-7055/drivers/s390/net/fsm.c
+++ linux-7060/drivers/s390/net/fsm.c
@@ -172,19 +172,11 @@ fsm_addtimer(fsm_timer *this, int millis
 	       this->fi->name, this, millisec);
 #endif
 
-#if LINUX_VERSION_CODE >= 0x020300
-	if (this->tl.list.next || this->tl.list.prev) {
+	if (timer_pending(&this->tl)) {
 		printk(KERN_WARNING "fsm(%s): timer already active!\n",
 			this->fi->name);
 		return -1;
 	}
-#else
-	if (this->tl.next || this->tl.prev) {
-		printk(KERN_WARNING "fsm(%s): timer already active!\n",
-			this->fi->name);
-		return -1;
-	}
-#endif
 	init_timer(&this->tl);
 	this->tl.function = (void *)fsm_expire_timer;
 	this->tl.data = (long)this;
@@ -204,13 +196,8 @@ fsm_modtimer(fsm_timer *this, int millis
 		this->fi->name, this, millisec);
 #endif
 
-#if LINUX_VERSION_CODE >= 0x020300
-	if (this->tl.list.next || this->tl.list.prev)
-		del_timer(&this->tl);
-#else
-	if (this->tl.next || this->tl.prev)
+	if (timer_pending(&this->tl))
 		del_timer(&this->tl);
-#endif
 	init_timer(&this->tl);
 	this->tl.function = (void *)fsm_expire_timer;
 	this->tl.data = (long)this;
diff -urNp linux-7055/include/linux/kernel.h linux-7060/include/linux/kernel.h
--- linux-7055/include/linux/kernel.h
+++ linux-7060/include/linux/kernel.h
@@ -200,6 +200,17 @@ extern void print_modules(void);
         const typeof( ((type *)0)->member ) *__mptr = (ptr);	\
         (type *)( (char *)__mptr - offsetof(type,member) );})
 
+/*
+ * Check at compile time that something is of a particular type.
+ * Always evaluates to 1 so you may use it easily in comparisons.
+ */
+#define typecheck(type,x) \
+({	type __dummy; \
+	typeof(x) __dummy2; \
+	(void)(&__dummy == &__dummy2); \
+	1; \
+})
+
 extern void __out_of_line_bug(int line) ATTRIB_NORET;
 #define out_of_line_bug() __out_of_line_bug(__LINE__)
 
diff -urNp linux-7055/include/linux/sched.h linux-7060/include/linux/sched.h
--- linux-7055/include/linux/sched.h
+++ linux-7060/include/linux/sched.h
@@ -754,6 +754,31 @@ extern void do_timer(struct pt_regs *);
 
 #define CURRENT_TIME (xtime.tv_sec)
 
+/*
+ *	These inlines deal with timer wrapping correctly. You are 
+ *	strongly encouraged to use them
+ *	1. Because people otherwise forget
+ *	2. Because if the timer wrap changes in future you won't have to
+ *	   alter your driver code.
+ *
+ * time_after(a,b) returns true if the time a is after time b.
+ *
+ * Do this with "<0" and ">=0" to only test the sign of the result. A
+ * good compiler would generate better code (and a really good compiler
+ * wouldn't care). Gcc is currently neither.
+ */
+#define time_after(a,b)		\
+	(typecheck(unsigned long, a) && \
+	 typecheck(unsigned long, b) && \
+	 ((long)(b) - (long)(a) < 0))
+#define time_before(a,b)	time_after(b,a)
+
+#define time_after_eq(a,b)	\
+	(typecheck(unsigned long, a) && \
+	 typecheck(unsigned long, b) && \
+	 ((long)(a) - (long)(b) >= 0))
+#define time_before_eq(a,b)	time_after_eq(b,a)
+
 extern int FASTCALL(wake_up_state(struct task_struct * tsk, unsigned int state));
 extern void FASTCALL(__wake_up(wait_queue_head_t *q, unsigned int mode, int nr));
 extern void FASTCALL(__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr));
diff -urNp linux-7055/include/linux/smp.h linux-7060/include/linux/smp.h
--- linux-7055/include/linux/smp.h
+++ linux-7060/include/linux/smp.h
@@ -119,5 +119,7 @@ static inline void smp_send_reschedule_a
  * Common definitions:
  */
 #define cpu()					smp_processor_id()
+#define get_cpu()				smp_processor_id()
+#define put_cpu()				do { } while (0)
 
 #endif
diff -urNp linux-7055/include/linux/timer.h linux-7060/include/linux/timer.h
--- linux-7055/include/linux/timer.h
+++ linux-7060/include/linux/timer.h
@@ -3,74 +3,56 @@
 
 #include <linux/config.h>
 #include <linux/list.h>
+#include <linux/spinlock.h>
+
+struct tvec_t_base_s;
 
-/*
- * In Linux 2.4, static timers have been removed from the kernel.
- * Timers may be dynamically created and destroyed, and should be initialized
- * by a call to init_timer() upon creation.
- *
- * The "data" field enables use of a common timeout function for several
- * timeouts. You can use this field to distinguish between the different
- * invocations.
- */
 struct timer_list {
-	struct list_head list;
+	struct list_head entry;
 	unsigned long expires;
 	unsigned long data;
 	void (*function)(unsigned long);
+
+	unsigned long magic;
+	unsigned long lock;
+	struct tvec_t_base_s *base;
 };
 
+#define TIMER_INITIALIZER(_function, _expires, _data) {		\
+		.function = (_function),			\
+		.expires = (_expires),				\
+		.data = (_data),				\
+		.lock = 0,					\
+		.base = NULL,					\
+	}
+
+/***
+ * init_timer - initialize a timer.
+ * @timer: the timer to be initialized
+ *
+ * init_timer() must be done to a timer prior calling *any* of the
+ * other timer functions.
+ */
+static inline void init_timer(struct timer_list * timer)
+{
+	timer->magic = 0;
+	timer->lock = 0;
+	timer->base = NULL;
+}
+
+extern int timer_pending(struct timer_list * timer);
 extern void add_timer(struct timer_list * timer);
 extern int del_timer(struct timer_list * timer);
-
+extern int mod_timer(struct timer_list *timer, unsigned long expires);
+  
 #ifdef CONFIG_SMP
-extern int del_timer_sync(struct timer_list * timer);
-extern void sync_timers(void);
+  extern int del_timer_sync(struct timer_list * timer);
 #else
-#define del_timer_sync(t)	del_timer(t)
-#define sync_timers()		do { } while (0)
+# define del_timer_sync(t) del_timer(t)
 #endif
 
-/*
- * mod_timer is a more efficient way to update the expire field of an
- * active timer (if the timer is inactive it will be activated)
- * mod_timer(a,b) is equivalent to del_timer(a); a->expires = b; add_timer(a).
- * If the timer is known to be not pending (ie, in the handler), mod_timer
- * is less efficient than a->expires = b; add_timer(a).
- */
-int mod_timer(struct timer_list *timer, unsigned long expires);
-
 extern void it_real_fn(unsigned long);
 
-static inline void init_timer(struct timer_list * timer)
-{
-	timer->list.next = timer->list.prev = NULL;
-}
-
-static inline int timer_pending (const struct timer_list * timer)
-{
-	return timer->list.next != NULL;
-}
-
-/*
- *	These inlines deal with timer wrapping correctly. You are 
- *	strongly encouraged to use them
- *	1. Because people otherwise forget
- *	2. Because if the timer wrap changes in future you wont have to
- *	   alter your driver code.
- *
- * time_after(a,b) returns true if the time a is after time b.
- *
- * Do this with "<0" and ">=0" to only test the sign of the result. A
- * good compiler would generate better code (and a really good compiler
- * wouldn't care). Gcc is currently neither.
- */
-#define time_after(a,b)		((long)(b) - (long)(a) < 0)
-#define time_before(a,b)	time_after(b,a)
-
-#define time_after_eq(a,b)	((long)(a) - (long)(b) >= 0)
-#define time_before_eq(a,b)	time_after_eq(b,a)
-
 #define RATE_LIMIT(interval)					\
 ({								\
 	static unsigned long expires;				\
diff -urNp linux-7055/include/net/ip_vs.h linux-7060/include/net/ip_vs.h
--- linux-7055/include/net/ip_vs.h
+++ linux-7060/include/net/ip_vs.h
@@ -412,7 +412,7 @@ struct ip_vs_conn {
 
 	/* counter and timer */
 	atomic_t		refcnt;		/* reference count */
-	struct timer_list	timer;		/* Expiration timer */
+	struct timer_list	conn_timer;	/* Expiration timer */
 	volatile unsigned long	timeout;	/* timeout */
 	struct ip_vs_timeout_table *timeout_table;
 
@@ -758,17 +758,6 @@ static __inline__ int ip_vs_todrop(void)
 
 
 /*
- *      Slow timer functions for IPVS
- *      (from ip_vs_timer.c)
- */
-extern void add_sltimer(struct timer_list * timer);
-extern int  del_sltimer(struct timer_list * timer);
-extern void mod_sltimer(struct timer_list *timer, unsigned long expires);
-extern void ip_vs_sltimer_init(void);
-extern void ip_vs_sltimer_cleanup(void);
-
-
-/*
  *      ip_vs_fwd_tag returns the forwarding tag of the connection
  */
 #define IP_VS_FWD_METHOD(cp)  (cp->flags & IP_VS_CONN_F_FWD_MASK)
diff -urNp linux-7055/kernel/ksyms.c linux-7060/kernel/ksyms.c
--- linux-7055/kernel/ksyms.c
+++ linux-7060/kernel/ksyms.c
@@ -417,6 +417,7 @@ EXPORT_SYMBOL(proc_doulongvec_minmax);
 /* interrupt handling */
 EXPORT_SYMBOL(add_timer);
 EXPORT_SYMBOL(del_timer);
+EXPORT_SYMBOL(timer_pending);
 EXPORT_SYMBOL(request_irq);
 EXPORT_SYMBOL(free_irq);
 #if !defined(CONFIG_IA64)	/* irq_stat is part of struct cpuinfo_ia64 */
diff -urNp linux-7055/kernel/timer.c linux-7060/kernel/timer.c
--- linux-7055/kernel/timer.c
+++ linux-7060/kernel/timer.c
@@ -13,6 +13,9 @@
  *              serialize accesses to xtime/lost_ticks).
  *                              Copyright (C) 1998  Andrea Arcangeli
  *  1999-03-10  Improved NTP compatibility by Ulrich Windl
+ *  2000-10-05  Implemented scalable SMP per-CPU timer handling.
+ *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar
+ *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar
  */
 
 #include <linux/config.h>
@@ -22,6 +25,7 @@
 #include <linux/smp_lock.h>
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>
+#include <linux/init.h>
 
 #include <asm/uaccess.h>
 
@@ -68,9 +72,8 @@ unsigned long event;
 extern int do_setitimer(int, struct itimerval *, struct itimerval *);
 
 unsigned long volatile jiffies __cacheline_aligned;
-
 /*
- * Event timer code
+ * per-CPU timer vector definitions:
  */
 #define TVN_BITS 6
 #define TVR_BITS 8
@@ -79,183 +82,318 @@ unsigned long volatile jiffies __cacheli
 #define TVN_MASK (TVN_SIZE - 1)
 #define TVR_MASK (TVR_SIZE - 1)
 
-struct timer_vec {
-	int index;
+#define TIMER_MAGIC	0x4b87ad6e
+
+typedef struct tvec_s {
 	struct list_head vec[TVN_SIZE];
-};
+} tvec_t;
 
-struct timer_vec_root {
-	int index;
+typedef struct tvec_root_s {
 	struct list_head vec[TVR_SIZE];
-};
+} tvec_root_t;
 
-static struct timer_vec tv5;
-static struct timer_vec tv4;
-static struct timer_vec tv3;
-static struct timer_vec tv2;
-static struct timer_vec_root tv1;
+struct tvec_t_base_s {
+	spinlock_t lock;
+	unsigned long timer_jiffies;
+	struct timer_list *running_timer;
+	tvec_root_t tv1;
+	tvec_t tv2;
+	tvec_t tv3;
+	tvec_t tv4;
+	tvec_t tv5;
+	unsigned int magic;
+} ____cacheline_aligned_in_smp;
 
-static struct timer_vec * const tvecs[] = {
-	(struct timer_vec *)&tv1, &tv2, &tv3, &tv4, &tv5
-};
+typedef struct tvec_t_base_s tvec_base_t;
 
-static struct list_head * run_timer_list_running;
+static inline void set_running_timer(tvec_base_t *base,
+					struct timer_list *timer)
+{
+#ifdef CONFIG_SMP
+	base->running_timer = timer;
+#endif
+}
 
-#define NOOF_TVECS (sizeof(tvecs) / sizeof(tvecs[0]))
+static tvec_base_t tvec_bases[NR_CPUS] =
+	{ [ 0 ... NR_CPUS-1] = { .lock = SPIN_LOCK_UNLOCKED} };
 
-void init_timervecs (void)
+static inline void check_kernel_timer(struct timer_list *timer)
 {
-	int i;
+	tvec_base_t *base = timer->base;
 
-	for (i = 0; i < TVN_SIZE; i++) {
-		INIT_LIST_HEAD(tv5.vec + i);
-		INIT_LIST_HEAD(tv4.vec + i);
-		INIT_LIST_HEAD(tv3.vec + i);
-		INIT_LIST_HEAD(tv2.vec + i);
-	}
-	for (i = 0; i < TVR_SIZE; i++)
-		INIT_LIST_HEAD(tv1.vec + i);
+	BUG_ON(timer->magic);
+	BUG_ON(base && (base->magic != TIMER_MAGIC));
 }
 
-static unsigned long timer_jiffies;
-
-static inline void internal_add_timer(struct timer_list *timer)
+static void internal_add_timer(tvec_base_t *base, struct timer_list *timer)
 {
-	/*
-	 * must be cli-ed when calling this
-	 */
 	unsigned long expires = timer->expires;
-	unsigned long idx = expires - timer_jiffies;
-	struct list_head * vec;
+	unsigned long idx = expires - base->timer_jiffies;
+	struct list_head *vec;
 
-	if (run_timer_list_running)
-		vec = run_timer_list_running;
-	else if (idx < TVR_SIZE) {
+	if (idx < TVR_SIZE) {
 		int i = expires & TVR_MASK;
-		vec = tv1.vec + i;
+		vec = base->tv1.vec + i;
 	} else if (idx < 1 << (TVR_BITS + TVN_BITS)) {
 		int i = (expires >> TVR_BITS) & TVN_MASK;
-		vec = tv2.vec + i;
+		vec = base->tv2.vec + i;
 	} else if (idx < 1 << (TVR_BITS + 2 * TVN_BITS)) {
 		int i = (expires >> (TVR_BITS + TVN_BITS)) & TVN_MASK;
-		vec =  tv3.vec + i;
+		vec = base->tv3.vec + i;
 	} else if (idx < 1 << (TVR_BITS + 3 * TVN_BITS)) {
 		int i = (expires >> (TVR_BITS + 2 * TVN_BITS)) & TVN_MASK;
-		vec = tv4.vec + i;
+		vec = base->tv4.vec + i;
 	} else if ((signed long) idx < 0) {
-		/* can happen if you add a timer with expires == jiffies,
+		/*
+		 * Can happen if you add a timer with expires == jiffies,
 		 * or you set a timer to go off in the past
 		 */
-		vec = tv1.vec + tv1.index;
-	} else if (idx <= 0xffffffffUL) {
-		int i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
-		vec = tv5.vec + i;
+		vec = base->tv1.vec + (base->timer_jiffies & TVR_MASK);
 	} else {
-		/* Can only get here on architectures with 64-bit jiffies */
-		INIT_LIST_HEAD(&timer->list);
-		return;
+		int i;
+		/* If the timeout is larger than 0xffffffff on 64-bit
+		 * architectures then we use the maximum timeout:
+		 */
+		if (idx > 0xffffffffUL) {
+			idx = 0xffffffffUL;
+			expires = idx + base->timer_jiffies;
+		}
+		i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
+		vec = base->tv5.vec + i;
 	}
 	/*
-	 * Timers are FIFO!
+	 * Timers are FIFO:
 	 */
-	list_add(&timer->list, vec->prev);
+	list_add_tail(&timer->entry, vec);
 }
 
-/* Initialize both explicitly - let's try to have them in the same cache line */
-spinlock_t timerlist_lock = SPIN_LOCK_UNLOCKED;
-
-#ifdef CONFIG_SMP
-volatile struct timer_list * volatile running_timer;
-#define timer_enter(t) do { running_timer = t; mb(); } while (0)
-#define timer_exit() do { running_timer = NULL; } while (0)
-#define timer_is_running(t) (running_timer == t)
-#define timer_synchronize(t) while (timer_is_running(t)) barrier()
-#else
-#define timer_enter(t)		do { } while (0)
-#define timer_exit()		do { } while (0)
-#endif
-
-void add_timer(struct timer_list *timer)
+static inline int trylock_timer(struct timer_list *timer)
 {
-	unsigned long flags;
+	return test_and_set_bit(0, &timer->lock);
+}
 
-	spin_lock_irqsave(&timerlist_lock, flags);
-	if (timer_pending(timer))
-		goto bug;
-	internal_add_timer(timer);
-	spin_unlock_irqrestore(&timerlist_lock, flags);
-	return;
-bug:
-	spin_unlock_irqrestore(&timerlist_lock, flags);
-	printk("bug: kernel timer added twice at %p.\n",
-			__builtin_return_address(0));
+static inline void lock_timer(struct timer_list *timer)
+{
+	while (unlikely(trylock_timer(timer)))
+		cpu_relax();
 }
 
-static inline int detach_timer (struct timer_list *timer)
+static inline void unlock_timer(struct timer_list *timer)
 {
-	if (!timer_pending(timer))
-		return 0;
-	list_del(&timer->list);
-	return 1;
+	if (unlikely(!test_and_clear_bit(0, &timer->lock)))
+		BUG();
 }
 
-int mod_timer(struct timer_list *timer, unsigned long expires)
+static int __mod_timer(struct timer_list *timer, unsigned long expires)
 {
-	int ret;
+	tvec_base_t *old_base, *new_base;
 	unsigned long flags;
+	int ret = 0;
 
-	spin_lock_irqsave(&timerlist_lock, flags);
+	local_irq_save(flags);
+	lock_timer(timer);
+	new_base = tvec_bases + smp_processor_id();
+repeat:
+	old_base = timer->base;
+
+	/*
+	 * Prevent deadlocks via ordering by old_base < new_base.
+	 */
+	if (old_base && (new_base != old_base)) {
+		if (old_base < new_base) {
+			spin_lock(&new_base->lock);
+			spin_lock(&old_base->lock);
+		} else {
+			spin_lock(&old_base->lock);
+			spin_lock(&new_base->lock);
+		}
+		/*
+		 * The timer base might have been cancelled while we were
+		 * trying to take the lock(s):
+		 */
+		if (timer->base != old_base) {
+			spin_unlock(&new_base->lock);
+			spin_unlock(&old_base->lock);
+			goto repeat;
+		}
+	} else {
+		spin_lock(&new_base->lock);
+		if (timer->base != old_base) {
+			spin_unlock(&new_base->lock);
+			goto repeat;
+		}
+	}
+
+	/*
+	 * Delete the previous timeout (if there was any), and install
+	 * the new one:
+	 */
+	if (old_base) {
+		check_kernel_timer(timer);
+		list_del(&timer->entry);
+		ret = 1;
+	}
 	timer->expires = expires;
-	ret = detach_timer(timer);
-	internal_add_timer(timer);
-	spin_unlock_irqrestore(&timerlist_lock, flags);
+	internal_add_timer(new_base, timer);
+	timer->base = new_base;
+
+	if (old_base && (new_base != old_base))
+		spin_unlock(&old_base->lock);
+	spin_unlock(&new_base->lock);
+	unlock_timer(timer);
+	local_irq_restore(flags);
+
 	return ret;
 }
 
-int del_timer(struct timer_list * timer)
-{
-	int ret;
-	unsigned long flags;
 
-	spin_lock_irqsave(&timerlist_lock, flags);
-	ret = detach_timer(timer);
-	timer->list.next = timer->list.prev = NULL;
-	spin_unlock_irqrestore(&timerlist_lock, flags);
-	return ret;
+/***
+ * timer_pending - is a timer pending?
+ * @timer: the timer in question
+ *
+ * timer_pending will tell whether a given timer is currently pending,
+ * or not. Callers must ensure serialization wrt. other operations done
+ * to this timer, eg. interrupt contexts, or other CPUs on SMP.
+ *
+ * return value: 1 if the timer is pending, 0 if not.
+ */
+inline int timer_pending(struct timer_list * timer)
+{
+	check_kernel_timer(timer);
+	return timer->base != NULL;
 }
 
-#ifdef CONFIG_SMP
-void sync_timers(void)
+/***
+ * add_timer - start a timer
+ * @timer: the timer to be added
+ *
+ * The kernel will do a ->function(->data) callback from the
+ * timer interrupt at the ->expired point in the future. The
+ * current time is 'jiffies'.
+ *
+ * The timer's ->expired, ->function (and if the handler uses it, ->data)
+ * fields must be set prior calling this function.
+ *
+ * Timers with an ->expired field in the past will be executed in the next
+ * timer tick. It's illegal to add an already pending timer.
+ */
+void add_timer(struct timer_list *timer)
 {
-	spin_unlock_wait(&global_bh_lock);
+  	BUG_ON(timer_pending(timer) || !timer->function);
+	check_kernel_timer(timer);
+
+	__mod_timer(timer, timer->expires);
 }
 
-/*
- * SMP specific function to delete periodic timer.
- * Caller must disable by some means restarting the timer
- * for new. Upon exit the timer is not queued and handler is not running
- * on any CPU. It returns number of times, which timer was deleted
- * (for reference counting).
+/***
+ * mod_timer - modify a timer's timeout
+ * @timer: the timer to be modified
+ *
+ * mod_timer is a more efficient way to update the expire field of an
+ * active timer (if the timer is inactive it will be activated)
+ *
+ * mod_timer(timer, expires) is equivalent to:
+ *
+ *     del_timer(timer); timer->expires = expires; add_timer(timer);
+ *
+ * Note that if there are multiple unserialized concurrent users of the
+ * same timer, then mod_timer() is the only safe way to modify the timeout,
+ * since add_timer() cannot modify an already running timer.
+ *
+ * The function returns whether it has modified a pending timer or not.
+ * (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an
+ * active timer returns 1.)
  */
+int mod_timer(struct timer_list *timer, unsigned long expires)
+{
+	BUG_ON(!timer->function);
+	check_kernel_timer(timer);
 
-int del_timer_sync(struct timer_list * timer)
+	/*
+	 * This is a common optimization triggered by the
+	 * networking code - if the timer is re-modified
+	 * to be the same thing then just return:
+	 */
+	if (timer->expires == expires && timer_pending(timer))
+		return 1;
+
+	return __mod_timer(timer, expires);
+}
+
+/***
+ * del_timer - deactive a timer.
+ * @timer: the timer to be deactivated
+ *
+ * del_timer() deactivates a timer - this works on both active and inactive
+ * timers.
+ *
+ * The function returns whether it has deactivated a pending timer or not.
+ * (ie. del_timer() of an inactive timer returns 0, del_timer() of an
+ * active timer returns 1.)
+ */
+int del_timer(struct timer_list *timer)
 {
-	int ret = 0;
+	unsigned long flags;
+	tvec_base_t *base;
 
-	for (;;) {
-		unsigned long flags;
-		int running;
+	check_kernel_timer(timer);
+
+repeat:
+ 	base = timer->base;
+	if (!base)
+		return 0;
+	spin_lock_irqsave(&base->lock, flags);
+	if (base != timer->base) {
+		spin_unlock_irqrestore(&base->lock, flags);
+		goto repeat;
+	}
+	list_del(&timer->entry);
+	smp_wmb();
+	timer->base = NULL;
+	spin_unlock_irqrestore(&base->lock, flags);
+
+	return 1;
+}
 
-		spin_lock_irqsave(&timerlist_lock, flags);
-		ret += detach_timer(timer);
-		timer->list.next = timer->list.prev = 0;
-		running = timer_is_running(timer);
-		spin_unlock_irqrestore(&timerlist_lock, flags);
+#ifdef CONFIG_SMP
+/***
+ * del_timer_sync - deactivate a timer and wait for the handler to finish.
+ * @timer: the timer to be deactivated
+ *
+ * This function only differs from del_timer() on SMP: besides deactivating
+ * the timer it also makes sure the handler has finished executing on other
+ * CPUs.
+ *
+ * Synchronization rules: callers must prevent restarting of the timer,
+ * otherwise this function is meaningless. It must not be called from
+ * interrupt contexts. Upon exit the timer is not queued and the handler
+ * is not running on any CPU.
+ *
+ * The function returns whether it has deactivated a pending timer or not.
+ */
+int del_timer_sync(struct timer_list *timer)
+{
+	tvec_base_t *base;
+	int i, ret;
 
-		if (!running)
-			break;
+del_again:
+	ret = del_timer(timer);
 
-		timer_synchronize(timer);
+	if (unlikely(!ret)) {
+		for (i = 0; i < NR_CPUS; i++) {
+			if (!cpu_online(i))
+				continue;
+
+			base = tvec_bases + i;
+			if (base->running_timer == timer) {
+				while (base->running_timer == timer)
+					cpu_relax();
+				break;
+			}
+		}
+		smp_rmb();
+		if (timer_pending(timer))
+			goto del_again;
 	}
 
 	return ret;
@@ -263,78 +401,82 @@ int del_timer_sync(struct timer_list * t
 #endif
 
 
-static inline void cascade_timers(struct timer_vec *tv)
+static int cascade(tvec_base_t *base, tvec_t *tv, int index)
 {
 	/* cascade all the timers from tv up one level */
-	struct list_head *head, *curr, *next;
+	struct list_head *head, *curr;
 
-	head = tv->vec + tv->index;
+	head = tv->vec + index;
 	curr = head->next;
 	/*
 	 * We are removing _all_ timers from the list, so we don't  have to
 	 * detach them individually, just clear the list afterwards.
 	 */
 	while (curr != head) {
-		struct timer_list *tmp;
+		struct timer_list *timer;
 
-		tmp = list_entry(curr, struct timer_list, list);
-		next = curr->next;
-		list_del(curr); // not needed
-		internal_add_timer(tmp);
-		curr = next;
+		timer = list_entry(curr, struct timer_list, entry);
+		BUG_ON(timer->base != base);
+		check_kernel_timer(timer);
+		curr = curr->next;
+		internal_add_timer(base, timer);
 	}
 	INIT_LIST_HEAD(head);
-	tv->index = (tv->index + 1) & TVN_MASK;
+
+	return index;
 }
 
-static inline void run_timer_list(void)
+/***
+ * __run_timers - run all expired timers (if any) on this CPU.
+ * @base: the timer vector to be processed.
+ *
+ * This function cascades all vectors and executes all expired timer
+ * vectors.
+ */
+#define INDEX(N) (base->timer_jiffies >> (TVR_BITS + N * TVN_BITS)) & TVN_MASK
+
+static inline void __run_timers(tvec_base_t *base)
 {
-	spin_lock_irq(&timerlist_lock);
-	while ((long)(jiffies - timer_jiffies) >= 0) {
-		LIST_HEAD(queued);
-		struct list_head *head, *curr;
-		if (!tv1.index) {
-			int n = 1;
-			do {
-				cascade_timers(tvecs[n]);
-			} while (tvecs[n]->index == 1 && ++n < NOOF_TVECS);
-		}
-		run_timer_list_running = &queued;
+	struct timer_list *timer;
+
+	spin_lock_irq(&base->lock);
+	while (time_after_eq(jiffies, base->timer_jiffies)) {
+		struct list_head work_list = LIST_HEAD_INIT(work_list);
+		struct list_head *head = &work_list;
+ 		int index = base->timer_jiffies & TVR_MASK;
+ 
+		/*
+		 * Cascade timers:
+		 */
+		if (!index &&
+			(!cascade(base, &base->tv2, INDEX(0))) &&
+				(!cascade(base, &base->tv3, INDEX(1))) &&
+					!cascade(base, &base->tv4, INDEX(2)))
+			cascade(base, &base->tv5, INDEX(3));
+		++base->timer_jiffies; 
+		list_splice_init(base->tv1.vec + index, &work_list);
 repeat:
-		head = tv1.vec + tv1.index;
-		curr = head->next;
-		if (curr != head) {
-			struct timer_list *timer;
+		if (!list_empty(head)) {
 			void (*fn)(unsigned long);
 			unsigned long data;
 
-			timer = list_entry(curr, struct timer_list, list);
+			timer = list_entry(head->next,struct timer_list,entry);
+			check_kernel_timer(timer);
  			fn = timer->function;
- 			data= timer->data;
+ 			data = timer->data;
 
-			detach_timer(timer);
-			timer->list.next = timer->list.prev = NULL;
-			timer_enter(timer);
-			spin_unlock_irq(&timerlist_lock);
+			list_del(&timer->entry);
+			set_running_timer(base, timer);
+			smp_wmb();
+			timer->base = NULL;
+			spin_unlock_irq(&base->lock);
 			fn(data);
-			spin_lock_irq(&timerlist_lock);
-			timer_exit();
+			spin_lock_irq(&base->lock);
 			goto repeat;
 		}
-		run_timer_list_running = NULL;
-		++timer_jiffies; 
-		tv1.index = (tv1.index + 1) & TVR_MASK;
-
-		curr = queued.next;
-		while (curr != &queued) {
-			struct timer_list *timer;
-
-			timer = list_entry(curr, struct timer_list, list);
-			curr = curr->next;
-			internal_add_timer(timer);
-		}			
 	}
-	spin_unlock_irq(&timerlist_lock);
+	set_running_timer(base, NULL);
+	spin_unlock_irq(&base->lock);
 }
 
 spinlock_t tqueue_lock = SPIN_LOCK_UNLOCKED;
@@ -638,33 +780,6 @@ static inline void calc_load(unsigned lo
 /* jiffies at the most recent update of wall time */
 unsigned long wall_jiffies;
 
-static inline void update_times(void)
-{
-	unsigned long ticks;
-
-	/*
-	 * update_times() is run from the raw timer_bh handler so we
-	 * just know that the irqs are locally enabled and so we don't
-	 * need to save/restore the flags of the local CPU here. -arca
-	 */
-	br_write_lock_irq(BR_XTIME_LOCK);
-	vxtime_lock();
-
-	ticks = jiffies - wall_jiffies;
-	if (ticks) {
-		wall_jiffies += ticks;
-		update_wall_time(ticks);
-	}
-	vxtime_unlock();
-	br_write_unlock_irq(BR_XTIME_LOCK);
-	calc_load(ticks);
-}
-
-void timer_bh(void)
-{
-	update_times();
-	run_timer_list();
-}
 
 void do_timer(struct pt_regs *regs)
 {
@@ -924,3 +1039,63 @@ asmlinkage long sys_nanosleep(struct tim
 	}
 	return 0;
 }
+
+static inline void update_times(void)
+{
+	unsigned long ticks;
+
+	/*
+	 * update_times() is run from the raw timer_bh handler so we
+	 * just know that the irqs are locally enabled and so we don't
+	 * need to save/restore the flags of the local CPU here. -arca
+	 */
+	br_write_lock_irq(BR_XTIME_LOCK);
+	vxtime_lock();
+
+	ticks = jiffies - wall_jiffies;
+	if (ticks) {
+		wall_jiffies += ticks;
+		update_wall_time(ticks);
+	}
+	vxtime_unlock();
+	br_write_unlock_irq(BR_XTIME_LOCK);
+	calc_load(ticks);
+}
+
+void timer_bh(void)
+{
+	int i;
+
+	update_times();
+
+	for (i = 0; i < smp_num_cpus; i++) {
+		tvec_base_t *base = tvec_bases + i;
+		if (time_after_eq(jiffies, base->timer_jiffies))
+			__run_timers(base);
+	}
+}
+
+/* dummy variable - keep it around for compatibility */
+spinlock_t timerlist_lock = SPIN_LOCK_UNLOCKED;
+
+void __init init_timervecs(void)
+{
+	int i, j;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		tvec_base_t *base = tvec_bases + i;
+
+		spin_lock_init(&base->lock);
+		base->magic = TIMER_MAGIC;
+		for (j = 0; j < TVN_SIZE; j++) {
+			INIT_LIST_HEAD(base->tv5.vec + j);
+			INIT_LIST_HEAD(base->tv4.vec + j);
+			INIT_LIST_HEAD(base->tv3.vec + j);
+			INIT_LIST_HEAD(base->tv2.vec + j);
+		}
+		for (j = 0; j < TVR_SIZE; j++)
+			INIT_LIST_HEAD(base->tv1.vec + j);
+
+		base->timer_jiffies = jiffies;
+	}
+}
diff -urNp linux-7055/net/ipv4/ipvs/Makefile linux-7060/net/ipv4/ipvs/Makefile
--- linux-7055/net/ipv4/ipvs/Makefile
+++ linux-7060/net/ipv4/ipvs/Makefile
@@ -12,7 +12,7 @@ O_TARGET :=	ipvs.o
 export-objs :=	ip_vs_core.o ip_vs_app.o
 
 ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o \
-		ip_vs_timer.o ip_vs_app.o ip_vs_sync.o ip_vs_est.o
+		ip_vs_app.o ip_vs_sync.o ip_vs_est.o
 
 ifeq ($(CONFIG_IP_VS),y)
   obj-y := $(ip_vs-objs)
diff -urNp linux-7055/net/ipv4/ipvs/ip_vs_conn.c linux-7060/net/ipv4/ipvs/ip_vs_conn.c
--- linux-7055/net/ipv4/ipvs/ip_vs_conn.c
+++ linux-7060/net/ipv4/ipvs/ip_vs_conn.c
@@ -301,7 +301,7 @@ struct ip_vs_conn *ip_vs_conn_out_get
 void ip_vs_conn_put(struct ip_vs_conn *cp)
 {
 	/* reset it expire in its timeout */
-	mod_sltimer(&cp->timer, jiffies+cp->timeout);
+	mod_timer(&cp->conn_timer, jiffies+cp->timeout);
 
 	__ip_vs_conn_put(cp);
 }
@@ -1222,8 +1222,8 @@ static void ip_vs_conn_expire(unsigned l
 	 */
 	if (likely(atomic_read(&cp->refcnt) == 1)) {
 		/* make sure that there is no timer on it now */
-		if (timer_pending(&cp->timer))
-			del_sltimer(&cp->timer);
+		if (timer_pending(&cp->conn_timer))
+			del_timer(&cp->conn_timer);
 
 		/* does anybody control me? */
 		if (cp->control)
@@ -1255,7 +1255,7 @@ static void ip_vs_conn_expire(unsigned l
 void ip_vs_conn_expire_now(struct ip_vs_conn *cp)
 {
 	cp->timeout = 0;
-	mod_sltimer(&cp->timer, jiffies);
+	mod_timer(&cp->conn_timer, jiffies);
 }
 
 /*
@@ -1276,9 +1276,9 @@ ip_vs_conn_new(int proto, __u32 caddr, _
 
 	memset(cp, 0, sizeof(*cp));
 	INIT_LIST_HEAD(&cp->c_list);
-	init_timer(&cp->timer);
-	cp->timer.data     = (unsigned long)cp;
-	cp->timer.function = ip_vs_conn_expire;
+	init_timer(&cp->conn_timer);
+	cp->conn_timer.data     = (unsigned long)cp;
+	cp->conn_timer.function = ip_vs_conn_expire;
 	ip_vs_timeout_attach(cp, ip_vs_timeout_table);
 	cp->protocol	   = proto;
 	cp->caddr	   = caddr;
@@ -1363,7 +1363,7 @@ ip_vs_conn_getinfo(char *buffer, char **
 				ntohl(cp->vaddr), ntohs(cp->vport),
 				ntohl(cp->daddr), ntohs(cp->dport),
 				ip_vs_state_name(cp->state),
-				(cp->timer.expires-jiffies)/HZ);
+				(cp->conn_timer.expires-jiffies)/HZ);
 			len += sprintf(buffer+len, "%-127s\n", temp);
 			if (pos >= offset+length) {
 				ct_read_unlock_bh(idx);
@@ -1400,7 +1400,7 @@ static inline int todrop_entry(struct ip
 	/* if the conn entry hasn't lasted for 60 seconds, don't drop it.
 	   This will leave enough time for normal connection to get
 	   through. */
-	if (cp->timeout+jiffies-cp->timer.expires < 60*HZ)
+	if (cp->timeout+jiffies-cp->conn_timer.expires < 60*HZ)
 		return 0;
 
 	/* Don't drop the entry if its number of incoming packets is not
diff -urNp linux-7055/net/ipv4/ipvs/ip_vs_core.c linux-7060/net/ipv4/ipvs/ip_vs_core.c
--- linux-7055/net/ipv4/ipvs/ip_vs_core.c
+++ linux-7060/net/ipv4/ipvs/ip_vs_core.c
@@ -1217,8 +1217,6 @@ static int __init ip_vs_init(void)
 		goto cleanup_nothing;
 	}
 
-	ip_vs_sltimer_init();
-
 	ret = ip_vs_conn_init();
 	if (ret < 0) {
 		IP_VS_ERR("can't setup connection table.\n");
@@ -1266,7 +1264,6 @@ static int __init ip_vs_init(void)
   cleanup_conn:
 	ip_vs_conn_cleanup();
   cleanup_sltimer:
-	ip_vs_sltimer_cleanup();
 	ip_vs_control_cleanup();
   cleanup_nothing:
 	return ret;
@@ -1280,7 +1277,6 @@ static void __exit ip_vs_cleanup(void)
 	nf_unregister_hook(&ip_vs_in_ops);
 	ip_vs_app_cleanup();
 	ip_vs_conn_cleanup();
-	ip_vs_sltimer_cleanup();
 	ip_vs_control_cleanup();
 	IP_VS_INFO("ipvs unloaded.\n");
 }
diff -urNp linux-7055/net/ipv4/ipvs/ip_vs_timer.c linux-7060/net/ipv4/ipvs/ip_vs_timer.c
--- linux-7055/net/ipv4/ipvs/ip_vs_timer.c
+++ linux-7060/net/ipv4/ipvs/ip_vs_timer.c
@@ -1,260 +0,0 @@
-/*
- * IPVS         An implementation of the IP virtual server support for the
- *              LINUX operating system.  IPVS is now implemented as a module
- *              over the Netfilter framework. IPVS can be used to build a
- *              high-performance and highly available server based on a
- *              cluster of servers.
- *
- * Version:     $Id: ip_vs_timer.c,v 1.8 2002/01/31 14:33:12 wensong Exp $
- *
- * Authors:     Wensong Zhang <wensong@linuxvirtualserver.org>
- *              Julian Anastasov <ja@ssi.bg>
- *
- *              This program is free software; you can redistribute it and/or
- *              modify it under the terms of the GNU General Public License
- *              as published by the Free Software Foundation; either version
- *              2 of the License, or (at your option) any later version.
- *
- * Changes:
- *
- */
-#include <linux/kernel.h>
-#include <linux/types.h>
-#include <linux/timer.h>
-
-#include <net/ip_vs.h>
-
-/*
- * The following block implements slow timers for IPVS, most code is stolen
- * from linux/kernel/timer.c.
- * Slow timer is used to avoid the overhead of cascading timers, when lots
- * of connection entries (>50,000) are cluttered in the system.
- */
-#define SHIFT_BITS 6
-#define TVN_BITS 8
-#define TVR_BITS 10
-#define TVN_SIZE (1 << TVN_BITS)
-#define TVR_SIZE (1 << TVR_BITS)
-#define TVN_MASK (TVN_SIZE - 1)
-#define TVR_MASK (TVR_SIZE - 1)
-
-struct sltimer_vec {
-	int index;
-	struct list_head vec[TVN_SIZE];
-};
-
-struct sltimer_vec_root {
-	int index;
-	struct list_head vec[TVR_SIZE];
-};
-
-static struct sltimer_vec sltv3 = { 0 };
-static struct sltimer_vec sltv2 = { 0 };
-static struct sltimer_vec_root sltv1 = { 0 };
-
-static struct sltimer_vec * const sltvecs[] = {
-	(struct sltimer_vec *)&sltv1, &sltv2, &sltv3
-};
-
-#define NOOF_SLTVECS (sizeof(sltvecs) / sizeof(sltvecs[0]))
-
-static void init_sltimervecs (void)
-{
-	int i;
-
-	for (i = 0; i < TVN_SIZE; i++) {
-		INIT_LIST_HEAD(sltv3.vec + i);
-		INIT_LIST_HEAD(sltv2.vec + i);
-	}
-	for (i = 0; i < TVR_SIZE; i++)
-		INIT_LIST_HEAD(sltv1.vec + i);
-}
-
-static unsigned long sltimer_jiffies = 0;
-
-static inline void internal_add_sltimer(struct timer_list *timer)
-{
-	/*
-	 * must hold the sltimer lock when calling this
-	 */
-	unsigned long expires = timer->expires;
-	unsigned long idx = expires - sltimer_jiffies;
-	struct list_head * vec;
-
-	if (idx < 1 << (SHIFT_BITS + TVR_BITS)) {
-		int i = (expires >> SHIFT_BITS) & TVR_MASK;
-		vec = sltv1.vec + i;
-	} else if (idx < 1 << (SHIFT_BITS + TVR_BITS + TVN_BITS)) {
-		int i = (expires >> (SHIFT_BITS+TVR_BITS)) & TVN_MASK;
-		vec = sltv2.vec + i;
-	} else if ((signed long) idx < 0) {
-		/*
-		 * can happen if you add a timer with expires == jiffies,
-		 * or you set a timer to go off in the past
-		 */
-		vec = sltv1.vec + sltv1.index;
-	} else if (idx <= 0xffffffffUL) {
-		int i = (expires >> (SHIFT_BITS+TVR_BITS+TVN_BITS)) & TVN_MASK;
-		vec = sltv3.vec + i;
-	} else {
-		/* Can only get here on architectures with 64-bit jiffies */
-		INIT_LIST_HEAD(&timer->list);
-	}
-	/*
-	 * Timers are FIFO!
-	 */
-	list_add(&timer->list, vec->prev);
-}
-
-
-static spinlock_t __ip_vs_sltimerlist_lock = SPIN_LOCK_UNLOCKED;
-
-void add_sltimer(struct timer_list *timer)
-{
-	spin_lock(&__ip_vs_sltimerlist_lock);
-	if (timer->list.next)
-		goto bug;
-	internal_add_sltimer(timer);
-  out:
-	spin_unlock(&__ip_vs_sltimerlist_lock);
-	return;
-
-  bug:
-	printk("bug: kernel sltimer added twice at %p.\n",
-	       __builtin_return_address(0));
-	goto out;
-}
-
-static inline int detach_sltimer(struct timer_list *timer)
-{
-	if (!timer_pending(timer))
-		return 0;
-	list_del(&timer->list);
-	return 1;
-}
-
-void mod_sltimer(struct timer_list *timer, unsigned long expires)
-{
-	int ret;
-
-	spin_lock(&__ip_vs_sltimerlist_lock);
-	timer->expires = expires;
-	ret = detach_sltimer(timer);
-	internal_add_sltimer(timer);
-	spin_unlock(&__ip_vs_sltimerlist_lock);
-}
-
-int del_sltimer(struct timer_list * timer)
-{
-	int ret;
-
-	spin_lock(&__ip_vs_sltimerlist_lock);
-	ret = detach_sltimer(timer);
-	timer->list.next = timer->list.prev = 0;
-	spin_unlock(&__ip_vs_sltimerlist_lock);
-	return ret;
-}
-
-
-static inline void cascade_sltimers(struct sltimer_vec *tv)
-{
-	/*
-	 * cascade all the timers from tv up one level
-	 */
-	struct list_head *head, *curr, *next;
-
-	head = tv->vec + tv->index;
-	curr = head->next;
-
-	/*
-	 * We are removing _all_ timers from the list, so we don't  have to
-	 * detach them individually, just clear the list afterwards.
-	 */
-	while (curr != head) {
-		struct timer_list *tmp;
-
-		tmp = list_entry(curr, struct timer_list, list);
-		next = curr->next;
-		list_del(curr); // not needed
-		internal_add_sltimer(tmp);
-		curr = next;
-	}
-	INIT_LIST_HEAD(head);
-	tv->index = (tv->index + 1) & TVN_MASK;
-}
-
-static inline void run_sltimer_list(void)
-{
-	spin_lock(&__ip_vs_sltimerlist_lock);
-	while ((long)(jiffies - sltimer_jiffies) >= 0) {
-		struct list_head *head, *curr;
-		if (!sltv1.index) {
-			int n = 1;
-			do {
-				cascade_sltimers(sltvecs[n]);
-			} while (sltvecs[n]->index == 1 && ++n < NOOF_SLTVECS);
-		}
-	  repeat:
-		head = sltv1.vec + sltv1.index;
-		curr = head->next;
-		if (curr != head) {
-			struct timer_list *timer;
-			void (*fn)(unsigned long);
-			unsigned long data;
-
-			timer = list_entry(curr, struct timer_list, list);
-			fn = timer->function;
-			data= timer->data;
-
-			detach_sltimer(timer);
-			timer->list.next = timer->list.prev = NULL;
-			spin_unlock(&__ip_vs_sltimerlist_lock);
-			fn(data);
-			spin_lock(&__ip_vs_sltimerlist_lock);
-			goto repeat;
-		}
-		sltimer_jiffies += 1<<SHIFT_BITS;
-		sltv1.index = (sltv1.index + 1) & TVR_MASK;
-	}
-	spin_unlock(&__ip_vs_sltimerlist_lock);
-}
-
-static struct timer_list slow_timer;
-
-/*
- *  Slow timer handler is activated every second
- */
-#define SLTIMER_PERIOD	     1*HZ
-
-static void sltimer_handler(unsigned long data)
-{
-	run_sltimer_list();
-
-	update_defense_level();
-	if (atomic_read(&ip_vs_dropentry))
-		ip_vs_random_dropentry();
-
-	mod_timer(&slow_timer, (jiffies + SLTIMER_PERIOD));
-}
-
-
-void ip_vs_sltimer_init(void)
-{
-	/*
-	 * Hook the slow_timer handler in the system timer.
-	 */
-	init_sltimervecs();
-
-	sltimer_jiffies = jiffies;
-
-	init_timer(&slow_timer);
-	slow_timer.function = sltimer_handler;
-	slow_timer.expires = jiffies+SLTIMER_PERIOD;
-	add_timer(&slow_timer);
-}
-
-
-void ip_vs_sltimer_cleanup(void)
-{
-	del_timer_sync(&slow_timer);
-}
