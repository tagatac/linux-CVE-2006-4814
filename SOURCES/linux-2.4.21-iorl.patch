diff -urNp linux-871/drivers/block/ll_rw_blk.c linux-880/drivers/block/ll_rw_blk.c
--- linux-871/drivers/block/ll_rw_blk.c
+++ linux-880/drivers/block/ll_rw_blk.c
@@ -401,9 +401,9 @@ void generic_unplug_device(void *data)
 	request_queue_t *q = (request_queue_t *) data;
 	unsigned long flags;
 
-	spin_lock_irqsave(&io_request_lock, flags);
+	spin_lock_irqsave(q->queue_lock, flags);
 	__generic_unplug_device(q);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 }
 
 /** blk_grow_request_list
@@ -427,7 +427,7 @@ int blk_grow_request_list(request_queue_
 	 * this causes system hangs during boot.
 	 * As a temporary fix, make the function non-blocking.
 	 */
-	spin_lock_irqsave(&io_request_lock, flags);
+	spin_lock_irqsave(q->queue_lock, flags);
 	while (q->nr_requests < nr_requests) {
 		struct request *rq;
 		int rw;
@@ -445,7 +445,7 @@ int blk_grow_request_list(request_queue_
 	q->batch_requests = q->nr_requests / 4;
 	if (q->batch_requests > 32)
 		q->batch_requests = 32;
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 	return q->nr_requests;
 }
 
@@ -472,7 +472,6 @@ static void blk_init_free_list(request_q
 
 	init_waitqueue_head(&q->wait_for_requests[0]);
 	init_waitqueue_head(&q->wait_for_requests[1]);
-	spin_lock_init(&q->queue_lock);
 }
 
 static int __make_request(request_queue_t * q, int rw, struct buffer_head * bh);
@@ -514,6 +513,7 @@ void blk_init_queue(request_queue_t * q,
 {
 	INIT_LIST_HEAD(&q->queue_head);
 	elevator_init(&q->elevator, ELEVATOR_LINUS);
+	q->queue_lock		= &io_request_lock;
 	blk_init_free_list(q);
 	q->request_fn     	= rfn;
 	q->back_merge_fn       	= ll_back_merge_fn;
@@ -632,9 +632,9 @@ static struct request *__get_request_wai
 		generic_unplug_device(q);
 		if (q->rq[rw].count == 0)
 			schedule();
-		spin_lock_irq(&io_request_lock);
+		spin_lock_irq(q->queue_lock);
 		rq = get_request(q, rw);
-		spin_unlock_irq(&io_request_lock);
+		spin_unlock_irq(q->queue_lock);
 	} while (rq == NULL);
 	remove_wait_queue(&q->wait_for_requests[rw], &wait);
 	current->state = TASK_RUNNING;
@@ -837,7 +837,7 @@ static inline void add_request(request_q
 	drive_stat_acct(req->rq_dev, req->cmd, req->nr_sectors, 1);
 
 	if (!q->plugged && q->head_active && insert_here == &q->queue_head) {
-		spin_unlock_irq(&io_request_lock);
+		spin_unlock_irq(q->queue_lock);
 		BUG();
 	}
 
@@ -1019,7 +1019,7 @@ again:
 	 * Now we acquire the request spinlock, we have to be mega careful
 	 * not to schedule or do something nonatomic
 	 */
-	spin_lock_irq(&io_request_lock);
+	spin_lock_irq(q->queue_lock);
 
 	insert_here = head->prev;
 	if (list_empty(head)) {
@@ -1106,7 +1106,7 @@ get_rq:
 		 */
 		if (rw_ahead) {
 			if (q->rq[rw].count < q->batch_requests) {
-				spin_unlock_irq(&io_request_lock);
+				spin_unlock_irq(q->queue_lock);
 				goto end_io;
 			}
 			req = get_request(q, rw);
@@ -1115,7 +1115,7 @@ get_rq:
 		} else {
 			req = get_request(q, rw);
 			if (req == NULL) {
-				spin_unlock_irq(&io_request_lock);
+				spin_unlock_irq(q->queue_lock);
 				freereq = __get_request_wait(q, rw);
 				goto again;
 			}
@@ -1140,7 +1140,7 @@ get_rq:
 out:
 	if (freereq)
 		blkdev_release_request(freereq);
-	spin_unlock_irq(&io_request_lock);
+	spin_unlock_irq(q->queue_lock);
 	return 0;
 end_io:
 	bh->b_end_io(bh, test_bit(BH_Uptodate, &bh->b_state));
diff -urNp linux-871/drivers/scsi/aic7xxx_old.c linux-880/drivers/scsi/aic7xxx_old.c
--- linux-871/drivers/scsi/aic7xxx_old.c
+++ linux-880/drivers/scsi/aic7xxx_old.c
@@ -7012,6 +7012,12 @@ do_aic7xxx_isr(int irq, void *dev_id, st
   if(!p)
     return;
   spin_lock_irqsave(&io_request_lock, cpu_flags);
+  if(p->flags & AHC_IN_ISR)
+  {
+    aic7xxx_isr(irq, dev_id, regs);
+    spin_unlock_irqrestore(&io_request_lock, cpu_flags);
+    return;
+  }
   p->flags |= AHC_IN_ISR;
   do
   {
diff -urNp linux-871/drivers/scsi/hosts.c linux-880/drivers/scsi/hosts.c
--- linux-871/drivers/scsi/hosts.c
+++ linux-880/drivers/scsi/hosts.c
@@ -148,11 +148,12 @@ struct Scsi_Host * scsi_register(Scsi_Ho
 	}
     }
     atomic_set(&retval->host_active,0);
-    retval->host_busy = 0;
+    atomic_set(&retval->host_busy,0);
     retval->host_failed = 0;
     if(j > 0xffff) panic("Too many extra bytes requested\n");
     retval->extra_bytes = j;
     retval->loaded_as_module = 1;
+    retval->host_lock = &io_request_lock;
     if (flag_new) {
 	shn = (Scsi_Host_Name *) kmalloc(sizeof(Scsi_Host_Name), GFP_ATOMIC);
         if (!shn) {
diff -urNp linux-871/drivers/scsi/hosts.h linux-880/drivers/scsi/hosts.h
--- linux-871/drivers/scsi/hosts.h
+++ linux-880/drivers/scsi/hosts.h
@@ -337,9 +337,11 @@ struct Scsi_Host
     unsigned int            eh_active:1; /* Indicates the eh thread is awake and active if
                                           this is true. */
     wait_queue_head_t       host_wait;
+#define SCSI_HAS_HOST_LOCK
+    spinlock_t		  * host_lock;
     Scsi_Host_Template    * hostt;
     atomic_t                host_active; /* commands checked out */
-    volatile unsigned short host_busy;   /* commands actually active on low-level */
+    atomic_t                host_busy;   /* commands actually active on low-level */
     volatile unsigned short host_failed; /* commands that failed. */
     
 /* public: */
diff -urNp linux-871/drivers/scsi/qlogicfc.c linux-880/drivers/scsi/qlogicfc.c
--- linux-871/drivers/scsi/qlogicfc.c
+++ linux-880/drivers/scsi/qlogicfc.c
@@ -1343,7 +1343,7 @@ int isp2x00_queuecommand(Scsi_Cmnd * Cmn
 
 	num_free = QLOGICFC_REQ_QUEUE_LEN - REQ_QUEUE_DEPTH(in_ptr, out_ptr);
 	num_free = (num_free > 2) ? num_free - 2 : 0;
-	host->can_queue = host->host_busy + num_free;
+	host->can_queue = atomic_read(&host->host_busy) + num_free;
 	if (host->can_queue > QLOGICFC_REQ_QUEUE_LEN)
 		host->can_queue = QLOGICFC_REQ_QUEUE_LEN;
 	host->sg_tablesize = QLOGICFC_MAX_SG(num_free);
@@ -1616,7 +1616,7 @@ void isp2x00_intr_handler(int irq, void 
 
 	num_free = QLOGICFC_REQ_QUEUE_LEN - REQ_QUEUE_DEPTH(in_ptr, out_ptr);
 	num_free = (num_free > 2) ? num_free - 2 : 0;
-	host->can_queue = host->host_busy + num_free;
+	host->can_queue = atomic_read(&host->host_busy) + num_free;
 	if (host->can_queue > QLOGICFC_REQ_QUEUE_LEN)
 		host->can_queue = QLOGICFC_REQ_QUEUE_LEN;
 	host->sg_tablesize = QLOGICFC_MAX_SG(num_free);
diff -urNp linux-871/drivers/scsi/qlogicisp.c linux-880/drivers/scsi/qlogicisp.c
--- linux-871/drivers/scsi/qlogicisp.c
+++ linux-880/drivers/scsi/qlogicisp.c
@@ -957,7 +957,7 @@ int isp1020_queuecommand(Scsi_Cmnd *Cmnd
 	hostdata->req_in_ptr = in_ptr;
 
 	num_free = QLOGICISP_REQ_QUEUE_LEN - REQ_QUEUE_DEPTH(in_ptr, out_ptr);
-	host->can_queue = host->host_busy + num_free;
+	host->can_queue = atomic_read(&host->host_busy) + num_free;
 	host->sg_tablesize = QLOGICISP_MAX_SG(num_free);
 
 	LEAVE("isp1020_queuecommand");
diff -urNp linux-871/drivers/scsi/scsi.c linux-880/drivers/scsi/scsi.c
--- linux-871/drivers/scsi/scsi.c
+++ linux-880/drivers/scsi/scsi.c
@@ -197,6 +197,8 @@ void  scsi_initialize_queue(Scsi_Device 
 
 	blk_init_queue(q, scsi_request_fn);
 	blk_queue_headactive(q, 0);
+	spin_lock_init(&SDpnt->device_lock);
+	q->queue_lock = &SDpnt->device_lock;
 	q->queuedata = (void *) SDpnt;
 
 	q->max_segments = SHpnt->sg_tablesize;
@@ -556,7 +558,7 @@ inline void __scsi_release_command(Scsi_
 	 */
 	if (SCpnt->host->in_recovery
 	    && !SCpnt->host->eh_active
-	    && SCpnt->host->host_busy == SCpnt->host->host_failed) {
+	    && atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
 		SCSI_LOG_ERROR_RECOVERY(5, printk("Waking error handler thread (%d)\n",
 			     atomic_read(&SCpnt->host->eh_wait->count)));
 		up(SCpnt->host->eh_wait);
@@ -633,11 +635,12 @@ int scsi_dispatch_cmd(Scsi_Cmnd * SCpnt)
 	unsigned long clock;
 #endif
 	struct Scsi_Host *host;
+	request_queue_t *q = &SCpnt->device->request_queue;
 	int rtn = 0;
 	unsigned long flags = 0;
 	unsigned long timeout;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 #if DEBUG
 	unsigned long *ret = 0;
@@ -695,63 +698,44 @@ int scsi_dispatch_cmd(Scsi_Cmnd * SCpnt)
 
 	SCpnt->state = SCSI_STATE_QUEUED;
 	SCpnt->owner = SCSI_OWNER_LOWLEVEL;
+	/*
+	 * Before we queue this command, check if the command
+	 * length exceeds what the host adapter can handle.
+	 */
+	if (CDB_SIZE(SCpnt) > host->max_cmd_len) {
+		SCSI_LOG_MLQUEUE(3, printk("scsi_dispatch_cmd : command too long %d(Max %d).\n", CDB_SIZE(SCpnt), host->max_cmd_len));
+		SCpnt->result = (DID_ABORT << 16);
+		spin_lock_irqsave(host->host_lock, flags);
+		if (host->hostt->use_new_eh_code)
+			scsi_done(SCpnt);
+		else
+			scsi_old_done(SCpnt);
+		spin_unlock_irqrestore(host->host_lock, flags);
+		return 1;
+	}
 	if (host->can_queue) {
 		SCSI_LOG_MLQUEUE(3, printk("queuecommand : routine at %p\n",
 					   host->hostt->queuecommand));
-		/*
-		 * Use the old error handling code if we haven't converted the driver
-		 * to use the new one yet.  Note - only the new queuecommand variant
-		 * passes a meaningful return value.
-		 */
-		if (host->hostt->use_new_eh_code) {
-			/*
-			 * Before we queue this command, check if the command
-			 * length exceeds what the host adapter can handle.
-			 */
-			if (CDB_SIZE(SCpnt) <= SCpnt->host->max_cmd_len) {
-				spin_lock_irqsave(&io_request_lock, flags);
-				rtn = host->hostt->queuecommand(SCpnt, scsi_done);
-				spin_unlock_irqrestore(&io_request_lock, flags);
-				if (rtn != 0) {
-					scsi_delete_timer(SCpnt);
-					scsi_mlqueue_insert(SCpnt, SCSI_MLQUEUE_HOST_BUSY);
-					SCSI_LOG_MLQUEUE(3, printk("queuecommand : request rejected\n"));                                
-				}
-			} else {
-				SCSI_LOG_MLQUEUE(3, printk("queuecommand : command too long.\n"));
-				SCpnt->result = (DID_ABORT << 16);
-				spin_lock_irqsave(&io_request_lock, flags);
-				scsi_done(SCpnt);
-				spin_unlock_irqrestore(&io_request_lock, flags);
-				rtn = 1;
-			}
-		} else {
-			/*
-			 * Before we queue this command, check if the command
-			 * length exceeds what the host adapter can handle.
-			 */
-			if (CDB_SIZE(SCpnt) <= SCpnt->host->max_cmd_len) {
-				spin_lock_irqsave(&io_request_lock, flags);
-				host->hostt->queuecommand(SCpnt, scsi_old_done);
-				spin_unlock_irqrestore(&io_request_lock, flags);
-			} else {
-				SCSI_LOG_MLQUEUE(3, printk("queuecommand : command too long.\n"));
-				SCpnt->result = (DID_ABORT << 16);
-				spin_lock_irqsave(&io_request_lock, flags);
-				scsi_old_done(SCpnt);
-				spin_unlock_irqrestore(&io_request_lock, flags);
-				rtn = 1;
-			}
+		spin_lock_irqsave(host->host_lock, flags);
+		if (host->hostt->use_new_eh_code)
+			rtn = host->hostt->queuecommand(SCpnt, scsi_done);
+		else
+			host->hostt->queuecommand(SCpnt, scsi_old_done);
+		spin_unlock_irqrestore(host->host_lock, flags);
+		if (host->hostt->use_new_eh_code && rtn != 0) {
+			scsi_delete_timer(SCpnt);
+			scsi_mlqueue_insert(SCpnt, SCSI_MLQUEUE_HOST_BUSY);
+			SCSI_LOG_MLQUEUE(3, printk("queuecommand : request rejected\n"));                                
 		}
 	} else {
 		int temp;
 
 		SCSI_LOG_MLQUEUE(3, printk("command() :  routine at %p\n", host->hostt->command));
-                spin_lock_irqsave(&io_request_lock, flags);
+                spin_lock_irqsave(host->host_lock, flags);
 		temp = host->hostt->command(SCpnt);
 		SCpnt->result = temp;
+                spin_unlock_irqrestore(host->host_lock, flags);
 #ifdef DEBUG_DELAY
-                spin_unlock_irqrestore(&io_request_lock, flags);
 		clock = jiffies + 4 * HZ;
 		while (time_before(jiffies, clock)) {
 			barrier();
@@ -759,14 +743,8 @@ int scsi_dispatch_cmd(Scsi_Cmnd * SCpnt)
 		}
 		printk("done(host = %d, result = %04x) : routine at %p\n",
 		       host->host_no, temp, host->hostt->command);
-                spin_lock_irqsave(&io_request_lock, flags);
 #endif
-		if (host->hostt->use_new_eh_code) {
-			scsi_done(SCpnt);
-		} else {
-			scsi_old_done(SCpnt);
-		}
-                spin_unlock_irqrestore(&io_request_lock, flags);
+		scsi_done(SCpnt);
 	}
 	SCSI_LOG_MLQUEUE(3, printk("leaving scsi_dispatch_cmnd()\n"));
 	return rtn;
@@ -835,8 +813,9 @@ void scsi_do_req(Scsi_Request * SRpnt, c
 {
 	Scsi_Device * SDpnt = SRpnt->sr_device;
 	struct Scsi_Host *host = SDpnt->host;
+	request_queue_t *q = &SRpnt->sr_device->request_queue;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	SCSI_LOG_MLQUEUE(4,
 			 {
@@ -932,8 +911,9 @@ void scsi_do_req(Scsi_Request * SRpnt, c
 void scsi_init_cmd_from_req(Scsi_Cmnd * SCpnt, Scsi_Request * SRpnt)
 {
 	struct Scsi_Host *host = SCpnt->host;
+	request_queue_t *q = &SRpnt->sr_device->request_queue;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	SCpnt->owner = SCSI_OWNER_MIDLEVEL;
 	SRpnt->sr_command = SCpnt;
@@ -1022,8 +1002,9 @@ void scsi_do_cmd(Scsi_Cmnd * SCpnt, cons
 		 int timeout, int retries)
 {
 	struct Scsi_Host *host = SCpnt->host;
+	request_queue_t *q = &SCpnt->device->request_queue;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	SCpnt->pid = scsi_pid++;
 	SCpnt->owner = SCSI_OWNER_MIDLEVEL;
@@ -1190,6 +1171,10 @@ void scsi_done(Scsi_Cmnd * SCpnt)
 	 * level drivers away from using io_request_lock.   Technically they should
 	 * all use their own locking.  I am adding a small spinlock to protect
 	 * this datastructure to make it safe for that day.  (ERY)
+	 *
+	 * We do *NOT* hold the io_request_lock for certain at this point.
+	 * Don't make any assumptions, and we also don't need any other lock
+	 * besides the bh queue lock.  (DL)
 	 */
 	if (!scsi_bh_queue_head) {
 		scsi_bh_queue_head = SCpnt;
@@ -1231,6 +1216,7 @@ void scsi_done(Scsi_Cmnd * SCpnt)
  */
 void scsi_bottom_half_handler(void)
 {
+	struct Scsi_Host *host;
 	Scsi_Cmnd *SCpnt;
 	Scsi_Cmnd *SCnext;
 	unsigned long flags;
@@ -1250,12 +1236,13 @@ void scsi_bottom_half_handler(void)
 		for (; SCpnt; SCpnt = SCnext) {
 			SCnext = SCpnt->bh_next;
 
+			host = SCpnt->host;
 			switch (scsi_decide_disposition(SCpnt)) {
 			case SUCCESS:
 				/*
 				 * Add to BH queue.
 				 */
-				SCSI_LOG_MLCOMPLETE(3, printk("Command finished %d %d 0x%x\n", SCpnt->host->host_busy,
+				SCSI_LOG_MLCOMPLETE(3, printk("Command finished %d %d 0x%x\n", atomic_read(&SCpnt->host->host_busy),
 						SCpnt->host->host_failed,
 							 SCpnt->result));
 
@@ -1268,7 +1255,7 @@ void scsi_bottom_half_handler(void)
 				 * keeping track of the number of tries, so we don't end up looping,
 				 * of course.
 				 */
-				SCSI_LOG_MLCOMPLETE(3, printk("Command needs retry %d %d 0x%x\n", SCpnt->host->host_busy,
+				SCSI_LOG_MLCOMPLETE(3, printk("Command needs retry %d %d 0x%x\n", atomic_read(&SCpnt->host->host_busy),
 				SCpnt->host->host_failed, SCpnt->result));
 
 				scsi_retry_command(SCpnt);
@@ -1294,7 +1281,7 @@ void scsi_bottom_half_handler(void)
 				SCSI_LOG_MLCOMPLETE(3, printk("Command failed %p %x active=%d busy=%d failed=%d\n",
 						    SCpnt, SCpnt->result,
 				  atomic_read(&SCpnt->host->host_active),
-						  SCpnt->host->host_busy,
+				  atomic_read(&SCpnt->host->host_busy),
 					      SCpnt->host->host_failed));
 
 				/*
@@ -1312,7 +1299,7 @@ void scsi_bottom_half_handler(void)
 					 * If the host is having troubles, then look to see if this was the last
 					 * command that might have failed.  If so, wake up the error handler.
 					 */
-					if (SCpnt->host->host_busy == SCpnt->host->host_failed) {
+					if (atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
 						SCSI_LOG_ERROR_RECOVERY(5, printk("Waking error handler thread (%d)\n",
 										  atomic_read(&SCpnt->host->eh_wait->count)));
 						up(SCpnt->host->eh_wait);
@@ -1369,8 +1356,9 @@ void scsi_finish_command(Scsi_Cmnd * SCp
 	Scsi_Device *device;
 	Scsi_Request * SRpnt;
 	unsigned long flags;
+	request_queue_t *q = &SCpnt->device->request_queue;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	host = SCpnt->host;
 	device = SCpnt->device;
@@ -1382,10 +1370,8 @@ void scsi_finish_command(Scsi_Cmnd * SCp
          * one execution context, but the device and host structures are
          * shared.
          */
-	spin_lock_irqsave(&io_request_lock, flags);
-	host->host_busy--;	/* Indicate that we are free */
-	device->device_busy--;	/* Decrement device usage counter. */
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	atomic_dec(&host->host_busy);	/* Indicate that we are free */
+	atomic_dec(&device->device_busy);/* Decrement device usage counter. */
 
         /*
          * Clear the flags which say that the device/host is no longer
@@ -1450,7 +1436,7 @@ void scsi_release_commandblocks(Scsi_Dev
 	Scsi_Cmnd *SCpnt, *SCnext;
 	unsigned long flags;
 
- 	spin_lock_irqsave(&device_request_lock, flags);
+ 	spin_lock_irqsave(SDpnt->request_queue.queue_lock, flags);
 	for (SCpnt = SDpnt->device_queue; SCpnt; SCpnt = SCnext) {
 		SDpnt->device_queue = SCnext = SCpnt->next;
 		list_del(&SCpnt->sc_list);
@@ -1458,7 +1444,7 @@ void scsi_release_commandblocks(Scsi_Dev
 	}
 	SDpnt->has_cmdblocks = 0;
 	SDpnt->queue_depth = 0;
-	spin_unlock_irqrestore(&device_request_lock, flags);
+	spin_unlock_irqrestore(SDpnt->request_queue.queue_lock, flags);
 }
 
 /*
@@ -1480,8 +1466,9 @@ void scsi_build_commandblocks(Scsi_Devic
 	struct Scsi_Host *host = SDpnt->host;
 	int j;
 	Scsi_Cmnd *SCpnt;
+	request_queue_t *q = &SDpnt->request_queue;
 
-	spin_lock_irqsave(&device_request_lock, flags);
+	spin_lock_irqsave(q->queue_lock, flags);
 
 	if (SDpnt->queue_depth == 0)
 	{
@@ -1530,7 +1517,7 @@ void scsi_build_commandblocks(Scsi_Devic
 	} else {
 		SDpnt->has_cmdblocks = 1;
 	}
-	spin_unlock_irqrestore(&device_request_lock, flags);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 }
 
 void __init scsi_host_no_insert(char *str, int n)
@@ -1898,7 +1885,11 @@ static int scsi_register_host(Scsi_Host_
 	   All lame drivers are going to fail due to the following 
 	   spinlock. For the time beeing let's use it only for drivers 
 	   using the new scsi code. NOTE: the detect routine could
-	   redefine the value tpnt->use_new_eh_code. (DB, 13 May 1998) */
+	   redefine the value tpnt->use_new_eh_code. (DB, 13 May 1998)
+	   Since we now allow drivers to specify their own per-host locks,
+	   this attempt at locking is horrible broken.  Instead, call into
+	   the detect routine unlocked and let the driver grab/release
+	   the driver specific lock after it's allocated. (DL, 11 Dec 2001 */
 
 	if (tpnt->use_new_eh_code) {
 		spin_lock_irqsave(&io_request_lock, flags);
@@ -2022,6 +2013,7 @@ static int scsi_register_host(Scsi_Host_
 	       (scsi_memory_upper_value - scsi_init_memory_start) / 1024);
 #endif
 
+
 	if (out_of_space) {
 		scsi_unregister_host(tpnt);	/* easiest way to clean up?? */
 		return 1;
@@ -2472,7 +2464,7 @@ static void scsi_dump_status(int level)
 	for (shpnt = scsi_hostlist; shpnt; shpnt = shpnt->next) {
 		printk(KERN_INFO " %d %d %d : %d %d\n",
 		       shpnt->host_failed,
-		       shpnt->host_busy,
+		       atomic_read(&shpnt->host_busy),
 		       atomic_read(&shpnt->host_active),
 		       shpnt->host_blocked,
 		       shpnt->host_self_blocked);
@@ -2698,10 +2690,10 @@ Scsi_Device * scsi_get_host_dev(struct S
         SDpnt->type = -1;
         SDpnt->queue_depth = 1;
         
-	scsi_build_commandblocks(SDpnt);
-
 	scsi_initialize_queue(SDpnt, SHpnt);
 
+	scsi_build_commandblocks(SDpnt);
+
 	SDpnt->online = TRUE;
 
         /*
@@ -2731,13 +2723,13 @@ void scsi_free_host_dev(Scsi_Device * SD
                 panic("Attempt to delete wrong device\n");
         }
 
-        blk_cleanup_queue(&SDpnt->request_queue);
-
         /*
          * We only have a single SCpnt attached to this device.  Free
          * it now.
          */
 	scsi_release_commandblocks(SDpnt);
+        blk_cleanup_queue(&SDpnt->request_queue);
+
         kfree(SDpnt);
 }
 
@@ -2828,9 +2820,9 @@ scsi_reset_provider(Scsi_Device *dev, in
 	} else {
 		unsigned long flags;
 
-		spin_lock_irqsave(&io_request_lock, flags);
+		spin_lock_irqsave(dev->host->host_lock, flags);
 		rtn = scsi_old_reset(SCpnt, flag);
-		spin_unlock_irqrestore(&io_request_lock, flags);
+		spin_unlock_irqrestore(dev->host->host_lock, flags);
 	}
 
 	scsi_delete_timer(SCpnt);
diff -urNp linux-871/drivers/scsi/scsi.h linux-880/drivers/scsi/scsi.h
--- linux-871/drivers/scsi/scsi.h
+++ linux-880/drivers/scsi/scsi.h
@@ -555,10 +555,11 @@ struct scsi_device {
 	struct Scsi_Host *host;
 	request_queue_t request_queue;
         atomic_t                device_active; /* commands checked out for device */
-	volatile unsigned short device_busy;	/* commands actually active on low-level */
+	atomic_t device_busy;	/* commands actually active on low-level */
 	int (*scsi_init_io_fn) (Scsi_Cmnd *);	/* Used to initialize
 						   new request */
 	Scsi_Cmnd *device_queue;	/* queue of SCSI Command structures */
+	spinlock_t	device_lock;
 	struct list_head sdev_free_q;	/* list of free cmds */
 
 /* public: */
diff -urNp linux-871/drivers/scsi/scsi_error.c linux-880/drivers/scsi/scsi_error.c
--- linux-871/drivers/scsi/scsi_error.c
+++ linux-880/drivers/scsi/scsi_error.c
@@ -223,7 +223,7 @@ void scsi_times_out(Scsi_Cmnd * SCpnt)
 
 	SCSI_LOG_TIMEOUT(3, printk("Command timed out active=%d busy=%d failed=%d\n",
 				   atomic_read(&SCpnt->host->host_active),
-				   SCpnt->host->host_busy,
+				   atomic_read(&SCpnt->host->host_busy),
 				   SCpnt->host->host_failed));
 
 	/*
@@ -234,7 +234,7 @@ void scsi_times_out(Scsi_Cmnd * SCpnt)
 		panic("Error handler thread not present at %p %p %s %d", 
 		      SCpnt, SCpnt->host, __FILE__, __LINE__);
 	}
-	if (SCpnt->host->host_busy == SCpnt->host->host_failed) {
+	if (atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
 		up(SCpnt->host->eh_wait);
 	}
 }
@@ -417,7 +417,7 @@ STATIC int scsi_request_sense(Scsi_Cmnd 
 	unsigned char scsi_result0[256], *scsi_result = NULL;
 	int saved_result;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(SCpnt->host->host_lock, 0);
 
 	scsi_result = (!SCpnt->host->hostt->unchecked_isa_dma)
 	    ? &scsi_result0[0] : kmalloc(512, GFP_ATOMIC | GFP_DMA);
@@ -598,10 +598,10 @@ STATIC void scsi_send_eh_cmnd(Scsi_Cmnd 
 	unsigned long flags;
 	struct Scsi_Host *host;
 
-	ASSERT_LOCK(&io_request_lock, 0);
-
 	host = SCpnt->host;
 
+	ASSERT_LOCK(host->host_lock, 0);
+
 	/*
 	 * We will use a queued command if possible, otherwise we will emulate the
 	 * queuing and calling of completion function ourselves.
@@ -621,9 +621,9 @@ STATIC void scsi_send_eh_cmnd(Scsi_Cmnd 
 		SCpnt->host->eh_action = &sem;
 		SCpnt->request.rq_status = RQ_SCSI_BUSY;
 
-		spin_lock_irqsave(&io_request_lock, flags);
+		spin_lock_irqsave(host->host_lock, flags);
 		host->hostt->queuecommand(SCpnt, scsi_eh_done);
-		spin_unlock_irqrestore(&io_request_lock, flags);
+		spin_unlock_irqrestore(host->host_lock, flags);
 
 		down(&sem);
 
@@ -646,10 +646,10 @@ STATIC void scsi_send_eh_cmnd(Scsi_Cmnd 
 			 * abort a timed out command or not.  Not sure how
 			 * we should treat them differently anyways.
 			 */
-			spin_lock_irqsave(&io_request_lock, flags);
+			spin_lock_irqsave(host->host_lock, flags);
 			if (SCpnt->host->hostt->eh_abort_handler)
 				SCpnt->host->hostt->eh_abort_handler(SCpnt);
-			spin_unlock_irqrestore(&io_request_lock, flags);
+			spin_unlock_irqrestore(host->host_lock, flags);
 			
 			SCpnt->request.rq_status = RQ_SCSI_DONE;
 			SCpnt->owner = SCSI_OWNER_ERROR_HANDLER;
@@ -666,9 +666,9 @@ STATIC void scsi_send_eh_cmnd(Scsi_Cmnd 
 		 * protection here, since we would end up waiting in the actual low
 		 * level driver, we don't know how to wake it up.
 		 */
-		spin_lock_irqsave(&io_request_lock, flags);
+		spin_lock_irqsave(host->host_lock, flags);
 		temp = host->hostt->command(SCpnt);
-		spin_unlock_irqrestore(&io_request_lock, flags);
+		spin_unlock_irqrestore(host->host_lock, flags);
 
 		SCpnt->result = temp;
 		/* Fall through to code below to examine status. */
@@ -772,10 +772,13 @@ STATIC int scsi_try_to_abort_command(Scs
 {
 	int rtn;
 	unsigned long flags;
+	struct Scsi_Host *host;
+
+	host = SCpnt->host;
 
 	SCpnt->eh_state = FAILED;	/* Until we come up with something better */
 
-	if (SCpnt->host->hostt->eh_abort_handler == NULL) {
+	if (host->hostt->eh_abort_handler == NULL) {
 		return FAILED;
 	}
 	/* 
@@ -787,9 +790,9 @@ STATIC int scsi_try_to_abort_command(Scs
 
 	SCpnt->owner = SCSI_OWNER_LOWLEVEL;
 
-	spin_lock_irqsave(&io_request_lock, flags);
-	rtn = SCpnt->host->hostt->eh_abort_handler(SCpnt);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_lock_irqsave(host->host_lock, flags);
+	rtn = host->hostt->eh_abort_handler(SCpnt);
+	spin_unlock_irqrestore(host->host_lock, flags);
 	return rtn;
 }
 
@@ -811,17 +814,20 @@ STATIC int scsi_try_bus_device_reset(Scs
 {
 	unsigned long flags;
 	int rtn;
+	struct Scsi_Host *host;
+
+	host = SCpnt->host;
 
 	SCpnt->eh_state = FAILED;	/* Until we come up with something better */
 
-	if (SCpnt->host->hostt->eh_device_reset_handler == NULL) {
+	if (host->hostt->eh_device_reset_handler == NULL) {
 		return FAILED;
 	}
 	SCpnt->owner = SCSI_OWNER_LOWLEVEL;
 
-	spin_lock_irqsave(&io_request_lock, flags);
-	rtn = SCpnt->host->hostt->eh_device_reset_handler(SCpnt);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_lock_irqsave(host->host_lock, flags);
+	rtn = host->hostt->eh_device_reset_handler(SCpnt);
+	spin_unlock_irqrestore(host->host_lock, flags);
 
 	if (rtn == SUCCESS)
 		SCpnt->eh_state = SUCCESS;
@@ -843,18 +849,21 @@ STATIC int scsi_try_bus_reset(Scsi_Cmnd 
 {
 	unsigned long flags;
 	int rtn;
+	struct Scsi_Host *host;
+
+	host = SCpnt->host;
 
 	SCpnt->eh_state = FAILED;	/* Until we come up with something better */
 	SCpnt->owner = SCSI_OWNER_LOWLEVEL;
 	SCpnt->serial_number_at_timeout = SCpnt->serial_number;
 
-	if (SCpnt->host->hostt->eh_bus_reset_handler == NULL) {
+	if (host->hostt->eh_bus_reset_handler == NULL) {
 		return FAILED;
 	}
 
-	spin_lock_irqsave(&io_request_lock, flags);
-	rtn = SCpnt->host->hostt->eh_bus_reset_handler(SCpnt);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_lock_irqsave(host->host_lock, flags);
+	rtn = host->hostt->eh_bus_reset_handler(SCpnt);
+	spin_unlock_irqrestore(host->host_lock, flags);
 
 	if (rtn == SUCCESS)
 		SCpnt->eh_state = SUCCESS;
@@ -866,7 +875,7 @@ STATIC int scsi_try_bus_reset(Scsi_Cmnd 
 	scsi_sleep(BUS_RESET_SETTLE_TIME);
 	if (SCpnt->eh_state == SUCCESS) {
 		Scsi_Device *SDloop;
-		for (SDloop = SCpnt->host->host_queue; SDloop; SDloop = SDloop->next) {
+		for (SDloop = host->host_queue; SDloop; SDloop = SDloop->next) {
 			if (SCpnt->channel == SDloop->channel) {
 				SDloop->was_reset = 1;
 				SDloop->expecting_cc_ua = 1;
@@ -890,17 +899,20 @@ STATIC int scsi_try_host_reset(Scsi_Cmnd
 {
 	unsigned long flags;
 	int rtn;
+	struct Scsi_Host *host;
+
+	host = SCpnt->host;
 
 	SCpnt->eh_state = FAILED;	/* Until we come up with something better */
 	SCpnt->owner = SCSI_OWNER_LOWLEVEL;
 	SCpnt->serial_number_at_timeout = SCpnt->serial_number;
 
-	if (SCpnt->host->hostt->eh_host_reset_handler == NULL) {
+	if (host->hostt->eh_host_reset_handler == NULL) {
 		return FAILED;
 	}
-	spin_lock_irqsave(&io_request_lock, flags);
-	rtn = SCpnt->host->hostt->eh_host_reset_handler(SCpnt);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_lock_irqsave(host->host_lock, flags);
+	rtn = host->hostt->eh_host_reset_handler(SCpnt);
+	spin_unlock_irqrestore(host->host_lock, flags);
 
 	if (rtn == SUCCESS)
 		SCpnt->eh_state = SUCCESS;
@@ -912,7 +924,7 @@ STATIC int scsi_try_host_reset(Scsi_Cmnd
 	scsi_sleep(HOST_RESET_SETTLE_TIME);
 	if (SCpnt->eh_state == SUCCESS) {
 		Scsi_Device *SDloop;
-		for (SDloop = SCpnt->host->host_queue; SDloop; SDloop = SDloop->next) {
+		for (SDloop = host->host_queue; SDloop; SDloop = SDloop->next) {
 			SDloop->was_reset = 1;
 			SDloop->expecting_cc_ua = 1;
 		}
@@ -1249,7 +1261,7 @@ STATIC void scsi_restart_operations(stru
 	Scsi_Device *SDpnt;
 	unsigned long flags;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(host->host_lock, 0);
 
 	/*
 	 * Next free up anything directly waiting upon the host.  This will be
@@ -1266,19 +1278,23 @@ STATIC void scsi_restart_operations(stru
 	 * now that error recovery is done, we will need to ensure that these
 	 * requests are started.
 	 */
-	spin_lock_irqsave(&io_request_lock, flags);
+	spin_lock_irqsave(host->host_lock, flags);
 	for (SDpnt = host->host_queue; SDpnt; SDpnt = SDpnt->next) {
 		request_queue_t *q;
-		if ((host->can_queue > 0 && (host->host_busy >= host->can_queue))
+		if ((host->can_queue > 0 && (atomic_read(&host->host_busy) >= host->can_queue))
 		    || (host->host_blocked)
 		    || (host->host_self_blocked)
 		    || (SDpnt->device_blocked)) {
 			break;
 		}
 		q = &SDpnt->request_queue;
+		spin_lock(q->queue_lock);
+		spin_unlock(host->host_lock);
 		q->request_fn(q);
+		spin_lock(host->host_lock);
+		spin_unlock(q->queue_lock);
 	}
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(host->host_lock, flags);
 }
 
 /*
@@ -1325,7 +1341,7 @@ STATIC int scsi_unjam_host(struct Scsi_H
 	Scsi_Cmnd *SCdone;
 	int timed_out;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(host->host_lock, 0);
 
 	SCdone = NULL;
 
diff -urNp linux-871/drivers/scsi/scsi_lib.c linux-880/drivers/scsi/scsi_lib.c
--- linux-871/drivers/scsi/scsi_lib.c
+++ linux-880/drivers/scsi/scsi_lib.c
@@ -70,7 +70,7 @@ static void __scsi_insert_special(reques
 {
 	unsigned long flags;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	rq->cmd = SPECIAL;
 	rq->special = data;
@@ -84,15 +84,15 @@ static void __scsi_insert_special(reques
 	 * head of the queue for things like a QUEUE_FULL message from a
 	 * device, or a host that is unable to accept a particular command.
 	 */
-	spin_lock_irqsave(&io_request_lock, flags);
 
+	spin_lock_irqsave(q->queue_lock, flags);
 	if (at_head)
 		list_add(&rq->queue, &q->queue_head);
 	else
 		list_add_tail(&rq->queue, &q->queue_head);
 
 	q->request_fn(q);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 }
 
 
@@ -167,7 +167,7 @@ int scsi_insert_special_req(Scsi_Request
  */
 int scsi_init_cmd_errh(Scsi_Cmnd * SCpnt)
 {
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	SCpnt->owner = SCSI_OWNER_MIDLEVEL;
 	SCpnt->reset_chain = NULL;
@@ -270,13 +270,12 @@ void scsi_setup_cmd_retry(Scsi_Cmnd *SCp
 void scsi_queue_next_request(request_queue_t * q, Scsi_Cmnd * SCpnt)
 {
 	int all_clear;
-	unsigned long flags;
 	Scsi_Device *SDpnt;
 	struct Scsi_Host *SHpnt;
+	unsigned long flags;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
-	spin_lock_irqsave(&io_request_lock, flags);
 	if (SCpnt != NULL) {
 
 		/*
@@ -286,13 +285,17 @@ void scsi_queue_next_request(request_que
 		 * the bad sector.
 		 */
 		SCpnt->request.special = (void *) SCpnt;
+		spin_lock_irqsave(q->queue_lock, flags);
 		list_add(&SCpnt->request.queue, &q->queue_head);
+		spin_unlock_irqrestore(q->queue_lock, flags);
 	}
 
 	/*
 	 * Just hit the requeue function for the queue.
 	 */
+	spin_lock_irqsave(q->queue_lock, flags);
 	q->request_fn(q);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 
 	SDpnt = (Scsi_Device *) q->queuedata;
 	SHpnt = SDpnt->host;
@@ -306,21 +309,24 @@ void scsi_queue_next_request(request_que
 	 */
 	if (SDpnt->single_lun
 	    && list_empty(&q->queue_head)
-	    && SDpnt->device_busy == 0) {
+	    && atomic_read(&SDpnt->device_busy) == 0) {
 		request_queue_t *q;
 
 		for (SDpnt = SHpnt->host_queue;
 		     SDpnt;
 		     SDpnt = SDpnt->next) {
 			if (((SHpnt->can_queue > 0)
-			     && (SHpnt->host_busy >= SHpnt->can_queue))
+			     && (atomic_read(&SHpnt->host_busy)
+				 >= SHpnt->can_queue))
 			    || (SHpnt->host_blocked)
 			    || (SHpnt->host_self_blocked)
 			    || (SDpnt->device_blocked)) {
 				break;
 			}
 			q = &SDpnt->request_queue;
+			spin_lock_irqsave(q->queue_lock, flags);
 			q->request_fn(q);
+			spin_unlock_irqrestore(q->queue_lock, flags);
 		}
 	}
 
@@ -336,7 +342,7 @@ void scsi_queue_next_request(request_que
 	if (SHpnt->some_device_starved) {
 		for (SDpnt = SHpnt->host_queue; SDpnt; SDpnt = SDpnt->next) {
 			request_queue_t *q;
-			if ((SHpnt->can_queue > 0 && (SHpnt->host_busy >= SHpnt->can_queue))
+			if ((SHpnt->can_queue > 0 && (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue))
 			    || (SHpnt->host_blocked) 
 			    || (SHpnt->host_self_blocked)) {
 				break;
@@ -345,14 +351,15 @@ void scsi_queue_next_request(request_que
 				continue;
 			}
 			q = &SDpnt->request_queue;
+			spin_lock_irqsave(q->queue_lock, flags);
 			q->request_fn(q);
+			spin_unlock_irqrestore(q->queue_lock, flags);
 			all_clear = 0;
 		}
 		if (SDpnt == NULL && all_clear) {
 			SHpnt->some_device_starved = 0;
 		}
 	}
-	spin_unlock_irqrestore(&io_request_lock, flags);
 }
 
 /*
@@ -390,7 +397,7 @@ static Scsi_Cmnd *__scsi_end_request(Scs
 	unsigned long flags;
 	int nsect;
 
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	req = &SCpnt->request;
 	req->errors = 0;
@@ -445,15 +452,17 @@ static Scsi_Cmnd *__scsi_end_request(Scs
 	if (req->waiting)
 		complete(req->waiting);
 
-	spin_lock_irqsave(&io_request_lock, flags);
+	spin_lock_irqsave(q->queue_lock, flags);
 	req_finished_io(req);
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(q->queue_lock, flags);
 
 	add_blkdev_randomness(MAJOR(req->rq_dev));
 
 	/*
-	 * This will goose the queue request function at the end, so we don't
-	 * need to worry about launching another command.
+	 * This used to be a call to scsi_release_command, but that could
+	 * result in infinite recursion if link status went down during
+	 * I/O.  Instead, we goose the queue request function conditionally
+	 * below.
 	 */
 	__scsi_release_command(SCpnt);
 
@@ -507,8 +516,6 @@ Scsi_Cmnd *scsi_end_request(Scsi_Cmnd * 
  */
 static void scsi_release_buffers(Scsi_Cmnd * SCpnt)
 {
-	ASSERT_LOCK(&io_request_lock, 0);
-
 	/*
 	 * Free up any indirection buffers we allocated for DMA purposes. 
 	 */
@@ -581,7 +588,7 @@ void scsi_io_completion(Scsi_Cmnd * SCpn
 	 *	would be used if we just wanted to retry, for example.
 	 *
 	 */
-	ASSERT_LOCK(&io_request_lock, 0);
+	ASSERT_LOCK(q->queue_lock, 0);
 
 	/*
 	 * Free up any indirection buffers we allocated for DMA purposes. 
@@ -809,7 +816,7 @@ struct Scsi_Device_Template *scsi_get_re
 	kdev_t dev = req->rq_dev;
 	int major = MAJOR(dev);
 
-	ASSERT_LOCK(&io_request_lock, 1);
+	ASSERT_LOCK(q->queue_lock, 1);
 
 	for (spnt = scsi_devicelist; spnt; spnt = spnt->next) {
 		/*
@@ -867,7 +874,7 @@ void scsi_request_fn(request_queue_t * q
 	struct Scsi_Host *SHpnt;
 	struct Scsi_Device_Template *STpnt;
 
-	ASSERT_LOCK(&io_request_lock, 1);
+	ASSERT_LOCK(q->queue_lock, 1);
 
 	SDpnt = (Scsi_Device *) q->queuedata;
 	if (!SDpnt) {
@@ -891,10 +898,18 @@ void scsi_request_fn(request_queue_t * q
 		/*
 		 * If the device cannot accept another request, then quit.
 		 */
-		if (SDpnt->device_blocked) {
+		if (SDpnt->device_blocked)
 			break;
-		}
-		if ((SHpnt->can_queue > 0 && (SHpnt->host_busy >= SHpnt->can_queue))
+
+		/*
+		 * If we couldn't find a request that could be queued, then we
+		 * can quit.
+		 */
+		if (list_empty(&q->queue_head))
+			break;
+
+		spin_lock(SHpnt->host_lock);
+		if ((SHpnt->can_queue > 0 && (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue))
 		    || (SHpnt->host_blocked) 
 		    || (SHpnt->host_self_blocked)) {
 			/*
@@ -905,13 +920,16 @@ void scsi_request_fn(request_queue_t * q
 			 * little help getting it started again
 			 * once the host isn't quite so busy.
 			 */
-			if (SDpnt->device_busy == 0) {
+			if (atomic_read(&SDpnt->device_busy) == 0) {
 				SDpnt->starved = 1;
 				SHpnt->some_device_starved = 1;
 			}
+			spin_unlock(SHpnt->host_lock);
 			break;
 		} else {
 			SDpnt->starved = 0;
+			atomic_inc(&SHpnt->host_busy);
+			spin_unlock(SHpnt->host_lock);
 		}
 
  		/*
@@ -934,21 +952,15 @@ void scsi_request_fn(request_queue_t * q
 			 */
 			SDpnt->was_reset = 0;
 			if (SDpnt->removable && !in_interrupt()) {
-				spin_unlock_irq(&io_request_lock);
+				atomic_dec(&SHpnt->host_busy);
+				spin_unlock_irq(q->queue_lock);
 				scsi_ioctl(SDpnt, SCSI_IOCTL_DOORLOCK, 0);
-				spin_lock_irq(&io_request_lock);
+				spin_lock_irq(q->queue_lock);
 				continue;
 			}
 		}
 
 		/*
-		 * If we couldn't find a request that could be queued, then we
-		 * can also quit.
-		 */
-		if (list_empty(&q->queue_head))
-			break;
-
-		/*
 		 * Loop through all of the requests in this queue, and find
 		 * one that is queueable.
 		 */
@@ -970,9 +982,10 @@ void scsi_request_fn(request_queue_t * q
 			SRpnt = (Scsi_Request *) req->special;
 
 			if( SRpnt->sr_magic == SCSI_REQ_MAGIC ) {
-				SCpnt = scsi_allocate_device(SRpnt->sr_device, 
+				SCpnt = scsi_allocate_device(SRpnt->sr_device,
 							     FALSE, FALSE);
 				if( !SCpnt ) {
+					atomic_dec(&SHpnt->host_busy);
 					break;
 				}
 				scsi_init_cmd_from_req(SCpnt, SRpnt);
@@ -1007,45 +1020,14 @@ void scsi_request_fn(request_queue_t * q
 			 * loop. Otherwise loop around and try another request.
 			 */
 			if (!SCpnt) {
+				atomic_dec(&SHpnt->host_busy);
 				break;
 			}
 		}
 
-		/*
-		 * Now bump the usage count for both the host and the
-		 * device.
-		 */
-		SHpnt->host_busy++;
-		SDpnt->device_busy++;
-
-		/*
-		 * Finally, before we release the lock, we copy the
-		 * request to the command block, and remove the
-		 * request from the request list.   Note that we always
-		 * operate on the queue head - there is absolutely no
-		 * reason to search the list, because all of the commands
-		 * in this queue are for the same device.
-		 */
-		blkdev_dequeue_request(req);
-
 		if (req != &SCpnt->request && req != &SRpnt->sr_request ) {
 			memcpy(&SCpnt->request, req, sizeof(struct request));
-
-			/*
-			 * We have copied the data out of the request block -
-			 * it is now in a field in SCpnt.  Release the request
-			 * block.
-			 */
-			blkdev_release_request(req);
 		}
-		/*
-		 * Now it is finally safe to release the lock.  We are
-		 * not going to noodle the request list until this
-		 * request has been queued and we loop back to queue
-		 * another.  
-		 */
-		req = NULL;
-		spin_unlock_irq(&io_request_lock);
 
 		if (SCpnt->request.cmd != SPECIAL) {
 			/*
@@ -1076,15 +1058,14 @@ void scsi_request_fn(request_queue_t * q
 				 * on highmem i/o, so mark the device as
 				 * starved and continue later instead
 				 */
-				spin_lock_irq(&io_request_lock);
-				SHpnt->host_busy--;
-				SDpnt->device_busy--;
-				if (SDpnt->device_busy == 0) {
+				spin_lock(SHpnt->host_lock);
+				atomic_dec(&SHpnt->host_busy);
+				if (atomic_read(&SDpnt->device_busy) == 0) {
 					SDpnt->starved = 1;
 					SHpnt->some_device_starved = 1;
 				}
+				spin_unlock(SHpnt->host_lock);
 				SCpnt->request.special = SCpnt;
-				list_add(&SCpnt->request.queue, &q->queue_head);
 				break;
 			}
 
@@ -1092,16 +1073,22 @@ void scsi_request_fn(request_queue_t * q
 			 * Initialize the actual SCSI command for this request.
 			 */
 			if (!STpnt->init_command(SCpnt)) {
+				blkdev_dequeue_request(req);
+				if (req != &SCpnt->request &&
+				    req != &SRpnt->sr_request ) {
+					blkdev_release_request(req);
+				}
+				req = NULL;
 				scsi_release_buffers(SCpnt);
+				spin_unlock_irq(q->queue_lock);
 				SCpnt = __scsi_end_request(SCpnt, 0, 
 							   SCpnt->request.nr_sectors, 0, 0);
+				spin_lock_irq(q->queue_lock);
 				if( SCpnt != NULL )
 				{
 					panic("Should not have leftover blocks\n");
 				}
-				spin_lock_irq(&io_request_lock);
-				SHpnt->host_busy--;
-				SDpnt->device_busy--;
+				atomic_dec(&SHpnt->host_busy);
 				continue;
 			}
 		}
@@ -1112,15 +1099,41 @@ void scsi_request_fn(request_queue_t * q
 		scsi_init_cmd_errh(SCpnt);
 
 		/*
-		 * Dispatch the command to the low-level driver.
+		 * Now bump the usage count for the device.
 		 */
-		scsi_dispatch_cmd(SCpnt);
+		atomic_inc(&SDpnt->device_busy);
 
 		/*
-		 * Now we need to grab the lock again.  We are about to mess
-		 * with the request queue and try to find another command.
+		 * Finally, before we release the lock, we copy the
+		 * request to the command block, and remove the
+		 * request from the request list.   Note that we always
+		 * operate on the queue head - there is absolutely no
+		 * reason to search the list, because all of the commands
+		 * in this queue are for the same device.
 		 */
-		spin_lock_irq(&io_request_lock);
+		blkdev_dequeue_request(req);
+
+		if (req != &SCpnt->request && req != &SRpnt->sr_request ) {
+			/*
+			 * Above, we copied the data out of the request block -
+			 * it is now in a field in SCpnt.  Release the request
+			 * block.
+			 */
+			blkdev_release_request(req);
+		}
+		req = NULL;
+		/*
+		 * Dispatch the command to the low-level driver.
+		 */
+		spin_unlock_irq(q->queue_lock);
+		scsi_dispatch_cmd(SCpnt);
+		spin_lock_irq(q->queue_lock);
+	}
+	if (!list_empty(&q->queue_head) && atomic_read(&SDpnt->device_busy) == 0) {
+		SDpnt->starved = 1;
+		spin_lock(SHpnt->host_lock);
+		SHpnt->some_device_starved = 1;
+		spin_unlock(SHpnt->host_lock);
 	}
 }
 
diff -urNp linux-871/drivers/scsi/scsi_obsolete.c linux-880/drivers/scsi/scsi_obsolete.c
--- linux-871/drivers/scsi/scsi_obsolete.c
+++ linux-880/drivers/scsi/scsi_obsolete.c
@@ -147,7 +147,7 @@ void scsi_old_times_out(Scsi_Cmnd * SCpn
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&io_request_lock, flags);
+	spin_lock_irqsave(SCpnt->host->host_lock, flags);
 
 	/* Set the serial_number_at_timeout to the current serial_number */
 	SCpnt->serial_number_at_timeout = SCpnt->serial_number;
@@ -202,14 +202,14 @@ void scsi_old_times_out(Scsi_Cmnd * SCpn
 		break;
 
 	}
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	spin_unlock_irqrestore(SCpnt->host->host_lock, flags);
 
 }
 
 /*
  *  From what I can find in scsi_obsolete.c, this function is only called
  *  by scsi_old_done and scsi_reset.  Both of these functions run with the
- *  io_request_lock already held, so we need do nothing here about grabbing
+ *  host->host_lock already held, so we need do nothing here about grabbing
  *  any locks.
  */
 static void scsi_request_sense(Scsi_Cmnd * SCpnt)
@@ -238,9 +238,9 @@ static void scsi_request_sense(Scsi_Cmnd
          * Ugly, ugly.  The newer interfaces all assume that the lock
          * isn't held.  Mustn't disappoint, or we deadlock the system.
          */
-        spin_unlock_irq(&io_request_lock);
+        spin_unlock_irq(SCpnt->host->host_lock);
 	scsi_dispatch_cmd(SCpnt);
-        spin_lock_irq(&io_request_lock);
+        spin_lock_irq(SCpnt->host->host_lock);
 }
 
 
@@ -661,9 +661,9 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
                          * assume that the lock isn't held.  Mustn't
                          * disappoint, or we deadlock the system.  
                          */
-                        spin_unlock_irq(&io_request_lock);
+                        spin_unlock_irq(host->host_lock);
 			scsi_dispatch_cmd(SCpnt);
-                        spin_lock_irq(&io_request_lock);
+                        spin_lock_irq(host->host_lock);
 		}
 		break;
 	default:
@@ -675,8 +675,8 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 #ifdef DEBUG
 		printk("Calling done function - at address %p\n", SCpnt->done);
 #endif
-		host->host_busy--;	/* Indicate that we are free */
-                device->device_busy--;	/* Decrement device usage counter. */
+		atomic_dec(&host->host_busy);	/* Indicate that we are free */
+                atomic_dec(&device->device_busy);/* Decrement device usage counter. */
 
 		SCpnt->result = result | ((exit & 0xff) << 24);
 		SCpnt->use_sg = SCpnt->old_use_sg;
@@ -689,7 +689,7 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
                  * use, the upper code is run from a bottom half handler, so
                  * it isn't an issue.
                  */
-                spin_unlock_irq(&io_request_lock);
+                spin_unlock_irq(host->host_lock);
 		SRpnt = SCpnt->sc_request;
 		if( SRpnt != NULL ) {
 			SRpnt->sr_result = SRpnt->sr_command->result;
@@ -701,7 +701,7 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 		}
 
 		SCpnt->done(SCpnt);
-                spin_lock_irq(&io_request_lock);
+                spin_lock_irq(host->host_lock);
 	}
 #undef CMD_FINISHED
 #undef REDO
@@ -740,10 +740,10 @@ static int scsi_abort(Scsi_Cmnd * SCpnt,
 			return 0;
 		}
 		if (SCpnt->internal_timeout & IN_ABORT) {
-			spin_unlock_irq(&io_request_lock);
+			spin_unlock_irq(host->host_lock);
 			while (SCpnt->internal_timeout & IN_ABORT)
 				barrier();
-			spin_lock_irq(&io_request_lock);
+			spin_lock_irq(host->host_lock);
 		} else {
 			SCpnt->internal_timeout |= IN_ABORT;
 			oldto = update_timeout(SCpnt, ABORT_TIMEOUT);
@@ -756,7 +756,7 @@ static int scsi_abort(Scsi_Cmnd * SCpnt,
 				       " the bus was reset\n",
 				       SCpnt->channel, SCpnt->target, SCpnt->lun);
 			}
-			if (!host->host_busy) {
+			if (atomic_read(&host->host_busy) == 0) {
 				SCpnt->internal_timeout &= ~IN_ABORT;
 				update_timeout(SCpnt, oldto);
 				return 0;
@@ -923,17 +923,17 @@ static int scsi_reset(Scsi_Cmnd * SCpnt,
 				return 0;
 			}
 		if (SCpnt->internal_timeout & IN_RESET) {
-			spin_unlock_irq(&io_request_lock);
+			spin_unlock_irq(host->host_lock);
 			while (SCpnt->internal_timeout & IN_RESET)
 				barrier();
-			spin_lock_irq(&io_request_lock);
+			spin_lock_irq(host->host_lock);
 		} else {
 			SCpnt->internal_timeout |= IN_RESET;
 			update_timeout(SCpnt, RESET_TIMEOUT);
 
 			if (reset_flags & SCSI_RESET_SYNCHRONOUS)
 				SCpnt->flags |= SYNC_RESET;
-			if (host->host_busy) {
+			if (atomic_read(&host->host_busy) != 0) {
 				for (SDpnt = host->host_queue; SDpnt; SDpnt = SDpnt->next) {
 					SCpnt1 = SDpnt->device_queue;
 					while (SCpnt1) {
@@ -970,7 +970,7 @@ static int scsi_reset(Scsi_Cmnd * SCpnt,
 				if (host->last_reset - jiffies > 20UL * HZ)
 					host->last_reset = jiffies;
 			} else {
-				host->host_busy++;
+				atomic_inc(&host->host_busy);
 				host->last_reset = jiffies;
 				host->resetting = 1;
 				SCpnt->flags |= (WAS_RESET | IS_RESETTING);
@@ -983,7 +983,7 @@ static int scsi_reset(Scsi_Cmnd * SCpnt,
 				if (time_before(host->last_reset, jiffies) ||
 				    (time_after(host->last_reset, jiffies + 20 * HZ)))
 					host->last_reset = jiffies;
-				host->host_busy--;
+				atomic_dec(&host->host_busy);
 			}
 			if (reset_flags & SCSI_RESET_SYNCHRONOUS)
 				SCpnt->flags &= ~SYNC_RESET;
diff -urNp linux-871/drivers/scsi/scsi_queue.c linux-880/drivers/scsi/scsi_queue.c
--- linux-871/drivers/scsi/scsi_queue.c
+++ linux-880/drivers/scsi/scsi_queue.c
@@ -103,7 +103,7 @@ int scsi_mlqueue_insert(Scsi_Cmnd * cmd,
 		 * If a host is inactive and cannot queue any commands, I don't see
 		 * how things could possibly work anyways.
 		 */
-		if (host->host_busy == 0) {
+		if (atomic_read(&host->host_busy) == 0) {
 			if (scsi_retry_command(cmd) == 0) {
 				return 0;
 			}
@@ -118,7 +118,7 @@ int scsi_mlqueue_insert(Scsi_Cmnd * cmd,
 		 * If a host is inactive and cannot queue any commands, I don't see
 		 * how things could possibly work anyways.
 		 */
-		if (cmd->device->device_busy == 0) {
+		if (atomic_read(&cmd->device->device_busy) == 0) {
 			if (scsi_retry_command(cmd) == 0) {
 				return 0;
 			}
@@ -137,10 +137,8 @@ int scsi_mlqueue_insert(Scsi_Cmnd * cmd,
 	 * Decrement the counters, since these commands are no longer
 	 * active on the host/device.
 	 */
-	spin_lock_irqsave(&io_request_lock, flags);
-	cmd->host->host_busy--;
-	cmd->device->device_busy--;
-	spin_unlock_irqrestore(&io_request_lock, flags);
+	atomic_dec(&cmd->host->host_busy);
+	atomic_dec(&cmd->device->device_busy);
 
 	/*
 	 * Insert this command at the head of the queue for it's device.
diff -urNp linux-871/drivers/scsi/scsi_scan.c linux-880/drivers/scsi/scsi_scan.c
--- linux-871/drivers/scsi/scsi_scan.c
+++ linux-880/drivers/scsi/scsi_scan.c
@@ -336,13 +336,9 @@ void scan_scsis(struct Scsi_Host *shpnt,
 		memset(SDpnt, 0, sizeof(Scsi_Device));
 		/*
 		 * Register the queue for the device.  All I/O requests will
-		 * come in through here.  We also need to register a pointer to
-		 * ourselves, since the queue handler won't know what device
-		 * the queue actually represents.   We could look it up, but it
-		 * is pointless work.
+		 * come in through here.
 		 */
 		scsi_initialize_queue(SDpnt, shpnt);
-		SDpnt->request_queue.queuedata = (void *) SDpnt;
 		/* Make sure we have something that is valid for DMA purposes */
 		scsi_result = ((!shpnt->unchecked_isa_dma)
 			       ? &scsi_result0[0] : kmalloc(512, GFP_DMA));
@@ -719,7 +715,7 @@ static int scan_scsis_single(unsigned in
 	}
 
 	SDpnt->device_blocked = FALSE;
-	SDpnt->device_busy = 0;
+	atomic_set(&SDpnt->device_busy,0);
 	SDpnt->single_lun = 0;
 	SDpnt->soft_reset =
 	    (scsi_result[7] & 1) && ((scsi_result[3] & 7) == 2);
diff -urNp linux-871/drivers/scsi/sg.c linux-880/drivers/scsi/sg.c
--- linux-871/drivers/scsi/sg.c
+++ linux-880/drivers/scsi/sg.c
@@ -2972,7 +2972,7 @@ static int sg_proc_dev_info(char * buffe
 	    PRINT_PROC("%d\t%d\t%d\t%d\t%d\t%d\t%d\t%d\t%d\n",
 	       scsidp->host->host_no, scsidp->channel, scsidp->id,
 	       scsidp->lun, (int)scsidp->type, (int)scsidp->access_count,
-	       (int)scsidp->queue_depth, (int)scsidp->device_busy,
+	       (int)scsidp->queue_depth, (int)atomic_read(&scsidp->device_busy),
 	       (int)scsidp->online);
 	else
 	    PRINT_PROC("-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\n");
@@ -3028,8 +3028,9 @@ static int sg_proc_host_info(char * buff
     	for ( ; k < shp->host_no; ++k)
 	    PRINT_PROC("-1\t-1\t-1\t-1\t-1\t-1\n");
 	PRINT_PROC("%u\t%hu\t%hd\t%hu\t%d\t%d\n",
-		   shp->unique_id, shp->host_busy, shp->cmd_per_lun,
-		   shp->sg_tablesize, (int)shp->unchecked_isa_dma,
+		   shp->unique_id, atomic_read(&shp->host_busy),
+		   shp->cmd_per_lun, shp->sg_tablesize,
+		   (int)shp->unchecked_isa_dma,
 		   (int)shp->hostt->emulated);
     }
     return 1;
diff -urNp linux-871/include/linux/blkdev.h linux-880/include/linux/blkdev.h
--- linux-871/include/linux/blkdev.h
+++ linux-880/include/linux/blkdev.h
@@ -143,7 +143,7 @@ struct request_queue
 	 * Is meant to protect the queue in the future instead of
 	 * io_request_lock
 	 */
-	spinlock_t		queue_lock;
+	spinlock_t		*queue_lock;
 
 	/*
 	 * Tasks wait here for free read and write requests
