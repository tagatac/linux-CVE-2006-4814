diff -urNp linux-1251/include/linux/mm.h linux-1252/include/linux/mm.h
--- linux-1251/include/linux/mm.h
+++ linux-1252/include/linux/mm.h
@@ -893,6 +893,32 @@ extern struct vm_area_struct *find_exten
 
 extern struct page * vmalloc_to_page(void *addr);
 
+/* Page pinning for direct IO.  Copy-on-write effects mean it's unsafe
+ * for the VM to unmap a pte if the process has requested direct IO to
+ * that page.  So we keep a count of outstanding direct IOs per page.
+ *
+ * We actually maintain this as a hash into an atomic_t[] array, so
+ * depending on the hash size there may be a certain amount of false
+ * sharing of page pins. */
+extern atomic_t *page_pin_array;
+extern unsigned int page_pin_mask;
+extern void page_pin_init(unsigned long);
+static inline unsigned int page_pin_hash(struct page *p) 
+{
+	unsigned int addr = (unsigned int)(unsigned long)p;
+	/* gcc usually optimises integer divide-by-constant pretty well. */
+	addr /= sizeof(struct page);  
+	addr ^= (addr >> 8);
+	return (addr & page_pin_mask);
+}
+#define page_pin_counter(p) (page_pin_array + page_pin_hash(p))
+#define pin_page_mappings(p) \
+	do {atomic_inc(page_pin_counter(p));} while (0)
+#define unpin_page_mappings(p) \
+	do {atomic_dec(page_pin_counter(p));} while (0)
+#define page_mapping_pinned(p) \
+	(atomic_read(page_pin_counter(p)) != 0)
+
 #endif /* __KERNEL__ */
 
 #endif
diff -urNp linux-1251/init/main.c linux-1252/init/main.c
--- linux-1251/init/main.c
+++ linux-1252/init/main.c
@@ -387,6 +387,7 @@ asmlinkage void __init start_kernel(void
 #endif
 	/* allocate large system hash tables using the bootmem allocator */
 	page_cache_init(max_low_pfn);
+	page_pin_init(max_low_pfn);
 	dcache_init_early(max_low_pfn);
 	inode_init_early(max_low_pfn);
 	buffer_init(max_low_pfn);
diff -urNp linux-1251/mm/memory.c linux-1252/mm/memory.c
--- linux-1251/mm/memory.c
+++ linux-1252/mm/memory.c
@@ -49,6 +49,7 @@
 #include <linux/slab.h>
 #include <linux/mm_inline.h>
 #include <linux/hugetlb.h>
+#include <linux/bootmem.h>
 
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -64,6 +65,19 @@ struct page *highmem_start_page;
 atomic_t lowmem_pagetables = ATOMIC_INIT(0);
 atomic_t highmem_pagetables = ATOMIC_INIT(0);
 
+atomic_t *page_pin_array = NULL;
+unsigned int page_pin_mask = 0;
+
+void page_pin_init(unsigned long mempages)
+{
+	page_pin_array = (atomic_t *)alloc_large_system_hash("Page-pin",
+							     sizeof(atomic_t),
+							     14,
+							     1,
+							     NULL,
+							     &page_pin_mask);
+}
+
 void vm_account(struct vm_area_struct *vma, pte_t pte, unsigned long address, long adj)
 {
 	struct mm_struct *mm = vma->vm_mm;
@@ -853,8 +867,8 @@ untouched_anonymous_page(struct mm_struc
  * Accessing a VM_IO area is even more dangerous, therefore the function
  * fails if pages is != NULL and a VM_IO area is found.
  */
-int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start,
-		int len, int write, int force, struct page **pages, struct vm_area_struct **vmas)
+int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start,
+		int len, int write, int force, struct page **pages, struct vm_area_struct **vmas, int pin)
 {
 	int i;
 	unsigned int flags;
@@ -875,6 +889,10 @@ int get_user_pages(struct task_struct *t
 		if ( !vma || (pages && vma->vm_flags & VM_IO) || !(flags & vma->vm_flags) )
 			return i ? : -EFAULT;
 		if (is_vm_hugetlb_page(vma)) {
+			/* We're not going to bother pinning these
+			 * pages.  hugetlbfs pages are already pinned
+			 * wrt page tables, and we'll detect this case
+			 * by testing PageCompound() on unpin. */
 			i = follow_pin_hugetlb_page(mm, vma, pages, vmas,
 						&start, &len, i);
 			continue;
@@ -908,13 +926,19 @@ int get_user_pages(struct task_struct *t
 				spin_lock(&mm->page_table_lock);
 			}
 			if (pages) {
-				pages[i] = get_page_map(map);
+				struct page *page;
+				pages[i] = page = get_page_map(map);
 				/* FIXME: call the correct function,
 				 * depending on the type of the found page
 				 */
-				if (!pages[i])
+				if (!page)
 					goto bad_page;
-				page_cache_get(pages[i]);
+				if (!PageReserved(page))
+					page_cache_get(page);
+				/* It's important to do this while still
+				 * holding page_table_lock! */
+				if (pin && !PageReserved(page))
+					pin_page_mappings(page);
 			}
 			if (vmas)
 				vmas[i] = vma;
@@ -933,14 +957,33 @@ out:
 	 */
 bad_page:
 	spin_unlock(&mm->page_table_lock);
-	while (i--)
-		page_cache_release(pages[i]);
+	while (i--) {
+		struct page *page = pages[i];
+		if (pin && !PageReserved(page) && !PageCompound(page))
+			unpin_page_mappings(page);
+		page_cache_release(page);
+	}
 	i = -EFAULT;
 	goto out;
 }
 
+int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start,
+		   int len, int write, int force, struct page **pages, struct vm_area_struct **vmas)
+{
+	return __get_user_pages(tsk, mm, start, 
+				len, write, force, pages, vmas, 0);
+}
+
 EXPORT_SYMBOL(get_user_pages);
 
+static inline void unpin_user_pages(struct page **pages, int count)
+{
+	int i;
+	for (i=0; i<count; i++)
+		if (!PageReserved(pages[i]) && !PageCompound(pages[i]))
+			unpin_page_mappings(pages[i]);
+}
+
 /*
  * Force in an entire range of pages from the current process's user VA,
  * and pin them in physical memory.  
@@ -973,8 +1016,8 @@ int map_user_kiobuf(int rw, struct kiobu
 	/* Try to fault in all of the necessary pages */
 	down_read(&mm->mmap_sem);
 	/* rw==READ means read from disk, write into memory area */
-	err = get_user_pages(current, mm, va, pgcount,
-			(rw==READ), 0, iobuf->maplist, NULL);
+	err = __get_user_pages(current, mm, va, pgcount,
+			(rw==READ), 0, iobuf->maplist, NULL, 1);
 	up_read(&mm->mmap_sem);
 	if (err < 0) {
 		unmap_kiobuf(iobuf);
@@ -1028,7 +1071,7 @@ void mark_dirty_kiobuf(struct kiobuf *io
 
 /*
  * Unmap all of the pages referenced by a kiobuf.  We release the pages,
- * and unlock them if they were locked. 
+ * and unpin and unlock them if they were locked. 
  */
 
 void unmap_kiobuf (struct kiobuf *iobuf) 
@@ -1036,6 +1079,7 @@ void unmap_kiobuf (struct kiobuf *iobuf)
 	int i;
 	struct page *map;
 	
+	unpin_user_pages(iobuf->maplist, iobuf->nr_pages);
 	for (i = 0; i < iobuf->nr_pages; i++) {
 		map = iobuf->maplist[i];
 		if (map) {
@@ -2082,6 +2126,8 @@ struct kvec *mm_map_user_kvec(struct mm_
 		if (likely(map != NULL)) {
 			flush_dcache_page(map);
 			get_page(map);
+			if (!PageReserved(map) && !PageCompound(map))
+				pin_page_mappings(map);
 		} else
 			printk (KERN_INFO "Mapped page missing [%d]\n", i);
 		spin_unlock(&mm->page_table_lock);
@@ -2132,6 +2178,8 @@ void unmap_kvec (struct kvec *vec, int d
 				SetPageDirty(map);
 				flush_dcache_page(map);	/* FIXME */
 			}
+			if (!PageCompound(map))
+				unpin_page_mappings(map);
 			__free_page(map);
 		}
 	}
diff -urNp linux-1251/mm/rmap.c linux-1252/mm/rmap.c
--- linux-1251/mm/rmap.c
+++ linux-1252/mm/rmap.c
@@ -426,6 +426,14 @@ static int try_to_unmap_one(struct page 
 		return SWAP_AGAIN;
 	}
 
+	/* The page is pinned for direct IO, can't be swapped right now. */
+	if (unlikely(page_mapping_pinned(page))) {
+		/* SWAP_FAIL instead of SWAP_AGAIN: this is a relatively
+		 * long-term condition as far as the VM lrus are
+		 * concerned. */
+		ret = SWAP_FAIL; 
+		goto out_unlock;
+	}
 
 	/* During mremap, it's possible pages are not in a VMA. */
 	vma = find_vma(mm, address);
