diff -urNp linux-1020/drivers/block/ll_rw_blk.c linux-1030/drivers/block/ll_rw_blk.c
--- linux-1020/drivers/block/ll_rw_blk.c
+++ linux-1030/drivers/block/ll_rw_blk.c
@@ -631,7 +631,7 @@ static struct request *__get_request_wai
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		generic_unplug_device(q);
 		if (q->rq[rw].count == 0)
-			schedule();
+			io_schedule_timeout(HZ);
 		spin_lock_irq(q->queue_lock);
 		rq = get_request(q, rw);
 		spin_unlock_irq(q->queue_lock);
diff -urNp linux-1020/fs/buffer.c linux-1030/fs/buffer.c
--- linux-1020/fs/buffer.c
+++ linux-1030/fs/buffer.c
@@ -190,7 +190,7 @@ void __wait_on_buffer(struct buffer_head
 		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 		if (!buffer_locked(bh))
 			break;
-		schedule();
+		io_schedule();
 	} while (buffer_locked(bh));
 	tsk->state = TASK_RUNNING;
 	remove_wait_queue(&bh->b_wait, &wait);
diff -urNp linux-1020/fs/iobuf.c linux-1030/fs/iobuf.c
--- linux-1020/fs/iobuf.c
+++ linux-1030/fs/iobuf.c
@@ -196,7 +196,7 @@ repeat:
 	set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 	if (atomic_read(&kiobuf->io_count) != 0) {
 		run_task_queue(&tq_disk);
-		schedule();
+		io_schedule();
 		if (atomic_read(&kiobuf->io_count) != 0)
 			goto repeat;
 	}
diff -urNp linux-1020/fs/proc/proc_misc.c linux-1030/fs/proc/proc_misc.c
--- linux-1020/fs/proc/proc_misc.c
+++ linux-1030/fs/proc/proc_misc.c
@@ -324,7 +324,7 @@ static int kstat_read_proc(char *page, c
 	int i, len = 0;
 	extern unsigned long total_forks;
 	unsigned long jif = jiffies;
-	unsigned int sum = 0, user = 0, nice = 0, system = 0;
+	unsigned int sum = 0, user = 0, nice = 0, system = 0, idle = 0, iowait = 0;
 	int major, disk;
 
 	for (i = 0 ; i < smp_num_cpus; i++) {
@@ -333,6 +333,8 @@ static int kstat_read_proc(char *page, c
 		user += kstat.per_cpu_user[cpu];
 		nice += kstat.per_cpu_nice[cpu];
 		system += kstat.per_cpu_system[cpu];
+		idle += kstat.per_cpu_idle[cpu];
+		iowait += kstat.per_cpu_iowait[cpu];
 #if !defined(CONFIG_ARCH_S390)
 		for (j = 0 ; j < NR_IRQS ; j++)
 			sum += kstat.irqs[cpu][j];
@@ -340,18 +342,17 @@ static int kstat_read_proc(char *page, c
 	}
 
 	proc_sprintf(page, &off, &len,
-		      "cpu  %u %u %u %lu\n", user, nice, system,
-		      jif * smp_num_cpus - (user + nice + system));
+		      "cpu  %u %u %u %u %u\n", user, nice, system,
+		      idle, iowait);
 	for (i = 0 ; i < smp_num_cpus; i++)
 		proc_sprintf(page, &off, &len,
-			"cpu%d %u %u %u %lu\n",
+			"cpu%d %u %u %u %u %u\n",
 			i,
 			kstat.per_cpu_user[cpu_logical_map(i)],
 			kstat.per_cpu_nice[cpu_logical_map(i)],
 			kstat.per_cpu_system[cpu_logical_map(i)],
-			jif - (  kstat.per_cpu_user[cpu_logical_map(i)] \
-				   + kstat.per_cpu_nice[cpu_logical_map(i)] \
-				   + kstat.per_cpu_system[cpu_logical_map(i)]));
+			kstat.per_cpu_idle[cpu_logical_map(i)],
+			kstat.per_cpu_iowait[cpu_logical_map(i)]);
 	proc_sprintf(page, &off, &len,
 		"page %u %u\n"
 		"swap %u %u\n"
@@ -391,10 +392,14 @@ static int kstat_read_proc(char *page, c
 	proc_sprintf(page, &off, &len,
 		"\nctxt %lu\n"
 		"btime %lu\n"
-		"processes %lu\n",
+		"processes %lu\n"
+		"procs_running %lu\n"
+		"procs_blocked %lu\n",
 		nr_context_switches(),
 		xtime.tv_sec - jif / HZ,
-		total_forks);
+		total_forks,
+		nr_running(),
+		nr_iowait());
 
 	return proc_calc_metrics(page, start, off, count, eof, len);
 }
diff -urNp linux-1020/include/linux/sched.h linux-1030/include/linux/sched.h
--- linux-1020/include/linux/sched.h
+++ linux-1030/include/linux/sched.h
@@ -171,6 +171,9 @@ extern unsigned long cache_decay_ticks;
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long FASTCALL(schedule_timeout(signed long timeout));
 asmlinkage void schedule(void);
+void io_schedule(void);
+long io_schedule_timeout(long timeout);
+
 
 extern int schedule_task(struct tq_struct *task);
 extern void flush_scheduled_tasks(void);
diff -urNp linux-1020/kernel/sched.c linux-1030/kernel/sched.c
--- linux-1020/kernel/sched.c
+++ linux-1030/kernel/sched.c
@@ -195,13 +195,13 @@ struct runqueue {
 	spinlock_t lock;
 	unsigned long nr_running, nr_switches, expired_timestamp,
 			nr_uninterruptible;
+	atomic_t nr_iowait;
 	struct mm_struct *prev_mm;
 	prio_array_t *active, *expired, arrays[2];
 	int prev_cpu_load[NR_CPUS];
 	int nr_cpus;
 	cpu_t cpu[MAX_NR_SIBLINGS];
 
-	atomic_t nr_iowait;
 } ____cacheline_aligned;
 
 static struct runqueue runqueues[NR_CPUS] __cacheline_aligned;
@@ -1239,6 +1239,10 @@ void scheduler_tick(int user_ticks, int 
 	if (p == cpu_idle_ptr(cpu)) {
 		if (local_bh_count(cpu) || local_irq_count(cpu) > 1)
 			kstat.per_cpu_system[cpu] += sys_ticks;
+		else if (atomic_read(&rq->nr_iowait) > 0)
+			kstat.per_cpu_iowait[cpu] += sys_ticks;
+		else
+			kstat.per_cpu_idle[cpu] += sys_ticks;
 		rebalance_tick(rq, cpu, 1);
 		return;
 	}
@@ -2555,4 +2559,3 @@ void __init sched_init(void)
 	atomic_inc(&init_mm.mm_count);
 	enter_lazy_tlb(&init_mm, current, smp_processor_id());
 }
-
diff -urNp linux-1020/mm/filemap.c linux-1030/mm/filemap.c
--- linux-1020/mm/filemap.c
+++ linux-1030/mm/filemap.c
@@ -858,7 +858,7 @@ void ___wait_on_page(struct page *page)
 		if (!PageLocked(page))
 			break;
 		sync_page(page);
-		schedule();
+		io_schedule();
 	} while (PageLocked(page));
 	__set_task_state(tsk, TASK_RUNNING);
 	remove_wait_queue(waitqueue, &wait);
@@ -931,7 +931,7 @@ static void __lock_page(struct page *pag
 		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 		if (PageLocked(page)) {
 			sync_page(page);
-			schedule();
+			io_schedule();
 		}
 		if (!TryLockPage(page))
 			break;
