diff -urNp linux-1060/drivers/block/ll_rw_blk.c linux-1070/drivers/block/ll_rw_blk.c
--- linux-1060/drivers/block/ll_rw_blk.c
+++ linux-1070/drivers/block/ll_rw_blk.c
@@ -658,8 +658,17 @@ static struct request *__get_request_wai
 	do {
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		generic_unplug_device(q);
-		if (q->rq[rw].count == 0)
+		if (q->rq[rw].count == 0) {
+			/*
+			 * All we care about is not to stall if any request
+			 * is released after we set TASK_UNINTERRUPTIBLE.
+			 * This is the most efficient place to unplug the queue
+			 * in case we hit the race and we can get the request
+			 * without waiting.
+			 */
+			generic_unplug_device(q);
 			io_schedule_timeout(HZ);
+		}
 		spin_lock_irq(q->queue_lock);
 		rq = get_request(q, rw);
 		spin_unlock_irq(q->queue_lock);
@@ -1304,8 +1313,15 @@ void submit_bh_rsector(int rw, struct bu
 	 */
 	bh->b_rdev = bh->b_dev;
 
+	get_bh(bh);
 	generic_make_request(rw, bh);
 
+	/* fix race condition with wait_on_buffer() */
+	smp_mb(); /* spin_unlock may have inclusive semantics */
+	if (waitqueue_active(&bh->b_wait))
+		wake_up(&bh->b_wait);
+	put_bh(bh);
+
 	switch (rw) {
 		case WRITE:
 			kstat.pgpgout += count;
diff -urNp linux-1060/fs/buffer.c linux-1070/fs/buffer.c
--- linux-1060/fs/buffer.c
+++ linux-1070/fs/buffer.c
@@ -186,10 +186,23 @@ void __wait_on_buffer(struct buffer_head
 	get_bh(bh);
 	add_wait_queue(&bh->b_wait, &wait);
 	do {
-		run_task_queue(&tq_disk);
 		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 		if (!buffer_locked(bh))
 			break;
+		/*
+		 * We must read tq_disk in TQ_ACTIVE after the
+		 * add_wait_queue effect is visible to other cpus.
+		 * We could unplug some line above it wouldn't matter
+		 * but we can't do that right after add_wait_queue
+		 * without an smp_mb() in between because spin_unlock
+		 * has inclusive semantics.
+		 * Doing it here is the most efficient place so we
+		 * don't do a suprious unplug if we get a racy
+		 * wakeup that make buffer_locked to return 0, and
+		 * doing it here avoids an explicit smp_mb() we
+		 * rely on the implicit one in set_task_state.
+		 */
+		run_task_queue(&tq_disk);
 		io_schedule();
 	} while (buffer_locked(bh));
 	tsk->state = TASK_RUNNING;
@@ -1609,6 +1622,9 @@ static int __block_write_full_page(struc
 
 	/* Done - end_buffer_io_async will unlock */
 	SetPageUptodate(page);
+
+	wakeup_page_waiters(page);
+
 	return 0;
 
 out:
@@ -1640,6 +1656,7 @@ out:
 	} while (bh != head);
 	if (need_unlock)
 		UnlockPage(page);
+	wakeup_page_waiters(page);
 	return err;
 }
 
@@ -1867,6 +1884,8 @@ int block_read_full_page(struct page *pa
 		else
 			submit_bh(READ, bh);
 	}
+
+	wakeup_page_waiters(page);
 	
 	return 0;
 }
@@ -2604,6 +2623,7 @@ int brw_page(int rw, struct page *page, 
 		submit_bh(rw, bh);
 		bh = next;
 	} while (bh != head);
+	wakeup_page_waiters(page);
 	return 0;
 }
 
diff -urNp linux-1060/fs/reiserfs/inode.c linux-1070/fs/reiserfs/inode.c
--- linux-1060/fs/reiserfs/inode.c
+++ linux-1070/fs/reiserfs/inode.c
@@ -2048,6 +2048,7 @@ static int reiserfs_write_full_page(stru
     */
     if (nr) {
         submit_bh_for_writepage(arr, nr) ;
+	wakeup_page_waiters(page);
     } else {
         UnlockPage(page) ;
     }
diff -urNp linux-1060/include/linux/pagemap.h linux-1070/include/linux/pagemap.h
--- linux-1060/include/linux/pagemap.h
+++ linux-1070/include/linux/pagemap.h
@@ -99,6 +99,8 @@ static inline void wait_on_page(struct p
 		___wait_on_page(page);
 }
 
+extern void wakeup_page_waiters(struct page * page);
+
 /*
  * Returns locked page at given index in given cache, creating it if needed.
  */
diff -urNp linux-1060/kernel/ksyms.c linux-1070/kernel/ksyms.c
--- linux-1060/kernel/ksyms.c
+++ linux-1070/kernel/ksyms.c
@@ -327,6 +327,7 @@ EXPORT_SYMBOL(filemap_fdatasync);
 EXPORT_SYMBOL(filemap_fdatawait);
 EXPORT_SYMBOL(lock_page);
 EXPORT_SYMBOL(unlock_page);
+EXPORT_SYMBOL_GPL(wakeup_page_waiters);
 
 /* device registration */
 EXPORT_SYMBOL(register_chrdev);
diff -urNp linux-1060/mm/filemap.c linux-1070/mm/filemap.c
--- linux-1060/mm/filemap.c
+++ linux-1070/mm/filemap.c
@@ -824,6 +824,21 @@ inline wait_queue_head_t *page_waitqueue
 	return &wait[hash];
 }
 
+/*
+ * This must be called after every submit_bh with end_io
+ * callbacks that would result into the blkdev layer waking
+ * up the page after a queue unplug.
+ */
+void wakeup_page_waiters(struct page * page)
+{
+	wait_queue_head_t * head;
+
+	head = page_waitqueue(page);
+	if (waitqueue_active(head))
+		wake_up(head);
+}
+  
+
 /* 
  * Wait for a page to get unlocked.
  *
