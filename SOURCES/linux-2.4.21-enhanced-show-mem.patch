diff -urNp linux-1227/mm/highmem.c linux-1228/mm/highmem.c
--- linux-1227/mm/highmem.c
+++ linux-1228/mm/highmem.c
@@ -208,6 +208,8 @@ static LIST_HEAD(emergency_pages);
 int nr_emergency_bhs;
 static LIST_HEAD(emergency_bhs);
 
+atomic_t bouncepages = ATOMIC_INIT(0);
+
 /*
  * Simple bounce buffer support for highmem pages.
  * This will be moved to the block layer in 2.5.
@@ -253,9 +255,10 @@ static inline void bounce_end_io (struct
 	page = bh->b_page;
 
 	spin_lock_irqsave(&emergency_lock, flags);
-	if (nr_emergency_pages >= POOL_SIZE)
+	if (nr_emergency_pages >= POOL_SIZE) {
 		__free_page(page);
-	else {
+		atomic_dec(&bouncepages);
+	} else {
 		/*
 		 * We are abusing page->list to manage
 		 * the highmem emergency pool:
@@ -299,6 +302,7 @@ static __init int init_emergency_pool(vo
 			printk("couldn't refill highmem emergency pages");
 			break;
 		}
+		atomic_inc(&bouncepages);
 		list_add(&page->list, &emergency_pages);
 		nr_emergency_pages++;
 	}
@@ -340,8 +344,10 @@ struct page *alloc_bounce_page (void)
 	struct page *page;
 
 	page = alloc_page(GFP_ATOMIC);
-	if (page)
+	if (page) {
+		atomic_inc(&bouncepages);
 		return page;
+	}
 	/*
 	 * No luck. First, kick the VM so it doesn't idle around while
 	 * we are using up our emergency rations.
diff -urNp linux-1227/mm/memory.c linux-1228/mm/memory.c
--- linux-1227/mm/memory.c
+++ linux-1228/mm/memory.c
@@ -61,6 +61,9 @@ unsigned long num_mappedpages;
 void * high_memory;
 struct page *highmem_start_page;
 
+atomic_t lowmem_pagetables = ATOMIC_INIT(0);
+atomic_t highmem_pagetables = ATOMIC_INIT(0);
+
 void vm_account(struct vm_area_struct *vma, pte_t pte, unsigned long address, long adj)
 {
 	struct mm_struct *mm = vma->vm_mm;
@@ -190,6 +193,12 @@ void free_one_pmd(pmd_t * dir)
 	pte = pmd_page(*dir);
 	pmd_clear(dir);
 	pgtable_remove_rmap(pte);
+
+	if ((pte->flags >> ZONE_SHIFT) < 2)
+		atomic_dec(&lowmem_pagetables);
+	else
+		atomic_dec(&highmem_pagetables);
+
 	pte_free(pte);
 }
 
@@ -252,6 +261,12 @@ pte_t *pte_alloc_map(struct mm_struct *m
 			pte_free(new);
 			goto out;
 		}
+
+		if ((new->flags >> ZONE_SHIFT) < 2)
+			atomic_inc(&lowmem_pagetables);
+		else
+			atomic_inc(&highmem_pagetables);
+
 		pgtable_add_rmap(new, mm, address);
 		pmd_populate(mm, pmd, new);
 	}
diff -urNp linux-1227/mm/oom_kill.c linux-1228/mm/oom_kill.c
--- linux-1227/mm/oom_kill.c
+++ linux-1228/mm/oom_kill.c
@@ -171,6 +171,9 @@ static void oom_kill(void)
 	struct task_struct *g, *p, *q;
 	extern wait_queue_head_t kswapd_done;
 
+	/* print the memory stats whenever we OOM kill */
+	show_mem();
+
 	read_lock(&tasklist_lock);
 	p = select_bad_process();
 
diff -urNp linux-1227/mm/page_alloc.c linux-1228/mm/page_alloc.c
--- linux-1227/mm/page_alloc.c
+++ linux-1228/mm/page_alloc.c
@@ -888,28 +888,24 @@ void show_free_areas_core(pg_data_t *pgd
 	unsigned type;
 	pg_data_t *tmpdat = pgdat;
 
-	printk("Free pages:      %6dkB (%6dkB HighMem)\n",
-		K(nr_free_pages()),
-		K(nr_free_highpages()));
-
 	while (tmpdat) {
 		zone_t *zone;
 		for (zone = tmpdat->node_zones;
 			       	zone < tmpdat->node_zones + MAX_NR_ZONES; zone++)
-			printk("Zone:%s freepages:%6lukB min:%6lukB low:%6lukB " 
-				       "high:%6lukB\n", 
+			printk("Zone:%s freepages:%6lu min:%6lu low:%6lu " 
+				       "high:%6lu\n", 
 					zone->name,
-					K(zone->free_pages),
-					K(zone->pages_min),
-					K(zone->pages_low),
-					K(zone->pages_high));
+					zone->free_pages,
+					zone->pages_min,
+					zone->pages_low,
+					zone->pages_high);
 			
 		tmpdat = tmpdat->node_next;
 	}
 
-	printk("Free pages:      %6dkB (%6dkB HighMem)\n",
-		nr_free_pages() << (PAGE_SHIFT-10),
-		nr_free_highpages() << (PAGE_SHIFT-10));
+	printk("Free pages:      %6d (%6d HighMem)\n",
+		nr_free_pages(),
+		nr_free_highpages());
 
 	printk("( Active: %d/%d, inactive_laundry: %d, inactive_clean: %d, free: %d )\n",
 		nr_active_anon_pages() + nr_active_cache_pages(),
@@ -918,29 +914,59 @@ void show_free_areas_core(pg_data_t *pgd
 		nr_inactive_clean_pages(),
 		nr_free_pages());
 
+	tmpdat = pgdat; 
+	while (tmpdat) {
+		zone_t *zone;
+		for (zone = tmpdat->node_zones;
+		     zone < tmpdat->node_zones + MAX_NR_ZONES; zone++)
+			printk("  aa:%ld ac:%ld id:%ld il:%ld ic:%ld fr:%ld\n",
+				zone->active_anon_pages,
+				zone->active_cache_pages,
+				zone->inactive_dirty_pages,
+				zone->inactive_laundry_pages,
+				zone->inactive_clean_pages,
+				zone->free_pages);
+
+		tmpdat = tmpdat->node_next;
+	}
+
 	for (type = 0; type < MAX_NR_ZONES; type++) {
 		struct list_head *head, *curr;
 		zone_t *zone = pgdat->node_zones + type;
- 		unsigned long nr, total, flags;
+		unsigned long total, flags;
+		unsigned long nr[MAX_ORDER];
 
 		total = 0;
 		if (zone->size) {
-			spin_lock_irqsave(&zone->lock, flags);
+			local_irq_save(flags);
+			if (!spin_trylock(&zone->lock)) {
+				printk("[%s zone locked]\n", zone->name);
+				local_irq_restore(flags);
+				continue;
+			}
 		 	for (order = 0; order < MAX_ORDER; order++) {
 				head = &(zone->free_area + order)->free_list;
 				curr = head;
-				nr = 0;
+				nr[order] = 0;
 				for (;;) {
 					if ((curr = curr->next) == head)
 						break;
-					nr++;
+					nr[order]++;
 				}
-				total += nr * (1 << order);
-				printk("%lu*%lukB ", nr, K(1UL) << order);
+				total += nr[order] * (1 << order);
+			}
+			spin_unlock(&zone->lock);
+			local_irq_restore(flags);
+			/*
+			 * Move the printk out of the irq-spinlock range
+			 * to avoid nmi_watchdog timer kicks off dump.
+			 */
+		 	for (order = 0; order < MAX_ORDER; order++) {
+				printk("%lu*%lukB ", nr[order], K(1UL) << order);
 			}
-			spin_unlock_irqrestore(&zone->lock, flags);
 		}
-		printk("= %lukB)\n", K(total));
+		if (zone->size)
+			printk("= %lukB)\n", K(total));
 	}
 
 #ifdef SWAP_CACHE_INFO
@@ -948,9 +974,26 @@ void show_free_areas_core(pg_data_t *pgd
 #endif	
 }
 
+extern int slabpages;
+extern int nr_threads;
+extern atomic_t lowmem_pagetables, highmem_pagetables;
+#ifdef CONFIG_HIGHMEM
+extern atomic_t bouncepages;
+extern int nr_emergency_pages;
+#endif
+
 void show_free_areas(void)
 {
 	show_free_areas_core(pgdat_list);
+	printk("%d pages of slabcache\n", slabpages);
+	printk("%d pages of kernel stacks\n", nr_threads * 2);
+	printk("%d lowmem pagetables, %d highmem pagetables\n", 
+		atomic_read(&lowmem_pagetables),
+		atomic_read(&highmem_pagetables));
+#ifdef CONFIG_HIGHMEM
+	printk("%d bounce buffer pages, %d are on the emergency list\n",
+		atomic_read(&bouncepages), nr_emergency_pages);
+#endif
 }
 
 /*
diff -urNp linux-1227/mm/slab.c linux-1228/mm/slab.c
--- linux-1227/mm/slab.c
+++ linux-1228/mm/slab.c
@@ -145,6 +145,8 @@ typedef unsigned int kmem_bufctl_t;
  */
 static unsigned long offslab_limit;
 
+int slabpages = 0;
+
 /*
  * slab_t
  *
@@ -499,7 +501,8 @@ static inline void * kmem_getpages (kmem
 	 * would be relatively rare and ignorable.
 	 */
 	flags |= cachep->gfpflags;
-	addr = (void*) __get_free_pages(flags, cachep->gfporder);
+	if ((addr = (void *)__get_free_pages(flags, cachep->gfporder)) != NULL)
+		slabpages += 1 << cachep->gfporder;
 	/* Assume that now we have the pages no one else can legally
 	 * messes with the 'struct page's.
 	 * However vm_scan() might try to test the structure to see if
@@ -520,6 +523,7 @@ static inline void kmem_freepages (kmem_
 	 * but their 'struct page's might be accessed in
 	 * vm_scan(). Shouldn't be a worry.
 	 */
+	slabpages -= i;
 	while (i--) {
 		PageClearSlab(page);
 		page++;
