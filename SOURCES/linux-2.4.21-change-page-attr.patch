diff -urNp linux-1270/arch/i386/mm/pageattr.c linux-1280/arch/i386/mm/pageattr.c
--- linux-1270/arch/i386/mm/pageattr.c
+++ linux-1280/arch/i386/mm/pageattr.c
@@ -32,7 +32,8 @@ static inline pte_t *lookup_address(unsi
     return pte_offset_kernel(pmd, address);
 } 
 
-static struct page *split_large_page(unsigned long address, pgprot_t prot)
+static struct page *split_large_page(unsigned long address, pgprot_t prot,
+					pgprot_t ref_prot)
 { 
 	int i; 
 	unsigned long addr;
@@ -45,7 +46,7 @@ static struct page *split_large_page(uns
 	pbase = (pte_t *)page_address(base);
 	for (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {
 		pbase[i] = mk_pte_phys(addr, 
-				      addr == address ? prot : PAGE_KERNEL);
+				      addr == address ? prot : ref_prot);
 	}
 	return base;
 } 
@@ -122,19 +123,22 @@ __change_page_attr(struct page *page, pg
 	kpte_page = virt_to_page(((unsigned long)kpte) & PAGE_MASK);
 	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL)) { 
 		if ((pte_val(*kpte) & _PAGE_PSE) == 0) {
-			pte_t old = *kpte;
-			pte_t standard = mk_pte(page, PAGE_KERNEL); 
-
 			set_pte_atomic(kpte, mk_pte(page, prot)); 
-			if (pte_same(old,standard))
-				atomic_inc(&kpte_page->count);
 		} else {
-			struct page *split = split_large_page(address, prot); 
+			pgprot_t ref_prot;
+			struct page *split;
+			extern char _etext;
+
+			ref_prot =
+			 ((address & LARGE_PAGE_MASK) < (unsigned long)&_etext)
+				? PAGE_KERNEL_EXEC : PAGE_KERNEL;
+			split = split_large_page(address, prot, ref_prot);
 			if (!split)
 				return -ENOMEM;
-			atomic_inc(&kpte_page->count); 	
-			set_pmd_pte(kpte,address,mk_pte(split, PAGE_KERNEL));
-		}	
+			set_pmd_pte(kpte, address, mk_pte(split, ref_prot));
+			kpte_page = split;
+		}
+		atomic_inc(&kpte_page->count);
 	} else if ((pte_val(*kpte) & _PAGE_PSE) == 0) { 
 		set_pte_atomic(kpte, mk_pte(page, PAGE_KERNEL));
 		atomic_dec(&kpte_page->count); 
diff -urNp linux-1270/arch/i386/vmlinux.lds.in linux-1280/arch/i386/vmlinux.lds.in
--- linux-1270/arch/i386/vmlinux.lds.in
+++ linux-1280/arch/i386/vmlinux.lds.in
@@ -27,6 +27,11 @@ SECTIONS
 	*(.gnu.warning)
 	} = 0x9090
 
+  . = ALIGN(4096);		/* kernel entry code */
+  entry_tramp_start = .;
+  .entry.text : { *(.entry.text) }
+  entry_tramp_end = .;
+
   _etext = .;			/* End of text section */
 
   .rodata : { *(.rodata) *(.rodata.*) }
@@ -54,11 +59,6 @@ SECTIONS
   . = ALIGN(8192);		/* init_task */
   .data.init_task : { *(.data.init_task) }
 
-  entry_tramp_start = .;
-  . = ALIGN(4096);		/* kernel entry code */
-  .entry.text : { *(.entry.text) }
-  entry_tramp_end = .;
-
   . = ALIGN(4096);		/* Init code and data */
   __init_begin = .;
   .text.init : { *(.text.init) }
diff -urNp linux-1270/arch/x86_64/mm/pageattr.c linux-1280/arch/x86_64/mm/pageattr.c
--- linux-1270/arch/x86_64/mm/pageattr.c
+++ linux-1280/arch/x86_64/mm/pageattr.c
@@ -26,7 +26,8 @@ static inline pte_t *lookup_address(unsi
         return pte_offset_kernel(pmd, address);
 } 
 
-static struct page *split_large_page(unsigned long address, pgprot_t prot)
+static struct page *split_large_page(unsigned long address, pgprot_t prot,
+					pgprot_t ref_prot)
 { 
 	int i; 
 	unsigned long addr;
@@ -39,7 +40,7 @@ static struct page *split_large_page(uns
 	pbase = (pte_t *)page_address(base);
 	for (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {
 		pbase[i] = mk_pte_phys(addr, 
-				      addr == address ? prot : PAGE_KERNEL);
+				      addr == address ? prot : ref_prot);
 	}
 	return base;
 } 
@@ -60,7 +61,8 @@ static void flush_kernel_map(void * addr
 
 /* no more special protections in this 2MB area - revert to a
    large page again. */
-static inline void revert_page(struct page *kpte_page, unsigned long address)
+static inline void revert_page(struct page *kpte_page, unsigned long address,
+				int is_kern_mapping)
 {
 	pgd_t *pgd;
 	pmd_t *pmd; 
@@ -72,7 +74,8 @@ static inline void revert_page(struct pa
 	if (!pmd) BUG(); 
 	if ((pmd_val(*pmd) & _PAGE_GLOBAL) == 0) BUG(); 
 	
-	large_pte = mk_pte_phys(__pa(address) & LARGE_PAGE_MASK, PAGE_KERNEL_LARGE); 
+	large_pte = mk_pte_phys(__pa(address) & LARGE_PAGE_MASK,
+		(is_kern_mapping) ? PAGE_KERNEL_EXEC_LARGE : PAGE_KERNEL_LARGE);
 	set_pte((pte_t *)pmd, large_pte);
 }	
  
@@ -90,7 +93,7 @@ static inline void revert_page(struct pa
  */
 static int 
 __change_page_attr(unsigned long address, struct page *page, pgprot_t prot, 
-		   struct page **oldpage) 
+		   struct page **oldpage, int is_kern_mapping)
 { 
 	pte_t *kpte; 
 	struct page *kpte_page;
@@ -101,18 +104,20 @@ __change_page_attr(unsigned long address
 	kpte_page = virt_to_page(((unsigned long)kpte) & PAGE_MASK);
 	if (pgprot_val(prot) != pgprot_val(PAGE_KERNEL)) { 
 		if ((pte_val(*kpte) & _PAGE_PSE) == 0) { 
-			pte_t old = *kpte;
-			pte_t standard = mk_pte(page, PAGE_KERNEL); 
-
 			set_pte(kpte, mk_pte(page, prot)); 
-			if (pte_same(old,standard))
-				atomic_inc(&kpte_page->count);
 		} else {
-			struct page *split = split_large_page(address, prot); 
+			pgprot_t ref_prot;
+			struct page *split;
+
+			ref_prot = (is_kern_mapping)
+				? PAGE_KERNEL_EXECUTABLE : PAGE_KERNEL;
+			split = split_large_page(address, prot, ref_prot);
 			if (!split)
 				return -ENOMEM;
-			set_pte(kpte,mk_pte(split, PAGE_KERNEL));
-		}	
+			set_pte(kpte, mk_pte(split, ref_prot));
+			kpte_page = split;
+		}
+		atomic_inc(&kpte_page->count);	
 	} else if ((pte_val(*kpte) & _PAGE_PSE) == 0) { 
 		set_pte(kpte, mk_pte(page, PAGE_KERNEL));
 		atomic_dec(&kpte_page->count); 
@@ -120,7 +125,7 @@ __change_page_attr(unsigned long address
 
 	if (atomic_read(&kpte_page->count) == 1) { 
 		*oldpage = kpte_page;
-		revert_page(kpte_page, address);
+		revert_page(kpte_page, address, is_kern_mapping);
 	} 
 	return 0;
 } 
@@ -145,13 +150,13 @@ int change_page_attr(struct page *page, 
 	for (i = 0; i < numpages; i++, page++) { 
 		fpage = fpage2 = NULL;
 		err = __change_page_attr((unsigned long)page_address(page), 
-					 page, prot, &fpage); 
+					 page, prot, &fpage, 0);
 		
 		/* Handle kernel mapping too which aliases part of the lowmem */
 		if (!err && page_to_phys(page) < KERNEL_TEXT_SIZE) { 
 			err = __change_page_attr((unsigned long) __START_KERNEL_map + 
 						 page_to_phys(page),
-						 page, prot, &fpage2); 
+						 page, prot, &fpage2, 1);
 		} 
 
 		if (err) 
diff -urNp linux-1270/include/asm-x86_64/pgtable.h linux-1280/include/asm-x86_64/pgtable.h
--- linux-1270/include/asm-x86_64/pgtable.h
+++ linux-1280/include/asm-x86_64/pgtable.h
@@ -251,6 +251,7 @@ extern inline void pgd_clear (pgd_t * pg
 #define __PAGE_KERNEL_LARGE     (__PAGE_KERNEL | _PAGE_PSE)
 #define __PAGE_KERNEL_LARGE_NOCACHE     (__PAGE_KERNEL_LARGE | _PAGE_PCD)
 #define __PAGE_KERNEL_EXECUTABLE (__PAGE_KERNEL & ~_PAGE_NX)
+#define __PAGE_KERNEL_EXEC_LARGE (__PAGE_KERNEL_EXECUTABLE | _PAGE_PSE)
 #define __PAGE_USER_NOCACHE_RO	\
 	(_PAGE_PRESENT | _PAGE_USER | _PAGE_DIRTY | _PAGE_ACCESSED) 
 
@@ -267,6 +268,7 @@ extern unsigned long __supported_pte_mas
 #define PAGE_KERNEL_LARGE_NOCACHE __pgprot(__PAGE_KERNEL_LARGE_NOCACHE|_PAGE_GLOBAL)
 #define PAGE_USER_NOCACHE_RO __pgprot(__PAGE_USER_NOCACHE_RO|_PAGE_GLOBAL)
 #define PAGE_KERNEL_EXECUTABLE __pgprot(__PAGE_KERNEL_EXECUTABLE|_PAGE_GLOBAL)
+#define PAGE_KERNEL_EXEC_LARGE __pgprot(__PAGE_KERNEL_EXEC_LARGE|_PAGE_GLOBAL)
 
 /*         xwr */
 #define __P000	PAGE_NONE
