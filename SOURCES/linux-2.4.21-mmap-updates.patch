diff -urNp linux-1170/fs/binfmt_aout.c linux-1180/fs/binfmt_aout.c
--- linux-1170/fs/binfmt_aout.c
+++ linux-1180/fs/binfmt_aout.c
@@ -45,7 +45,7 @@ static void set_brk(unsigned long start,
 	end = PAGE_ALIGN(end);
 	if (end <= start)
 		return;
-	do_brk(start, end - start);
+	do_brk_locked(start, end - start);
 }
 
 /*
@@ -341,7 +341,7 @@ static int load_aout_binary(struct linux
 		map_size = ex.a_text+ex.a_data;
 #endif
 
-		error = do_brk(text_addr & PAGE_MASK, map_size);
+		error = do_brk_locked(text_addr & PAGE_MASK, map_size);
 		if (error != (text_addr & PAGE_MASK)) {
 			send_sig(SIGKILL, current, 0);
 			return error;
@@ -375,7 +375,7 @@ static int load_aout_binary(struct linux
 
 		if (!bprm->file->f_op->mmap||((fd_offset & ~PAGE_MASK) != 0)) {
 			loff_t pos = fd_offset;
-			do_brk(N_TXTADDR(ex), ex.a_text+ex.a_data);
+			do_brk_locked(N_TXTADDR(ex), ex.a_text+ex.a_data);
 			bprm->file->f_op->read(bprm->file,(char *)N_TXTADDR(ex),
 					ex.a_text+ex.a_data, &pos);
 			flush_icache_range((unsigned long) N_TXTADDR(ex),
@@ -472,7 +472,7 @@ static int load_aout_library(struct file
 			error_time = jiffies;
 		}
 
-		do_brk(start_addr, ex.a_text + ex.a_data + ex.a_bss);
+		do_brk_locked(start_addr, ex.a_text + ex.a_data + ex.a_bss);
 		
 		file->f_op->read(file, (char *)start_addr,
 			ex.a_text + ex.a_data, &pos);
@@ -496,7 +496,7 @@ static int load_aout_library(struct file
 	len = PAGE_ALIGN(ex.a_text + ex.a_data);
 	bss = ex.a_text + ex.a_data + ex.a_bss;
 	if (bss > len) {
-		error = do_brk(start_addr + len, bss - len);
+		error = do_brk_locked(start_addr + len, bss - len);
 		retval = error;
 		if (error != start_addr + len)
 			goto out;
diff -urNp linux-1170/fs/binfmt_elf.c linux-1180/fs/binfmt_elf.c
--- linux-1170/fs/binfmt_elf.c
+++ linux-1180/fs/binfmt_elf.c
@@ -85,7 +85,7 @@ static int set_brk(unsigned long start, 
 	start = ELF_PAGEALIGN(start);
 	end = ELF_PAGEALIGN(end);
 	if (end > start) {
-		unsigned long addr = do_brk(start, end - start);
+		unsigned long addr = do_brk_locked(start, end - start);
 		if (BAD_ADDR(addr))
 			return addr;
 	}
@@ -413,7 +413,7 @@ static unsigned long load_elf_interp(str
 
 	/* Map the last of the bss segment */
 	if (last_bss > elf_bss) {
-		error = do_brk(elf_bss, last_bss - elf_bss);
+		error = do_brk_locked(elf_bss, last_bss - elf_bss);
 		if (BAD_ADDR(error))
 			goto out_close;
 	}
@@ -453,7 +453,7 @@ static unsigned long load_aout_interp(st
 		goto out;
 	}
 
-	do_brk(0, text_data);
+	do_brk_locked(0, text_data);
 	if (!interpreter->f_op || !interpreter->f_op->read)
 		goto out;
 	if (interpreter->f_op->read(interpreter, addr, text_data, &offset) < 0)
@@ -461,7 +461,7 @@ static unsigned long load_aout_interp(st
 	flush_icache_range((unsigned long)addr,
 	                   (unsigned long)addr + text_data);
 
-	do_brk(ELF_PAGESTART(text_data + ELF_MIN_ALIGN - 1),
+	do_brk_locked(ELF_PAGESTART(text_data + ELF_MIN_ALIGN - 1),
 		interp_ex->a_bss);
 	elf_entry = interp_ex->a_entry;
 
@@ -1037,7 +1037,7 @@ static int load_elf_library(struct file 
 	len = ELF_PAGESTART(elf_phdata->p_filesz + elf_phdata->p_vaddr + ELF_MIN_ALIGN - 1);
 	bss = elf_phdata->p_memsz + elf_phdata->p_vaddr;
 	if (bss > len)
-		do_brk(len, bss - len);
+		do_brk_locked(len, bss - len);
 	error = 0;
 
 out_free_ph:
diff -urNp linux-1170/include/linux/mm.h linux-1180/include/linux/mm.h
--- linux-1170/include/linux/mm.h
+++ linux-1180/include/linux/mm.h
@@ -715,7 +715,8 @@ extern void lock_vma_mappings(struct vm_
 extern void unlock_vma_mappings(struct vm_area_struct *);
 extern void insert_vm_struct(struct mm_struct *, struct vm_area_struct *);
 extern void __insert_vm_struct(struct mm_struct *, struct vm_area_struct *);
-extern void build_mmap_rb(struct mm_struct *);
+extern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,
+	rb_node_t **, rb_node_t *);
 extern void exit_mmap(struct mm_struct *);
 
 extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);
@@ -740,6 +741,7 @@ out:
 extern int do_munmap(struct mm_struct *, unsigned long, size_t, int acct);
 
 extern unsigned long do_brk(unsigned long, unsigned long);
+extern unsigned long do_brk_locked(unsigned long, unsigned long);
 
 #ifdef __i386__
 extern void arch_remove_exec_range(struct mm_struct *mm, unsigned long limit);
diff -urNp linux-1170/ipc/shm.c linux-1180/ipc/shm.c
--- linux-1170/ipc/shm.c
+++ linux-1180/ipc/shm.c
@@ -166,6 +166,8 @@ static int shm_mmap(struct file * file, 
 {
 	UPDATE_ATIME(file->f_dentry->d_inode);
 	vma->vm_ops = &shm_vm_ops;
+	if (!(vma->vm_flags & VM_WRITE))
+		vma->vm_flags &= ~VM_MAYWRITE;
 	shm_inc(file->f_dentry->d_inode->i_ino);
 	return 0;
 }
diff -urNp linux-1170/kernel/fork.c linux-1180/kernel/fork.c
--- linux-1170/kernel/fork.c
+++ linux-1180/kernel/fork.c
@@ -186,6 +186,7 @@ static struct task_struct *dup_task_stru
 static inline int dup_mmap(struct mm_struct * mm)
 {
 	struct vm_area_struct * mpnt, *tmp, **pprev;
+	rb_node_t **rb_link, *rb_parent;
 	int retval;
 	unsigned long charge;
 
@@ -199,6 +200,9 @@ static inline int dup_mmap(struct mm_str
 	mm->non_executable_cache = NON_EXECUTABLE_CACHE(current);
 	mm->rss = 0;
 	mm->cpu_vm_mask = 0;
+	mm->mm_rb = RB_ROOT;
+	rb_link = &mm->mm_rb.rb_node;
+	rb_parent = NULL;
 	pprev = &mm->mmap;
 
 	/*
@@ -252,11 +256,17 @@ static inline int dup_mmap(struct mm_str
 
 		/*
 		 * Link in the new vma and copy the page table entries:
-		 * link in first so that swapoff can see swap entries.
+		 * link in first so that swapoff can see swap entries,
+		 * and try_to_unmap_one's find_vma find the new vma.
 		 */
 		spin_lock(&mm->page_table_lock);
 		*pprev = tmp;
 		pprev = &tmp->vm_next;
+
+		__vma_link_rb(mm, tmp, rb_link, rb_parent);
+		rb_link = &tmp->vm_rb.rb_right;
+		rb_parent = &tmp->vm_rb;
+
 		mm->map_count++;
 		retval = copy_page_range(mm, current->mm, tmp);
 		spin_unlock(&mm->page_table_lock);
@@ -268,7 +278,6 @@ static inline int dup_mmap(struct mm_str
 			goto fail_nomem;
 	}
 	retval = 0;
-	build_mmap_rb(mm);
 out:
 	flush_tlb_mm(current->mm);
 	return retval;
diff -urNp linux-1170/kernel/ksyms.c linux-1180/kernel/ksyms.c
--- linux-1170/kernel/ksyms.c
+++ linux-1180/kernel/ksyms.c
@@ -104,6 +104,7 @@ EXPORT_SYMBOL(__stop___kallsyms);
 EXPORT_SYMBOL(do_mmap_pgoff);
 EXPORT_SYMBOL(do_munmap);
 EXPORT_SYMBOL(do_brk);
+EXPORT_SYMBOL(do_brk_locked);
 EXPORT_SYMBOL(exit_mm);
 EXPORT_SYMBOL(exit_files);
 EXPORT_SYMBOL(exit_fs);
diff -urNp linux-1170/mm/filemap.c linux-1180/mm/filemap.c
--- linux-1170/mm/filemap.c
+++ linux-1180/mm/filemap.c
@@ -2174,7 +2174,7 @@ static void nopage_sequential_readahead(
 		if (vma->vm_raend > (vma->vm_pgoff + ra_window + ra_window)) {
 			unsigned long window = ra_window << PAGE_SHIFT;
 
-			end = vma->vm_start + (vma->vm_raend << PAGE_SHIFT);
+			end = vma->vm_start + ((vma->vm_raend - vma->vm_pgoff) << PAGE_SHIFT);
 			end -= window + window;
 			filemap_sync(vma, end - window, window, MS_INVALIDATE);
 		}
@@ -2423,6 +2423,9 @@ int filemap_sync(struct vm_area_struct *
 	unsigned long end = address + size;
 	int error = 0;
 
+	if ((vma->vm_flags & VM_HUGETLB) || address < vma->vm_start || end > vma->vm_end)
+		return error;
+
 	/* Aquire the lock early; it may be possible to avoid dropping
 	 * and reaquiring it repeatedly.
 	 */
diff -urNp linux-1170/mm/mmap.c linux-1180/mm/mmap.c
--- linux-1170/mm/mmap.c
+++ linux-1180/mm/mmap.c
@@ -378,8 +378,8 @@ static inline void __vma_link_list(struc
 	}
 }
 
-static inline void __vma_link_rb(struct mm_struct * mm, struct vm_area_struct * vma,
-				 rb_node_t ** rb_link, rb_node_t * rb_parent)
+void __vma_link_rb(struct mm_struct * mm, struct vm_area_struct * vma,
+		 rb_node_t ** rb_link, rb_node_t * rb_parent)
 {
 	rb_link_node(&vma->vm_rb, rb_parent, rb_link);
 	rb_insert_color(&vma->vm_rb, &mm->mm_rb);
@@ -528,6 +528,16 @@ unsigned long do_mmap_pgoff(struct file 
 	if (mm->map_count > max_map_count)
 		return -ENOMEM;
 
+	/*
+	 * If the application expects PROT_READ to imply PROT_EXEC, and
+	 * the mapping isn't for a file whose underlying filesystem is
+	 * mounted w/noexec, then enable PROT_EXEC.
+	 */
+	if ((prot & (PROT_EXEC | PROT_READ)) == PROT_READ &&
+	    !(current->flags & PF_RELOCEXEC) &&
+	    !(file && (file->f_vfsmnt->mnt_flags & MNT_NOEXEC)))
+		prot |= PROT_EXEC;
+
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
@@ -920,6 +930,14 @@ int expand_stack(struct vm_area_struct *
 	 */
 	address &= PAGE_MASK;
  	spin_lock(&vma->vm_mm->page_table_lock);
+
+	/* check if another thread has already expanded the stack */
+	if (address >= vma->vm_start) {
+		spin_unlock(&vma->vm_mm->page_table_lock);
+		vm_validate_enough("exiting expand_stack - NOTHING TO DO");
+		return 0;
+	}
+
 	grow = (vma->vm_start - address) >> PAGE_SHIFT;
 
 	/* Overcommit.. */
@@ -1373,22 +1391,19 @@ out:
 	return addr;
 }
 
-/* Build the RB tree corresponding to the VMA list. */
-void build_mmap_rb(struct mm_struct * mm)
+/* locking version of do_brk. */
+unsigned long do_brk_locked(unsigned long addr, unsigned long len)
 {
-	struct vm_area_struct * vma;
-	rb_node_t ** rb_link, * rb_parent;
+	unsigned long ret;
 
-	mm->mm_rb = RB_ROOT;
-	rb_link = &mm->mm_rb.rb_node;
-	rb_parent = NULL;
-	for (vma = mm->mmap; vma; vma = vma->vm_next) {
-		__vma_link_rb(mm, vma, rb_link, rb_parent);
-		rb_parent = &vma->vm_rb;
-		rb_link = &rb_parent->rb_right;
-	}
+	down_write(&current->mm->mmap_sem);
+	ret = do_brk(addr, len);
+	up_write(&current->mm->mmap_sem);
+
+	return ret;
 }
 
+
 /* Release all mmaps. */
 void exit_mmap(struct mm_struct * mm)
 {
diff -urNp linux-1170/mm/mprotect.c linux-1180/mm/mprotect.c
--- linux-1170/mm/mprotect.c
+++ linux-1180/mm/mprotect.c
@@ -62,6 +62,7 @@ static inline void change_pte_range(stru
 			entry = vm_ptep_get_and_clear(vma, address + start, pte);
 			vm_set_pte(vma, address + start,
 				   pte, pte_modify(entry, newprot));
+			update_mmu_cache(vma, address + start, entry);
 		}
 		address += PAGE_SIZE;
 		pte++;
@@ -386,6 +387,16 @@ asmlinkage long sys_mprotect(unsigned lo
 		/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */
 
 		newflags = prot | (vma->vm_flags & ~(PROT_READ | PROT_WRITE | PROT_EXEC));
+		/*
+		 * If the application expects PROT_READ to imply PROT_EXEC, and
+		 * the mapping isn't for a region whose permissions disallow
+		 * execute access, then enable PROT_EXEC.
+		 */
+		if ((prot & (PROT_EXEC | PROT_READ)) == PROT_READ &&
+		    !(current->flags & PF_RELOCEXEC) &&
+		    (vma->vm_flags & VM_MAYEXEC))
+			newflags |= PROT_EXEC;
+
 		if ((newflags & ~(newflags >> 4)) & 0xf) {
 			error = -EACCES;
 			goto out;
diff -urNp linux-1170/mm/mremap.c linux-1180/mm/mremap.c
--- linux-1170/mm/mremap.c
+++ linux-1180/mm/mremap.c
@@ -144,8 +144,10 @@ static int move_one_page(struct vm_area_
 		dst = alloc_one_pte_map(mm, new_addr);
 		if (src == NULL)
 			src = get_one_pte_map_nested(mm, old_addr);
-		error = copy_one_pte(vma, src, dst, old_addr, new_addr, &pte_chain);
-		pte_unmap_nested(src);
+		if (src) {
+			error = copy_one_pte(vma, src, dst, old_addr, new_addr, &pte_chain);
+			pte_unmap_nested(src);
+		}
 		pte_unmap(dst);
 	}
 	flush_tlb_page(vma, old_addr);
@@ -306,6 +308,13 @@ unsigned long do_mremap(unsigned long ad
 
 		if (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)
 			goto out;
+		/*
+		 * Allow new_len == 0 only if new_addr == addr
+		 * to preserve truncation in place (that was working
+		 * safe and some app may depend on it).
+		 */
+		if (unlikely(!new_len && new_addr != addr))
+			goto out;
 
 		/* Check if the location we're moving into overlaps the
 		 * old location at all, and fail if it does.
@@ -316,7 +325,9 @@ unsigned long do_mremap(unsigned long ad
 		if ((addr <= new_addr) && (addr+old_len) > new_addr)
 			goto out;
 
-		do_munmap(current->mm, new_addr, new_len, 1);
+		ret = do_munmap(current->mm, new_addr, new_len, 1);
+		if (ret && new_len)
+			goto out;
 	}
 
 	/*
@@ -324,9 +335,11 @@ unsigned long do_mremap(unsigned long ad
 	 * the unnecessary pages..
 	 * do_munmap does all the needed commit accounting
 	 */
-	ret = addr;
 	if (old_len >= new_len) {
-		do_munmap(current->mm, addr+new_len, old_len - new_len, 1);
+		ret = do_munmap(current->mm, addr+new_len, old_len-new_len, 1);
+		if (ret && old_len != new_len)
+			goto out;
+		ret = addr;
 		if (!(flags & MREMAP_FIXED) || (new_addr == addr))
 			goto out;
 		old_len = new_len;
