diff -urNp linux-1150/arch/ia64/config.in linux-1151/arch/ia64/config.in
--- linux-1150/arch/ia64/config.in
+++ linux-1151/arch/ia64/config.in
@@ -89,6 +89,29 @@ fi
 
 define_bool CONFIG_KCORE_ELF y	# On IA-64, we always want an ELF /proc/kcore.
 
+if [ "$CONFIG_HUGETLB_PAGE" = "y"  -o "$CONFIG_HUGETLBFS" = "y" ]; then
+	define_int CONFIG_FORCE_MAX_ZONEORDER 15
+	if [ "$CONFIG_MCKINLEY" = "y" ]; then
+		choice 'IA-64 Huge TLB Page Size'		\
+			"4GB	CONFIG_HUGETLB_PAGE_SIZE_4GB	\
+			1GB	CONFIG_HUGETLB_PAGE_SIZE_1GB	\
+			256MB	CONFIG_HUGETLB_PAGE_SIZE_256MB	\
+			64MB	CONFIG_HUGETLB_PAGE_SIZE_64MB	\
+			16MB	CONFIG_HUGETLB_PAGE_SIZE_16MB	\
+			4MB	CONFIG_HUGETLB_PAGE_SIZE_4MB	\
+			1MB	CONFIG_HUGETLB_PAGE_SIZE_1MB	\
+			256KB	CONFIG_HUGETLB_PAGE_SIZE_256KB" 256MB
+	else
+		choice 'IA-64 Huge TLB Page Size'		\
+			"256MB	CONFIG_HUGETLB_PAGE_SIZE_256MB	\
+			64MB	CONFIG_HUGETLB_PAGE_SIZE_64MB	\
+			16MB	CONFIG_HUGETLB_PAGE_SIZE_16MB	\
+			4MB	CONFIG_HUGETLB_PAGE_SIZE_4MB	\
+			1MB	CONFIG_HUGETLB_PAGE_SIZE_1MB	\
+			256KB	CONFIG_HUGETLB_PAGE_SIZE_256KB" 256MB
+		fi
+fi
+
 bool 'Use PAL_HALT_LIGHT in idle loop' CONFIG_IA64_PAL_IDLE
 bool 'SMP support' CONFIG_SMP
 bool 'Support running of Linux/x86 binaries' CONFIG_IA32_SUPPORT
diff -urNp linux-1150/arch/ia64/kernel/ivt.S linux-1151/arch/ia64/kernel/ivt.S
--- linux-1150/arch/ia64/kernel/ivt.S
+++ linux-1151/arch/ia64/kernel/ivt.S
@@ -114,6 +114,10 @@ ENTRY(vhpt_miss)
 	 *	- the faulting virtual address has no L1, L2, or L3 mapping
 	 */
 	mov r16=cr.ifa				// get address that caused the TLB miss
+#ifdef CONFIG_HUGETLB_PAGE
+        movl r18=PAGE_SHIFT
+        mov r25=cr.itir
+#endif
 	;;
 	rsm psr.dt				// use physical addressing for data
 	mov r31=pr				// save the predicate registers
@@ -121,8 +125,18 @@ ENTRY(vhpt_miss)
 	shl r21=r16,3				// shift bit 60 into sign bit
 	shr.u r17=r16,61			// get the region number into r17
 	;;
+	shr r22=r21,3
+#ifdef CONFIG_HUGETLB_PAGE
+        extr.u r26=r25,2,6
+        ;;
+        cmp.eq p8,p0=HPAGE_SHIFT,r26
+        ;;
+(p8)    dep r25=r18,r25,2,6
+(p8)    shr r22=r22,HPAGE_SHIFT-PAGE_SHIFT
+#endif
+	;;
 	cmp.eq p6,p7=5,r17			// is IFA pointing into to region 5?
-	shr.u r18=r16,PGDIR_SHIFT		// get bits 33-63 of the faulting address
+	shr.u r18=r22,PGDIR_SHIFT		// get bits 33-63 of the faulting address
 	;;
 (p7)	dep r17=r17,r19,(PAGE_SHIFT-3),3	// put region number bits in place
 	srlz.d					// ensure "rsm psr.dt" has taken effect
@@ -133,7 +147,7 @@ ENTRY(vhpt_miss)
 (p6)	dep r17=r18,r19,3,(PAGE_SHIFT-3)	// r17=PTA + IFA(33,42)*8
 (p7)	dep r17=r18,r17,3,(PAGE_SHIFT-6)	// r17=PTA + (((IFA(61,63) << 7) | IFA(33,39))*8)
 	cmp.eq p7,p6=0,r21			// unused address bits all zeroes?
-	shr.u r18=r16,PMD_SHIFT			// shift L2 index into position
+	shr.u r18=r22,PMD_SHIFT			// shift L2 index into position
 	;;
 	ld8 r17=[r17]				// fetch the L1 entry (may be 0)
 	;;
@@ -141,7 +155,7 @@ ENTRY(vhpt_miss)
 	dep r17=r18,r17,3,(PAGE_SHIFT-3)	// compute address of L2 page table entry
 	;;
 (p7)	ld8 r20=[r17]				// fetch the L2 entry (may be 0)
-	shr.u r19=r16,PAGE_SHIFT		// shift L3 index into position
+	shr.u r19=r22,PAGE_SHIFT		// shift L3 index into position
 	;;
 (p7)	cmp.eq.or.andcm p6,p7=r20,r0		// was L2 entry NULL?
 	dep r21=r19,r20,3,(PAGE_SHIFT-3)	// compute address of L3 page table entry
@@ -160,6 +174,10 @@ ENTRY(vhpt_miss)
 (p6)	br.cond.spnt.many page_fault		// handle bad address/page not present (page fault)
 	mov cr.ifa=r22
 
+#ifdef CONFIG_HUGETLB_PAGE
+(p8)    mov cr.itir=r25                         // change to default page-size for VHPT
+#endif
+
 	/*
 	 * Now compute and insert the TLB entry for the virtual page table.  We never
 	 * execute in a page table page so there is no need to set the exception deferral
diff -urNp linux-1150/arch/ia64/kernel/sys_ia64.c linux-1151/arch/ia64/kernel/sys_ia64.c
--- linux-1150/arch/ia64/kernel/sys_ia64.c
+++ linux-1151/arch/ia64/kernel/sys_ia64.c
@@ -29,6 +29,10 @@ arch_get_unmapped_area (struct file *fil
 
 	if (len > RGN_MAP_LIMIT)
 		return -ENOMEM;
+#ifdef CONFIG_HUGETLB_PAGE
+	if (rgn_index(addr) == REGION_HPAGE)
+		addr = 0;
+#endif
 	if (!addr)
 		addr = TASK_UNMAPPED_BASE;
 
diff -urNp linux-1150/arch/ia64/mm/Makefile linux-1151/arch/ia64/mm/Makefile
--- linux-1150/arch/ia64/mm/Makefile
+++ linux-1151/arch/ia64/mm/Makefile
@@ -12,5 +12,6 @@ O_TARGET := mm.o
 export-objs := init.o
 
 obj-y	 := init.o fault.o tlb.o extable.o
+obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
 
 include $(TOPDIR)/Rules.make
diff -urNp linux-1150/arch/ia64/mm/hugetlbpage.c linux-1151/arch/ia64/mm/hugetlbpage.c
--- linux-1150/arch/ia64/mm/hugetlbpage.c
+++ linux-1151/arch/ia64/mm/hugetlbpage.c
@@ -0,0 +1,593 @@
+/*
+ * IA-64 Huge TLB Page Support for Kernel.
+ *
+ * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/sysctl.h>
+#include <linux/mm_inline.h>
+#include <asm/mman.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+
+#define TASK_HPAGE_BASE (REGION_HPAGE << REGION_SHIFT)
+
+static long    htlbpagemem;
+int     htlbpool_max;
+int     htlbpage_max;
+static long    htlbzone_pages;
+
+static LIST_HEAD(htlbpage_freelist);
+static spinlock_t htlbpage_lock = SPIN_LOCK_UNLOCKED;
+
+void free_huge_page(struct page *page);
+
+#define CHECK_LINK(p) BUG_ON((p) != (struct page *)(p)->lru.next)
+
+static void check_huge_page(struct page *page0)
+{
+       int i;
+       struct page *tmp;
+
+       BUG_ON(page_count(page0) != 1);
+       CHECK_LINK(page0);
+
+       BUG_ON(page0->flags & (1 << PG_reserved));
+       tmp = page0;
+       for (i = 0; i < (HPAGE_SIZE / PAGE_SIZE); i++) {
+               if (i && (tmp->flags & (1 << PG_locked | 1 << PG_reserved))) {
+                       printk("hm, tmp: %p (%d), ->flags: %08lx\n", tmp, i, tmp->flags);
+                       BUG();
+               }
+               if (i && page_count(tmp)) {
+                       printk("hm, tmp: %p (%d), page_count(): %d\n", tmp, i, page_count(tmp));
+                       BUG();
+               }
+               if (tmp->mapping) {
+                       printk("hm, tmp: %p (%d), ->mapping: %p\n", tmp, i, tmp->mapping);
+                       BUG();
+               }
+               tmp++;
+       }
+}
+
+static struct page *alloc_hugetlb_page(void)
+{
+	int i;
+	struct page *page;
+
+	spin_lock(&htlbpage_lock);
+	if (list_empty(&htlbpage_freelist)) {
+		spin_unlock(&htlbpage_lock);
+		return NULL;
+	}
+
+	page = list_entry(htlbpage_freelist.next, struct page, list);
+	list_del(&page->list);
+	htlbpagemem--;
+	spin_unlock(&htlbpage_lock);
+	check_huge_page(page);
+	page->lru.prev = (void *)free_huge_page;
+	for (i = 0; i < (HPAGE_SIZE/PAGE_SIZE); ++i)
+		clear_highpage(&page[i]);
+	return page;
+}
+
+static pte_t *
+huge_pte_alloc (struct mm_struct *mm, unsigned long addr)
+{
+	unsigned long taddr = htlbpage_to_page(addr);
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, taddr);
+	pmd = pmd_alloc(mm, pgd, taddr);
+	if (pmd)
+		pte = pte_alloc_map(mm, pmd, taddr);
+	return pte;
+}
+
+static pte_t *
+huge_pte_offset (struct mm_struct *mm, unsigned long addr)
+{
+	unsigned long taddr = htlbpage_to_page(addr);
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, taddr);
+	pmd = pmd_offset(pgd, taddr);
+	pte = pte_offset_map(pmd, taddr);
+	return pte;
+}
+
+#define mk_pte_huge(entry) { pte_val(entry) |= _PAGE_P; }
+
+static void
+set_huge_pte (struct mm_struct *mm, struct vm_area_struct *vma,
+	      struct page *page, pte_t * page_table, int write_access)
+{
+	pte_t entry;
+
+	mm->rss += (HPAGE_SIZE / PAGE_SIZE);
+	if (write_access) {
+		entry =
+		    pte_mkwrite(pte_mkdirty(mk_pte(page, vma->vm_page_prot)));
+	} else
+		entry = pte_wrprotect(mk_pte(page, vma->vm_page_prot));
+	entry = pte_mkyoung(entry);
+	mk_pte_huge(entry);
+	vm_set_pte(vma, vma->vm_start, page_table, entry);
+	return;
+}
+/*
+ * This function checks for proper alignment of input addr and len parameters.
+ */
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	if (REGION_NUMBER(addr) != REGION_HPAGE)
+		return -EINVAL;
+
+	return 0;
+}
+/* This function checks if the address and address+len falls out of HugeTLB region.  It
+ * return -EINVAL if any part of address range falls in HugeTLB region.
+ */
+int is_hugepage_only_range(unsigned long addr, unsigned long len)
+{
+	if (REGION_NUMBER(addr) == REGION_HPAGE)
+		return -EINVAL;
+	if (REGION_NUMBER(addr+len) == REGION_HPAGE)
+		return -EINVAL;
+	return 0;
+}
+
+/*
+ * Same as generic free_pgtables(), except constant PGDIR_* and pgd_offset
+ * are hugetlb region specific.
+ */
+void hugetlb_free_pgtables(struct mm_struct * mm, struct vm_area_struct *prev,
+	unsigned long start, unsigned long end)
+{
+	unsigned long first = start & HUGETLB_PGDIR_MASK;
+	unsigned long last = end + HUGETLB_PGDIR_SIZE - 1;
+	unsigned long start_index, end_index;
+
+	if (!prev) {
+		prev = mm->mmap;
+		if (!prev)
+			goto no_mmaps;
+		if (prev->vm_end > start) {
+			if (last > prev->vm_start)
+				last = prev->vm_start;
+			goto no_mmaps;
+		}
+	}
+	for (;;) {
+		struct vm_area_struct *next = prev->vm_next;
+
+		if (next) {
+			if (next->vm_start < start) {
+				prev = next;
+				continue;
+			}
+			if (last > next->vm_start)
+				last = next->vm_start;
+		}
+		if (prev->vm_end > first)
+			first = prev->vm_end + HUGETLB_PGDIR_SIZE - 1;
+		break;
+	}
+no_mmaps:
+	if (last < first)
+		return;
+	/*
+	 * If the PGD bits are not consecutive in the virtual address, the
+	 * old method of shifting the VA >> by PGDIR_SHIFT doesn't work.
+	 */
+	start_index = pgd_index(htlbpage_to_page(first));
+	end_index = pgd_index(htlbpage_to_page(last));
+	if (end_index > start_index) {
+		clear_page_tables(mm, start_index, end_index - start_index);
+		flush_tlb_pgtables(mm, first & HUGETLB_PGDIR_MASK,
+				   last & HUGETLB_PGDIR_MASK);
+	}
+}
+
+int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
+			struct vm_area_struct *vma)
+{
+	pte_t *src_pte, *dst_pte, entry;
+	struct page *ptepage;
+	unsigned long addr = vma->vm_start;
+	unsigned long end = vma->vm_end;
+
+	while (addr < end) {
+		dst_pte = huge_pte_alloc(dst, addr);
+		if (!dst_pte)
+			goto nomem;
+		src_pte = huge_pte_offset(src, addr);
+		entry = *src_pte;
+		ptepage = pte_page(entry);
+		get_page(ptepage);
+		vm_set_pte(vma, vma->vm_start, dst_pte, entry);
+		dst->rss += (HPAGE_SIZE / PAGE_SIZE);
+		addr += HPAGE_SIZE;
+	}
+	return 0;
+nomem:
+	return -ENOMEM;
+}
+
+int
+follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		    struct page **pages, struct vm_area_struct **vmas,
+		    unsigned long *st, int *length, int i)
+{
+	pte_t *ptep, pte;
+	unsigned long start = *st;
+	unsigned long pstart;
+	int len = *length;
+	struct page *page;
+
+	do {
+		pstart = start & HPAGE_MASK;
+		ptep = huge_pte_offset(mm, start);
+		pte = *ptep;
+
+back1:
+		page = pte_page(pte);
+		if (pages) {
+			page += ((start & ~HPAGE_MASK) >> PAGE_SHIFT);
+			get_page(page);
+			pages[i] = page;
+		}
+		if (vmas)
+			vmas[i] = vma;
+		i++;
+		len--;
+		start += PAGE_SIZE;
+		if (((start & HPAGE_MASK) == pstart) && len &&
+				(start < vma->vm_end))
+			goto back1;
+	} while (len && start < vma->vm_end);
+	*length = len;
+	*st = start;
+	return i;
+}
+
+struct vm_area_struct *hugepage_vma(struct mm_struct *mm, unsigned long addr)
+{
+	if (REGION_NUMBER(addr) == REGION_HPAGE) {
+		struct vm_area_struct *vma = find_vma(mm, addr);
+		if (vma && is_vm_hugetlb_page(vma))
+			return vma;
+	}
+	return NULL;
+}
+
+struct page *follow_huge_addr(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr, int write)
+{
+	struct page *page;
+	pte_t *ptep;
+
+	ptep = huge_pte_offset(mm, addr);
+	page = pte_page(*ptep);
+	page += ((addr & ~HPAGE_MASK) >> PAGE_SHIFT);
+	get_page(page);
+	return page;
+}
+int pmd_huge(pmd_t pmd)
+{
+	return 0;
+}
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address, pmd_t *pmd, int write)
+{
+	return NULL;
+}
+
+void free_huge_page(struct page *page)
+{
+	set_page_count(page, 1);
+
+	check_huge_page(page);
+
+	INIT_LIST_HEAD(&page->list);
+
+	spin_lock(&htlbpage_lock);
+	list_add(&page->list, &htlbpage_freelist);
+	htlbpagemem++;
+	spin_unlock(&htlbpage_lock);
+}
+
+void huge_page_release(struct page *page)
+{
+	CHECK_LINK(page);
+
+	if (!put_page_testzero(page))
+		return;
+
+	free_huge_page(page);
+}
+
+void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long address;
+	pte_t *pte;
+	struct page *page;
+
+	BUG_ON(start & (HPAGE_SIZE - 1));
+	BUG_ON(end & (HPAGE_SIZE - 1));
+
+	for (address = start; address < end; address += HPAGE_SIZE) {
+		pte = huge_pte_offset(mm, address);
+		if (pte_none(*pte))
+			continue;
+		page = pte_page(*pte);
+		vm_pte_clear(vma, address, pte);
+		flush_tlb_range(vma, address, address + HPAGE_SIZE);
+		huge_page_release(page);
+	}
+	mm->rss -= (end - start) >> PAGE_SHIFT;
+}
+
+void zap_hugepage_range(struct vm_area_struct *vma, unsigned long start, unsigned long length)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	spin_lock(&mm->page_table_lock);
+	unmap_hugepage_range(vma, start, start + length);
+	spin_unlock(&mm->page_table_lock);
+}
+
+int hugetlb_prefault(struct address_space *mapping, struct vm_area_struct *vma)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long addr;
+	int ret = 0;
+
+	BUG_ON(vma->vm_start & ~HPAGE_MASK);
+	BUG_ON(vma->vm_end & ~HPAGE_MASK);
+
+	spin_lock(&mm->page_table_lock);
+	for (addr = vma->vm_start; addr < vma->vm_end; addr += HPAGE_SIZE) {
+		unsigned long idx;
+		pte_t *pte = huge_pte_alloc(mm, addr);
+		struct page *page;
+
+		if (!pte) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		if (!pte_none(*pte))
+			continue;
+
+		idx = ((addr - vma->vm_start) >> HPAGE_SHIFT)
+			+ (vma->vm_pgoff >> (HPAGE_SHIFT - PAGE_SHIFT));
+		page = find_get_page(mapping, idx);
+		if (!page) {
+			page = alloc_hugetlb_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			ret = add_to_page_cache_unique_nolru(page, mapping, idx, page_hash(mapping, idx));
+			unlock_page(page);
+			if (ret) {
+				free_huge_page(page);
+				goto out;
+			}
+		}
+		CHECK_LINK(page);
+		set_huge_pte(mm, vma, page, pte, vma->vm_flags & VM_WRITE);
+	}
+out:
+	spin_unlock(&mm->page_table_lock);
+	return ret;
+}
+
+unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
+		unsigned long pgoff, unsigned long flags)
+{
+	struct vm_area_struct *vmm;
+
+	if (len > RGN_MAP_LIMIT)
+		return -ENOMEM;
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	/* This code assumes that REGION_HPAGE != 0. */
+	if ((REGION_NUMBER(addr) != REGION_HPAGE) || (addr & (HPAGE_SIZE - 1)))
+		addr = TASK_HPAGE_BASE;
+	else
+		addr = ALIGN(addr, HPAGE_SIZE);
+	for (vmm = find_vma(current->mm, addr); ; vmm = vmm->vm_next) {
+		/* At this point:  (!vmm || addr < vmm->vm_end). */
+		if (REGION_OFFSET(addr) + len > RGN_MAP_LIMIT)
+			return -ENOMEM;
+		if (!vmm || (addr + len) <= vmm->vm_start)
+			return addr;
+		addr = ALIGN(vmm->vm_end, HPAGE_SIZE);
+	}
+}
+void update_and_free_page(struct page *page)
+{
+	int j;
+	struct page *map;
+
+	map = page;
+	htlbzone_pages--;
+	for (j = 0; j < (HPAGE_SIZE / PAGE_SIZE); j++) {
+		map->flags &= ~(1 << PG_locked | 1 << PG_error | 1 << PG_referenced |
+				1 << PG_dirty | 1 << PG_reserved);
+		set_page_count(map, 0);
+		map++;
+	}
+	set_page_count(page, 1);
+	free_pages_ok(page, HUGETLB_PAGE_ORDER);
+}
+
+int try_to_free_low(int count)
+{
+	struct list_head *p;
+	struct page *page, *map;
+
+	map = NULL;
+	spin_lock(&htlbpage_lock);
+	list_for_each(p, &htlbpage_freelist) {
+		if (map) {
+			list_del(&map->list);
+			update_and_free_page(map);
+			htlbpagemem--;
+			map = NULL;
+			if (++count == 0)
+				break;
+		}
+		page = list_entry(p, struct page, list);
+		if (!PageHighMem(page))
+			map = page;
+	}
+	if (map) {
+		list_del(&map->list);
+		update_and_free_page(map);
+		htlbpagemem--;
+		count++;
+	}
+	spin_unlock(&htlbpage_lock);
+	return count;
+}
+
+int set_hugetlb_mem_size(int count)
+{
+	int  lcount;
+	struct page *page ;
+
+	if (count < 0)
+		lcount = count;
+	else
+		lcount = count - htlbzone_pages;
+
+	if (lcount == 0)
+		return (int)htlbzone_pages;
+	if (lcount > 0) {	/* Increase the mem size. */
+		while (lcount--) {
+			page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+			if (page == NULL)
+				break;
+			spin_lock(&htlbpage_lock);
+			list_add(&page->list, &htlbpage_freelist);
+			htlbpagemem++;
+			htlbzone_pages++;
+			spin_unlock(&htlbpage_lock);
+		}
+		return (int) htlbzone_pages;
+	}
+	/* Shrink the memory size. */
+	lcount = try_to_free_low(lcount);
+	while (lcount++) {
+		page = alloc_hugetlb_page();
+		if (page == NULL)
+			break;
+		spin_lock(&htlbpage_lock);
+		update_and_free_page(page);
+		spin_unlock(&htlbpage_lock);
+	}
+	return (int) htlbzone_pages;
+}
+
+#define HPAGE_FACTOR (HPAGE_SIZE / 1024 / 1024)
+
+int hugetlb_sysctl_handler(ctl_table *table, int write, struct file *file, void *buffer, size_t *length)
+{
+	int ret = proc_dointvec(table, write, file, buffer, length);	
+	/*
+	 * htlbpool_max is in units of MB.
+	 * htlbpage_max is in units of hugepages.
+	 *
+	 * Be careful about 32-bit overflows:
+	 */
+	if (write) {
+		htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+		htlbpage_max = set_hugetlb_mem_size(htlbpage_max);
+		htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	}
+	return ret;
+}
+
+static int __init hugetlb_setup(char *s)
+{
+	if (sscanf(s, "%d", &htlbpage_max) <= 0)
+		htlbpage_max = 0;
+	htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	return 1;
+}
+__setup("hugepages=", hugetlb_setup);
+
+static int __init hugetlbpool_setup(char *s)
+{
+	        if (sscanf(s, "%d", &htlbpool_max) <= 0)
+			                htlbpool_max = 0;
+		        htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+			        return 1;
+}
+__setup("hugetlbpool=", hugetlbpool_setup);
+
+static int __init hugetlb_init(void)
+{
+	int i, j;
+	struct page *page;
+
+	for (i = 0; i < htlbpage_max; ++i) {
+		page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+		if (!page)
+			break;
+		spin_lock(&htlbpage_lock);
+		list_add(&page->list, &htlbpage_freelist);
+		spin_unlock(&htlbpage_lock);
+	}
+	htlbpage_max = htlbpagemem = htlbzone_pages = i;
+	printk("Total HugeTLB memory allocated, %ld\n", htlbpagemem);
+	return 0;
+}
+module_init(hugetlb_init);
+
+int hugetlb_report_meminfo(char *buf)
+{
+	return sprintf(buf,
+			"HugePages_Total: %5lu\n"
+			"HugePages_Free:  %5lu\n"
+			"Hugepagesize:    %5lu kB\n",
+			htlbzone_pages,
+			htlbpagemem,
+			HPAGE_SIZE/1024);
+}
+
+int is_hugepage_mem_enough(size_t size)
+{
+	if (size > (htlbpagemem << HPAGE_SHIFT))
+		return 0;
+	return 1;
+}
+
+static struct page *hugetlb_nopage(struct vm_area_struct * area, unsigned long address, int unused)
+{
+	BUG();
+	return NULL;
+}
+
+struct vm_operations_struct hugetlb_vm_ops = {
+       .nopage = hugetlb_nopage,
+};
diff -urNp linux-1150/arch/ia64/mm/init.c linux-1151/arch/ia64/mm/init.c
--- linux-1150/arch/ia64/mm/init.c
+++ linux-1151/arch/ia64/mm/init.c
@@ -19,6 +19,7 @@
 #include <linux/efi.h>
 #include <linux/highmem.h>
 #include <linux/config.h>
+#include <linux/mm_inline.h>
 
 #include <asm/bitops.h>
 #include <asm/dma.h>
@@ -59,13 +60,18 @@ do_check_pgt_cache (int low, int high)
 
 	if (pgtable_cache_size > high) {
 		do {
-			if (pgd_quicklist)
-				free_page((unsigned long)pgd_alloc_one_fast(0)), ++freed;
-			if (pmd_quicklist)
-				free_page((unsigned long)pmd_alloc_one_fast(0, 0)), ++freed;
-			if (pte_quicklist)
-				__free_page((struct page *)pte_alloc_one_fast(0, 0)), ++freed;
-
+			if (pgd_quicklist) {
+				free_page((unsigned long)pgd_alloc_one_fast(0)); 
+				++freed;
+			}
+			if (pmd_quicklist) {
+				free_page((unsigned long)pmd_alloc_one_fast(0, 0)); 
+				++freed;
+			}
+			if (pte_quicklist) {
+				__free_page((struct page *)pte_alloc_one_fast(0, 0)); 
+				++freed;
+			}
 		} while (pgtable_cache_size > low);
 	}
 	return freed;
diff -urNp linux-1150/fs/hugetlbfs/inode.c linux-1151/fs/hugetlbfs/inode.c
--- linux-1150/fs/hugetlbfs/inode.c
+++ linux-1151/fs/hugetlbfs/inode.c
@@ -136,10 +136,7 @@ static int hugetlbfs_file_mmap(struct fi
 		return -EINVAL;
 	if (vma->vm_end - vma->vm_start < HPAGE_SIZE)
 		return -EINVAL;
-#ifdef CONFIG_IA64
-	if (vma->vm_start < (REGION_HPAGE << REGION_SHIFT))
-		return -EINVAL;
-#endif
+	
 	down(&inode->i_sem);
 
 	UPDATE_ATIME(inode);
diff -urNp linux-1150/include/asm-ia64/mmu_context.h linux-1151/include/asm-ia64/mmu_context.h
--- linux-1150/include/asm-ia64/mmu_context.h
+++ linux-1151/include/asm-ia64/mmu_context.h
@@ -118,6 +118,9 @@ reload_context (mm_context_t context)
 	rr2 = rr0 + 2*rid_incr;
 	rr3 = rr0 + 3*rid_incr;
 	rr4 = rr0 + 4*rid_incr;
+#ifdef  CONFIG_HUGETLB_PAGE
+	        rr4 = (rr4 & (~(0xfcUL))) | (HPAGE_SHIFT << 2);
+#endif
 	ia64_set_rr(0x0000000000000000, rr0);
 	ia64_set_rr(0x2000000000000000, rr1);
 	ia64_set_rr(0x4000000000000000, rr2);
diff -urNp linux-1150/include/asm-ia64/page.h linux-1151/include/asm-ia64/page.h
--- linux-1150/include/asm-ia64/page.h
+++ linux-1151/include/asm-ia64/page.h
@@ -30,6 +30,36 @@
 #define PAGE_MASK		(~(PAGE_SIZE - 1))
 #define PAGE_ALIGN(addr)	(((addr) + PAGE_SIZE - 1) & PAGE_MASK)
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+# if defined(CONFIG_HUGETLB_PAGE_SIZE_4GB)
+#  define HPAGE_SHIFT   32
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_1GB)
+#  define HPAGE_SHIFT   30
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_256MB)
+#  define HPAGE_SHIFT   28
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_64MB)
+#  define HPAGE_SHIFT   26
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_16MB)
+#  define HPAGE_SHIFT   24
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_4MB)
+#  define HPAGE_SHIFT   22
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_1MB)
+#  define HPAGE_SHIFT   20
+# elif defined(CONFIG_HUGETLB_PAGE_SIZE_256KB)
+#  define HPAGE_SHIFT   18
+# else
+#  error Unsupported IA-64 HugeTLB Page Size!
+# endif
+
+# define REGION_HPAGE   (4UL)   /* note: this is hardcoded in mmu_context.h:reload_context()!*/
+# define REGION_SHIFT   61
+# define HPAGE_SIZE     (__IA64_UL_CONST(1) << HPAGE_SHIFT)
+# define HPAGE_MASK     (~(HPAGE_SIZE - 1))
+# define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+# define ARCH_HAS_HUGEPAGE_ONLY_RANGE
+#endif /* CONFIG_HUGETLB_PAGE */
+
 #ifdef __ASSEMBLY__
 # define __pa(x)		((x) - PAGE_OFFSET)
 # define __va(x)		((x) + PAGE_OFFSET)
@@ -98,6 +128,12 @@ typedef union ia64_va {
 #define REGION_SIZE		REGION_NUMBER(1)
 #define REGION_KERNEL	7
 
+#ifdef CONFIG_HUGETLB_PAGE
+# define htlbpage_to_page(x)    ((REGION_NUMBER(x) << 61)                               \
+		                                 | (REGION_OFFSET(x) >> (HPAGE_SHIFT-PAGE_SHIFT)))
+# define HUGETLB_PAGE_ORDER     (HPAGE_SHIFT - PAGE_SHIFT)
+#endif
+
 #define BUG() do { printk("kernel BUG at %s:%d!\n", __FILE__, __LINE__); *(int *)0=0; } while (0)
 #define PAGE_BUG(page) do { BUG(); } while (0)
 
diff -urNp linux-1150/include/asm-ia64/pgtable.h linux-1151/include/asm-ia64/pgtable.h
--- linux-1150/include/asm-ia64/pgtable.h
+++ linux-1151/include/asm-ia64/pgtable.h
@@ -269,7 +269,7 @@ extern unsigned long vmalloc_end;
 #define pte_mkclean(pte)	(__pte(pte_val(pte) & ~_PAGE_D))
 #define pte_mkdirty(pte)	(__pte(pte_val(pte) | _PAGE_D))
 
-#define pte_pfn(x) 		(pte_val(x) >> PAGE_SHIFT)
+#define pte_pfn(x) 		((pte_val(x) & _PFN_MASK) >> PAGE_SHIFT)
 
 /*
  * Macro to make mark a page protection value as "uncacheable".  Note
@@ -450,6 +450,12 @@ extern unsigned long empty_zero_page[PAG
 /* We provide our own get_unmapped_area to cope with VA holes for userland */
 #define HAVE_ARCH_UNMAPPED_AREA
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HUGETLB_PGDIR_SHIFT	(HPAGE_SHIFT + 2*(PAGE_SHIFT-3))
+#define HUGETLB_PGDIR_SIZE	(__IA64_UL(1) << HUGETLB_PGDIR_SHIFT)
+#define HUGETLB_PGDIR_MASK	(~(HUGETLB_PGDIR_SIZE-1))
+#endif
+
 /*
  * No page table caches to initialise
  */
diff -urNp linux-1150/include/linux/hugetlb.h linux-1151/include/linux/hugetlb.h
--- linux-1150/include/linux/hugetlb.h
+++ linux-1151/include/linux/hugetlb.h
@@ -48,8 +48,13 @@ mark_mm_hugetlb(struct mm_struct *mm, st
 		mm->used_hugetlb = 1;
 }
 
-#ifndef ARCH_HAS_VALID_HUGEPAGE_RANGE
-#define check_valid_hugepage_range(addr, len)	0
+#ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
+#define is_hugepage_only_range(addr, len)       0
+#define hugetlb_free_pgtables(mm, prev, start, end) do { } while (0)
+#else
+int is_hugepage_only_range(unsigned long addr, unsigned long len);
+void hugetlb_free_pgtables(struct mm_struct * mm, struct vm_area_struct * prev,
+	unsigned long start, unsigned long end);
 #endif
 
 #else /* !CONFIG_HUGETLB_PAGE */
@@ -76,7 +81,8 @@ static inline int is_vm_hugetlb_page(str
 #define follow_pin_huge_pmd(mm, addr, pmd, w)	0
 #define is_aligned_hugepage_range(addr, len)	0
 #define pmd_huge(x)	0
-#define check_valid_hugepage_range(addr, len)	0
+#define is_hugepage_only_range(addr, len)       0
+#define hugetlb_free_pgtables(mm, prev, start, end) do { } while (0)
 #define zap_one_hugepage(vma, address, size)	({ BUG(); 0; })
 
 #ifndef HPAGE_MASK
diff -urNp linux-1150/include/linux/mmzone.h linux-1151/include/linux/mmzone.h
--- linux-1150/include/linux/mmzone.h
+++ linux-1151/include/linux/mmzone.h
@@ -14,7 +14,11 @@
  * Free memory management - zoned buddy allocator.
  */
 
+#ifndef CONFIG_FORCE_MAX_ZONEORDER
 #define MAX_ORDER 11
+#else
+#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
+#endif
 
 typedef struct free_area_struct {
 	struct list_head	free_list;
diff -urNp linux-1150/mm/memory.c linux-1151/mm/memory.c
--- linux-1150/mm/memory.c
+++ linux-1151/mm/memory.c
@@ -492,8 +492,6 @@ static inline int zap_pte_range(mmu_gath
 	pte_t * ptep, *mapping;
 	int freed = 0;
 
-	if (unlikely(pmd_large(*pmd)))
-		return zap_one_hugepage(tlb_vma(tlb), address, size);
 	if (pmd_none(*pmd))
 		return 0;
 	if (pmd_bad(*pmd)) {
@@ -579,6 +577,11 @@ void zap_page_range(struct vm_area_struc
 	unsigned long start, end, addr, block;
 	int freed;
 
+	if (is_vm_hugetlb_page(vma)) {
+		zap_hugepage_range(vma, address, size);
+		return;
+	}
+
 	/*
 	 * Break the work up into blocks of ZAP_BLOCK_SIZE pages:
 	 * this decreases lock-hold time for the page_table_lock
diff -urNp linux-1150/mm/mmap.c linux-1151/mm/mmap.c
--- linux-1150/mm/mmap.c
+++ linux-1151/mm/mmap.c
@@ -790,10 +790,27 @@ extern unsigned long arch_get_unmapped_a
 unsigned long get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags, unsigned long exec)
 {
 	if (flags & MAP_FIXED) {
+		unsigned long ret;
+		
 		if (addr > TASK_SIZE - len)
 			return -ENOMEM;
 		if (addr & ~PAGE_MASK)
 			return -EINVAL;
+		if (file && is_file_hugepages(file))  {
+			/*
+			 * Make sure that addr and length are properly aligned.
+			 */
+			ret = is_aligned_hugepage_range(addr, len);
+		} else {
+			/*
+			 * Ensure that a normal request is not falling in a
+			 * reserved hugepage range.  For some archs like IA-64,
+			 * there is a separate region for hugepages.
+			 */			
+			ret = is_hugepage_only_range(addr, len);
+		}
+		if (ret)
+			return ret;
 		return addr;
 	}
 
@@ -1277,7 +1294,11 @@ int do_munmap(struct mm_struct *mm, unsi
 	if (extra)
 		kmem_cache_free(vm_area_cachep, extra);
 
-	free_pgtables(mm, prev, addr, addr+len);
+	if (is_hugepage_only_range(addr, len))
+		hugetlb_free_pgtables(mm, prev, addr, addr+len);
+	else
+		free_pgtables(mm, prev, addr, addr+len);
+
 	if(acct) vm_validate_enough("exit -ok- do_munmap");
 
 	return 0;
