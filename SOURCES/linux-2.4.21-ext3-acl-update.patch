diff -urNp linux-5810/fs/ext3/acl.c linux-5820/fs/ext3/acl.c
--- linux-5810/fs/ext3/acl.c
+++ linux-5820/fs/ext3/acl.c
@@ -1,7 +1,7 @@
 /*
  * linux/fs/ext3/acl.c
  *
- * Copyright (C) 2001 by Andreas Gruenbacher, <a.gruenbacher@computer.org>
+ * Copyright (C) 2001-2003 by Andreas Gruenbacher, <agruen@suse.de>
  */
 
 #include <linux/module.h>
@@ -20,7 +20,7 @@ static struct posix_acl *
 ext3_acl_from_disk(const void *value, size_t size)
 {
 	const char *end = (char *)value + size;
-	int n, count;
+	size_t n, count;
 	struct posix_acl *acl;
 
 	if (!value)
@@ -86,7 +86,7 @@ ext3_acl_to_disk(const struct posix_acl 
 {
 	ext3_acl_header *ext_acl;
 	char *e;
-	int n;
+	size_t n;
 
 	*size = ext3_acl_size(acl->a_count);
 	ext_acl = (ext3_acl_header *)kmalloc(sizeof(ext3_acl_header) +
@@ -128,16 +128,17 @@ fail:
 /*
  * Inode operation get_posix_acl().
  *
- * inode->i_sem: down
- * BKL held [before 2.5.x]
+ * inode->i_sem: don't care
+ * BKL: held
  */
 struct posix_acl *
 ext3_get_acl(struct inode *inode, int type)
 {
+	const size_t max_size = ext3_acl_size(EXT3_ACL_MAX_ENTRIES);
+	struct ext3_inode_info *ei = EXT3_I(inode);
 	int name_index;
 	char *value;
-	struct posix_acl *acl, **p_acl;
-	const size_t size = ext3_acl_size(EXT3_ACL_MAX_ENTRIES);
+	struct posix_acl *acl;
 	int retval;
 
 	if (!IS_POSIXACL(inode))
@@ -145,50 +146,57 @@ ext3_get_acl(struct inode *inode, int ty
 
 	switch(type) {
 		case ACL_TYPE_ACCESS:
-			p_acl = &EXT3_I(inode)->i_acl;
+			if (ei->i_acl != EXT3_ACL_NOT_CACHED)
+				return posix_acl_dup(ei->i_acl);
 			name_index = EXT3_XATTR_INDEX_POSIX_ACL_ACCESS;
 			break;
 
 		case ACL_TYPE_DEFAULT:
-			p_acl = &EXT3_I(inode)->i_default_acl;
+			if (ei->i_default_acl != EXT3_ACL_NOT_CACHED)
+				return posix_acl_dup(ei->i_default_acl);
 			name_index = EXT3_XATTR_INDEX_POSIX_ACL_DEFAULT;
 			break;
 
 		default:
 			return ERR_PTR(-EINVAL);
 	}
-	if (*p_acl != EXT3_ACL_NOT_CACHED)
-		return posix_acl_dup(*p_acl);
-	value = kmalloc(size, GFP_KERNEL);
+	value = kmalloc(max_size, GFP_KERNEL);
 	if (!value)
 		return ERR_PTR(-ENOMEM);
 
-	retval = ext3_xattr_get(inode, name_index, "", value, size);
-
-	if (retval == -ENODATA || retval == -ENOSYS)
-		*p_acl = acl = NULL;
-	else if (retval < 0)
-		acl = ERR_PTR(retval);
-	else {
+	retval = ext3_xattr_get(inode, name_index, "", value, max_size);
+	acl = ERR_PTR(retval);
+	if (retval > 0)
 		acl = ext3_acl_from_disk(value, retval);
-		if (!IS_ERR(acl))
-			*p_acl = posix_acl_dup(acl);
-	}
+	else if (retval == -ENODATA || retval == -ENOSYS)
+		acl = NULL;
 	kfree(value);
+
+	if (!IS_ERR(acl)) {
+		switch(type) {
+			case ACL_TYPE_ACCESS:
+				ei->i_acl = posix_acl_dup(acl);
+				break;
+
+			case ACL_TYPE_DEFAULT:
+				ei->i_default_acl = posix_acl_dup(acl);
+				break;
+		}
+	}
 	return acl;
 }
 
 /*
- * inode->i_sem: down unless called from ext3_new_inode
- * BKL held [before 2.5.x]
+ * inode->i_sem: down, or inode is just being initialized
+ * BKL: held
  */
 static int
 ext3_do_set_acl(handle_t *handle, struct inode *inode, int type,
 		struct posix_acl *acl)
 {
+	struct ext3_inode_info *ei = EXT3_I(inode);
 	int name_index;
 	void *value = NULL;
-	struct posix_acl **p_acl;
 	size_t size;
 	int error;
 
@@ -198,7 +206,6 @@ ext3_do_set_acl(handle_t *handle, struct
 	switch(type) {
 		case ACL_TYPE_ACCESS:
 			name_index = EXT3_XATTR_INDEX_POSIX_ACL_ACCESS;
-			p_acl = &EXT3_I(inode)->i_acl;
 			if (acl) {
 				mode_t mode = inode->i_mode;
 				error = posix_acl_equiv_mode(acl, &mode);
@@ -215,7 +222,6 @@ ext3_do_set_acl(handle_t *handle, struct
 
 		case ACL_TYPE_DEFAULT:
 			name_index = EXT3_XATTR_INDEX_POSIX_ACL_DEFAULT;
-			p_acl = &EXT3_I(inode)->i_default_acl;
 			if (!S_ISDIR(inode->i_mode))
 				return acl ? -EACCES : 0;
 			break;
@@ -237,15 +243,31 @@ ext3_do_set_acl(handle_t *handle, struct
 	if (value)
 		kfree(value);
 	if (!error) {
-		if (*p_acl && *p_acl != EXT3_ACL_NOT_CACHED)
-			posix_acl_release(*p_acl);
-		*p_acl = posix_acl_dup(acl);
+		switch(type) {
+			case ACL_TYPE_ACCESS:
+				if (ei->i_acl != EXT3_ACL_NOT_CACHED)
+					posix_acl_release(ei->i_acl);
+				ei->i_acl = posix_acl_dup(acl);
+				break;
+
+			case ACL_TYPE_DEFAULT:
+				if (ei->i_default_acl != EXT3_ACL_NOT_CACHED)
+					posix_acl_release(ei->i_default_acl);
+				ei->i_default_acl = posix_acl_dup(acl);
+				break;
+		}
 	}
 	return error;
 }
 
-static int
-__ext3_permission(struct inode *inode, int mask, int lock)
+/*
+ * Inode operation permission().
+ *
+ * inode->i_sem: don't care
+ * BKL: held
+ */
+int
+ext3_permission(struct inode *inode, int mask)
 {
 	int mode = inode->i_mode;
 
@@ -259,29 +281,24 @@ __ext3_permission(struct inode *inode, i
 	if (current->fsuid == inode->i_uid) {
 		mode >>= 6;
 	} else if (IS_POSIXACL(inode)) {
-		/* ACL can't contain additional permissions if
-		   the ACL_MASK entry is 0 */
-		if (!(mode & S_IRWXG))
-			goto check_groups;
-		if (EXT3_I(inode)->i_acl == EXT3_ACL_NOT_CACHED) {
-			struct posix_acl *acl;
+		struct ext3_inode_info *ei = EXT3_I(inode);
 
-			if (lock) {
-				down(&inode->i_sem);
-				acl = ext3_get_acl(inode, ACL_TYPE_ACCESS);
-				up(&inode->i_sem);
-			} else
-				acl = ext3_get_acl(inode, ACL_TYPE_ACCESS);
+		/* The access ACL cannot grant access if the group class
+		   permission bits don't contain all requested permissions. */
+		if (((mode >> 3) & mask & S_IRWXO) != mask)
+			goto check_groups;
+		if (ei->i_acl == EXT3_ACL_NOT_CACHED) {
+			struct posix_acl *acl =
+				ext3_get_acl(inode, ACL_TYPE_ACCESS);
 
 			if (IS_ERR(acl))
 				return PTR_ERR(acl);
 			posix_acl_release(acl);
-			if (EXT3_I(inode)->i_acl == EXT3_ACL_NOT_CACHED)
+			if (ei->i_acl == EXT3_ACL_NOT_CACHED)
 				return -EIO;
 		}
-		if (EXT3_I(inode)->i_acl) {
-			int error = posix_acl_permission(inode,
-				EXT3_I(inode)->i_acl, mask);
+		if (ei->i_acl) {
+			int error = posix_acl_permission(inode, ei->i_acl,mask);
 			if (error == -EACCES)
 				goto check_capabilities;
 			return error;
@@ -308,32 +325,11 @@ check_capabilities:
 }
 
 /*
- * Inode operation permission().
- *
- * inode->i_sem: up
- * BKL held [before 2.5.x]
- */
-int
-ext3_permission(struct inode *inode, int mask)
-{
-	return __ext3_permission(inode, mask, 1);
-}
-
-/*
- * Used internally if i_sem is already down.
- */
-int
-ext3_permission_locked(struct inode *inode, int mask)
-{
-	return __ext3_permission(inode, mask, 0);
-}
-
-/*
  * Initialize the ACLs of a new inode. Called from ext3_new_inode.
  *
- * dir->i_sem: down
+ * dir->i_sem: don't care
  * inode->i_sem: up (access to inode is still exclusive)
- * BKL held [before 2.5.x]
+ * BKL: held
  */
 int
 ext3_init_acl(handle_t *handle, struct inode *inode, struct inode *dir)
@@ -398,7 +394,7 @@ cleanup:
  * file mode.
  *
  * inode->i_sem: down
- * BKL held [before 2.5.x]
+ * BKL: held
  */
 int
 ext3_acl_chmod(handle_t *handle, struct inode *inode)
@@ -492,7 +488,8 @@ ext3_xattr_get_acl_default(struct inode 
 }
 
 static int
-ext3_xattr_set_acl(struct inode *inode, int type, const void *value, size_t size)
+ext3_xattr_set_acl(struct inode *inode, int type, const void *value,
+		   size_t size)
 {
 	handle_t *handle;
 	struct posix_acl *acl;
diff -urNp linux-5810/fs/ext3/ialloc.c linux-5820/fs/ext3/ialloc.c
--- linux-5810/fs/ext3/ialloc.c
+++ linux-5820/fs/ext3/ialloc.c
@@ -511,6 +511,10 @@ repeat:
 	inode->u.ext3_i.i_state = EXT3_STATE_NEW;
 	err = ext3_mark_inode_dirty(handle, inode);
 	if (err) goto fail;
+
+#ifdef CONFIG_EXT3_FS_XATTR
+	init_rwsem(&inode->u.ext3_i.xattr_sem);
+#endif
 	
 	unlock_super (sb);
 	if(DQUOT_ALLOC_INODE(inode)) {
diff -urNp linux-5810/fs/ext3/inode.c linux-5820/fs/ext3/inode.c
--- linux-5810/fs/ext3/inode.c
+++ linux-5820/fs/ext3/inode.c
@@ -2322,6 +2322,9 @@ void ext3_read_inode(struct inode * inod
 				   le32_to_cpu(iloc.raw_inode->i_block[0]));
 	}
 	ext3_set_inode_flags(inode);
+#ifdef CONFIG_EXT3_FS_XATTR
+	init_rwsem(&inode->u.ext3_i.xattr_sem);
+#endif
 #ifdef CONFIG_EXT3_FS_POSIX_ACL
 	if (inode->u.ext3_i.i_file_acl) {
 		/* The filesystem is mounted with ACL support, and there
diff -urNp linux-5810/fs/ext3/xattr.c linux-5820/fs/ext3/xattr.c
--- linux-5810/fs/ext3/xattr.c
+++ linux-5820/fs/ext3/xattr.c
@@ -1,7 +1,7 @@
 /*
  * linux/fs/ext3/xattr.c
  *
- * Copyright (C) 2001 by Andreas Gruenbacher, <a.gruenbacher@computer.org>
+ * Copyright (C) 2001-2003 by Andreas Gruenbacher, <agruen@suse.de>
  *
  * Fix by Harrison Xing <harrison@mountainviewdata.com>.
  * Ext3 code with a lot of help from Eric Jarman <ejarman@acm.org>.
@@ -43,17 +43,12 @@
  *
  * Locking strategy
  * ----------------
- * The VFS already holds the BKL and the inode->i_sem semaphore when any of
- * the xattr inode operations are called, so we are guaranteed that only one
- * processes accesses extended attributes of an inode at any time.
- *
- * For writing we also grab the ext3_xattr_sem semaphore. This ensures that
- * only a single process is modifying an extended attribute block, even
- * if the block is shared among inodes.
- *
- * Note for porting to 2.5
- * -----------------------
- * The BKL will no longer be held in the xattr inode operations.
+ * EXT3_I(inode)->i_file_acl is protected by EXT3_I(inode)->xattr_sem.
+ * EA blocks are only changed if they are exclusive to an inode, so
+ * holding xattr_sem also means that nothing but the EA block's reference
+ * count will change. Multiple writers to an EA block are synchronized
+ * by the bh lock. No more than a single bh lock is held at any time,
+ * which avoids deadlocks.
  */
 
 #include <linux/fs.h>
@@ -64,14 +59,7 @@
 #include <linux/ext3_xattr.h>
 #include <linux/mbcache.h>
 #include <linux/quotaops.h>
-#include <asm/semaphore.h>
-#include <linux/compatmac.h>
-
-#define EXT3_EA_USER "user."
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
-# define mark_buffer_dirty(bh) mark_buffer_dirty(bh, 1)
-#endif
+#include <linux/rwsem.h>
 
 #define HDR(bh) ((struct ext3_xattr_header *)((bh)->b_data))
 #define ENTRY(ptr) ((struct ext3_xattr_entry *)(ptr))
@@ -103,9 +91,8 @@ static int ext3_xattr_set_handle2(handle
 #ifdef CONFIG_EXT3_FS_XATTR_SHARING
 
 static int ext3_xattr_cache_insert(struct buffer_head *);
-static struct buffer_head *ext3_xattr_cache_find(struct inode *,
+static struct buffer_head *ext3_xattr_cache_find(handle_t *, struct inode *,
 						 struct ext3_xattr_header *);
-static void ext3_xattr_cache_remove(struct buffer_head *);
 static void ext3_xattr_rehash(struct ext3_xattr_header *,
 			      struct ext3_xattr_entry *);
 
@@ -113,91 +100,11 @@ static struct mb_cache *ext3_xattr_cache
 
 #else
 # define ext3_xattr_cache_insert(bh) 0
-# define ext3_xattr_cache_find(inode, header) NULL
+# define ext3_xattr_cache_find(handle, inode, header) NULL
 # define ext3_xattr_cache_remove(bh) while(0) {}
 # define ext3_xattr_rehash(header, entry) while(0) {}
 #endif
 
-/*
- * If a file system does not share extended attributes among inodes,
- * we should not need the ext3_xattr_sem semaphore. However, the
- * filesystem may still contain shared blocks, so we always take
- * the lock.
- */
-
-DECLARE_MUTEX(ext3_xattr_sem);
-
-static inline int
-ext3_xattr_new_block(handle_t *handle, struct inode *inode,
-		     int * errp, int force)
-{
-	struct super_block *sb = inode->i_sb;
-	int goal = le32_to_cpu(EXT3_SB(sb)->s_es->s_first_data_block) +
-		EXT3_I(inode)->i_block_group * EXT3_BLOCKS_PER_GROUP(sb);
-
-	/* How can we enforce the allocation? */
-	int block = ext3_new_block(handle, inode, goal, 0, 0, errp);
-#ifdef OLD_QUOTAS
-	if (!*errp)
-		inode->i_blocks += inode->i_sb->s_blocksize >> 9;
-#endif
-	return block;
-}
-
-static inline int
-ext3_xattr_quota_alloc(struct inode *inode, int force)
-{
-	/* How can we enforce the allocation? */
-#ifdef OLD_QUOTAS
-	int error = DQUOT_ALLOC_BLOCK(inode->i_sb, inode, 1);
-	if (!error)
-		inode->i_blocks += inode->i_sb->s_blocksize >> 9;
-#else
-	int error = DQUOT_ALLOC_BLOCK(inode, 1);
-#endif
-	return error;
-}
-
-#ifdef OLD_QUOTAS
-
-static inline void
-ext3_xattr_quota_free(struct inode *inode)
-{
-	DQUOT_FREE_BLOCK(inode->i_sb, inode, 1);
-	inode->i_blocks -= inode->i_sb->s_blocksize >> 9;
-}
-
-static inline void
-ext3_xattr_free_block(handle_t *handle, struct inode * inode,
-		      unsigned long block)
-{
-	ext3_free_blocks(handle, inode, block, 1);
-	inode->i_blocks -= inode->i_sb->s_blocksize >> 9;
-}
-
-#else
-# define ext3_xattr_quota_free(inode) \
-	DQUOT_FREE_BLOCK(inode, 1)
-# define ext3_xattr_free_block(handle, inode, block) \
-	ext3_free_blocks(handle, inode, block, 1)
-#endif
-
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,18)
-
-static inline struct buffer_head *
-sb_bread(struct super_block *sb, int block)
-{
-	return bread(sb->s_dev, block, sb->s_blocksize);
-}
-
-static inline struct buffer_head *
-sb_getblk(struct super_block *sb, int block)
-{
-	return getblk(sb->s_dev, block, sb->s_blocksize);
-}
-
-#endif
-
 struct ext3_xattr_handler *ext3_xattr_handlers[EXT3_XATTR_INDEX_MAX];
 rwlock_t ext3_handler_lock = RW_LOCK_UNLOCKED;
 
@@ -280,8 +187,8 @@ ext3_xattr_handler(int name_index)
 /*
  * Inode operation getxattr()
  *
- * dentry->d_inode->i_sem down
- * BKL held [before 2.5.x]
+ * dentry->d_inode->i_sem: don't care
+ * BKL: held
  */
 ssize_t
 ext3_getxattr(struct dentry *dentry, const char *name,
@@ -299,8 +206,8 @@ ext3_getxattr(struct dentry *dentry, con
 /*
  * Inode operation listxattr()
  *
- * dentry->d_inode->i_sem down
- * BKL held [before 2.5.x]
+ * dentry->d_inode->i_sem: don't care
+ * BKL: held
  */
 ssize_t
 ext3_listxattr(struct dentry *dentry, char *buffer, size_t size)
@@ -312,7 +219,7 @@ ext3_listxattr(struct dentry *dentry, ch
  * Inode operation setxattr()
  *
  * dentry->d_inode->i_sem down
- * BKL held [before 2.5.x]
+ * BKL: held
  */
 int
 ext3_setxattr(struct dentry *dentry, const char *name,
@@ -332,8 +239,8 @@ ext3_setxattr(struct dentry *dentry, con
 /*
  * Inode operation removexattr()
  *
- * dentry->d_inode->i_sem down
- * BKL held [before 2.5.x]
+ * dentry->d_inode->i_sem: down
+ * BKL: held
  */
 int
 ext3_removexattr(struct dentry *dentry, const char *name)
@@ -363,29 +270,32 @@ ext3_xattr_get(struct inode *inode, int 
 {
 	struct buffer_head *bh = NULL;
 	struct ext3_xattr_entry *entry;
-	unsigned int block, size;
+	size_t size, name_len;
 	char *end;
-	int name_len, error;
+	int error;
 
 	ea_idebug(inode, "name=%d.%s, buffer=%p, buffer_size=%ld",
 		  name_index, name, buffer, (long)buffer_size);
 
 	if (name == NULL)
 		return -EINVAL;
+	down_read(&EXT3_I(inode)->xattr_sem);
+	error = -ENODATA;
 	if (!EXT3_I(inode)->i_file_acl)
-		return -ENODATA;
-	block = EXT3_I(inode)->i_file_acl;
-	ea_idebug(inode, "reading block %d", block);
-	bh = sb_bread(inode->i_sb, block);
+		goto cleanup;
+	ea_idebug(inode, "reading block %d", EXT3_I(inode)->i_file_acl);
+	bh = sb_bread(inode->i_sb, EXT3_I(inode)->i_file_acl);
+	error = -EIO;
 	if (!bh)
-		return -EIO;
+		goto cleanup;
 	ea_bdebug(bh, "b_count=%d, refcount=%d",
 		atomic_read(&(bh->b_count)), le32_to_cpu(HDR(bh)->h_refcount));
 	end = bh->b_data + bh->b_size;
 	if (HDR(bh)->h_magic != cpu_to_le32(EXT3_XATTR_MAGIC) ||
 	    HDR(bh)->h_blocks != cpu_to_le32(1)) {
 bad_block:	ext3_error(inode->i_sb, "ext3_xattr_get",
-			"inode %ld: bad block %d", inode->i_ino, block);
+			"inode %ld: bad block %d", inode->i_ino,
+			EXT3_I(inode)->i_file_acl);
 		error = -EIO;
 		goto cleanup;
 	}
@@ -442,6 +352,7 @@ found:
 
 cleanup:
 	brelse(bh);
+	up_read(&EXT3_I(inode)->xattr_sem);
 
 	return error;
 }
@@ -461,27 +372,30 @@ ext3_xattr_list(struct inode *inode, cha
 {
 	struct buffer_head *bh = NULL;
 	struct ext3_xattr_entry *entry;
-	unsigned int block, size = 0;
+	size_t size = 0;
 	char *buf, *end;
 	int error;
 
 	ea_idebug(inode, "buffer=%p, buffer_size=%ld",
 		  buffer, (long)buffer_size);
 
+	down_read(&EXT3_I(inode)->xattr_sem);
+	error = 0;
 	if (!EXT3_I(inode)->i_file_acl)
-		return 0;
-	block = EXT3_I(inode)->i_file_acl;
-	ea_idebug(inode, "reading block %d", block);
-	bh = sb_bread(inode->i_sb, block);
+		goto cleanup;
+	ea_idebug(inode, "reading block %d", EXT3_I(inode)->i_file_acl);
+	bh = sb_bread(inode->i_sb, EXT3_I(inode)->i_file_acl);
+	error = -EIO;
 	if (!bh)
-		return -EIO;
+		goto cleanup;
 	ea_bdebug(bh, "b_count=%d, refcount=%d",
 		atomic_read(&(bh->b_count)), le32_to_cpu(HDR(bh)->h_refcount));
 	end = bh->b_data + bh->b_size;
 	if (HDR(bh)->h_magic != cpu_to_le32(EXT3_XATTR_MAGIC) ||
 	    HDR(bh)->h_blocks != cpu_to_le32(1)) {
 bad_block:	ext3_error(inode->i_sb, "ext3_xattr_list",
-			"inode %ld: bad block %d", inode->i_ino, block);
+			"inode %ld: bad block %d", inode->i_ino,
+			EXT3_I(inode)->i_file_acl);
 		error = -EIO;
 		goto cleanup;
 	}
@@ -526,6 +440,7 @@ bad_block:	ext3_error(inode->i_sb, "ext3
 
 cleanup:
 	brelse(bh);
+	up_read(&EXT3_I(inode)->xattr_sem);
 
 	return error;
 }
@@ -541,19 +456,17 @@ static void ext3_xattr_update_super_bloc
 		return;
 
 	lock_super(sb);
-	ext3_journal_get_write_access(handle, EXT3_SB(sb)->s_sbh);
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
-	EXT3_SB(sb)->s_feature_compat |= EXT3_FEATURE_COMPAT_EXT_ATTR;
-#endif
-	EXT3_SB(sb)->s_es->s_feature_compat |=
-		cpu_to_le32(EXT3_FEATURE_COMPAT_EXT_ATTR);
-	sb->s_dirt = 1;
-	ext3_journal_dirty_metadata(handle, EXT3_SB(sb)->s_sbh);
+	if (ext3_journal_get_write_access(handle, EXT3_SB(sb)->s_sbh) == 0) {
+		EXT3_SB(sb)->s_es->s_feature_compat |=
+			cpu_to_le32(EXT3_FEATURE_COMPAT_EXT_ATTR);
+		sb->s_dirt = 1;
+		ext3_journal_dirty_metadata(handle, EXT3_SB(sb)->s_sbh);
+	}
 	unlock_super(sb);
 }
 
 /*
- * ext3_xattr_set()
+ * ext3_xattr_set_handle()
  *
  * Create, replace or remove an extended attribute for this inode. Buffer
  * is NULL to remove an existing extended attribute, and non-NULL to
@@ -573,9 +486,8 @@ ext3_xattr_set_handle(handle_t *handle, 
 	struct buffer_head *bh = NULL;
 	struct ext3_xattr_header *header = NULL;
 	struct ext3_xattr_entry *here, *last;
-	unsigned int name_len;
-	int block = EXT3_I(inode)->i_file_acl;
-	int min_offs = sb->s_blocksize, not_found = 1, free, error;
+	size_t name_len, free, min_offs = sb->s_blocksize;
+	int not_found = 1, error;
 	char *end;
 	
 	/*
@@ -604,11 +516,10 @@ ext3_xattr_set_handle(handle_t *handle, 
 	name_len = strlen(name);
 	if (name_len > 255 || value_len > sb->s_blocksize)
 		return -ERANGE;
-	down(&ext3_xattr_sem);
-
-	if (block) {
+	down_write(&EXT3_I(inode)->xattr_sem);
+	if (EXT3_I(inode)->i_file_acl) {
 		/* The inode already has an extended attribute block. */
-		bh = sb_bread(sb, block);
+		bh = sb_bread(sb, EXT3_I(inode)->i_file_acl);
 		error = -EIO;
 		if (!bh)
 			goto cleanup;
@@ -620,7 +531,8 @@ ext3_xattr_set_handle(handle_t *handle, 
 		if (header->h_magic != cpu_to_le32(EXT3_XATTR_MAGIC) ||
 		    header->h_blocks != cpu_to_le32(1)) {
 bad_block:		ext3_error(sb, "ext3_xattr_set",
-				"inode %ld: bad block %d", inode->i_ino, block);
+				"inode %ld: bad block %d", inode->i_ino,
+				EXT3_I(inode)->i_file_acl);
 			error = -EIO;
 			goto cleanup;
 		}
@@ -631,7 +543,7 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 			if ((char *)next >= end)
 				goto bad_block;
 			if (!here->e_value_block && here->e_value_size) {
-				int offs = le16_to_cpu(here->e_value_offs);
+				size_t offs = le16_to_cpu(here->e_value_offs);
 				if (offs < min_offs)
 					min_offs = offs;
 			}
@@ -651,7 +563,7 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 			if ((char *)next >= end)
 				goto bad_block;
 			if (!last->e_value_block && last->e_value_size) {
-				int offs = le16_to_cpu(last->e_value_offs);
+				size_t offs = le16_to_cpu(last->e_value_offs);
 				if (offs < min_offs)
 					min_offs = offs;
 			}
@@ -675,39 +587,53 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 		error = 0;
 		if (value == NULL)
 			goto cleanup;
-		else
-			free -= EXT3_XATTR_LEN(name_len);
 	} else {
 		/* Request to create an existing attribute? */
 		error = -EEXIST;
 		if (flags & XATTR_CREATE)
 			goto cleanup;
 		if (!here->e_value_block && here->e_value_size) {
-			unsigned int size = le32_to_cpu(here->e_value_size);
+			size_t size = le32_to_cpu(here->e_value_size);
 
 			if (le16_to_cpu(here->e_value_offs) + size > 
 			    sb->s_blocksize || size > sb->s_blocksize)
 				goto bad_block;
 			free += EXT3_XATTR_SIZE(size);
 		}
+		free += EXT3_XATTR_LEN(name_len);
 	}
-	free -= EXT3_XATTR_SIZE(value_len);
 	error = -ENOSPC;
-	if (free < 0)
+	if (free < EXT3_XATTR_LEN(name_len) + EXT3_XATTR_SIZE(value_len))
 		goto cleanup;
 
 	/* Here we know that we can set the new attribute. */
 
 	if (header) {
+		struct mb_cache_entry *ce;
+		/* assert(header == HDR(bh)); */
+		if (header->h_refcount != cpu_to_le32(1))
+			goto skip_get_write_access;
+		/* ext3_journal_get_write_access() requires an unlocked bh,
+		   which complicates things here. */
+		error = ext3_journal_get_write_access(handle, bh);
+		if (error)
+			goto cleanup;
+		ce = mb_cache_entry_get(ext3_xattr_cache, bh->b_dev,
+					bh->b_blocknr);
+		lock_buffer(bh);
 		if (header->h_refcount == cpu_to_le32(1)) {
 			ea_bdebug(bh, "modifying in-place");
-			ext3_xattr_cache_remove(bh);
-			error = ext3_journal_get_write_access(handle, bh);
-			if (error)
-				goto cleanup;
+			if (ce)
+				mb_cache_entry_free(ce);
+			/* keep the buffer locked while modifying it. */
 		} else {
 			int offset;
 
+			if (ce)
+				mb_cache_entry_release(ce);
+			unlock_buffer(bh);
+			journal_release_buffer(handle, bh);
+skip_get_write_access:
 			ea_bdebug(bh, "cloning");
 			header = kmalloc(bh->b_size, GFP_KERNEL);
 			error = -ENOMEM;
@@ -733,6 +659,8 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 		last = here = ENTRY(header+1);
 	}
 
+	/* Iff we are modifying the block in-place, bh is locked here. */
+
 	if (not_found) {
 		/* Insert the new name. */
 		int size = EXT3_XATTR_LEN(name_len);
@@ -743,13 +671,24 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 		here->e_name_len = name_len;
 		memcpy(here->e_name, name, name_len);
 	} else {
-		/* Remove the old value. */
 		if (!here->e_value_block && here->e_value_size) {
 			char *first_val = (char *)header + min_offs;
 			int offs = le16_to_cpu(here->e_value_offs);
 			char *val = (char *)header + offs;
 			size_t size = EXT3_XATTR_SIZE(
 				le32_to_cpu(here->e_value_size));
+
+			if (size == EXT3_XATTR_SIZE(value_len)) {
+				/* The old and the new value have the same
+				   size. Just replace. */
+				here->e_value_size = cpu_to_le32(value_len);
+				memset(val + size - EXT3_XATTR_PAD, 0,
+				       EXT3_XATTR_PAD); /* Clear pad bytes. */
+				memcpy(val, value, value_len);
+				goto skip_replace;
+			}
+
+			/* Remove the old value. */
 			memmove(first_val + size, first_val, val - first_val);
 			memset(first_val, 0, size);
 			here->e_value_offs = 0;
@@ -766,20 +705,12 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 			}
 		}
 		if (value == NULL) {
-			/* Remove this attribute. */
-			if (EXT3_XATTR_NEXT(ENTRY(header+1)) == last) {
-				/* This block is now empty. */
-				error = ext3_xattr_set_handle2(handle, inode,
-							       bh, NULL);
-				goto cleanup;
-			} else {
-				/* Remove the old name. */
-				int size = EXT3_XATTR_LEN(name_len);
-				last = ENTRY((char *)last - size);
-				memmove(here, (char*)here + size,
-					(char*)last - (char*)here);
-				memset(last, 0, size);
-			}
+			/* Remove the old name. */
+			int size = EXT3_XATTR_LEN(name_len);
+			last = ENTRY((char *)last - size);
+			memmove(here, (char*)here + size,
+				(char*)last - (char*)here);
+			memset(last, 0, size);
 		}
 	}
 
@@ -796,21 +727,31 @@ bad_block:		ext3_error(sb, "ext3_xattr_s
 			memcpy(val, value, value_len);
 		}
 	}
-	ext3_xattr_rehash(header, here);
 
-	error = ext3_xattr_set_handle2(handle, inode, bh, header);
+skip_replace:
+	if (IS_LAST_ENTRY(ENTRY(header+1))) {
+		/* This block is now empty. */
+		if (bh && header == HDR(bh))
+			unlock_buffer(bh);  /* we were modifying in-place. */
+		error = ext3_xattr_set_handle2(handle, inode, bh, NULL);
+	} else {
+		ext3_xattr_rehash(header, here);
+		if (bh && header == HDR(bh))
+			unlock_buffer(bh);  /* we were modifying in-place. */
+		error = ext3_xattr_set_handle2(handle, inode, bh, header);
+	}
 
 cleanup:
 	brelse(bh);
 	if (!(bh && header == HDR(bh)))
 		kfree(header);
-	up(&ext3_xattr_sem);
+	up_write(&EXT3_I(inode)->xattr_sem);
 
 	return error;
 }
 
 /*
- * Second half of ext3_xattr_set(): Update the file system.
+ * Second half of ext3_xattr_set_handle(): Update the file system.
  */
 static int
 ext3_xattr_set_handle2(handle_t *handle, struct inode *inode,
@@ -821,45 +762,51 @@ ext3_xattr_set_handle2(handle_t *handle,
 	int error;
 
 	if (header) {
-		new_bh = ext3_xattr_cache_find(inode, header);
+		new_bh = ext3_xattr_cache_find(handle, inode, header);
 		if (new_bh) {
-			/*
-			 * We found an identical block in the cache.
-			 * The old block will be released after updating
-			 * the inode.
-			 */
-			ea_bdebug(new_bh, "%s block %ld",
-				(old_bh == new_bh) ? "keeping" : "reusing",
-				new_bh->b_blocknr);
-			
-			error = -EDQUOT;
-			if (ext3_xattr_quota_alloc(inode, 1))
-				goto cleanup;
-			
-			error = ext3_journal_get_write_access(handle, new_bh);
-			if (error)
-				goto cleanup;
-			HDR(new_bh)->h_refcount = cpu_to_le32(
-				le32_to_cpu(HDR(new_bh)->h_refcount) + 1);
-			ea_bdebug(new_bh, "refcount now=%d",
-				le32_to_cpu(HDR(new_bh)->h_refcount));
+			/* We found an identical block in the cache. */
+			if (new_bh == old_bh)
+				ea_bdebug(new_bh, "keeping this block");
+			else {
+				/* The old block is released after updating
+				   the inode. */
+				ea_bdebug(new_bh, "reusing block");
+
+				error = -EDQUOT;
+				/* How can we enforce the allocation? */
+				if (DQUOT_ALLOC_BLOCK(inode, 1)) {
+					unlock_buffer(new_bh);
+					journal_release_buffer(handle, new_bh);
+					goto cleanup;
+				}
+				HDR(new_bh)->h_refcount = cpu_to_le32(1 +
+					le32_to_cpu(HDR(new_bh)->h_refcount));
+				ea_bdebug(new_bh, "refcount now=%d",
+					le32_to_cpu(HDR(new_bh)->h_refcount));
+			}
+			unlock_buffer(new_bh);
 		} else if (old_bh && header == HDR(old_bh)) {
-			/* Keep this block. */
+                       /* Keep this block. No need to lock the block as we
+		        * don't need to change the reference count. */
 			new_bh = old_bh;
 			get_bh(new_bh);
 			ext3_xattr_cache_insert(new_bh);
 		} else {
 			/* We need to allocate a new block */
-			int force = EXT3_I(inode)->i_file_acl != 0;
-			int block = ext3_xattr_new_block(handle, inode,
-							 &error, force);
+			int goal = le32_to_cpu(EXT3_SB(inode->i_sb)->s_es->
+							   s_first_data_block) +
+				   EXT3_I(inode)->i_block_group *
+				   EXT3_BLOCKS_PER_GROUP(inode->i_sb);
+			/* How can we enforce the allocation? */
+			int block = ext3_new_block(handle, inode, goal, 0, 0,
+						   &error);
 			if (error)
 				goto cleanup;
 			ea_idebug(inode, "creating block %d", block);
 
 			new_bh = sb_getblk(sb, block);
 			if (!new_bh) {
-getblk_failed:			ext3_xattr_free_block(handle, inode, block);
+getblk_failed:			ext3_free_blocks(handle, inode, block, 1);
 				error = -EIO;
 				goto cleanup;
 			}
@@ -890,19 +837,24 @@ getblk_failed:			ext3_xattr_free_block(h
 
 	error = 0;
 	if (old_bh && old_bh != new_bh) {
+		struct mb_cache_entry *ce;
 		/*
-		 * If there was an old block, and we are not still using it,
-		 * we now release the old block.
+		 * If there was an old block and we are no longer using it,
+		 * release the old block.
 		*/
-		unsigned int refcount = le32_to_cpu(HDR(old_bh)->h_refcount);
 
 		error = ext3_journal_get_write_access(handle, old_bh);
 		if (error)
 			goto cleanup;
-		if (refcount == 1) {
+		ce = mb_cache_entry_get(ext3_xattr_cache, old_bh->b_dev,
+					old_bh->b_blocknr);
+		lock_buffer(old_bh);
+		if (HDR(old_bh)->h_refcount == cpu_to_le32(1)) {
 			/* Free the old block. */
+			if (ce)
+				mb_cache_entry_free(ce);
 			ea_bdebug(old_bh, "freeing");
-			ext3_xattr_free_block(handle, inode, old_bh->b_blocknr);
+			ext3_free_blocks(handle, inode, old_bh->b_blocknr, 1);
 
 			/* ext3_forget() calls bforget() for us, but we
 			   let our caller release old_bh, so we need to
@@ -911,12 +863,16 @@ getblk_failed:			ext3_xattr_free_block(h
 			ext3_forget(handle, 1, inode, old_bh,old_bh->b_blocknr);
 		} else {
 			/* Decrement the refcount only. */
-			refcount--;
-			HDR(old_bh)->h_refcount = cpu_to_le32(refcount);
-			ext3_xattr_quota_free(inode);
+			if (ce)
+				mb_cache_entry_release(ce);
+			HDR(old_bh)->h_refcount = cpu_to_le32(
+				le32_to_cpu(HDR(old_bh)->h_refcount) - 1);
+			DQUOT_FREE_BLOCK(inode, 1);
 			ext3_journal_dirty_metadata(handle, old_bh);
-			ea_bdebug(old_bh, "refcount now=%d", refcount);
+			ea_bdebug(old_bh, "refcount now=%d",
+				  le32_to_cpu(HDR(old_bh)->h_refcount));
 		}
+		unlock_buffer(old_bh);
 	}
 
 cleanup:
@@ -962,46 +918,58 @@ ext3_xattr_set(struct inode *inode, int 
 void
 ext3_xattr_delete_inode(handle_t *handle, struct inode *inode)
 {
-	struct buffer_head *bh;
-	unsigned int block = EXT3_I(inode)->i_file_acl;
-
-	if (!block)
-		return;
-	down(&ext3_xattr_sem);
+	struct buffer_head *bh = NULL;
+	struct mb_cache_entry *ce;
 
-	bh = sb_bread(inode->i_sb, block);
+	down_write(&EXT3_I(inode)->xattr_sem);
+	if (!EXT3_I(inode)->i_file_acl)
+		goto cleanup;
+	bh = sb_bread(inode->i_sb, EXT3_I(inode)->i_file_acl);
 	if (!bh) {
 		ext3_error(inode->i_sb, "ext3_xattr_delete_inode",
-			"inode %ld: block %d read error", inode->i_ino, block);
+			"inode %ld: block %d read error", inode->i_ino,
+			EXT3_I(inode)->i_file_acl);
 		goto cleanup;
 	}
 	ea_bdebug(bh, "b_count=%d", atomic_read(&(bh->b_count)));
 	if (HDR(bh)->h_magic != cpu_to_le32(EXT3_XATTR_MAGIC) ||
 	    HDR(bh)->h_blocks != cpu_to_le32(1)) {
 		ext3_error(inode->i_sb, "ext3_xattr_delete_inode",
-			"inode %ld: bad block %d", inode->i_ino, block);
+			"inode %ld: bad block %d", inode->i_ino,
+			EXT3_I(inode)->i_file_acl);
 		goto cleanup;
 	}
-	ext3_journal_get_write_access(handle, bh);
-	ea_bdebug(bh, "refcount now=%d", le32_to_cpu(HDR(bh)->h_refcount) - 1);
+	if (ext3_journal_get_write_access(handle, bh) != 0)
+		goto cleanup;
+	ce = mb_cache_entry_get(ext3_xattr_cache, bh->b_dev,
+				bh->b_blocknr);
+	lock_buffer(bh);
 	if (HDR(bh)->h_refcount == cpu_to_le32(1)) {
-		ext3_xattr_cache_remove(bh);
-		ext3_xattr_free_block(handle, inode, block);
-		ext3_forget(handle, 1, inode, bh, block);
-		bh = NULL;
+		if (ce)
+			mb_cache_entry_free(ce);
+		ext3_free_blocks(handle, inode, EXT3_I(inode)->i_file_acl, 1);
+
+		/* ext3_forget() calls bforget() for us, but we release
+		   old_bh blow, so we need to duplicate the handle before. */
+		get_bh(bh);
+		ext3_forget(handle, 1, inode, bh, EXT3_I(inode)->i_file_acl);
 	} else {
+		if (ce)
+			mb_cache_entry_release(ce);
 		HDR(bh)->h_refcount = cpu_to_le32(
 			le32_to_cpu(HDR(bh)->h_refcount) - 1);
 		ext3_journal_dirty_metadata(handle, bh);
 		if (IS_SYNC(inode))
 			handle->h_sync = 1;
-		ext3_xattr_quota_free(inode);
+		DQUOT_FREE_BLOCK(inode, 1);
 	}
+	ea_bdebug(bh, "refcount now=%d", le32_to_cpu(HDR(bh)->h_refcount));
+	unlock_buffer(bh);
 	EXT3_I(inode)->i_file_acl = 0;
 
 cleanup:
 	brelse(bh);
-	up(&ext3_xattr_sem);
+	up_write(&EXT3_I(inode)->xattr_sem);
 }
 
 /*
@@ -1073,6 +1041,7 @@ ext3_xattr_cmp(struct ext3_xattr_header 
 		if (IS_LAST_ENTRY(entry2))
 			return 1;
 		if (entry1->e_hash != entry2->e_hash ||
+		    entry1->e_name_index != entry2->e_name_index ||
 		    entry1->e_name_len != entry2->e_name_len ||
 		    entry1->e_value_size != entry2->e_value_size ||
 		    memcmp(entry1->e_name, entry2->e_name, entry1->e_name_len))
@@ -1101,7 +1070,8 @@ ext3_xattr_cmp(struct ext3_xattr_header 
  * not found or an error occurred.
  */
 static struct buffer_head *
-ext3_xattr_cache_find(struct inode *inode, struct ext3_xattr_header *header)
+ext3_xattr_cache_find(handle_t *handle, struct inode *inode,
+		      struct ext3_xattr_header *header)
 {
 	__u32 hash = le32_to_cpu(header->h_hash);
 	struct mb_cache_entry *ce;
@@ -1109,50 +1079,50 @@ ext3_xattr_cache_find(struct inode *inod
 	if (!header->h_hash)
 		return NULL;  /* never share */
 	ea_idebug(inode, "looking for cached blocks [%x]", (int)hash);
+again:
 	ce = mb_cache_entry_find_first(ext3_xattr_cache, 0, inode->i_dev, hash);
 	while (ce) {
-		struct buffer_head *bh = sb_bread(inode->i_sb, ce->e_block);
+		struct buffer_head *bh;
+
+		if (IS_ERR(ce)) {
+			if (PTR_ERR(ce) == -EAGAIN)
+				goto again;
+			break;
+		}
 
+		bh = sb_bread(inode->i_sb, ce->e_block);
 		if (!bh) {
 			ext3_error(inode->i_sb, "ext3_xattr_cache_find",
 				"inode %ld: block %ld read error",
 				inode->i_ino, ce->e_block);
-		} else if (le32_to_cpu(HDR(bh)->h_refcount) >
-			   EXT3_XATTR_REFCOUNT_MAX) {
-			ea_idebug(inode, "block %ld refcount %d>%d",ce->e_block,
-				le32_to_cpu(HDR(bh)->h_refcount),
-				EXT3_XATTR_REFCOUNT_MAX);
-		} else if (!ext3_xattr_cmp(header, HDR(bh))) {
-			ea_bdebug(bh, "b_count=%d",atomic_read(&(bh->b_count)));
-			mb_cache_entry_release(ce);
-			return bh;
+		} else {
+			/* ext3_journal_get_write_access() requires an unlocked
+			   bh, which complicates things here. */
+			if (ext3_journal_get_write_access(handle, bh) != 0)
+				return NULL;
+			lock_buffer(bh);
+			if (le32_to_cpu(HDR(bh)->h_refcount) >
+			    EXT3_XATTR_REFCOUNT_MAX) {
+				ea_idebug(inode, "block %ld refcount %d>%d",
+					  ce->e_block,
+					  le32_to_cpu(HDR(bh)->h_refcount),
+					  EXT3_XATTR_REFCOUNT_MAX);
+			} else if (!ext3_xattr_cmp(header, HDR(bh))) {
+				ea_bdebug(bh, "b_count=%d",
+					  atomic_read(&(bh->b_count)));
+				mb_cache_entry_release(ce);
+				/* buffer will be unlocked by caller */
+				return bh;
+			}
+			unlock_buffer(bh);
+			journal_release_buffer(handle, bh);
+			brelse(bh);
 		}
-		brelse(bh);
 		ce = mb_cache_entry_find_next(ce, 0, inode->i_dev, hash);
 	}
 	return NULL;
 }
 
-/*
- * ext3_xattr_cache_remove()
- *
- * Remove the cache entry of a block from the cache. Called when a
- * block becomes invalid.
- */
-static void
-ext3_xattr_cache_remove(struct buffer_head *bh)
-{
-	struct mb_cache_entry *ce;
-
-	ce = mb_cache_entry_get(ext3_xattr_cache, bh->b_dev, bh->b_blocknr);
-	if (ce) {
-		ea_bdebug(bh, "removing (%d cache entries remaining)",
-			  atomic_read(&ext3_xattr_cache->c_entry_count)-1);
-		mb_cache_entry_free(ce);
-	} else 
-		ea_bdebug(bh, "no cache entry");
-}
-
 #define NAME_HASH_SHIFT 5
 #define VALUE_HASH_SHIFT 16
 
diff -urNp linux-5810/fs/ext3/xattr_user.c linux-5820/fs/ext3/xattr_user.c
--- linux-5810/fs/ext3/xattr_user.c
+++ linux-5820/fs/ext3/xattr_user.c
@@ -45,11 +45,7 @@ ext3_xattr_user_get(struct inode *inode,
 		return -EINVAL;
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
-#ifdef CONFIG_EXT3_FS_POSIX_ACL
-	error = ext3_permission_locked(inode, MAY_READ);
-#else
 	error = permission(inode, MAY_READ);
-#endif
 	if (error)
 		return error;
 
@@ -70,11 +66,7 @@ ext3_xattr_user_set(struct inode *inode,
 		return -EPERM;
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
-#ifdef CONFIG_EXT3_FS_POSIX_ACL
-	error = ext3_permission_locked(inode, MAY_WRITE);
-#else
 	error = permission(inode, MAY_WRITE);
-#endif
 	if (error)
 		return error;
   
diff -urNp linux-5810/fs/mbcache.c linux-5820/fs/mbcache.c
--- linux-5810/fs/mbcache.c
+++ linux-5820/fs/mbcache.c
@@ -54,6 +54,10 @@
 		printk(KERN_ERR f); \
 		printk("\n"); \
 	} while(0)
+
+#define MB_CACHE_WRITER ((unsigned short)~0U >> 1)
+
+DECLARE_WAIT_QUEUE_HEAD(mb_cache_queue);
 		
 MODULE_AUTHOR("Andreas Gruenbacher <a.gruenbacher@computer.org>");
 MODULE_DESCRIPTION("Meta block cache (for extended attributes)");
@@ -69,7 +73,6 @@ EXPORT_SYMBOL(mb_cache_entry_insert);
 EXPORT_SYMBOL(mb_cache_entry_release);
 EXPORT_SYMBOL(mb_cache_entry_takeout);
 EXPORT_SYMBOL(mb_cache_entry_free);
-EXPORT_SYMBOL(mb_cache_entry_dup);
 EXPORT_SYMBOL(mb_cache_entry_get);
 #if !defined(MB_CACHE_INDEXES_COUNT) || (MB_CACHE_INDEXES_COUNT > 0)
 EXPORT_SYMBOL(mb_cache_entry_find_first);
@@ -135,7 +138,8 @@ __mb_cache_entry_forget(struct mb_cache_
 {
 	struct mb_cache *cache = ce->e_cache;
 
-	mb_assert(atomic_read(&ce->e_used) == 0);
+	mb_assert(!ce->e_used);
+	mb_assert(!ce->e_queued);
 	if (cache->c_op.free && cache->c_op.free(ce, gfp_mask)) {
 		/* free failed -- put back on the lru list
 		   for freeing later. */
@@ -152,7 +156,13 @@ __mb_cache_entry_forget(struct mb_cache_
 static inline void
 __mb_cache_entry_release_unlock(struct mb_cache_entry *ce)
 {
-	if (atomic_dec_and_test(&ce->e_used)) {
+	/* Wake up all processes queuing for this cache entry. */
+	if (ce->e_queued)
+		wake_up_all(&mb_cache_queue);
+	if (ce->e_used >= MB_CACHE_WRITER)
+		ce->e_used -= MB_CACHE_WRITER;
+	ce->e_used--;
+	if (!(ce->e_used || ce->e_queued)) {
 		if (__mb_cache_entry_is_hashed(ce))
 			list_add_tail(&ce->e_lru_list, &mb_cache_lru_list);
 		else {
@@ -403,7 +413,8 @@ mb_cache_entry_alloc(struct mb_cache *ca
 		INIT_LIST_HEAD(&ce->e_lru_list);
 		INIT_LIST_HEAD(&ce->e_block_list);
 		ce->e_cache = cache;
-		atomic_set(&ce->e_used, 1);
+		ce->e_used = 1 + MB_CACHE_WRITER;
+		ce->e_queued = 0;
 	}
 	return ce;
 }
@@ -506,25 +517,12 @@ mb_cache_entry_free(struct mb_cache_entr
 
 
 /*
- * mb_cache_entry_dup()
- *
- * Duplicate a handle to a cache entry (does not duplicate the cache entry
- * itself). After the call, both the old and the new handle must be released.
- */
-struct mb_cache_entry *
-mb_cache_entry_dup(struct mb_cache_entry *ce)
-{
-	atomic_inc(&ce->e_used);
-	return ce;
-}
-
-
-/*
  * mb_cache_entry_get()
  *
  * Get a cache entry  by device / block number. (There can only be one entry
  * in the cache per device and block.) Returns NULL if no such cache entry
- * exists.
+ * exists. The returned cache entry is locked for exclusive access ("single
+ * writer").
  */
 struct mb_cache_entry *
 mb_cache_entry_get(struct mb_cache *cache, kdev_t dev, unsigned long block)
@@ -532,14 +530,36 @@ mb_cache_entry_get(struct mb_cache *cach
 	unsigned int bucket = (HASHDEV(dev) + block) % cache->c_bucket_count;
 	struct list_head *l;
 	struct mb_cache_entry *ce;
+	struct task_struct *tsk = current;
 
 	spin_lock(&mb_cache_spinlock);
 	list_for_each(l, &cache->c_block_hash[bucket]) {
 		ce = list_entry(l, struct mb_cache_entry, e_block_list);
 		if (ce->e_dev == dev && ce->e_block == block) {
+			if (ce->e_used > 0) {
+				DECLARE_WAITQUEUE(wait, tsk);
+
+				add_wait_queue(&mb_cache_queue, &wait);
+				while (ce->e_used > 0) {
+					ce->e_queued++;
+					set_task_state(tsk, 
+						       TASK_UNINTERRUPTIBLE);
+					spin_unlock(&mb_cache_spinlock);
+					schedule();
+					spin_lock(&mb_cache_spinlock);
+					ce->e_queued--;
+				}
+				remove_wait_queue(&mb_cache_queue, &wait);
+				set_task_state(tsk, TASK_RUNNING);
+			}
+			ce->e_used += 1 + MB_CACHE_WRITER;
+			
+			if (!__mb_cache_entry_is_hashed(ce)) {
+				__mb_cache_entry_release_unlock(ce);
+				return NULL;
+			}
 			if (!list_empty(&ce->e_lru_list))
 				list_del_init(&ce->e_lru_list);
-			atomic_inc(&ce->e_used);
 			goto cleanup;
 		}
 	}
@@ -561,9 +581,36 @@ __mb_cache_entry_find(struct list_head *
 			list_entry(l, struct mb_cache_entry,
 			           e_indexes[index].o_list);
 		if (ce->e_dev == dev && ce->e_indexes[index].o_key == key) {
+			struct task_struct *tsk = current;
+
+			/* Incrementing before holding the lock gives readers
+			   priority over writers. */
+			ce->e_used++;
+
+			if (ce->e_used >= MB_CACHE_WRITER) {
+				DECLARE_WAITQUEUE(wait, tsk);
+				
+				add_wait_queue(&mb_cache_queue, &wait);
+				while (ce->e_used >= MB_CACHE_WRITER) {
+					ce->e_queued++;
+					set_task_state(tsk, 
+						       TASK_UNINTERRUPTIBLE);
+					spin_unlock(&mb_cache_spinlock);
+					schedule();
+					spin_lock(&mb_cache_spinlock);
+					ce->e_queued--;
+				}
+				remove_wait_queue(&mb_cache_queue, &wait);
+				set_task_state(tsk, TASK_RUNNING);
+			}
+			
+			if (!__mb_cache_entry_is_hashed(ce)) {
+				__mb_cache_entry_release_unlock(ce);
+				spin_lock(&mb_cache_spinlock);
+				return ERR_PTR(-EAGAIN);
+			}
 			if (!list_empty(&ce->e_lru_list))
 				list_del_init(&ce->e_lru_list);
-			atomic_inc(&ce->e_used);
 			return ce;
 		}
 		l = l->next;
@@ -577,7 +624,8 @@ __mb_cache_entry_find(struct list_head *
  *
  * Find the first cache entry on a given device with a certain key in
  * an additional index. Additonal matches can be found with
- * mb_cache_entry_find_next(). Returns NULL if no match was found.
+ * mb_cache_entry_find_next(). Returns NULL if no match was found. The
+ * returned cache entry is locked for shared access ("multiple readers").
  *
  * @cache: the cache to search
  * @index: the number of the additonal index to search (0<=index<indexes_count)
diff -urNp linux-5810/include/linux/ext3_acl.h linux-5820/include/linux/ext3_acl.h
--- linux-5810/include/linux/ext3_acl.h
+++ linux-5820/include/linux/ext3_acl.h
@@ -7,6 +7,7 @@
 #include <linux/init.h>
 #include <linux/posix_acl.h>
 #include <linux/xattr_acl.h>
+#include <linux/stat.h>
 
 #define EXT3_ACL_VERSION	0x0001
 #define EXT3_ACL_MAX_ENTRIES	32
@@ -63,7 +64,6 @@ static inline int ext3_acl_count(size_t 
 
 /* acl.c */
 extern int ext3_permission (struct inode *, int);
-extern int ext3_permission_locked (struct inode *, int);
 extern struct posix_acl *ext3_get_acl (struct inode *, int);
 extern int ext3_set_acl (struct inode *, int, struct posix_acl *);
 extern int ext3_acl_chmod (handle_t *, struct inode *);
@@ -89,8 +89,10 @@ ext3_acl_chmod(handle_t *handle, struct 
 static inline int
 ext3_init_acl(handle_t *handle, struct inode *inode, struct inode *dir)
 {
-	inode->i_mode &= ~current->fs->umask;
-	ext3_mark_inode_dirty(handle, inode);
+	if (!S_ISLNK(inode->i_mode)) {
+		inode->i_mode &= ~current->fs->umask;
+		ext3_mark_inode_dirty(handle, inode);
+	}
 	return 0;
 }
 
diff -urNp linux-5810/include/linux/ext3_fs_i.h linux-5820/include/linux/ext3_fs_i.h
--- linux-5810/include/linux/ext3_fs_i.h
+++ linux-5820/include/linux/ext3_fs_i.h
@@ -42,6 +42,16 @@ struct ext3_inode_info {
 	__u32	i_prealloc_count;
 #endif
 	__u32	i_dir_start_lookup;
+#ifdef CONFIG_EXT3_FS_XATTR
+	/*
+	 * Extended attributes can be read independently of the main file
+	 * data. Taking i_sem even when reading would cause contention
+	 * between readers of EAs and writers of regular file data, so
+	 * instead we synchronize on xattr_sem when reading or changing
+	 * EAs.
+	 */
+	struct rw_semaphore xattr_sem;
+#endif
 #ifdef CONFIG_EXT3_FS_POSIX_ACL
 	struct posix_acl	*i_acl;
 	struct posix_acl	*i_default_acl;
diff -urNp linux-5810/include/linux/mbcache.h linux-5820/include/linux/mbcache.h
--- linux-5810/include/linux/mbcache.h
+++ linux-5820/include/linux/mbcache.h
@@ -35,7 +35,8 @@ struct mb_cache_entry_index {
 struct mb_cache_entry {
 	struct list_head		e_lru_list;
 	struct mb_cache			*e_cache;
-	atomic_t			e_used;
+	unsigned short			e_used;
+	unsigned short			e_queued;
 	kdev_t				e_dev;
 	unsigned long			e_block;
 	struct list_head		e_block_list;
@@ -58,7 +59,6 @@ void mb_cache_entry_rehash(struct mb_cac
 void mb_cache_entry_release(struct mb_cache_entry *);
 void mb_cache_entry_takeout(struct mb_cache_entry *);
 void mb_cache_entry_free(struct mb_cache_entry *);
-struct mb_cache_entry *mb_cache_entry_dup(struct mb_cache_entry *);
 struct mb_cache_entry *mb_cache_entry_get(struct mb_cache *, kdev_t,
 					  unsigned long);
 #if !defined(MB_CACHE_INDEXES_COUNT) || (MB_CACHE_INDEXES_COUNT > 0)
