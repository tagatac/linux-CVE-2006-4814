diff -urNp linux-1240/fs/buffer.c linux-1250/fs/buffer.c
--- linux-1240/fs/buffer.c
+++ linux-1250/fs/buffer.c
@@ -48,6 +48,7 @@
 #include <linux/module.h>
 #include <linux/completion.h>
 #include <linux/mm_inline.h>
+#include <linux/bootmem.h>
 
 #include <asm/uaccess.h>
 #include <asm/io.h>
@@ -2955,50 +2956,18 @@ void show_buffers(void)
  */
 void __init buffer_init(unsigned long mempages)
 {
-	int order, i;
-	unsigned int nr_hash;
+	hash_table =
+		alloc_large_system_hash("Buffer cache",
+					sizeof(struct buffer_head *),
+					14,
+					1,
+					&bh_hash_shift,
+					&bh_hash_mask);
 
-	/* The buffer cache hash table is less important these days,
-	 * trim it a bit.
-	 */
-	mempages >>= 14;
-
-	mempages *= sizeof(struct buffer_head *);
-
-	for (order = 0; (1 << order) < mempages; order++)
-		;
-
-	/* try to allocate something until we get it or we're asking
-	   for something that is really too small */
-
-	do {
-		unsigned long tmp;
-
-		nr_hash = (PAGE_SIZE << order) / sizeof(struct buffer_head *);
-		bh_hash_mask = (nr_hash - 1);
-
-		tmp = nr_hash;
-		bh_hash_shift = 0;
-		while((tmp >>= 1UL) != 0UL)
-			bh_hash_shift++;
-
-		hash_table = (struct buffer_head **)
-		    __get_free_pages(GFP_ATOMIC, order);
-	} while (hash_table == NULL && --order > 0);
-	printk(KERN_INFO "Buffer cache hash table entries: %d (order: %d, %ld bytes)\n",
-	       nr_hash, order, (PAGE_SIZE << order));
-
-	if (!hash_table)
-		panic("Failed to allocate buffer hash table\n");
-
-	/* Setup hash chains. */
-	for(i = 0; i < nr_hash; i++)
-		hash_table[i] = NULL;
+	memset(hash_table, 0, sizeof(struct buffer_head *) << bh_hash_shift);
 
 	/* Setup lru lists. */
-	for(i = 0; i < NR_LIST; i++)
-		lru_list[i] = NULL;
-
+	memset(lru_list, 0, sizeof(lru_list[0]) * NR_LIST);
 }
 
 
diff -urNp linux-1240/fs/dcache.c linux-1250/fs/dcache.c
--- linux-1240/fs/dcache.c
+++ linux-1250/fs/dcache.c
@@ -24,6 +24,7 @@
 #include <linux/cache.h>
 #include <linux/module.h>
 #include <linux/spinlock.h>
+#include <linux/bootmem.h>
 
 #include <asm/uaccess.h>
 
@@ -1213,11 +1214,6 @@ void flush_dentry_attributes (void)
 
 static void __init dcache_init(unsigned long mempages)
 {
-	struct list_head *d;
-	unsigned long order;
-	unsigned int nr_hash;
-	int i;
-
 	/* 
 	 * A constructor could be added for stable state like the lists,
 	 * but it is probably not worth it because of the cache nature
@@ -1233,43 +1229,28 @@ static void __init dcache_init(unsigned 
 					 NULL, NULL);
 	if (!dentry_cache)
 		panic("Cannot create dentry cache");
+}
 
-#if PAGE_SHIFT < 13
-	mempages >>= (13 - PAGE_SHIFT);
-#endif
-	mempages *= sizeof(struct list_head);
-	for (order = 0; ((1UL << order) << PAGE_SHIFT) < mempages; order++)
-		;
-
-	do {
-		unsigned long tmp;
-
-		nr_hash = (1UL << order) * PAGE_SIZE /
-			sizeof(struct list_head);
-		d_hash_mask = (nr_hash - 1);
-
-		tmp = nr_hash;
-		d_hash_shift = 0;
-		while ((tmp >>= 1UL) != 0UL)
-			d_hash_shift++;
-
-		dentry_hashtable = (struct list_head *)
-			__get_free_pages(GFP_ATOMIC, order);
-	} while (dentry_hashtable == NULL && --order >= 0);
-
-	printk(KERN_INFO "Dentry cache hash table entries: %d (order: %ld, %ld bytes)\n",
-			nr_hash, order, (PAGE_SIZE << order));
+void __init dcache_init_early(unsigned long mempages)
+{
+	struct list_head *p;
+	int loop;
 
-	if (!dentry_hashtable)
-		panic("Failed to allocate dcache hash table\n");
+	dentry_hashtable =
+		alloc_large_system_hash("Dentry cache",
+					sizeof(struct list_head),
+					13,
+					1,
+					&d_hash_shift,
+					&d_hash_mask);
 
-	d = dentry_hashtable;
-	i = nr_hash;
+	p = dentry_hashtable;
+	loop = 1 << d_hash_shift;
 	do {
-		INIT_LIST_HEAD(d);
-		d++;
-		i--;
-	} while (i);
+		INIT_LIST_HEAD(p);
+		p++;
+		loop--;
+	} while (loop);
 }
 
 static void init_buffer_head(void * foo, kmem_cache_t * cachep, unsigned long flags)
diff -urNp linux-1240/fs/inode.c linux-1250/fs/inode.c
--- linux-1240/fs/inode.c
+++ linux-1250/fs/inode.c
@@ -18,6 +18,7 @@
 #include <linux/swapctl.h>
 #include <linux/prefetch.h>
 #include <linux/locks.h>
+#include <linux/bootmem.h>
 
 /*
  * New inode.c implementation.
@@ -1216,48 +1217,30 @@ int bmap(struct inode * inode, int block
 /*
  * Initialize the hash tables.
  */
-void __init inode_init(unsigned long mempages)
+void __init inode_init_early(unsigned long mempages)
 {
-	struct list_head *head;
-	unsigned long order;
-	unsigned int nr_hash;
-	int i;
-
-	mempages >>= (14 - PAGE_SHIFT);
-	mempages *= sizeof(struct list_head);
-	for (order = 0; ((1UL << order) << PAGE_SHIFT) < mempages; order++)
-		;
-
-	do {
-		unsigned long tmp;
-
-		nr_hash = (1UL << order) * PAGE_SIZE /
-			sizeof(struct list_head);
-		i_hash_mask = (nr_hash - 1);
-
-		tmp = nr_hash;
-		i_hash_shift = 0;
-		while ((tmp >>= 1UL) != 0UL)
-			i_hash_shift++;
-
-		inode_hashtable = (struct list_head *)
-			__get_free_pages(GFP_ATOMIC, order);
-	} while (inode_hashtable == NULL && --order >= 0);
-
-	printk(KERN_INFO "Inode cache hash table entries: %d (order: %ld, %ld bytes)\n",
-			nr_hash, order, (PAGE_SIZE << order));
+	struct list_head *p;
+	int loop;
 
-	if (!inode_hashtable)
-		panic("Failed to allocate inode hash table\n");
+	inode_hashtable =
+		alloc_large_system_hash("Inode cache",
+					sizeof(struct list_head),
+					14,
+					1,
+					&i_hash_shift,
+					&i_hash_mask);
 
-	head = inode_hashtable;
-	i = nr_hash;
+	p = inode_hashtable;
+	loop = 1 << i_hash_shift;
 	do {
-		INIT_LIST_HEAD(head);
-		head++;
-		i--;
-	} while (i);
+		INIT_LIST_HEAD(p);
+		p++;
+		loop--;
+	} while (loop);
+}
 
+void __init inode_init(unsigned long mempages)
+{
 	/* inode slab cache */
 	inode_cachep = kmem_cache_create("inode_cache", sizeof(struct inode),
 					 0, SLAB_HWCACHE_ALIGN, init_once,
diff -urNp linux-1240/include/linux/bootmem.h linux-1250/include/linux/bootmem.h
--- linux-1240/include/linux/bootmem.h
+++ linux-1250/include/linux/bootmem.h
@@ -65,4 +65,11 @@ extern void * __init __alloc_bootmem_nod
 #define alloc_bootmem_low_pages_node(pgdat, x) \
 	__alloc_bootmem_node((pgdat), (x), PAGE_SIZE, 0)
 
+extern void *__init alloc_large_system_hash(const char *tablename,
+					    unsigned long bucketsize,
+					    int scale,
+					    int consider_highmem,
+					    unsigned int *_hash_shift,
+					    unsigned int *_hash_mask);
+
 #endif /* _LINUX_BOOTMEM_H */
diff -urNp linux-1240/include/linux/fs.h linux-1250/include/linux/fs.h
--- linux-1240/include/linux/fs.h
+++ linux-1250/include/linux/fs.h
@@ -207,6 +207,7 @@ extern void update_mctime (struct inode 
 #define UPDATE_ATIME(inode) update_atime (inode)
 
 extern void buffer_init(unsigned long);
+extern void inode_init_early(unsigned long);
 extern void inode_init(unsigned long);
 extern void mnt_init(unsigned long);
 extern void files_init(unsigned long mempages);
@@ -1122,6 +1123,7 @@ extern int filp_close(struct file *, fl_
 extern char * getname(const char *);
 
 /* fs/dcache.c */
+extern void dcache_init_early(unsigned long);
 extern void vfs_caches_init(unsigned long);
 
 #define __getname()	kmem_cache_alloc(names_cachep, SLAB_KERNEL)
diff -urNp linux-1240/include/linux/mmzone.h linux-1250/include/linux/mmzone.h
--- linux-1240/include/linux/mmzone.h
+++ linux-1250/include/linux/mmzone.h
@@ -20,6 +20,19 @@
 #define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
 #endif
 
+/*
+ * four large system hash tables (page, inode, dentry and buffer head) have
+ * been found to be somewhat more efficient at order 14 on larger systems (such
+ * as PPC64), so these hash tables are now allocated with the bootmem allocator
+ * so that a larger allocation size limit can be imposed than kmalloc() will
+ * support
+ */
+#if MAX_ORDER > 14
+#define MAX_SYS_HASH_TABLE_ORDER MAX_ORDER
+#else
+#define MAX_SYS_HASH_TABLE_ORDER 14
+#endif
+
 typedef struct free_area_struct {
 	struct list_head	free_list;
 	unsigned long		*map;
diff -urNp linux-1240/init/main.c linux-1250/init/main.c
--- linux-1240/init/main.c
+++ linux-1250/init/main.c
@@ -385,6 +385,11 @@ asmlinkage void __init start_kernel(void
 		initrd_start = 0;
 	}
 #endif
+	/* allocate large system hash tables using the bootmem allocator */
+	page_cache_init(max_low_pfn);
+	dcache_init_early(max_low_pfn);
+	inode_init_early(max_low_pfn);
+	buffer_init(max_low_pfn);
 	mem_init();
 	kmem_cache_sizes_init();
 	pidhash_init();
@@ -404,8 +409,6 @@ asmlinkage void __init start_kernel(void
 	fork_init(num_mappedpages);
 	proc_caches_init();
 	vfs_caches_init(num_physpages);
-	buffer_init(num_physpages);
-	page_cache_init(num_physpages);
 #if defined(CONFIG_ARCH_S390)
 	ccwcache_init();
 #endif
diff -urNp linux-1240/mm/filemap.c linux-1250/mm/filemap.c
--- linux-1240/mm/filemap.c
+++ linux-1250/mm/filemap.c
@@ -24,6 +24,7 @@
 #include <linux/mm.h>
 #include <linux/mm_inline.h>
 #include <linux/iobuf.h>
+#include <linux/bootmem.h>
 
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -3557,29 +3558,12 @@ generic_file_write(struct file *file,con
 
 void __init page_cache_init(unsigned long mempages)
 {
-	unsigned long htable_size, order;
-
-	htable_size = mempages;
-	htable_size *= sizeof(struct page *);
-	for(order = 0; (PAGE_SIZE << order) < htable_size; order++)
-		;
-
-	do {
-		unsigned long tmp = (PAGE_SIZE << order) / sizeof(struct page *);
-
-		page_hash_bits = 0;
-		while((tmp >>= 1UL) != 0UL)
-			page_hash_bits++;
-
-		page_hash_table = (struct page **)
-			__get_free_pages(GFP_ATOMIC, order);
-	} while(page_hash_table == NULL && --order > 0);
-
-	printk("Page-cache hash table entries: %d (order: %ld, %ld bytes)\n",
-	       (1 << page_hash_bits), order, (PAGE_SIZE << order));
-	if (!page_hash_table)
-		panic("Failed to allocate page hash table\n");
-	memset((void *)page_hash_table, 0, PAGE_HASH_SIZE * sizeof(struct page *));
+	page_hash_table = alloc_large_system_hash("Page-cache",	
+						  sizeof(struct page *),
+						  12,
+						  1,
+						  &page_hash_bits,
+						  NULL);
 }
 
 /* address_space_map
diff -urNp linux-1240/mm/page_alloc.c linux-1250/mm/page_alloc.c
--- linux-1240/mm/page_alloc.c
+++ linux-1250/mm/page_alloc.c
@@ -30,6 +30,9 @@ pg_data_t *pgdat_list;
 
 int vm_defragment;
 
+static unsigned long __initdata heuristic_lowmem_pages;
+static unsigned long __initdata heuristic_all_pages;
+
 /*
  *
  * The zone_table array is used to look up the address of the
@@ -1188,16 +1191,21 @@ void __init free_area_init_core(int nid,
 	if (zone_start_paddr & ~PAGE_MASK)
 		BUG();
 
-	totalpages = 0;
+	totalpages = realtotalpages = 0;
 	for (i = 0; i < MAX_NR_ZONES; i++) {
 		unsigned long size = zones_size[i];
+		unsigned long realsize = size;
+		if (zholes_size)
+			realsize -= zholes_size[i];
+
 		totalpages += size;
+		realtotalpages += realsize;
+
+		if (i == ZONE_DMA || i == ZONE_NORMAL)
+			heuristic_lowmem_pages += realsize;
+		heuristic_all_pages += realsize;
 	}
-	realtotalpages = totalpages;
-	if (zholes_size)
-		for (i = 0; i < MAX_NR_ZONES; i++)
-			realtotalpages -= zholes_size[i];
-			
+
 	printk("On node %d totalpages: %lu\n", nid, realtotalpages);
 
 	/*
@@ -1422,3 +1430,73 @@ void __init reset_highmem_zone(int highm
 
 }
 #endif
+
+static inline int long_log2(unsigned long x) __attribute__((pure));
+static inline int long_log2(unsigned long x)
+{
+	int r = 0;
+	for (x >>= 1; x > 0; x >>= 1)
+		r++;
+	return r;
+}
+
+/*
+ * allocate a large system hash table from bootmem
+ * - it is assumed that the hash table must contain an exact power-of-2
+ *   quantity of entries
+ */
+void *__init alloc_large_system_hash(const char *tablename,
+				     unsigned long bucketsize,
+				     int scale,
+				     int consider_highmem,
+				     unsigned int *_hash_shift,
+				     unsigned int *_hash_mask)
+{
+	unsigned long estimate, mem, max, log2qty, size;
+	void *table;
+
+	/* determine applicable memory size, rounded up to nearest megabyte */
+	mem = consider_highmem ? heuristic_all_pages : heuristic_lowmem_pages;
+	mem += (1UL << (20 - PAGE_SHIFT)) - 1;
+	mem >>= 20 - PAGE_SHIFT;
+	mem <<= 20 - PAGE_SHIFT;
+
+	/* estimate the number of buckets by requesting 1 bucket per 2^scale
+	 * bytes of memory (rounded up to nearest power of 2 in size) */
+	if (scale > PAGE_SHIFT)
+		estimate = mem >> (scale - PAGE_SHIFT);
+	else
+		estimate = mem << (PAGE_SHIFT - scale);
+
+	estimate = 1UL << (long_log2(estimate - 1UL) + 1);
+
+	/* limit the allocation size */
+	max = (1UL << (PAGE_SHIFT + MAX_SYS_HASH_TABLE_ORDER)) / bucketsize;
+	if (estimate > max)
+		estimate = max;
+
+	log2qty = long_log2(estimate);
+
+	do {
+		size = bucketsize << log2qty;
+
+		table = alloc_bootmem(size);
+
+	} while (!table && size > PAGE_SIZE && --log2qty);
+
+	if (!table)
+		panic("Failed to allocate %s hash table\n", tablename);
+
+	printk("%s hash table entries: %u (order: %d, %lu KB)\n",
+	       tablename,
+	       (1U << log2qty),
+	       long_log2(size) - PAGE_SHIFT,
+	       size / 1024);
+
+	if (_hash_shift)
+		*_hash_shift = log2qty;
+	if (_hash_mask)
+		*_hash_mask = (1 << log2qty) - 1;
+
+	return table;
+}
