diff -urNp linux-275/arch/ia64/config.in linux-276/arch/ia64/config.in
--- linux-275/arch/ia64/config.in
+++ linux-276/arch/ia64/config.in
@@ -19,7 +19,8 @@ comment 'General setup'
 
 define_bool CONFIG_IA64 y
 define_bool CONFIG_HIGHPTE n
-define_bool CONFIG_HIGHMEM n
+define_bool CONFIG_HIGHMEM y
+define_bool CONFIG_HIGHIO y
 
 define_bool CONFIG_ISA n
 define_bool CONFIG_EISA n
diff -urNp linux-275/arch/ia64/kernel/setup.c linux-276/arch/ia64/kernel/setup.c
--- linux-275/arch/ia64/kernel/setup.c
+++ linux-276/arch/ia64/kernel/setup.c
@@ -53,6 +53,7 @@
 #define MAX(a,b)	((a) > (b) ? (a) : (b))
 
 extern char _end;
+extern int blk_nohighio;
 
 #ifdef CONFIG_NUMA
  struct cpuinfo_ia64 *boot_cpu_data;
@@ -214,9 +215,8 @@ find_memory (void)
 {
 #	define KERNEL_END	((unsigned long) &_end)
 	unsigned long bootmap_size;
-	unsigned long max_pfn;
 	int n = 0;
-
+	
 	/*
 	 * none of the entries in this table overlap
 	 */
@@ -321,6 +321,13 @@ setup_arch (char **cmdline_p)
 	machvec_init(acpi_get_sysname());
 #endif
 
+#ifndef CONFIG_HIGHIO
+	        blk_nohighio = 1;
+#endif
+
+	/* setup max_low_pfn by chipset */
+	max_low_pfn_init();
+		
 	/*
 	 *  Set `iobase' to the appropriate address in region 6 (uncached access range).
 	 *
@@ -665,3 +672,42 @@ cpu_init (void)
 
 	platform_cpu_init();
 }
+
+static int __init highio_setup(char *str)
+{
+       printk("ia64: disabling HIGHMEM block I/O\n");
+       blk_nohighio = 1;
+       return 1;
+}
+__setup("nohighio", highio_setup);
+		
+ 
+static void 
+max_low_pfn_init (void) {
+	 
+       unsigned long max_dma;
+ 
+       max_dma = virt_to_phys((void *) MAX_DMA_ADDRESS) >> PAGE_SHIFT;
+
+#ifdef CONFIG_IA64_GENERIC
+       if (strcmp(ia64_mv.name, "dig") == 0) {
+               if (max_pfn < max_dma) 
+                       max_low_pfn = max_pfn;
+               else  
+                       max_low_pfn = max_dma;
+       } else {
+               max_low_pfn = max_pfn;
+       }
+#elif CONFIG_IA64_DIG
+      if (max_pfn < max_dma)
+             max_low_pfn = max_pfn;
+      else
+               max_low_pfn = max_dma;
+#else
+       max_low_pfn = max_pfn;
+#endif 
+
+}
+
+
+
diff -urNp linux-275/arch/ia64/lib/Makefile linux-276/arch/ia64/lib/Makefile
--- linux-275/arch/ia64/lib/Makefile
+++ linux-276/arch/ia64/lib/Makefile
@@ -7,14 +7,14 @@
 
 L_TARGET = lib.a
 
-export-objs := io.o swiotlb.o
+export-objs := io.o swiotlb.o fancy.o
 
 obj-y := __divsi3.o __udivsi3.o __modsi3.o __umodsi3.o					\
 	__divdi3.o __udivdi3.o __moddi3.o __umoddi3.o					\
 	carta_random.o checksum.o clear_page.o csum_partial_copy.o					\
 	clear_user.o strncpy_from_user.o strlen_user.o strnlen_user.o			\
 	flush.o ip_fast_csum.o io.o do_csum.o						\
-	memset.o strlen.o swiotlb.o
+	memset.o strlen.o swiotlb.o fancy.o
 
 obj-$(CONFIG_ITANIUM) += copy_page.o copy_user.o memcpy.o
 obj-$(CONFIG_MCKINLEY) += copy_page_mck.o memcpy_mck.o
diff -urNp linux-275/arch/ia64/lib/fancy.c linux-276/arch/ia64/lib/fancy.c
--- linux-275/arch/ia64/lib/fancy.c
+++ linux-276/arch/ia64/lib/fancy.c
@@ -0,0 +1,176 @@
+/* pci mapping routines 
+ * 
+ * these routines assume that the memory we are mapping is
+ * already accessible by the device. This means that no bouncing
+ * or re-mapping needs to take place. We can basically just return
+ * the physical address of the memory. This is ensured by the
+ * 'upper' layers in the kernel, ie, the bounce buffering code.
+ */
+ 
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/pci.h>
+
+#include <asm/io.h>
+#include <asm/scatterlist.h>
+
+
+#define flush_write_buffers() do {} while (0)
+
+	
+/* Map a single buffer of the indicated size for DMA in streaming mode.
+ * The 32-bit bus address to use is returned.
+
+ *
+ * Once the device is given the dma address, the device owns this memory
+ * until either pci_unmap_single or pci_dma_sync_single is performed.
+ */
+dma_addr_t fancy_pci_map_single(struct pci_dev *hwdev, void *ptr,
+                                       size_t size, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       flush_write_buffers();
+       return virt_to_bus(ptr);
+}
+
+/* Unmap a single streaming mode DMA translation.  The dma_addr and size
+ * must match what was provided for in a previous pci_map_single call.  All
+ * other usages are undefined.
+ *
+ * After this call, reads by the cpu to the buffer are guarenteed to see
+ * whatever the device wrote there.
+ */
+void fancy_pci_unmap_single(struct pci_dev *hwdev, dma_addr_t dma_addr,
+                                   size_t size, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       /* Nothing to do */
+}
+
+/*
+ * pci_{map,unmap}_single_page maps a kernel page to a dma_addr_t. identical
+ * to pci_map_single, but takes a struct page instead of a virtual address
+*/
+dma_addr_t fancy_pci_map_page(struct pci_dev *hwdev, struct page *page,
+                                     unsigned long offset, size_t size, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+
+       return (page - mem_map) * PAGE_SIZE + offset;
+}
+
+void fancy_pci_unmap_page(struct pci_dev *hwdev, dma_addr_t dma_address,
+                                 size_t size, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       /* Nothing to do */
+}
+
+/* Map a set of buffers described by scatterlist in streaming
+ * mode for DMA.  This is the scather-gather version of the
+ * above pci_map_single interface.  Here the scatter gather list
+ * elements are each tagged with the appropriate dma address
+ * and length.  They are obtained via sg_dma_{address,length}(SG).
+ *
+ * NOTE: An implementation may be able to use a smaller number of
+ *       DMA address/length pairs than there are SG table elements.
+ *       (for example via virtual mapping capabilities)
+ *       The routine returns the number of addr/length pairs actually
+ *       used, at most nents.
+ *
+ * Device ownership issues as mentioned above for pci_map_single are
+ * the same here.
+ */
+int fancy_pci_map_sg(struct pci_dev *hwdev, struct scatterlist *sg,
+                            int nents, int direction)
+{
+       int i;
+
+       if (direction == PCI_DMA_NONE)
+               BUG();
+ 
+       /*
+        * temporary 2.4 hack
+        */
+       for (i = 0; i < nents; i++ ) {
+               if (sg[i].address && sg[i].page)
+                       BUG();
+               else if (!sg[i].address && !sg[i].page)
+                       BUG();
+ 
+               if (sg[i].address)
+                       sg[i].dma_address = virt_to_bus(sg[i].address);
+               else
+                       sg[i].dma_address = page_to_bus(sg[i].page) + sg[i].offset;
+	       sg[i].dma_length = sg[i].length;
+       }
+ 
+       flush_write_buffers();
+       return nents;
+}
+
+/* Unmap a set of streaming mode DMA translations.
+ * Again, cpu read rules concerning calls here are the same as for
+ * pci_unmap_single() above.
+ */
+void fancy_pci_unmap_sg(struct pci_dev *hwdev, struct scatterlist *sg,
+                               int nents, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       /* Nothing to do */
+}
+
+/* Make physical memory consistent for a single
+ * streaming mode DMA translation after a transfer.
+ *
+ * If you perform a pci_map_single() but wish to interrogate the
+ * buffer using the cpu, yet do not wish to teardown the PCI dma
+ * mapping, you must call this function before doing so.  At the
+ * next point you give the PCI dma address back to the card, the
+ * device again owns the buffer.
+ */
+void fancy_pci_dma_sync_single(struct pci_dev *hwdev,
+                                      dma_addr_t dma_handle,
+                                      size_t size, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       flush_write_buffers();
+}
+
+/* Make physical memory consistent for a set of streaming
+ * mode DMA translations after a transfer.
+ *
+ * The same as pci_dma_sync_single but for a scatter-gather list,
+ * same rules and usage.
+ */
+void fancy_pci_dma_sync_sg(struct pci_dev *hwdev,
+                                  struct scatterlist *sg,
+                                  int nelems, int direction)
+{
+       if (direction == PCI_DMA_NONE)
+               BUG();
+       flush_write_buffers();
+}
+
+#define PCI_DMA_BUS_IS_PHYS (1)
+#define pci_dac_dma_supported(pci_dev, mask)      (0)
+
+EXPORT_SYMBOL(fancy_pci_map_single);
+EXPORT_SYMBOL(fancy_pci_unmap_single);
+EXPORT_SYMBOL(fancy_pci_map_sg);
+EXPORT_SYMBOL(fancy_pci_unmap_sg);
+EXPORT_SYMBOL(fancy_pci_dma_sync_single);
+EXPORT_SYMBOL(fancy_pci_dma_sync_sg);
+
+
+
+
diff -urNp linux-275/arch/ia64/mm/init.c linux-276/arch/ia64/mm/init.c
--- linux-275/arch/ia64/mm/init.c
+++ linux-276/arch/ia64/mm/init.c
@@ -1,4 +1,5 @@
 /*
+ *
  * Initialize MMU support.
  *
  * Copyright (C) 1998-2002 Hewlett-Packard Co
@@ -16,6 +17,8 @@
 #include <linux/slab.h>
 #include <linux/swap.h>
 #include <linux/efi.h>
+#include <linux/highmem.h>
+#include <linux/config.h>
 
 #include <asm/bitops.h>
 #include <asm/dma.h>
@@ -42,6 +45,7 @@ unsigned long MAX_DMA_ADDRESS = PAGE_OFF
 #define LARGE_GAP 0x40000000 /* Use virtual mem map if a hole is > than this */
 
 static unsigned long totalram_pages;
+static unsigned long totalhigh_pages;
 
 unsigned long vmalloc_end = VMALLOC_END_INIT;
 
@@ -66,6 +70,8 @@ do_check_pgt_cache (int low, int high)
 	return freed;
 }
 
+pte_t *kmap_pte;
+
 /*
  * This performs some platform-dependent address space initialization.
  * On IA-64, we want to setup the VM area for the register backing
@@ -183,8 +189,13 @@ si_meminfo (struct sysinfo *val)
 	val->sharedram = 0;
 	val->freeram = nr_free_pages();
 	val->bufferram = atomic_read(&buffermem_pages);
+#ifndef CONFIG_HIGHMEM
 	val->totalhigh = 0;
 	val->freehigh = 0;
+#else
+	val->totalhigh = totalhigh_pages;
+	val->freehigh = nr_free_highpages();
+#endif
 	val->mem_unit = PAGE_SIZE;
 	return;
 }
@@ -460,16 +471,34 @@ virtual_memmap_init (u64 start, u64 end,
 	return 0;
 }
 
+static unsigned long 
+memmap_init_nohighmem(struct page *start, struct page *end,
+	int zone, unsigned long start_paddr, int highmem) 
+{
+	struct page *page;
+
+	for (page = start; page < end; page++) {
+		set_page_zone(page, zone);
+		set_page_count(page, 0);
+		SetPageReserved(page);
+		INIT_LIST_HEAD(&page->list);
+		set_page_address(page, __va(start_paddr));
+		start_paddr += PAGE_SIZE;
+	}
+	return start_paddr;
+}
+
+
 unsigned long
 arch_memmap_init (memmap_init_callback_t *memmap_init, struct page *start,
 	struct page *end, int zone, unsigned long start_paddr, int highmem)
 {
 	if (!vmem_map) 
-		memmap_init(start,end,zone,page_to_phys(start),highmem);
+		memmap_init_nohighmem(start,end,zone,page_to_phys(start),highmem);
 	else {
 		struct memmap_init_callback_data args;
 
-		args.memmap_init = memmap_init;
+		args.memmap_init = memmap_init_nohighmem;
 		args.start = start;
 		args.end = end;
 		args.zone = zone;
@@ -533,6 +562,7 @@ paging_init (void)
 	unsigned long max_dma;
 	unsigned long zones_size[MAX_NR_ZONES];
 	unsigned long zholes_size[MAX_NR_ZONES];
+	int zone;
 #ifndef CONFIG_DISCONTIGMEM
 	unsigned long max_gap;
 #endif
@@ -557,9 +587,20 @@ paging_init (void)
 		zones_size[ZONE_DMA] = max_dma;
 		zholes_size[ZONE_DMA] = max_dma - num_dma_physpages;
 		if (num_physpages > num_dma_physpages) {
-			zones_size[ZONE_NORMAL] = max_low_pfn - max_dma;
-			zholes_size[ZONE_NORMAL] = (max_low_pfn - max_dma)
-					- (num_physpages - num_dma_physpages);
+#ifdef CONFIG_IA64_GENERIC
+			if (strcmp(ia64_mv.name, "dig") == 0)
+				zone = ZONE_HIGHMEM;    
+			else
+				zone = ZONE_NORMAL;
+									
+#elif CONFIG_IA64_DIG
+			zone = ZONE_HIGHMEM;
+#else
+			zone = ZONE_NORMAL;
+#endif
+			zones_size[zone] = max_pfn - max_dma;
+			zholes_size[zone] = (max_pfn - max_dma)
+				- (num_physpages - num_dma_physpages);
 		}
 	}
 
@@ -578,7 +619,7 @@ paging_init (void)
 
 		/* allocate virtual mem_map */
 
-		map_size = PAGE_ALIGN(max_low_pfn*sizeof(struct page));
+		map_size = PAGE_ALIGN(max_pfn*sizeof(struct page));
 		vmalloc_end -= map_size;
 		vmem_map = (struct page *) vmalloc_end;
 		efi_memmap_walk(create_mem_map_page_table, 0);
@@ -603,6 +644,22 @@ count_reserved_pages (u64 start, u64 end
 	return 0;
 }
 
+static int
+count_highmem_pages (u64 start, u64 end, void *arg)
+{
+       unsigned long num_high = 0;
+       unsigned long *count = arg;
+       struct page *pg;
+
+       for (pg = virt_to_page(start); pg < virt_to_page(end); ++pg)
+               if (page_to_phys(pg)>(0xffffffff)) {
+                       ++num_high;
+                       set_bit(PG_highmem, &pg->flags);
+		}
+       *count += num_high;
+       return 0;
+}
+
 void
 mem_init (void)
 {
@@ -622,14 +679,24 @@ mem_init (void)
 	if (!mem_map)
 		BUG();
 
-	max_mapnr = max_low_pfn;
-	high_memory = __va(max_low_pfn * PAGE_SIZE);
+	max_mapnr = max_pfn;
+	highmem_start_page = mem_map + max_low_pfn;
+	high_memory = __va(max_pfn * PAGE_SIZE);
 
 	totalram_pages += free_all_bootmem();
 
 	reserved_pages = 0;
 	efi_memmap_walk(count_reserved_pages, &reserved_pages);
 
+#ifdef CONFIG_IA64_GENERIC
+ 	if (strcmp(ia64_mv.name, "dig") == 0) { 
+		totalhigh_pages = 0;
+		efi_memmap_walk(count_highmem_pages, &totalhigh_pages);
+	}
+#elif CONFIG_IA64_DIG
+	totalhigh_pages = 0;
+	efi_memmap_walk(count_highmem_pages, &totalhigh_pages);
+#endif	
 	codesize =  (unsigned long) &_etext - (unsigned long) &_stext;
 	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
 	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
diff -urNp linux-275/include/asm-ia64/highmem.h linux-276/include/asm-ia64/highmem.h
--- linux-275/include/asm-ia64/highmem.h
+++ linux-276/include/asm-ia64/highmem.h
@@ -0,0 +1,64 @@
+/*
+ * highmem.h: virtual kernel memory mappings for high memory
+ *
+ * Used in CONFIG_HIGHMEM systems for memory pages which
+ * are not addressable by direct kernel virtual addresses.
+ *
+ * Copyright (C) 1999 Gerhard Wichert, Siemens AG
+ *                   Gerhard.Wichert@pdb.siemens.de
+ *
+ *
+ * Redesigned the x86 32-bit VM architecture to deal with 
+ * up to 16 Terabyte physical memory. With current x86 CPUs
+ * we now support up to 64 Gigabytes physical RAM.
+ * Modified for use on IA64.
+ * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>
+ */
+
+#ifndef _ASM_HIGHMEM_H
+#define _ASM_HIGHMEM_H
+
+#ifdef __KERNEL__
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <asm/kmap_types.h>
+#include <asm/pgtable.h>
+
+/* undef for production */
+#define HIGHMEM_DEBUG 0
+
+/* declarations for highmem.c */
+#define PKMAP_BASE (0x0UL)
+#define LAST_PKMAP 0
+#define LAST_PKMAP_MASK 0
+#define PKMAP_NR(virt)  0
+#define PKMAP_ADDR(nr)  0
+#define kmap_prot PAGE_KERNEL
+
+extern void * FASTCALL(kmap_high(struct page *page, int nonblocking));
+extern void FASTCALL(kunmap_high(struct page *page));
+
+extern unsigned long highstart_pfn, highend_pfn;
+extern pte_t *kmap_pte;
+
+#define kmap_init(void) do {} while (0)
+
+/*
+ * For 64-bit platforms, such as ia64, kmap returns page_address, 
+ * since all of physical memory is directly mapped by kernel virtual memory. 
+ * kunmap is thus is a no-op.
+ */
+#define kmap(page) page_address(page)
+#define kmap_nonblock(page) page_address(page)
+#define kunmap(page) do {} while (0)
+#define kmap_atomic(page, type) page_address(page)
+#define kunmap_atomic(kvaddr, type) do {} while (0)
+
+#define kmap_atomic_to_page(ptr) virt_to_page(ptr)
+
+#endif /* __KERNEL__ */
+
+#endif /* _ASM_HIGHMEM_H */
+
diff -urNp linux-275/include/asm-ia64/machvec_dig.h linux-276/include/asm-ia64/machvec_dig.h
--- linux-275/include/asm-ia64/machvec_dig.h
+++ linux-276/include/asm-ia64/machvec_dig.h
@@ -5,6 +5,12 @@ extern ia64_mv_setup_t dig_setup;
 extern ia64_mv_irq_init_t dig_irq_init;
 extern ia64_mv_pci_fixup_t iosapic_pci_fixup;
 extern ia64_mv_map_nr_t map_nr_dense;
+extern ia64_mv_pci_map_single fancy_pci_map_single;
+extern ia64_mv_pci_unmap_single fancy_pci_unmap_single;
+extern ia64_mv_pci_map_sg fancy_pci_map_sg;
+extern ia64_mv_pci_unmap_sg fancy_pci_unmap_sg;
+extern ia64_mv_pci_dma_sync_single fancy_pci_dma_sync_single;
+extern ia64_mv_pci_dma_sync_sg fancy_pci_dma_sync_sg;
 
 /*
  * This stuff has dual use!
@@ -13,10 +19,18 @@ extern ia64_mv_map_nr_t map_nr_dense;
  * platform's machvec structure.  When compiling a non-generic kernel,
  * the macros are used directly.
  */
-#define platform_name		"dig"
-#define platform_setup		dig_setup
-#define platform_irq_init	dig_irq_init
-#define platform_pci_fixup	iosapic_pci_fixup
-#define platform_map_nr		map_nr_dense
+#define platform_name			"dig"
+#define platform_setup			dig_setup
+#define platform_irq_init		dig_irq_init
+#define platform_pci_fixup		iosapic_pci_fixup
+#define platform_map_nr			map_nr_dense
+#define platform_pci_dma_init		((ia64_mv_mca_init_t *) machvec_noop)
+#define platform_pci_map_single		fancy_pci_map_single
+#define platform_pci_unmap_single 	fancy_pci_unmap_single
+#define platform_pci_map_sg		fancy_pci_map_sg
+#define platform_pci_unmap_sg   	fancy_pci_unmap_sg
+#define platform_pci_dma_sync_single 	fancy_pci_dma_sync_single
+#define platform_pci_dma_sync_sg  	fancy_pci_dma_sync_sg
+
 
 #endif /* _ASM_IA64_MACHVEC_DIG_h */
diff -urNp linux-275/include/asm-ia64/pgalloc.h linux-276/include/asm-ia64/pgalloc.h
--- linux-275/include/asm-ia64/pgalloc.h
+++ linux-276/include/asm-ia64/pgalloc.h
@@ -131,7 +131,7 @@ pte_alloc_one_fast (struct mm_struct *mm
 static inline pte_t*
 pte_alloc_one (struct mm_struct *mm, unsigned long addr)
 {
-	pte_t *pte = (pte_t *) __get_free_page(GFP_KERNEL);
+	pte_t *pte = (pte_t *) __get_free_page(GFP_KERNEL | __GFP_HIGHMEM);
 
 	if (__builtin_expect(pte != NULL, 1))
 		clear_page(pte);
diff -urNp linux-275/mm/slab.c linux-276/mm/slab.c
--- linux-275/mm/slab.c
+++ linux-276/mm/slab.c
@@ -452,8 +452,13 @@ void __init kmem_cache_sizes_init(void)
 		 * allow tighter packing of the smaller caches. */
 		snprintf(name, sizeof(name), "size-%Zd",sizes->cs_size);
 		if (!(sizes->cs_cachep =
+#ifdef CONFIG_IA64
+			kmem_cache_create(name, sizes->cs_size,
+					0, SLAB_CACHE_DMA|SLAB_HWCACHE_ALIGN, NULL, NULL))) {
+#else
 			kmem_cache_create(name, sizes->cs_size,
 					0, SLAB_HWCACHE_ALIGN, NULL, NULL))) {
+#endif
 			BUG();
 		}
 
@@ -787,6 +792,10 @@ next:
 	cachep->gfpflags = 0;
 	if (flags & SLAB_CACHE_DMA)
 		cachep->gfpflags |= GFP_DMA;
+#ifdef CONFIG_IA64
+	else
+		cachep->gfpflags |= __GFP_HIGHMEM;
+#endif
 	spin_lock_init(&cachep->spinlock);
 	cachep->objsize = size;
 	INIT_LIST_HEAD(&cachep->slabs_full);
@@ -1342,7 +1351,9 @@ static inline void * __kmem_cache_alloc 
 	unsigned long save_flags;
 	void* objp;
 
+#ifndef CONFIG_IA64
 	kmem_cache_alloc_head(cachep, flags);
+#endif
 try_again:
 	local_irq_save(save_flags);
 #ifdef CONFIG_SMP
