diff -urNp linux-7050/drivers/scsi/hosts.h linux-7055/drivers/scsi/hosts.h
--- linux-7050/drivers/scsi/hosts.h
+++ linux-7055/drivers/scsi/hosts.h
@@ -443,6 +443,14 @@ struct Scsi_Host
      * when the device becomes less busy that we need to feed them.
      */
     unsigned some_device_starved:1;
+#ifndef __GENKSYMS__ /* preserve KMI/ABI ksyms compatibility for mod linkage */
+
+    /*
+     * This becomes true any time we have a timer active because either a
+     * device or host is blocked.
+     */
+    unsigned unblock_timer_active:1;
+#endif
    
     void (*select_queue_depths)(struct Scsi_Host *, Scsi_Device *);
 
diff -urNp linux-7050/drivers/scsi/osst.c linux-7055/drivers/scsi/osst.c
--- linux-7050/drivers/scsi/osst.c
+++ linux-7055/drivers/scsi/osst.c
@@ -4177,11 +4177,19 @@ static int os_scsi_tape_open(struct inod
 	STp->in_use       = 1;
 	STp->rew_at_close = (MINOR(inode->i_rdev) & 0x80) == 0;
 
-	if (STp->device->host->hostt->module)
-		 __MOD_INC_USE_COUNT(STp->device->host->hostt->module);
-	if (osst_template.module)
-		 __MOD_INC_USE_COUNT(osst_template.module);
-	STp->device->access_count++;
+	spin_lock_irq(STp->device->request_queue.queue_lock);
+	if (STp->device->online) {
+		if (STp->device->host->hostt->module)
+			 __MOD_INC_USE_COUNT(STp->device->host->hostt->module);
+		if (osst_template.module)
+			 __MOD_INC_USE_COUNT(osst_template.module);
+		STp->device->access_count++;
+		spin_unlock_irq(STp->device->request_queue.queue_lock);
+	} else {
+		STp->in_use = 0;
+		spin_unlock_irq(STp->device->request_queue.queue_lock);
+		return -ENODEV;
+	}
 
 	if (mode != STp->current_mode) {
 #if DEBUG
@@ -4519,12 +4527,14 @@ err_out:
 	}
 	STp->in_use = 0;
 	STp->header_ok = 0;
+	spin_lock_irq(STp->device->request_queue.queue_lock);
 	STp->device->access_count--;
 
 	if (STp->device->host->hostt->module)
 	    __MOD_DEC_USE_COUNT(STp->device->host->hostt->module);
 	if (osst_template.module)
 	    __MOD_DEC_USE_COUNT(osst_template.module);
+	spin_unlock_irq(STp->device->request_queue.queue_lock);
 
 	return retval;
 }
diff -urNp linux-7050/drivers/scsi/scsi.c linux-7055/drivers/scsi/scsi.c
--- linux-7050/drivers/scsi/scsi.c
+++ linux-7055/drivers/scsi/scsi.c
@@ -213,8 +213,13 @@ void  scsi_initialize_queue(Scsi_Device 
 	q->queue_lock = &SDpnt->device_lock;
 	q->queuedata = (void *) SDpnt;
 
-	q->max_segments = SHpnt->sg_tablesize;
-
+/*
+ * scsi_malloc() can only dish out items of PAGE_SIZE or less, so we cannot
+ * build a request that requires an sg table allocation of more than that.
+ */
+	q->max_segments = min_t(int, SHpnt->sg_tablesize,
+				PAGE_SIZE / sizeof(struct scatterlist));
+	
         if (SHpnt->hostt->vary_io)
                 blk_queue_large_superbh(q, SHpnt->max_sectors, q->max_segments);
         else
@@ -555,17 +560,6 @@ inline void __scsi_release_command(Scsi_
 						SCpnt->host->in_recovery,
 						SCpnt->host->eh_active));
 	}
-	/*
-	 * If the host is having troubles, then look to see if this was the last
-	 * command that might have failed.  If so, wake up the error handler.
-	 */
-	if (SCpnt->host->in_recovery
-	    && !SCpnt->host->eh_active
-	    && atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
-		SCSI_LOG_ERROR_RECOVERY(5, printk("Waking error handler thread (%d)\n",
-			     atomic_read(&SCpnt->host->eh_wait->count)));
-		up(SCpnt->host->eh_wait);
-	}
 
 	spin_unlock_irqrestore(&SDpnt->device_request_lock, flags);
 
@@ -1220,7 +1214,7 @@ void scsi_softirq_handler(struct softirq
 	Scsi_Cmnd *SCpnt;
 	int cpu = smp_processor_id();
 	struct list_head *item;
-
+	unsigned long flags;
 
 	while (1 == 1) {
 		local_irq_disable();
@@ -1291,16 +1285,9 @@ void scsi_softirq_handler(struct softirq
 				SCpnt->host->host_failed++;
 				SCpnt->owner = SCSI_OWNER_ERROR_HANDLER;
 				SCpnt->state = SCSI_STATE_FAILED;
+				spin_lock_irqsave(SCpnt->host->host_lock, flags);
 				SCpnt->host->in_recovery = 1;
-				/*
-				 * If the host is having troubles, then look to see if this was the last
-				 * command that might have failed.  If so, wake up the error handler.
-				 */
-				if (atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
-					SCSI_LOG_ERROR_RECOVERY(5, printk("Waking error handler thread (%d)\n",
-									  atomic_read(&SCpnt->host->eh_wait->count)));
-					up(SCpnt->host->eh_wait);
-				}
+				spin_unlock_irqrestore(SCpnt->host->host_lock, flags);
 			} else {
 				/*
 				 * We only get here if the error recovery thread has died.
@@ -1308,6 +1295,19 @@ void scsi_softirq_handler(struct softirq
 				scsi_finish_command(SCpnt);
 			}
 		}
+		/*
+		 * Only wake the eh thread if there are no
+		 * timers set to go off.  If there are timers
+		 * set to go off, then when they go off the
+		 * scsi_timeout_block() routine will notice
+		 * that we are in recovery and wake the eh
+		 * thread for us.
+		 */
+		if (host->in_recovery && host->unblock_timer_active == 0 &&
+		    atomic_read(&host->host_busy) == host->host_failed) {
+			SCSI_LOG_ERROR_RECOVERY(5, printk("Waking error handler thread (%d)\n", atomic_read(&host->eh_wait->count)));
+			up(SCpnt->host->eh_wait);
+		}
 	}			/* while(1==1) */
 
 }
@@ -1352,6 +1352,7 @@ void scsi_finish_command(Scsi_Cmnd * SCp
 	Scsi_Request * SRpnt;
 	unsigned long flags;
 	request_queue_t *q = &SCpnt->device->request_queue;
+	int host_was_blocked = SCpnt->host->host_blocked;
 
 	ASSERT_LOCK(q->queue_lock, 0);
 
@@ -1374,7 +1375,9 @@ void scsi_finish_command(Scsi_Cmnd * SCp
          * for both the queue full condition on a device, and for a
          * host full condition on the host.
          */
+        spin_lock_irqsave(host->host_lock, flags);
         host->host_blocked = FALSE;
+        spin_unlock_irqrestore(host->host_lock, flags);
         device->device_blocked = FALSE;
 
 	/*
@@ -1408,6 +1411,38 @@ void scsi_finish_command(Scsi_Cmnd * SCp
 	}
 
 	SCpnt->done(SCpnt);
+
+	if (host_was_blocked) {
+	/*
+	 * We don't really need to do this for normal block devices (sd)
+	 * since they use scsi_io_completion for their done routine and
+	 * it will goose the queue for us.  However, the character and
+	 * special devices that use scsi_do_req and scsi_do_cmd don't
+	 * typically goose the queue after a completion event.  This isn't
+	 * a bad thing as long as every time they submit a command to
+	 * scsi_do_{req,cmd}, the q is able to send the command to the host
+	 * immediately.  However, should the host ever return from
+	 * queuecommand with a non-0 status, signalling that the command
+	 * was *not* queued, the failure to goose the queue upon completion
+	 * could result in a hung device.  So, to avoid that, we goose the
+	 * queue here if we have a device hanging condition.
+	 */
+		for (device = host->host_queue; device; device = device->next) {
+			q = &device->request_queue;
+			if (!device->device_blocked &&
+			    !list_empty(&q->queue_head) &&
+			    atomic_read(&device->device_busy) == 0) {
+				spin_lock_irq(q->queue_lock);
+				q->request_fn(q);
+				spin_unlock_irq(q->queue_lock);
+			}
+		}
+	} else if (!list_empty(&q->queue_head) &&
+		   atomic_read(&device->device_busy) == 0) {
+		spin_lock_irq(q->queue_lock);
+		q->request_fn(q);
+		spin_unlock_irq(q->queue_lock);
+	}
 }
 
 static int scsi_register_host(Scsi_Host_Template *);
@@ -1431,7 +1466,6 @@ void scsi_release_commandblocks(Scsi_Dev
 	Scsi_Cmnd *SCpnt, *SCnext;
 	unsigned long flags;
 
- 	spin_lock_irqsave(SDpnt->request_queue.queue_lock, flags);
 	for (SCpnt = SDpnt->device_queue; SCpnt; SCpnt = SCnext) {
 		SDpnt->device_queue = SCnext = SCpnt->next;
 		list_del(&SCpnt->sc_list);
@@ -1439,7 +1473,6 @@ void scsi_release_commandblocks(Scsi_Dev
 	}
 	SDpnt->has_cmdblocks = 0;
 	SDpnt->queue_depth = 0;
-	spin_unlock_irqrestore(SDpnt->request_queue.queue_lock, flags);
 }
 
 /*
@@ -1463,7 +1496,31 @@ void scsi_build_commandblocks(Scsi_Devic
 	Scsi_Cmnd *SCpnt;
 	request_queue_t *q = &SDpnt->request_queue;
 
-	spin_lock_irqsave(q->queue_lock, flags);
+	/*
+	 * Only init things once.
+	 */
+	if (SDpnt->has_cmdblocks)
+		return;
+
+	/*
+	 * Init the spin lock that protects alloc/free of command blocks
+	 */
+	spin_lock_init(&SDpnt->device_request_lock);
+
+	/*
+	 * Init the list we keep free commands on
+	 */
+	INIT_LIST_HEAD(&SDpnt->sdev_free_q);
+
+	/*
+	 * Init the list we keep commands to be retried on
+	 */
+	INIT_LIST_HEAD(&SDpnt->sdev_retry_q);
+
+        /*
+         * Initialize the object that we will use to wait for command blocks.
+         */
+	init_waitqueue_head(&SDpnt->scpnt_wait);
 
 	if (SDpnt->queue_depth == 0)
 	{
@@ -1472,7 +1529,6 @@ void scsi_build_commandblocks(Scsi_Devic
 			SDpnt->queue_depth = 1; /* live to fight another day */
 	}
 	SDpnt->device_queue = NULL;
-	INIT_LIST_HEAD(&SDpnt->sdev_free_q);
 
 	for (j = 0; j < SDpnt->queue_depth; j++) {
 		SCpnt = (Scsi_Cmnd *)
@@ -1512,7 +1568,6 @@ void scsi_build_commandblocks(Scsi_Devic
 	} else {
 		SDpnt->has_cmdblocks = 1;
 	}
-	spin_unlock_irqrestore(q->queue_lock, flags);
 }
 
 void __init scsi_host_no_insert(char *str, int n)
@@ -1803,8 +1858,23 @@ static int proc_scsi_gen_write(struct fi
 			goto out;	/* there is no such device attached */
 
 		err = -EBUSY;
-		if (scd->access_count)
+		spin_lock_irq(scd->request_queue.queue_lock);
+		if (scd->access_count != 0) {
+			printk(KERN_INFO "scsi%d: remove-single-device %d %d %d failed, device busy(%d).\n", host, channel, id, lun, scd->access_count);
+			spin_unlock_irq(scd->request_queue.queue_lock);
+			goto out;
+		}
+		if (scd->online == 0) {
+			/*
+			 * If we are busy, or some other context is already
+			 * removing this device, then we leave it alone.
+			 */
+			printk(KERN_INFO "scsi%d: remove-single-device %d %d %d failed, device offline.\n", host, channel, id, lun);
+			spin_unlock_irq(scd->request_queue.queue_lock);
 			goto out;
+		}
+		scd->online = 0;
+		spin_unlock_irq(scd->request_queue.queue_lock);
 
 		SDTpnt = scsi_devicelist;
 		while (SDTpnt != NULL) {
@@ -1836,6 +1906,11 @@ static int proc_scsi_gen_write(struct fi
 			blk_cleanup_queue(&scd->request_queue);
 			kfree((char *) scd);
 		} else {
+			/*
+			 * Oops, we didn't detach, might as well set it
+			 * back online.
+			 */
+			scd->online = 1;
 			goto out;
 		}
 		err = 0;
@@ -2721,16 +2796,10 @@ Scsi_Device * scsi_get_host_dev(struct S
         SDpnt->queue_depth = 1;
 	spin_lock_init(&SDpnt->device_request_lock);
         
-	scsi_initialize_queue(SDpnt, SHpnt);
-
 	scsi_build_commandblocks(SDpnt);
+	scsi_initialize_queue(SDpnt, SHpnt);
 
 	SDpnt->online = TRUE;
-
-        /*
-         * Initialize the object that we will use to wait for command blocks.
-         */
-	init_waitqueue_head(&SDpnt->scpnt_wait);
         return SDpnt;
 }
 
@@ -2754,12 +2823,12 @@ void scsi_free_host_dev(Scsi_Device * SD
                 panic("Attempt to delete wrong device\n");
         }
 
+        blk_cleanup_queue(&SDpnt->request_queue);
         /*
          * We only have a single SCpnt attached to this device.  Free
          * it now.
          */
 	scsi_release_commandblocks(SDpnt);
-        blk_cleanup_queue(&SDpnt->request_queue);
 
         kfree(SDpnt);
 }
diff -urNp linux-7050/drivers/scsi/scsi.h linux-7055/drivers/scsi/scsi.h
--- linux-7050/drivers/scsi/scsi.h
+++ linux-7055/drivers/scsi/scsi.h
@@ -345,6 +345,7 @@ extern const char *const scsi_device_typ
 #define SUGGEST_ABORT       0x20
 #define SUGGEST_REMAP       0x30
 #define SUGGEST_DIE         0x40
+#define SUGGEST_DELAYED_RETRY 0x50
 #define SUGGEST_SENSE       0x80
 #define SUGGEST_IS_OK       0xff
 
@@ -614,11 +615,18 @@ struct scsi_device {
 	unsigned starved:1;	/* unable to process commands because
 				   host busy */
 #ifndef __GENKSYMS__ /* preserve KMI/ABI ksyms compatibility for mod linkage */
-        unsigned no_start_on_add:1;	/* do not issue start on add */
+	unsigned no_start_on_add:1;	/* do not issue start on add */
+	unsigned unblock_timer_active:1; /* we've set a timer on this device
+					    to unblock it */
+	unsigned retry_aborted_cmd:1;	/* retry write commands that fail with
+					sense code of ABORTED_CMD infinitely */
 #endif
 
 	// Flag to allow revalidate to succeed in sd_open
 	int allow_revalidate;
+#ifndef __GENKSYMS__ /* preserve KMI/ABI ksyms compatibility for mod linkage */
+	struct list_head sdev_retry_q;	/* list of cmds to be retried */
+#endif
 };
 
 
diff -urNp linux-7050/drivers/scsi/scsi_error.c linux-7055/drivers/scsi/scsi_error.c
--- linux-7050/drivers/scsi/scsi_error.c
+++ linux-7055/drivers/scsi/scsi_error.c
@@ -163,6 +163,8 @@ int scsi_delete_timer(Scsi_Cmnd * SCset)
  */
 void scsi_times_out(Scsi_Cmnd * SCpnt)
 {
+	unsigned long flags;
+
 	/* 
 	 * Notify the low-level code that this operation failed and we are
 	 * reposessing the command.  
@@ -201,12 +203,23 @@ void scsi_times_out(Scsi_Cmnd * SCpnt)
 
 	/* Set the serial_number_at_timeout to the current serial_number */
 	SCpnt->serial_number_at_timeout = SCpnt->serial_number;
-
 	SCpnt->eh_state = FAILED;
 	SCpnt->state = SCSI_STATE_TIMEOUT;
 	SCpnt->owner = SCSI_OWNER_ERROR_HANDLER;
 
+	if( SCpnt->host->eh_wait == NULL ) {
+	/*
+	 * This is too severe, try to survive.
+	 *	panic("Error handler thread not present at %p %p %s %d", 
+	 *		 SCpnt, SCpnt->host, __FILE__, __LINE__);
+	 */
+		scsi_finish_command(SCpnt);
+		return;
+	}
+
+	spin_lock_irqsave(SCpnt->host->host_lock, flags);
 	SCpnt->host->in_recovery = 1;
+	spin_unlock_irqrestore(SCpnt->host->host_lock, flags);
 	SCpnt->host->host_failed++;
 
 	SCSI_LOG_TIMEOUT(3, printk("Command timed out active=%d busy=%d failed=%d\n",
@@ -215,14 +228,15 @@ void scsi_times_out(Scsi_Cmnd * SCpnt)
 				   SCpnt->host->host_failed));
 
 	/*
-	 * If the host is having troubles, then look to see if this was the last
-	 * command that might have failed.  If so, wake up the error handler.
+	 * Only wake the eh thread if there are no
+	 * timers set to go off.  If there are timers
+	 * set to go off, then when they go off the
+	 * scsi_timeout_block() routine will notice
+	 * that we are in recovery and wake the eh
+	 * thread for us.
 	 */
-	if( SCpnt->host->eh_wait == NULL ) {
-		panic("Error handler thread not present at %p %p %s %d", 
-		      SCpnt, SCpnt->host, __FILE__, __LINE__);
-	}
-	if (atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
+	if (!SCpnt->host->unblock_timer_active &&
+	    atomic_read(&SCpnt->host->host_busy) == SCpnt->host->host_failed) {
 		up(SCpnt->host->eh_wait);
 	}
 }
@@ -379,6 +393,11 @@ STATIC int scsi_eh_retry_command(Scsi_Cm
 {
 	do {
 		scsi_setup_cmd_retry(SCpnt);
+        	/*
+        	 * Zero the sense information from the last time we tried
+        	 * this command.
+        	 */
+		memset((void *) SCpnt->sense_buffer, 0, sizeof SCpnt->sense_buffer);
 		scsi_send_eh_cmnd(SCpnt, SCpnt->timeout_per_command);
 	} while (SCpnt->eh_state == NEEDS_RETRY);
 
@@ -684,6 +703,10 @@ STATIC void scsi_send_eh_cmnd(Scsi_Cmnd 
 		case SUCCESS:
 			SCpnt->eh_state = ret;
 			break;
+		/* Error Handling commands are sync, so don't queue to MLQUEUE */
+		case ADD_TO_MLQUEUE:
+			SCpnt->eh_state = NEEDS_RETRY;
+			break;
 		}
 	} else {
 		SCpnt->eh_state = FAILED;
@@ -1048,6 +1071,7 @@ int scsi_decide_disposition(Scsi_Cmnd * 
 	 */
 	switch (status_byte(SCpnt->result)) {
 	case QUEUE_FULL:
+	case BUSY:
 		/*
 		 * The case of trying to send too many commands to a tagged queueing
 		 * device.
@@ -1069,8 +1093,6 @@ int scsi_decide_disposition(Scsi_Cmnd * 
 		 * Who knows?  FIXME(eric)
 		 */
 		return SUCCESS;
-	case BUSY:
-		goto maybe_retry;
 
 	case RESERVATION_CONFLICT:
 		printk("scsi%d (%d,%d,%d) : RESERVATION CONFLICT\n", 
@@ -1191,6 +1213,9 @@ STATIC int scsi_check_sense(Scsi_Cmnd * 
 		return /* SOFT_ERROR */ SUCCESS;
 
 	case ABORTED_COMMAND:
+		if (SCpnt->device->retry_aborted_cmd &&
+		    SCpnt->sc_data_direction == SCSI_DATA_WRITE)
+			return ADD_TO_MLQUEUE;
 		return NEEDS_RETRY;
 	case NOT_READY:
 	case UNIT_ATTENTION:
@@ -1255,12 +1280,20 @@ STATIC void scsi_restart_operations(stru
 	ASSERT_LOCK(host->host_lock, 0);
 
 	/*
+	 * We just completed error handling, we can't be blocked now or else
+	 * we hang forever.
+	 */
+	spin_lock_irqsave(host->host_lock, flags);
+	host->host_blocked = 0;
+	host->host_self_blocked = 0;
+	spin_unlock_irqrestore(host->host_lock, flags);
+
+	/*
 	 * Next free up anything directly waiting upon the host.  This will be
 	 * requests for character device operations, and also for ioctls to queued
 	 * block devices.
 	 */
 	SCSI_LOG_ERROR_RECOVERY(5, printk("scsi_error.c: Waking up host to restart\n"));
-
 	wake_up(&host->host_wait);
 
 	/*
@@ -1269,23 +1302,14 @@ STATIC void scsi_restart_operations(stru
 	 * now that error recovery is done, we will need to ensure that these
 	 * requests are started.
 	 */
-	spin_lock_irqsave(host->host_lock, flags);
 	for (SDpnt = host->host_queue; SDpnt; SDpnt = SDpnt->next) {
 		request_queue_t *q;
-		if ((host->can_queue > 0 && (atomic_read(&host->host_busy) >= host->can_queue))
-		    || (host->host_blocked)
-		    || (host->host_self_blocked)
-		    || (SDpnt->device_blocked)) {
-			break;
-		}
+		SDpnt->device_blocked = 0;
 		q = &SDpnt->request_queue;
-		spin_lock(q->queue_lock);
-		spin_unlock(host->host_lock);
+		spin_lock_irqsave(q->queue_lock, flags);
 		q->request_fn(q);
-		spin_lock(host->host_lock);
-		spin_unlock(q->queue_lock);
+		spin_unlock_irqrestore(q->queue_lock, flags);
 	}
-	spin_unlock_irqrestore(host->host_lock, flags);
 }
 
 /*
@@ -1332,6 +1356,7 @@ STATIC int scsi_unjam_host(struct Scsi_H
 	LIST_HEAD(SCdone);
 	struct list_head *item, *tmp_item;
 	int timed_out;
+	unsigned long flags;
 
 	ASSERT_LOCK(host->host_lock, 0);
 
@@ -1346,7 +1371,8 @@ STATIC int scsi_unjam_host(struct Scsi_H
 			if (SCpnt->state == SCSI_STATE_FAILED
 			    || SCpnt->state == SCSI_STATE_TIMEOUT
 			    || SCpnt->state == SCSI_STATE_INITIALIZING
-			    || SCpnt->state == SCSI_STATE_UNUSED) {
+			    || SCpnt->state == SCSI_STATE_UNUSED
+			    || SCpnt->state == SCSI_STATE_MLQUEUE) {
 				continue;
 			}
 			/*
@@ -1415,7 +1441,7 @@ STATIC int scsi_unjam_host(struct Scsi_H
 				SCpnt->host->host_failed--;
 				scsi_eh_finish_command(&SCdone, SCpnt);
 			}
-			if (result != NEEDS_RETRY) {
+			if (result != NEEDS_RETRY && result != ADD_TO_MLQUEUE) {
 				continue;
 			}
 			/* 
@@ -1814,7 +1840,9 @@ STATIC int scsi_unjam_host(struct Scsi_H
 	 *
 	 * Start by marking that the host is no longer in error recovery.
 	 */
+	spin_lock_irqsave(host->host_lock, flags);
 	host->in_recovery = 0;
+	spin_unlock_irqrestore(host->host_lock, flags);
 
 	/*
 	 * Take the list of commands, and stick them in the bottom half queue.
diff -urNp linux-7050/drivers/scsi/scsi_lib.c linux-7055/drivers/scsi/scsi_lib.c
--- linux-7050/drivers/scsi/scsi_lib.c
+++ linux-7055/drivers/scsi/scsi_lib.c
@@ -229,6 +229,7 @@ void scsi_setup_cmd_retry(Scsi_Cmnd *SCp
 	SCpnt->cmd_len = SCpnt->old_cmd_len;
 	SCpnt->sc_data_direction = SCpnt->sc_old_data_direction;
 	SCpnt->underflow = SCpnt->old_underflow;
+	SCpnt->result = 0;
 }
 
 /*
@@ -269,7 +270,6 @@ void scsi_setup_cmd_retry(Scsi_Cmnd *SCp
  */
 void scsi_queue_next_request(request_queue_t * q, Scsi_Cmnd * SCpnt)
 {
-	int all_clear;
 	Scsi_Device *SDpnt;
 	struct Scsi_Host *SHpnt;
 	unsigned long flags;
@@ -287,15 +287,17 @@ void scsi_queue_next_request(request_que
 		SCpnt->request.special = (void *) SCpnt;
 		spin_lock_irqsave(q->queue_lock, flags);
 		list_add(&SCpnt->request.queue, &q->queue_head);
+		q->request_fn(q);
 		spin_unlock_irqrestore(q->queue_lock, flags);
-	}
+	} else {
 
-	/*
-	 * Just hit the requeue function for the queue.
-	 */
-	spin_lock_irqsave(q->queue_lock, flags);
-	q->request_fn(q);
-	spin_unlock_irqrestore(q->queue_lock, flags);
+		/*
+		 * Just hit the requeue function for the queue.
+		 */
+		spin_lock_irqsave(q->queue_lock, flags);
+		q->request_fn(q);
+		spin_unlock_irqrestore(q->queue_lock, flags);
+	}
 
 	SDpnt = (Scsi_Device *) q->queuedata;
 	SHpnt = SDpnt->host;
@@ -332,34 +334,48 @@ void scsi_queue_next_request(request_que
 
 	/*
 	 * Now see whether there are other devices on the bus which
-	 * might be starved.  If so, hit the request function.  If we
-	 * don't find any, then it is safe to reset the flag.  If we
-	 * find any device that it is starved, it isn't safe to reset the
-	 * flag as the queue function releases the lock and thus some
-	 * other device might have become starved along the way.
+	 * might be starved.  If so, hit the request function.  This
+	 * routine runs with no locks held, so it's entirely possible
+	 * for some other piece of code to set a device starved and
+	 * the some_device_starved flag while we are running.  In order
+	 * to avoid races, we first clear the flag, then scan the
+	 * device list, and if we don't clear the starved status on
+	 * all of the devices, then we turn the some_device_starved
+	 * flag back on ourselves.  If some other code turns the flag
+	 * back on while we are running, then that's perfectly fine.
+	 * If we haven't gotten to the device that caused the flag to
+	 * get turned back on yet then we will shortly, else it will
+	 * get taken care of the next time this function is called.
 	 */
-	all_clear = 1;
+	if (SHpnt->host_blocked || SHpnt->host_self_blocked)
+		return;
+
+	spin_lock_irqsave(SHpnt->host_lock, flags);
 	if (SHpnt->some_device_starved) {
+		SHpnt->some_device_starved = 0;
+		spin_unlock_irqrestore(SHpnt->host_lock, flags);
 		for (SDpnt = SHpnt->host_queue; SDpnt; SDpnt = SDpnt->next) {
 			request_queue_t *q;
-			if ((SHpnt->can_queue > 0 && (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue))
-			    || (SHpnt->host_blocked) 
-			    || (SHpnt->host_self_blocked)) {
+			if (SHpnt->can_queue > 0 &&
+			   (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue))
 				break;
-			}
-			if (SDpnt->device_blocked || !SDpnt->starved) {
+			if (SDpnt->device_blocked || !SDpnt->starved)
 				continue;
-			}
 			q = &SDpnt->request_queue;
 			spin_lock_irqsave(q->queue_lock, flags);
 			q->request_fn(q);
 			spin_unlock_irqrestore(q->queue_lock, flags);
-			all_clear = 0;
+			if (SDpnt->starved)
+				break;
 		}
-		if (SDpnt == NULL && all_clear) {
-			SHpnt->some_device_starved = 0;
+		if (SDpnt != NULL) {
+			/* We didn't get them all, turn the flag back on */
+			spin_lock_irqsave(SHpnt->host_lock, flags);
+			SHpnt->some_device_starved = 1;
+			spin_unlock_irqrestore(SHpnt->host_lock, flags);
 		}
-	}
+	} else
+		spin_unlock_irqrestore(SHpnt->host_lock, flags);
 }
 
 /*
@@ -884,34 +900,34 @@ void scsi_request_fn(request_queue_t * q
 			return;
 
 		/*
-		 * If the device cannot accept another request, then quit.
+		 * There are several points in time (device scan time, hot
+		 * add or removal of devices, hosts that unblock themselves
+		 * at inopportune moments) at which it is possible that
+		 * we may get called with a device that is only partially
+		 * initialized.  Check the has_cmdblocks value and bail
+		 * if the device isn't fully configured.
 		 */
-		if (SDpnt->device_blocked)
+		if (!SDpnt->has_cmdblocks)
+			return;
+
+		/*
+		 * If we are blocked, then quit.
+		 */
+		if (SDpnt->device_blocked ||
+		    SHpnt->host_blocked ||
+		    SHpnt->host_self_blocked)
 			break;
 
 		/*
 		 * If we couldn't find a request that could be queued, then we
 		 * can quit.
 		 */
-		if (list_empty(&q->queue_head))
+		if (list_empty(&q->queue_head) &&
+		    list_empty(&SDpnt->sdev_retry_q))
 			break;
 
 		spin_lock(SHpnt->host_lock);
-		if ((SHpnt->can_queue > 0 && (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue))
-		    || (SHpnt->host_blocked) 
-		    || (SHpnt->host_self_blocked)) {
-			/*
-			 * If we are unable to process any commands at all for
-			 * this device, then we consider it to be starved.
-			 * What this means is that there are no outstanding
-			 * commands for this device and hence we need a
-			 * little help getting it started again
-			 * once the host isn't quite so busy.
-			 */
-			if (atomic_read(&SDpnt->device_busy) == 0) {
-				SDpnt->starved = 1;
-				SHpnt->some_device_starved = 1;
-			}
+		if (SHpnt->can_queue > 0 && (atomic_read(&SHpnt->host_busy) >= SHpnt->can_queue)) {
 			spin_unlock(SHpnt->host_lock);
 			break;
 		} else {
@@ -949,6 +965,27 @@ void scsi_request_fn(request_queue_t * q
 		}
 
 		/*
+		 * See if we have any commands that need a simple retry first.
+		 * If so, handle it and then start this loop over again.
+		 * When we are all out of these commands, then we fall
+		 * through to regular block layer command processing.
+		 */
+		if (!list_empty(&SDpnt->sdev_retry_q)) {
+			/* no need to check that we aren't over the queue
+			 * depth, our command is already allocated out of
+			 * our total number of commands which happens to
+			 * equal maximum queue depth.
+			 */
+			atomic_inc(&SDpnt->device_busy);
+			SCpnt = list_entry(SDpnt->sdev_retry_q.next, Scsi_Cmnd, sc_list);
+			list_del(&SCpnt->sc_list);
+			spin_unlock_irq(q->queue_lock);
+			scsi_dispatch_cmd(SCpnt);
+			spin_lock_irq(q->queue_lock);
+			continue;
+		}
+
+		/*
 		 * Loop through all of the requests in this queue, and find
 		 * one that is queueable.
 		 */
@@ -1043,16 +1080,13 @@ void scsi_request_fn(request_queue_t * q
 				 * probably we ran out of sgtable memory, or
 				 * __init_io() wanted to revert to a single
 				 * segment request. this would require bouncing
-				 * on highmem i/o, so mark the device as
-				 * starved and continue later instead
+				 * on highmem i/o, so tack the command onto the
+				 * request struct and drop out of the loop,
+				 * the test outside the loop will make sure
+				 * we set the device as starved if it's really
+				 * needed.
 				 */
-				spin_lock(SHpnt->host_lock);
 				atomic_dec(&SHpnt->host_busy);
-				if (atomic_read(&SDpnt->device_busy) == 0) {
-					SDpnt->starved = 1;
-					SHpnt->some_device_starved = 1;
-				}
-				spin_unlock(SHpnt->host_lock);
 				SCpnt->request.special = SCpnt;
 				break;
 			}
@@ -1117,8 +1151,16 @@ void scsi_request_fn(request_queue_t * q
 		scsi_dispatch_cmd(SCpnt);
 		spin_lock_irq(q->queue_lock);
 	}
-	if (!list_empty(&q->queue_head) && atomic_read(&SDpnt->device_busy) == 0) {
+	/*
+	 * If the device is blocked then don't set it starved, that just
+	 * makes scsi_queue_next_request call our queue needlessly.  We
+	 * also only need to set it starved if there are 0 commands
+	 * outstanding and requests in the request queue.
+	 */
+	if (!SDpnt->device_blocked && !list_empty(&q->queue_head) &&
+	     atomic_read(&SDpnt->device_busy) == 0) {
 		SDpnt->starved = 1;
+		wmb();
 		spin_lock(SHpnt->host_lock);
 		SHpnt->some_device_starved = 1;
 		spin_unlock(SHpnt->host_lock);
@@ -1169,8 +1211,11 @@ void scsi_block_requests(struct Scsi_Hos
 void scsi_unblock_requests(struct Scsi_Host * SHpnt)
 {
 	Scsi_Device *SDloop;
+	unsigned long flags;
 
+	spin_lock_irqsave(SHpnt->host_lock, flags);
 	SHpnt->host_self_blocked = FALSE;
+	spin_unlock_irqrestore(SHpnt->host_lock, flags);
 	/* Now that we are unblocked, try to start the queues. */
 	for (SDloop = SHpnt->host_queue; SDloop; SDloop = SDloop->next)
 		scsi_queue_next_request(&SDloop->request_queue, NULL);
diff -urNp linux-7050/drivers/scsi/scsi_obsolete.c linux-7055/drivers/scsi/scsi_obsolete.c
--- linux-7050/drivers/scsi/scsi_obsolete.c
+++ linux-7055/drivers/scsi/scsi_obsolete.c
@@ -275,6 +275,9 @@ static int check_sense(Scsi_Cmnd * SCpnt
 		return SUGGEST_IS_OK;
 
 	case ABORTED_COMMAND:
+		if (SCpnt->device->retry_aborted_cmd &&
+		    SCpnt->sc_data_direction == SCSI_DATA_WRITE)
+			return SUGGEST_DELAYED_RETRY;
 		return SUGGEST_RETRY;
 	case NOT_READY:
 	case UNIT_ATTENTION:
@@ -363,6 +366,7 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 #define MAYREDO  1
 #define REDO     3
 #define PENDING  4
+#define DELAYED_REDO 8
 
 #ifdef DEBUG
 	printk("In scsi_done(host = %d, result = %06x)\n", host->host_no, result);
@@ -447,6 +451,9 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 							status = MAYREDO;
 							exit = DRIVER_SENSE | SUGGEST_RETRY;
 							break;
+						case SUGGEST_DELAYED_RETRY:
+							status = DELAYED_REDO;
+							break;
 						case SUGGEST_ABORT:
 #ifdef DEBUG
 							printk("SENSE SUGGEST ABORT - status = CMD_FINISHED");
@@ -485,6 +492,9 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 						status = MAYREDO;
 						exit = DRIVER_SENSE | SUGGEST_RETRY;
 						break;
+					case SUGGEST_DELAYED_RETRY:
+						status = DELAYED_REDO;
+						break;
 					case SUGGEST_ABORT:
 						status = CMD_FINISHED;
 						exit = DRIVER_SENSE | SUGGEST_ABORT;
@@ -503,8 +513,7 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 
 				case BUSY:
 				case QUEUE_FULL:
-					update_timeout(SCpnt, oldto);
-					status = REDO;
+					status = DELAYED_REDO;
 					break;
 
 				case RESERVATION_CONFLICT:
@@ -559,6 +568,8 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 		}
 		break;
 	case DID_BUS_BUSY:
+		status = DELAYED_REDO;
+		break;
 	case DID_PARITY:
 		status = REDO;
 		break;
@@ -602,6 +613,9 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 				status = MAYREDO;
 				exit = DRIVER_SENSE | SUGGEST_RETRY;
 				break;
+			case SUGGEST_DELAYED_RETRY:
+				status = DELAYED_REDO;
+				break;
 			case SUGGEST_ABORT:
 				status = CMD_FINISHED;
 				exit = DRIVER_SENSE | SUGGEST_ABORT;
@@ -649,39 +663,35 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 		if (SCpnt->flags & WAS_SENSE)
 			scsi_request_sense(SCpnt);
 		else {
-			memcpy((void *) SCpnt->cmnd,
-			       (void *) SCpnt->data_cmnd,
-			       sizeof(SCpnt->data_cmnd));
-			memset((void *) SCpnt->sense_buffer, 0,
-			       sizeof(SCpnt->sense_buffer));
-			SCpnt->request_buffer = SCpnt->buffer;
-			SCpnt->request_bufflen = SCpnt->bufflen;
-			SCpnt->use_sg = SCpnt->old_use_sg;
-			SCpnt->cmd_len = SCpnt->old_cmd_len;
-			SCpnt->sc_data_direction = SCpnt->sc_old_data_direction;
-			SCpnt->underflow = SCpnt->old_underflow;
-			SCpnt->result = 0;
                         /*
                          * Ugly, ugly.  The newer interfaces all
                          * assume that the lock isn't held.  Mustn't
                          * disappoint, or we deadlock the system.  
                          */
                         spin_unlock_irq(host->host_lock);
-			scsi_dispatch_cmd(SCpnt);
+			scsi_retry_command(SCpnt);
                         spin_lock_irq(host->host_lock);
 		}
 		break;
+	case DELAYED_REDO:
+		spin_unlock_irq(host->host_lock);
+		scsi_mlqueue_insert(SCpnt, SCSI_MLQUEUE_DEVICE_BUSY);
+		spin_lock_irq(host->host_lock);
+		break;
 	default:
 		INTERNAL_ERROR;
 	}
 
 	if (status == CMD_FINISHED) {
 		Scsi_Request *SRpnt;
+		int host_was_blocked = host->host_blocked;
 #ifdef DEBUG
 		printk("Calling done function - at address %p\n", SCpnt->done);
 #endif
 		atomic_dec(&host->host_busy);	/* Indicate that we are free */
                 atomic_dec(&device->device_busy);/* Decrement device usage counter. */
+		host->host_blocked = 0;
+		device->device_blocked = 0;
 
 		SCpnt->result = result | ((exit & 0xff) << 24);
 		SCpnt->use_sg = SCpnt->old_use_sg;
@@ -706,12 +716,48 @@ void scsi_old_done(Scsi_Cmnd * SCpnt)
 		}
 
 		SCpnt->done(SCpnt);
+	/*
+	 * We don't really need to do this for normal block devices (sd)
+	 * since they use scsi_io_completion for their done routine and
+	 * it will goose the queue for us.  However, the character and
+	 * special devices that use scsi_do_req and scsi_do_cmd don't
+	 * typically goose the queue after a completion event.  This isn't
+	 * a bad thing as long as every time they submit a command to
+	 * scsi_do_{req,cmd}, the q is able to send the command to the host
+	 * immediately.  However, should the host ever return from
+	 * queuecommand with a non-0 status, signalling that the command
+	 * was *not* queued, the failure to goose the queue upon completion
+	 * could result in a hung device.  So, to avoid that, we goose the
+	 * queue here if we have a device hanging condition.
+	 */
+		if (host_was_blocked) {
+			for (device = host->host_queue;
+			     device;
+			     device = device->next) {
+				struct request_queue *q = &device->request_queue;
+				if (!device->device_blocked &&
+		    		    !list_empty(&q->queue_head) &&
+				    atomic_read(&device->device_busy) == 0) {
+					spin_lock_irq(q->queue_lock);
+					q->request_fn(q);
+					spin_unlock_irq(q->queue_lock);
+				}
+			}
+		} else if (!host->host_blocked && !device->device_blocked &&
+		    !list_empty(&device->request_queue.queue_head) &&
+		     atomic_read(&device->device_busy) == 0) {
+			struct request_queue *q = &device->request_queue;
+			spin_lock_irq(q->queue_lock);
+			q->request_fn(q);
+			spin_unlock_irq(q->queue_lock);
+		}
                 spin_lock_irq(host->host_lock);
 	}
 #undef CMD_FINISHED
 #undef REDO
 #undef MAYREDO
 #undef PENDING
+#undef DELAYED_REDO
 }
 
 /*
diff -urNp linux-7050/drivers/scsi/scsi_queue.c linux-7055/drivers/scsi/scsi_queue.c
--- linux-7050/drivers/scsi/scsi_queue.c
+++ linux-7055/drivers/scsi/scsi_queue.c
@@ -56,6 +56,78 @@
 
 static const char RCSid[] = "$Header: /mnt/ide/home/eric/CVSROOT/linux/drivers/scsi/scsi_queue.c,v 1.1 1997/10/21 11:16:38 eric Exp $";
 
+/*
+ * Function:	scsi_timeout_block()
+ *
+ * Purpose:	Unblock a host or device after a time interval has passed
+ *
+ * Arguments:	cmd    - command that provides reference to either the host
+ *			 or device to be unblocked.
+ *
+ * Lock Status: No locks held (called by core timer expiration code)
+ *
+ * Returns:	Nothing
+ *
+ * Note:	We enter here for either a blocked host or device.  We simply
+ *		unblock both and then call the request queue for the
+ *		device passed in.  That will get things going again.  If
+ *		it was the host that was blocked, then we'll be nice and
+ *		call request_fn for all the device queues.
+ */
+void scsi_timeout_block(Scsi_Cmnd * cmd)
+{
+	int host_was_blocked = cmd->host->host_blocked;
+	struct scsi_device *device = cmd->device;
+	struct request_queue *q = &device->request_queue;
+	struct request *req;
+	unsigned long flags;
+
+	device->device_blocked = 0;
+	device->unblock_timer_active = 0;
+	spin_lock_irqsave(cmd->host->host_lock, flags);
+	cmd->host->host_blocked = 0;
+	for (device = cmd->host->host_queue; device; device = device->next)
+		if (device->unblock_timer_active)
+			break;
+	if (!device)
+		cmd->host->unblock_timer_active = 0;
+	spin_unlock_irqrestore(cmd->host->host_lock, flags);
+
+	if (cmd->host->eh_wait != NULL && cmd->host->in_recovery) {
+		/*
+		 * We don't goose any queues if we are in recovery.
+		 * Instead we wait until all timers that are already set
+		 * have fired and host_busy == host_failed.  If all the
+		 * commands have already finished, then we have to start
+		 * the eh thread (it's normally started at the end of
+		 * command completion processing, but it won't get started
+		 * if one of these timers is active).
+		 */
+		if (cmd->host->unblock_timer_active == 0 &&
+		    atomic_read(&cmd->host->host_busy) == cmd->host->host_failed)
+			up(cmd->host->eh_wait);
+	} else if (!host_was_blocked) {
+		spin_lock_irq(q->queue_lock);
+		q->request_fn(q);
+		spin_unlock_irq(q->queue_lock);
+	} else {
+		/*
+		 * OK, the host was blocked.  Walk the device queue and kick
+		 * any devices that aren't blocked.
+		 */
+		for (device = cmd->host->host_queue;
+		     device;
+		     device = device->next) {
+			if (device->device_blocked)
+				continue;
+			q = &device->request_queue;
+			spin_lock_irq(q->queue_lock);
+			q->request_fn(q);
+			spin_unlock_irq(q->queue_lock);
+		}
+	}
+}
+		
 
 /*
  * Function:    scsi_mlqueue_insert()
@@ -79,70 +151,73 @@ static const char RCSid[] = "$Header: /m
 int scsi_mlqueue_insert(Scsi_Cmnd * cmd, int reason)
 {
 	struct Scsi_Host *host;
+	struct scsi_device *device;
+	struct request_queue *q;
 	unsigned long flags;
 
 	SCSI_LOG_MLQUEUE(1, printk("Inserting command %p into mlqueue\n", cmd));
 
+	host = cmd->host;
+	device = cmd->device;
+	q = &device->request_queue;
+
 	/*
-	 * We are inserting the command into the ml queue.  First, we
-	 * cancel the timer, so it doesn't time out.
+	 * Decrement the counters, since these commands are no longer
+	 * active on the host/device.
 	 */
-	scsi_delete_timer(cmd);
+	atomic_dec(&host->host_busy);
+	atomic_dec(&device->device_busy);
 
-	host = cmd->host;
+	/* Clear any bad state info from the last try before putting back
+	 * on the queue.
+	 */
+	scsi_setup_cmd_retry(cmd);
+	memset((void *)&cmd->sense_buffer, 0, sizeof cmd->sense_buffer);
 
 	/*
-	 * Next, set the appropriate busy bit for the device/host.
+	 * Register the fact that we own the thing for now.
 	 */
-	if (reason == SCSI_MLQUEUE_HOST_BUSY) {
+	cmd->state = SCSI_STATE_MLQUEUE;
+	cmd->owner = SCSI_OWNER_MIDLEVEL;
+
+	if (host->eh_wait != NULL && host->in_recovery) {
 		/*
-		 * Protect against race conditions.  If the host isn't busy,
-		 * assume that something actually completed, and that we should
-		 * be able to queue a command now.  Note that there is an implicit
-		 * assumption that every host can always queue at least one command.
-		 * If a host is inactive and cannot queue any commands, I don't see
-		 * how things could possibly work anyways.
+		 * We don't goose any queues if we are in recovery.
+		 * We don't need to start the eh thread either,
+		 * scsi_softirq_handler will do so if we are ready.
 		 */
-		if (atomic_read(&host->host_busy) == 0) {
-			if (scsi_retry_command(cmd) == 0) {
-				return 0;
-			}
-		}
-		host->host_blocked = TRUE;
-	} else {
+		spin_lock_irqsave(q->queue_lock, flags);
+		list_add(&cmd->sc_list, &device->sdev_retry_q);
+		spin_unlock_irqrestore(q->queue_lock, flags);
+	} else if (reason == SCSI_MLQUEUE_HOST_BUSY) {
 		/*
-		 * Protect against race conditions.  If the device isn't busy,
-		 * assume that something actually completed, and that we should
-		 * be able to queue a command now.  Note that there is an implicit
-		 * assumption that every host can always queue at least one command.
-		 * If a host is inactive and cannot queue any commands, I don't see
-		 * how things could possibly work anyways.
+		 * Next, set the appropriate busy bit for the device/host.
 		 */
-		if (atomic_read(&cmd->device->device_busy) == 0) {
-			if (scsi_retry_command(cmd) == 0) {
-				return 0;
-			}
+		spin_lock_irqsave(host->host_lock, flags);
+		host->host_blocked = TRUE;
+		if (atomic_read(&host->host_busy) == 0 &&
+		    !host->unblock_timer_active) {
+			scsi_add_timer(cmd, HZ/5, scsi_timeout_block);
+			host->unblock_timer_active = 1;
 		}
-		cmd->device->device_blocked = TRUE;
+		spin_unlock(host->host_lock);
+		spin_lock(q->queue_lock);
+		list_add(&cmd->sc_list, &device->sdev_retry_q);
+		spin_unlock_irqrestore(q->queue_lock, flags);
+	} else {
+		spin_lock_irqsave(q->queue_lock, flags);
+		device->device_blocked = TRUE;
+		list_add(&cmd->sc_list, &device->sdev_retry_q);
+		if (atomic_read(&device->device_busy) == 0) {
+			scsi_add_timer(cmd, HZ/5, scsi_timeout_block);
+			device->unblock_timer_active = 1;
+			spin_unlock(q->queue_lock);
+			spin_lock(host->host_lock);
+			host->unblock_timer_active = 1;
+			spin_unlock_irqrestore(host->host_lock, flags);
+		} else
+			spin_unlock_irqrestore(q->queue_lock, flags);
 	}
 
-	/*
-	 * Register the fact that we own the thing for now.
-	 */
-	cmd->state = SCSI_STATE_MLQUEUE;
-	cmd->owner = SCSI_OWNER_MIDLEVEL;
-
-	/*
-	 * Decrement the counters, since these commands are no longer
-	 * active on the host/device.
-	 */
-	atomic_dec(&cmd->host->host_busy);
-	atomic_dec(&cmd->device->device_busy);
-
-	/*
-	 * Insert this command at the head of the queue for it's device.
-	 * It will go before all other commands that are already in the queue.
-	 */
-	scsi_insert_special_cmd(cmd, 1);
 	return 0;
 }
diff -urNp linux-7050/drivers/scsi/scsi_scan.c linux-7055/drivers/scsi/scsi_scan.c
--- linux-7050/drivers/scsi/scsi_scan.c
+++ linux-7055/drivers/scsi/scsi_scan.c
@@ -38,6 +38,7 @@
 #define BLIST_ISROM     	0x200	/* Treat as (removable) CD-ROM */
 #define BLIST_LARGELUN		0x400	/* LUNs larger than 7 despite reporting as SCSI 2 */
 #define BLIST_NOSTARTONADD	0x800	/* do not do automatic start on add */
+#define BLIST_RETRY_ABORTED_CMD	0x1000  /* infinite retries of ABORTED_CMD writes */
 
 static void print_inquiry(unsigned char *data);
 static int scan_scsis_single(unsigned int channel, unsigned int dev,
@@ -426,7 +427,6 @@ void scan_scsis(struct Scsi_Host *shpnt,
 		 * Register the queue for the device.  All I/O requests will
 		 * come in through here.
 		 */
-		spin_lock_init(&SDpnt->device_request_lock);
 		scsi_initialize_queue(SDpnt, shpnt);
 		/* Make sure we have something that is valid for DMA purposes */
 		scsi_result = ((!shpnt->unchecked_isa_dma)
@@ -446,11 +446,6 @@ void scan_scsis(struct Scsi_Host *shpnt,
 
 	initialize_merge_fn(SDpnt);
 
-        /*
-         * Initialize the object that we will use to wait for command blocks.
-         */
-	init_waitqueue_head(&SDpnt->scpnt_wait);
-
 	/*
 	 * Next, hook the device to the host in question.
 	 */
@@ -877,6 +872,15 @@ static int scan_scsis_single(unsigned in
 		SDpnt->single_lun = 1;
 
 	/*
+	 * Some devices will return write commands uncompleted with a sense
+	 * key of ABORTED_CMD.  On these devices, the condition that caused
+	 * the command to be aborted is temporary (like you might expect
+	 * from a BUSY status message) and should be retried infinitely.
+	 */
+	if (bflags & BLIST_RETRY_ABORTED_CMD)
+		SDpnt->retry_aborted_cmd = 1;
+
+	/*
 	 * These devices need this "key" to unlock the devices so we can use it
 	 */
 	if ((bflags & BLIST_KEY) != 0) {
@@ -927,7 +931,6 @@ static int scan_scsis_single(unsigned in
 	 * the queue actually represents.   We could look it up, but it
 	 * is pointless work.
 	 */
-	spin_lock_init(&SDpnt->device_request_lock);
 	scsi_initialize_queue(SDpnt, shpnt);
 	SDpnt->host = shpnt;
 	initialize_merge_fn(SDpnt);
@@ -937,11 +940,6 @@ static int scan_scsis_single(unsigned in
 	 */
 	SDpnt->online = TRUE;
 
-        /*
-         * Initialize the object that we will use to wait for command blocks.
-         */
-	init_waitqueue_head(&SDpnt->scpnt_wait);
-
 	/*
 	 * Since we just found one device, there had damn well better be one in the list
 	 * already.
diff -urNp linux-7050/drivers/scsi/sd.c linux-7055/drivers/scsi/sd.c
--- linux-7050/drivers/scsi/sd.c
+++ linux-7055/drivers/scsi/sd.c
@@ -487,6 +487,7 @@ static int sd_open(struct inode *inode, 
 	int target, retval = -ENXIO;
 	Scsi_Device * SDev;
 	target = DEVICE_NR(inode->i_rdev);
+	struct request_queue *q;
 
 	SCSI_LOG_HLQUEUE(1, printk("target=%d, max=%d\n", target, sd_template.dev_max));
 
@@ -508,18 +509,26 @@ static int sd_open(struct inode *inode, 
 
 	while (rscsi_disks[target].device->busy) {
 		barrier();
-		cpu_relax();
+		yield();
 	}
 	/*
 	 * The following code can sleep.
 	 * Module unloading must be prevented
 	 */
 	SDev = rscsi_disks[target].device;
-	if (SDev->host->hostt->module)
-		__MOD_INC_USE_COUNT(SDev->host->hostt->module);
-	if (sd_template.module)
-		__MOD_INC_USE_COUNT(sd_template.module);
-	SDev->access_count++;
+	q = &SDev->request_queue;
+	spin_lock_irq(q->queue_lock);
+	if (SDev->online) {
+		if (SDev->host->hostt->module)
+			__MOD_INC_USE_COUNT(SDev->host->hostt->module);
+		if (sd_template.module)
+			__MOD_INC_USE_COUNT(sd_template.module);
+		SDev->access_count++;
+		spin_unlock_irq(q->queue_lock);
+	} else {
+		spin_unlock_irq(q->queue_lock);
+		return -ENODEV;
+	}
 
 	if (rscsi_disks[target].device->removable) {
 		SDev->allow_revalidate = 1;
@@ -570,11 +579,13 @@ static int sd_open(struct inode *inode, 
 	return 0;
 
 error_out:
+	spin_lock_irq(q->queue_lock);
 	SDev->access_count--;
 	if (SDev->host->hostt->module)
 		__MOD_DEC_USE_COUNT(SDev->host->hostt->module);
 	if (sd_template.module)
 		__MOD_DEC_USE_COUNT(sd_template.module);
+	spin_unlock_irq(q->queue_lock);
 	return retval;	
 }
 
diff -urNp linux-7050/drivers/scsi/sg.c linux-7055/drivers/scsi/sg.c
--- linux-7050/drivers/scsi/sg.c
+++ linux-7055/drivers/scsi/sg.c
@@ -265,19 +265,26 @@ static int sg_open(struct inode * inode,
     Sg_fd * sfp;
     int res;
     int retval = -EBUSY;
+    struct request_queue *q;
 
     SCSI_LOG_TIMEOUT(3, printk("sg_open: dev=%d, flags=0x%x\n", dev, flags));
     sdp = sg_get_dev(dev);
     if ((! sdp) || (! sdp->device))
         return -ENXIO;
-    if (sdp->detached)
-    	return -ENODEV;
+    q = &sdp->device->request_queue;
 
+    spin_lock_irq(q->queue_lock);
+    if (sdp->device->online && !sdp->detached) {
      /* This driver's module count bumped by fops_get in <linux/fs.h> */
      /* Prevent the device driver from vanishing while we sleep */
-     if (sdp->device->host->hostt->module)
-        __MOD_INC_USE_COUNT(sdp->device->host->hostt->module);
-    sdp->device->access_count++;
+	     if (sdp->device->host->hostt->module)
+	        __MOD_INC_USE_COUNT(sdp->device->host->hostt->module);
+	    sdp->device->access_count++;
+	    spin_unlock_irq(q->queue_lock);
+    } else {
+	    spin_unlock_irq(q->queue_lock);
+	    return -ENODEV;
+    }
 
     if (! ((flags & O_NONBLOCK) ||
 	   scsi_block_when_processing_errors(sdp->device))) {
@@ -330,9 +337,11 @@ static int sg_open(struct inode * inode,
     return 0;
 
 error_out:
+    spin_lock_irq(q->queue_lock);
     sdp->device->access_count--;
     if ((! sdp->detached) && sdp->device->host->hostt->module)
         __MOD_DEC_USE_COUNT(sdp->device->host->hostt->module);
+    spin_unlock_irq(q->queue_lock);
     return retval;
 }
 
diff -urNp linux-7050/drivers/scsi/sr.c linux-7055/drivers/scsi/sr.c
--- linux-7050/drivers/scsi/sr.c
+++ linux-7055/drivers/scsi/sr.c
@@ -518,6 +518,8 @@ struct block_device_operations sr_bdops 
 
 static int sr_open(struct cdrom_device_info *cdi, int purpose)
 {
+	struct request_queue *q;
+	struct scsi_device *sdpnt;
 	check_disk_change(cdi->dev);
 
 	if (MINOR(cdi->dev) >= sr_template.dev_max
@@ -531,11 +533,19 @@ static int sr_open(struct cdrom_device_i
 	if (!scsi_block_when_processing_errors(scsi_CDs[MINOR(cdi->dev)].device)) {
 		return -ENXIO;
 	}
-	scsi_CDs[MINOR(cdi->dev)].device->access_count++;
-	if (scsi_CDs[MINOR(cdi->dev)].device->host->hostt->module)
-		__MOD_INC_USE_COUNT(scsi_CDs[MINOR(cdi->dev)].device->host->hostt->module);
+	sdpnt = scsi_CDs[MINOR(cdi->dev)].device;
+	q = &sdpnt->request_queue;
+	spin_lock_irq(q->queue_lock);
+	if (!sdpnt->online) {
+		spin_unlock_irq(q->queue_lock);
+		return -ENODEV;
+	}
+	sdpnt->access_count++;
+	if (sdpnt->host->hostt->module)
+		__MOD_INC_USE_COUNT(sdpnt->host->hostt->module);
 	if (sr_template.module)
 		__MOD_INC_USE_COUNT(sr_template.module);
+	spin_unlock_irq(q->queue_lock);
 
 	/* If this device did not have media in the drive at boot time, then
 	 * we would have been unable to get the sector size.  Check to see if
diff -urNp linux-7050/drivers/scsi/st.c linux-7055/drivers/scsi/st.c
--- linux-7050/drivers/scsi/st.c
+++ linux-7055/drivers/scsi/st.c
@@ -950,9 +950,17 @@ static int st_open(struct inode *inode, 
 	write_unlock_irqrestore(&st_dev_arr_lock, flags);
 	STp->rew_at_close = STp->autorew_dev = (MINOR(inode->i_rdev) & 0x80) == 0;
 
-	if (STp->device->host->hostt->module)
-		__MOD_INC_USE_COUNT(STp->device->host->hostt->module);
-	STp->device->access_count++;
+	spin_lock_irq(STp->device->request_queue.queue_lock);
+	if (STp->device->online) {
+		if (STp->device->host->hostt->module)
+			__MOD_INC_USE_COUNT(STp->device->host->hostt->module);
+		STp->device->access_count++;
+		spin_unlock_irq(STp->device->request_queue.queue_lock);
+	} else {
+		spin_unlock_irq(STp->device->request_queue.queue_lock);
+		STp->in_use = 0;
+		return (-ENODEV);
+	}
 
 	if (!scsi_block_when_processing_errors(STp->device)) {
 		retval = (-ENXIO);
@@ -1019,9 +1027,11 @@ static int st_open(struct inode *inode, 
 		STp->buffer = NULL;
 	}
 	STp->in_use = 0;
+	spin_lock_irq(STp->device->request_queue.queue_lock);
 	STp->device->access_count--;
 	if (STp->device->host->hostt->module)
 	    __MOD_DEC_USE_COUNT(STp->device->host->hostt->module);
+	spin_unlock_irq(STp->device->request_queue.queue_lock);
 	return retval;
 
 }
