diff -urNp linux-1000/arch/i386/mm/init.c linux-1020/arch/i386/mm/init.c
--- linux-1000/arch/i386/mm/init.c
+++ linux-1020/arch/i386/mm/init.c
@@ -461,6 +461,7 @@ static int __init free_pages_init(void)
 #ifdef CONFIG_HIGHMEM
 	for (pfn = highend_pfn-1; pfn >= highstart_pfn; pfn--)
 		one_highpage_init((struct page *) (mem_map + pfn), pfn, bad_ppro);
+	reset_highmem_zone(totalhigh_pages);
 	totalram_pages += totalhigh_pages;
 #endif
 	return reservedpages;
diff -urNp linux-1000/fs/buffer.c linux-1020/fs/buffer.c
--- linux-1000/fs/buffer.c
+++ linux-1020/fs/buffer.c
@@ -120,7 +120,7 @@ union bdflush_param {
 		int dummy5;	/* unused */
 	} b_un;
 	unsigned int data[N_PARAM];
-} bdf_prm = {{30, 500, 0, 0, 5*HZ, 30*HZ, 60, 20, 0}};
+} bdf_prm = {{50, 500, 0, 0, 5*HZ, 30*HZ, 80, 50, 0}};
 
 /* These are the min and max parameter values that we will allow to be assigned */
 int bdflush_min[N_PARAM] = {  0,  1,    0,   0,  0,   1*HZ,   0, 0, 0};
@@ -1035,21 +1035,6 @@ static int balance_dirty_state(void)
 	return -1;
 }
 
-static int bdflush_stop(void)
-{
-	unsigned long dirty, tot, dirty_limit;
-
-	dirty = size_buffers_type[BUF_DIRTY] >> PAGE_SHIFT;
-	tot = nr_free_buffer_pages();
-
-	dirty *= 100;
-	dirty_limit = tot * bdf_prm.b_un.nfract_stop_bdflush;
-
-	if (dirty > dirty_limit)
-		return 0;
-	return 1;
-}
-
 /*
  * if a new dirty buffer is created we need to balance bdflush.
  *
@@ -1405,6 +1390,7 @@ try_to_free:	
  * We rotate the buffers on the buffer_lru list, trying to reclaim
  * them. 
  */
+#ifdef CONFIG_HIGHMEM
 int try_to_reclaim_buffers(int priority, unsigned int gfp_mask)
 {
 	int todo = nr_used_buffer_heads >> priority;
@@ -1412,6 +1398,18 @@ int try_to_reclaim_buffers(int priority,
 	struct buffer_head * bh;
 	struct page * page;
 	int reclaimed = 0;
+	
+	if (RATE_LIMIT(HZ))
+		kmem_cache_shrink(kiobuf_cachep);
+
+	/*
+	 * Since removing buffer heads can be bad for performance, we
+	 * don't bother reclaiming any if the buffer heads take up less
+	 * than 10% of pageable low memory.
+	 */
+	if (nr_used_buffer_heads <
+	    (freeable_lowmem() * PAGE_SIZE) / (sizeof(struct buffer_head) * 10))
+		return 0;
 
 	spin_lock(&unused_list_lock);
 	while (todo-- && !list_empty(&buffer_lru)) {
@@ -1437,6 +1435,7 @@ int try_to_reclaim_buffers(int priority,
 
 	return reclaimed;
 }
+#endif /* CONFIG_HIGHMEM */
 
 /*
  * We don't have to release all buffers here, but
@@ -3134,7 +3133,7 @@ int bdflush(void *startup)
 				break;
 			ndirty -= NRSYNC;
 		}
-		if (ndirty > 0 || bdflush_stop()) {
+		if (ndirty > 0 || balance_dirty_state() < 1) {
 			run_task_queue(&tq_disk);
 			interruptible_sleep_on(&bdflush_wait);
 		}
diff -urNp linux-1000/fs/dcache.c linux-1020/fs/dcache.c
--- linux-1000/fs/dcache.c
+++ linux-1020/fs/dcache.c
@@ -573,7 +573,9 @@ int shrink_dcache_memory(int priority, u
 	count = dentry_stat.nr_unused / priority;
 
 	prune_dcache(count);
-	return kmem_cache_shrink(dentry_cache);
+	if (RATE_LIMIT(HZ))
+		return kmem_cache_shrink(dentry_cache);
+	return 0;
 }
 
 #define NAME_ALLOC_LEN(len)	((len+16) & ~15)
@@ -1303,7 +1305,7 @@ void __init vfs_caches_init(unsigned lon
 {
 	bh_cachep = kmem_cache_create("buffer_head",
 			sizeof(struct buffer_head), 0,
-			SLAB_HWCACHE_ALIGN, init_buffer_head, NULL);
+			0, init_buffer_head, NULL);
 	if(!bh_cachep)
 		panic("Cannot create buffer head SLAB cache");
 
diff -urNp linux-1000/fs/inode.c linux-1020/fs/inode.c
--- linux-1000/fs/inode.c
+++ linux-1020/fs/inode.c
@@ -474,9 +474,10 @@ void sync_inodes(kdev_t dev)
 static void try_to_sync_unused_inodes(void * arg)
 {
 	struct super_block * sb;
-	int nr_inodes = inodes_stat.nr_unused;
+	int nr_inodes;
 
 	spin_lock(&inode_lock);
+	nr_inodes = inodes_stat.nr_unused;
 	spin_lock(&sb_lock);
 	sb = sb_entry(super_blocks.next);
 	for (; nr_inodes && sb != sb_entry(&super_blocks); sb = sb_entry(sb->s_list.next)) {
@@ -622,7 +623,7 @@ static void dispose_list(struct list_hea
 			truncate_inode_pages(&inode->i_data, 0);
 		clear_inode(inode);
 		destroy_inode(inode);
-		inodes_stat.nr_inodes--;
+		/* inodes_stat.nr_inodes maintained by caller */
 	}
 }
 
@@ -657,6 +658,7 @@ static int invalidate_list(struct list_h
 		busy = 1;
 	}
 	/* only unused inodes may be cached with i_count zero */
+	inodes_stat.nr_inodes -= count;
 	inodes_stat.nr_unused -= count;
 	return busy;
 }
@@ -766,9 +768,10 @@ void prune_icache(int goal)
 		list_add(tmp, freeable);
 		inode->i_state |= I_FREEING;
 		count++;
-		if (!--goal)
+		if (--goal <= 0)
 			break;
 	}
+	inodes_stat.nr_inodes -= count;
 	inodes_stat.nr_unused -= count;
 	spin_unlock(&inode_lock);
 
@@ -780,15 +783,15 @@ void prune_icache(int goal)
 	 * from here or we're either synchronously dogslow
 	 * or we deadlock with oom.
 	 */
-	if (goal)
+	if (goal > 0)
 		schedule_task(&unused_inodes_flush_task);
 
 #ifdef CONFIG_HIGHMEM
 	/* Excuse the double negative; the code below is emergency. */
-	if (!goal)
+	if (goal <= 0)
 		return;
-	if (freeable_lowmem() * PAGE_SIZE < 10 * inodes_stat.nr_unused *
-			sizeof(struct inode))
+	if (inodes_stat.nr_unused <
+	    (freeable_lowmem() * PAGE_SIZE) / (sizeof(struct inode) * 10))
 		return;
 	wakeup_bdflush();
 	/*
@@ -808,7 +811,9 @@ void prune_icache(int goal)
 	avg_pages -= atomic_read(&buffermem_pages) + swapper_space.nrpages;
 	avg_pages = avg_pages / (inodes_stat.nr_inodes + 1);
 	spin_lock(&inode_lock);
-	while (goal--) {
+	while (goal-- > 0) {
+		int invalidated_inode;
+
 		if (list_empty(&inode_unused_pagecache))
 			break;
 		entry = inode_unused_pagecache.prev;
@@ -832,13 +837,21 @@ void prune_icache(int goal)
 		 * skip it for now; bdflush (which we woke up above) will
 		 * eventually make the inode freeable.
 		 */
+		invalidated_inode = 0;
 		if (list_empty(&inode->i_mapping->dirty_pages) &&
-				!inode_has_buffers(inode))
+				!inode_has_buffers(inode)) {
 			invalidate_inode_pages(inode);
-
+			invalidated_inode = 1;
+		}
 		/* release inode */
 		spin_lock(&inode_lock);
 		inode->i_state &= ~I_LOCK;
+		if (invalidated_inode && !(inode->i_state & I_FREEING)) {
+			/* wasn't called earlier because I_LOCK was set */
+			__refile_inode(inode);
+		}
+		/* wake up any potential waiters in wait_on_inode() */
+		wake_up(&inode->i_wait);
 	}
 	spin_unlock(&inode_lock);
 #endif /* CONFIG_HIGHMEM */
@@ -846,7 +859,7 @@ void prune_icache(int goal)
 
 int shrink_icache_memory(int priority, int gfp_mask)
 {
-	int count = 0;
+	int count;
 
 	/*
 	 * Nasty deadlock avoidance..
@@ -859,9 +872,11 @@ int shrink_icache_memory(int priority, i
 		return 0;
 
 	count = inodes_stat.nr_unused / priority;
-
-	prune_icache(count);
-	return kmem_cache_shrink(inode_cachep);
+	if (count > 0)
+		prune_icache(count);
+	if (RATE_LIMIT(HZ))
+		return kmem_cache_shrink(inode_cachep);
+	return 0;
 }
 
 /*
diff -urNp linux-1000/fs/iobuf.c linux-1020/fs/iobuf.c
--- linux-1000/fs/iobuf.c
+++ linux-1020/fs/iobuf.c
@@ -6,12 +6,12 @@
  * 
  */
 
-#include <linux/iobuf.h>
 #include <linux/slab.h>
+#include <linux/iobuf.h>
 #include <linux/vmalloc.h>
 
 
-static kmem_cache_t *kiobuf_cachep;
+kmem_cache_t *kiobuf_cachep;
 
 void end_kio_request(struct kiobuf *kiobuf, int uptodate)
 {
diff -urNp linux-1000/include/asm-ppc64/pgalloc.h linux-1020/include/asm-ppc64/pgalloc.h
--- linux-1000/include/asm-ppc64/pgalloc.h
+++ linux-1020/include/asm-ppc64/pgalloc.h
@@ -36,39 +36,22 @@ pgd_alloc_one_fast (struct mm_struct *mm
 static inline pgd_t*
 pgd_alloc (struct mm_struct *mm)
 {
-	/* the VM system never calls pgd_alloc_one_fast(), so we do it here. */
-	pgd_t *pgd = pgd_alloc_one_fast(mm);
+	pgd_t *pgd = (pgd_t *)__get_free_page(GFP_KERNEL);
 
-	if (pgd == NULL) {
-		pgd = (pgd_t *)__get_free_page(GFP_KERNEL);
-		if (pgd != NULL)
-			clear_page(pgd);
-	}
+	if (pgd != NULL)
+		clear_page(pgd);
 	return pgd;
 }
 
 static inline void
 pgd_free (pgd_t *pgd)
 {
-	*(unsigned long *)pgd = (unsigned long) pgd_quicklist;
-	pgd_quicklist = (unsigned long *) pgd;
-	++pgtable_cache_size;
+        free_page((unsigned long)pgd);
 }
 
 #define pgd_populate(MM, PGD, PMD)	pgd_set(PGD, PMD)
 
-static inline pmd_t*
-pmd_alloc_one_fast (struct mm_struct *mm, unsigned long addr)
-{
-	unsigned long *ret = (unsigned long *)pmd_quicklist;
-
-	if (ret != NULL) {
-		pmd_quicklist = (unsigned long *)(*ret);
-		ret[0] = 0;
-		--pgtable_cache_size;
-	}
-	return (pmd_t *)ret;
-}
+#define pmd_alloc_one_fast(mm, address)		(0)
 
 static inline pmd_t*
 pmd_alloc_one (struct mm_struct *mm, unsigned long addr)
@@ -113,9 +96,7 @@ pte_free_kernel(pte_t *pte)
 static inline void
 pmd_free (pmd_t *pmd)
 {
-	*(unsigned long *)pmd = (unsigned long) pmd_quicklist;
-	pmd_quicklist = (unsigned long *) pmd;
-	++pgtable_cache_size;
+        free_page((unsigned long)pmd);
 }
 
 #define pte_alloc_one_fast(mm, address)		(0)
diff -urNp linux-1000/include/linux/fs.h linux-1020/include/linux/fs.h
--- linux-1000/include/linux/fs.h
+++ linux-1020/include/linux/fs.h
@@ -1532,9 +1532,13 @@ extern int brw_page(int, struct page *, 
 
 typedef int (get_block_t)(struct inode*,long,struct buffer_head*,int);
 
+#ifdef CONFIG_HIGHMEM
+extern int try_to_reclaim_buffers(int, unsigned int);
+#else
+#define try_to_reclaim_buffers(x, y) do { ; } while(0)
+#endif
 /* Generic buffer handling for block filesystems.. */
 extern int try_to_release_page(struct page * page, int gfp_mask);
-extern int try_to_reclaim_buffers(int, unsigned int);
 extern int discard_bh_page(struct page *, unsigned long, int);
 #define block_flushpage(page, offset) discard_bh_page(page, offset, 1)
 #define block_invalidate_page(page) discard_bh_page(page, 0, 0)
diff -urNp linux-1000/include/linux/highmem.h linux-1020/include/linux/highmem.h
--- linux-1000/include/linux/highmem.h
+++ linux-1020/include/linux/highmem.h
@@ -10,8 +10,9 @@ extern struct page *highmem_start_page;
 
 #include <asm/highmem.h>
 
-/* declarations for linux/mm/highmem.c */
-unsigned int nr_free_highpages(void);
+/* prototypes for functions in mm/page_alloc.c */
+extern unsigned int nr_free_highpages(void);
+extern void __init reset_highmem_zone(int);
 
 extern struct buffer_head *create_bounce(int rw, struct buffer_head * bh_orig);
 extern int superbh_will_bounce(unsigned long pfn, struct buffer_head *bh);
diff -urNp linux-1000/include/linux/iobuf.h linux-1020/include/linux/iobuf.h
--- linux-1000/include/linux/iobuf.h
+++ linux-1020/include/linux/iobuf.h
@@ -11,6 +11,7 @@
 #include <linux/mm.h>
 #include <linux/init.h>
 #include <linux/wait.h>
+#include <linux/slab.h>
 #include <asm/atomic.h>
 
 /*
@@ -76,6 +77,7 @@ int	expand_kiobuf(struct kiobuf *, int);
 void	kiobuf_wait_for_io(struct kiobuf *);
 extern int alloc_kiobuf_bhs(struct kiobuf *);
 extern void free_kiobuf_bhs(struct kiobuf *);
+extern kmem_cache_t *kiobuf_cachep;
 
 /* fs/buffer.c */
 
diff -urNp linux-1000/include/linux/mm.h linux-1020/include/linux/mm.h
--- linux-1000/include/linux/mm.h
+++ linux-1020/include/linux/mm.h
@@ -353,9 +353,13 @@ typedef struct page {
 #define TryLockPage(page)	test_and_set_bit(PG_locked, &(page)->flags)
 #define PageChecked(page)	test_bit(PG_checked, &(page)->flags)
 #define SetPageChecked(page)	set_bit(PG_checked, &(page)->flags)
+#define ClearPageChecked(page)	clear_bit(PG_checked, &(page)->flags)
 #define PageLaunder(page)	test_bit(PG_launder, &(page)->flags)
 #define SetPageLaunder(page)	set_bit(PG_launder, &(page)->flags)
 #define ClearPageLaunder(page)	clear_bit(PG_launder, &(page)->flags)
+#define ClearPageReferenced(page) clear_bit(PG_referenced, &(page)->flags)
+#define ClearPageError(page)    clear_bit(PG_error, &(page)->flags)
+#define ClearPageArch1(page)    clear_bit(PG_arch_1, &(page)->flags)
 
 /*
  * inlines for acquisition and release of PG_chainlock
diff -urNp linux-1000/include/linux/mm_inline.h linux-1020/include/linux/mm_inline.h
--- linux-1000/include/linux/mm_inline.h
+++ linux-1020/include/linux/mm_inline.h
@@ -69,21 +69,25 @@ static inline int page_dirty(struct page
  * Returns 1 if the page is backed by ram/swap, 0 if the page is
  * backed by a file in a filesystem on permanent storage.
  */
+extern int shmem_writepage(struct page *);
 static inline int page_anon(struct page * page)
 {
-	/* Pages of an mmap()d file won't trigger this unless they get
-	 * referenced on the inactive list and really are in the working
-	 * set of the process... */
-	if (page->pte.direct)
-		return 1;
-
 	if (!page->mapping && !page->buffers)
 		return 1;
 
 	if (PageSwapCache(page))
 		return 1;
 
-	/* TODO: ramfs, tmpfs shm segments and ramdisk */
+	/*
+	 * Files on tmpfs that are are shared writable mmapped are often
+	 * database shared memory segments, which should receive the same
+	 * priority as anonymous memory.  Yes this is ugly.
+	 */
+	if (page->mapping && page->mapping->i_mmap_shared &&
+			page->mapping->a_ops->writepage == shmem_writepage)
+		return 1;
+
+	/* TODO: ramfs and ramdisk */
 
 	return 0;
 }
diff -urNp linux-1000/include/linux/timer.h linux-1020/include/linux/timer.h
--- linux-1000/include/linux/timer.h
+++ linux-1020/include/linux/timer.h
@@ -71,4 +71,13 @@ static inline int timer_pending (const s
 #define time_after_eq(a,b)	((long)(a) - (long)(b) >= 0)
 #define time_before_eq(a,b)	time_after_eq(b,a)
 
+#define RATE_LIMIT(interval)					\
+({								\
+	static unsigned long expires;				\
+	int ok = time_after(jiffies, expires);			\
+	if (ok)							\
+		expires = jiffies + (interval);			\
+	ok;							\
+})
+
 #endif
diff -urNp linux-1000/kernel/fork.c linux-1020/kernel/fork.c
--- linux-1000/kernel/fork.c
+++ linux-1020/kernel/fork.c
@@ -290,6 +290,10 @@ static struct mm_struct * mm_init(struct
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->pgd = pgd_alloc(mm);
 	mm->def_flags = 0;
+	if (current->mm)
+		mm->rlimit_rss = current->mm->rlimit_rss;
+	else
+		mm->rlimit_rss = RLIM_INFINITY;
 	if (mm->pgd)
 		return mm;
 	free_mm(mm);
diff -urNp linux-1000/mm/filemap.c linux-1020/mm/filemap.c
--- linux-1000/mm/filemap.c
+++ linux-1020/mm/filemap.c
@@ -101,11 +101,11 @@ static inline void remove_page_from_inod
 		mapping->a_ops->removepage(page);
 	
 	list_del(&page->list);
-	if (!mapping->nrpages) 
-		refile_inode(mapping->host);
 	page->mapping = NULL;
 	wmb();
 	mapping->nrpages--;
+	if (!mapping->nrpages) 
+		refile_inode(mapping->host);
 }
 
 static inline void remove_page_from_hash_queue(struct page * page)
@@ -664,10 +664,13 @@ static inline void __add_to_page_cache(s
 	struct address_space *mapping, unsigned long offset,
 	struct page **hash)
 {
-	unsigned long flags;
-
-	flags = page->flags & ~(1 << PG_uptodate | 1 << PG_error | 1 << PG_dirty | 1 << PG_referenced | 1 << PG_arch_1 | 1 << PG_checked);
-	page->flags = flags | (1 << PG_locked);
+	ClearPageUptodate(page);
+	ClearPageError(page);
+	ClearPageDirty(page);
+	ClearPageReferenced(page);
+	ClearPageArch1(page);
+	ClearPageChecked(page);
+	LockPage(page);
 	page_cache_get(page);
 	page->index = offset;
 	add_page_to_inode_queue(mapping, page);
diff -urNp linux-1000/mm/memory.c linux-1020/mm/memory.c
--- linux-1000/mm/memory.c
+++ linux-1020/mm/memory.c
@@ -895,17 +895,18 @@ static inline void zeromap_pte_range(pte
 static inline int zeromap_pmd_range(struct mm_struct *mm, pmd_t * pmd, unsigned long address,
                                     unsigned long size, pgprot_t prot)
 {
-	unsigned long end;
+	unsigned long base, end;
 
+	base = address & PGDIR_MASK;
 	address &= ~PGDIR_MASK;
 	end = address + size;
 	if (end > PGDIR_SIZE)
 		end = PGDIR_SIZE;
 	do {
-		pte_t * pte = pte_alloc_map(mm, pmd, address);
+		pte_t * pte = pte_alloc_map(mm, pmd, base + address);
 		if (!pte)
 			return -ENOMEM;
-		zeromap_pte_range(pte, address, end - address, prot);
+		zeromap_pte_range(pte, base + address, end - address, prot);
 		pte_unmap(pte);
 		address = (address + PMD_SIZE) & PMD_MASK;
 		pmd++;
@@ -1351,20 +1352,9 @@ static int do_anonymous_page(struct mm_s
 {
 	pte_t entry;
 	struct page * page = ZERO_PAGE(addr);
-	struct pte_chain * pte_chain;
+	struct pte_chain * pte_chain = NULL;
 	int ret;
 
-	pte_chain = pte_chain_alloc(GFP_ATOMIC);
-	if (!pte_chain) {
-		pte_unmap(page_table);
-		spin_unlock(&mm->page_table_lock);
-		pte_chain = pte_chain_alloc(GFP_KERNEL);
-		if (!pte_chain)
-			goto no_mem;
-		spin_lock(&mm->page_table_lock);
-		page_table = pte_offset_map(pmd, addr);
-	}
-
 	/* Read-only mapping of ZERO_PAGE. */
 	entry = pte_wrprotect(mk_pte(ZERO_PAGE(addr), vma->vm_page_prot));
 
@@ -1374,6 +1364,9 @@ static int do_anonymous_page(struct mm_s
 		pte_unmap(page_table);
 		spin_unlock(&mm->page_table_lock);
 
+		pte_chain = pte_chain_alloc(GFP_KERNEL);
+		if (!pte_chain)
+			goto no_mem;
 		page = alloc_page(GFP_HIGHUSER);
 		if (!page)
 			goto no_mem;
@@ -1395,8 +1388,8 @@ static int do_anonymous_page(struct mm_s
 	}
 
 	set_pte(page_table, entry);
-	/* ignores ZERO PAGE */
-	pte_chain = page_add_rmap(page, page_table, pte_chain);
+	if (write_access)
+		pte_chain = page_add_rmap(page, page_table, pte_chain);
 
 	/* No need to invalidate - it was non-present before */
 	update_mmu_cache(vma, addr, entry);
diff -urNp linux-1000/mm/page_alloc.c linux-1020/mm/page_alloc.c
--- linux-1000/mm/page_alloc.c
+++ linux-1020/mm/page_alloc.c
@@ -122,7 +122,9 @@ static void __free_pages_ok (struct page
 		BUG();
 	if (page->pte.direct)
 		BUG();
-	page->flags &= ~((1<<PG_referenced) | (1<<PG_dirty) | (1 << PG_fresh_page));
+	ClearPageReferenced(page);
+	ClearPageDirty(page);
+	ClearPageFresh(page);
 	
 	zone = page_zone(page);
 
@@ -281,19 +283,18 @@ struct page *_alloc_pages(unsigned int g
  * If we are able to directly reclaim pages, we move pages from the
  * inactive_clean list onto the free list until the zone has enough
  * free pages or until the inactive_clean pages are exhausted.
- * If we cannot do this work ourselves, call kswapd.
  */
 void FASTCALL(fixup_freespace(zone_t * zone, int direct_reclaim));
 void fixup_freespace(zone_t * zone, int direct_reclaim)
 {
 	if (direct_reclaim) {
 		struct page * page;
+		int worktodo = max_t(int, 64, zone->pages_min-zone->free_pages);
 		do {
 			if ((page = reclaim_page(zone)))
 				__free_pages_ok(page, 0);
-		} while (page && zone->free_pages <= zone->pages_min);
-	} else
-		wakeup_kswapd(GFP_ATOMIC);
+		} while (page && worktodo-- > 0);
+	}
 }
 
 #define PAGES_KERNEL	0
@@ -318,7 +319,7 @@ static struct page * __alloc_pages_limit
 		if (!z)
 			break;
 		if (!z->size)
-			BUG();
+			continue;
 
 		/*
 		 * We allocate if the number of (free + inactive_clean)
@@ -341,12 +342,11 @@ static struct page * __alloc_pages_limit
 
 		if (z->free_pages + z->inactive_clean_pages >= water_mark) {
 			struct page *page = NULL;
-			/* If possible, reclaim a page directly. */
-			if (direct_reclaim)
+
+			page = rmqueue(z, order);
+			/* Fall back to direct reclaim if possible */
+			if (!page && direct_reclaim)
 				page = reclaim_page(z);
-			/* If that fails, fall back to rmqueue. */
-			if (!page)
-				page = rmqueue(z, order);
 			if (page)
 				return page;
 		}
@@ -385,21 +385,21 @@ try_again:
 	 * First, see if we have any zones with lots of free memory.
 	 *
 	 * We allocate free memory first because it doesn't contain
-	 * any data we would want to cache.
+	 * any data we would want to cache.  Make sure to stay above
+	 * the watermark that triggers kswapd, otherwise we could
+	 * upset zone balancing.
 	 */
 	zone = zonelist->zones;
 	if (!*zone)
 		return NULL;
-	min = 1UL << order;
 	for (;;) {
 		zone_t *z = *(zone++);
 		if (!z)
 			break;
 		if (!z->size)
-			BUG();
+			continue;
 
-		min += z->pages_min;
-		if (z->free_pages > min) {
+		if (z->free_pages > z->pages_low) {
 			page = rmqueue(z, order);
 			if (page)
 				return page;
@@ -1147,3 +1147,37 @@ static int __init setup_mem_frac(char *s
 }
 
 __setup("memfrac=", setup_mem_frac);
+
+#ifdef CONFIG_HIGHMEM
+void __init reset_highmem_zone(int highmempages)
+{
+
+	pg_data_t	*pgdat;
+	int		sum;
+
+	sum = 0;
+	pgdat = pgdat_list;
+
+	/* sum up the highpages */
+	while (pgdat) {
+		sum += (pgdat->node_zones+ZONE_HIGHMEM)->pages_high;
+		pgdat = pgdat->node_next;
+	}
+
+	pgdat = pgdat_list;
+	/* zero the watermarks and the free count if there's no at least high pages */
+	if (highmempages <= sum) {
+
+		while (pgdat) {
+			(pgdat->node_zones+ZONE_HIGHMEM)->size = 0;
+			(pgdat->node_zones+ZONE_HIGHMEM)->pages_min = 0;
+			(pgdat->node_zones+ZONE_HIGHMEM)->pages_low = 0;
+			(pgdat->node_zones+ZONE_HIGHMEM)->pages_high = 0;
+			(pgdat->node_zones+ZONE_HIGHMEM)->pages_plenty = 0;
+			(pgdat->node_zones+ZONE_HIGHMEM)->free_pages = 0;
+			pgdat = pgdat->node_next;
+		}
+	}
+
+}
+#endif
diff -urNp linux-1000/mm/rmap.c linux-1020/mm/rmap.c
--- linux-1000/mm/rmap.c
+++ linux-1020/mm/rmap.c
@@ -13,11 +13,9 @@
 
 /*
  * Locking:
- * - the page->pte_chain is protected by the PG_chainlock bit,
- *   which nests within the zone lru_lock, then the
  * - the page->pte.chain is protected by the PG_chainlock bit,
- *   which nests within the lru lock, then the
- *   mm->page_table_lock, and then the page lock.
+ *   which nests within the the mm->page_table_lock,
+ *   which nests within the page lock.
  * - because swapout locking is opposite to the locking order
  *   in the page fault path, the swapout path uses trylocks
  *   on the mm->page_table_lock
@@ -41,33 +39,56 @@
  * here, the page struct for the page table page contains the process
  * it belongs to and the offset within that process.
  *
- * We use an array of pte pointers in this structure to minimise cache
- * misses while traversing reverse maps.
+ * We use an array of pte pointers in this structure to minimise cache misses
+ * while traversing reverse maps.
  */
-#define NRPTE ((L1_CACHE_BYTES - sizeof(void *))/sizeof(pte_addr_t))
+#define NRPTE ((L1_CACHE_BYTES - sizeof(unsigned long))/sizeof(pte_addr_t))
 
+/*
+ * next_and_idx encodes both the address of the next pte_chain and the
+ * offset of the highest-index used pte in ptes[].
+ */
 struct pte_chain {
-	struct pte_chain *next;
+	unsigned long next_and_idx;
 	pte_addr_t ptes[NRPTE];
 } ____cacheline_aligned;
 
 static kmem_cache_t	*pte_chain_cache;
 
+static inline struct pte_chain *pte_chain_next(struct pte_chain *pte_chain)
+{
+	return (struct pte_chain *)(pte_chain->next_and_idx & ~NRPTE);
+}
+
+static inline struct pte_chain *pte_chain_ptr(unsigned long pte_chain_addr)
+{
+	return (struct pte_chain *)(pte_chain_addr & ~NRPTE);
+}
+
+static inline int pte_chain_idx(struct pte_chain *pte_chain)
+{
+	return pte_chain->next_and_idx & NRPTE;
+}
+
+static inline unsigned long
+pte_chain_encode(struct pte_chain *pte_chain, int idx)
+{
+	return (unsigned long)pte_chain | idx;
+}
+
 /*
  * pte_chain list management policy:
  *
- * - If a page has a pte_chain list then it is shared by at least two
- *   processes, or by a process which has recently done a fork+exec,
- *   because a single sharing uses PageDirect.
- * - The pageout code collapses pte_chains with a single user back into
- *   PageDirect pointers. This is done lazily so a process can do a number
- *   of fork+exec sequences without having to allocate and free pte_chains.
+ * - If a page has a pte_chain list then it is shared by at least two processes,
+ *   because a single sharing uses PageDirect. (Well, this isn't true yet,
+ *   coz this code doesn't collapse singletons back to PageDirect on the remove
+ *   path).
  * - A pte_chain list has free space only in the head member - all succeeding
  *   members are 100% full.
  * - If the head element has free space, it occurs in its leading slots.
  * - All free space in the pte_chain is at the start of the head member.
- * - Insertion into the pte_chain puts a pte pointer in the last free slot
- *   of the head member.
+ * - Insertion into the pte_chain puts a pte pointer in the last free slot of
+ *   the head member.
  * - Removal from a pte chain moves the head pte of the head member onto the
  *   victim pte and frees the head member if it became empty.
  */
@@ -102,7 +123,8 @@ struct pte_chain * pte_chain_alloc(int g
  */
 void __pte_chain_free(struct pte_chain *pte_chain)
 {
-	pte_chain->next = NULL;
+	if (pte_chain->next_and_idx)
+		pte_chain->next_and_idx = 0;
 	kmem_cache_free(pte_chain_cache, pte_chain);
 }
 
@@ -121,13 +143,12 @@ void __pte_chain_free(struct pte_chain *
  * page are over or under their RSS limit.
  * Caller needs to hold the pte_chain_lock.
  *
- * If the page has a single-entry pte_chain, collapse that back to a
- * PageDirect representation.  This way, it's only done under memory
- * pressure, giving a slight speedup to fork+exec for active forkers.
+ * If the page has a single-entry pte_chain, collapse that back to a PageDirect
+ * representation.  This way, it's only done under memory pressure.
  */
 int page_referenced(struct page * page, int * rsslimit)
 {
-	int referenced = 0, under_rsslimit = 0;
+	int referenced = 0, over_rsslimit = 0;
 	struct mm_struct * mm;
 	struct pte_chain * pc;
 
@@ -136,18 +157,19 @@ int page_referenced(struct page * page, 
 
 	if (PageDirect(page)) {
 		pte_t *pte = rmap_ptep_map(page->pte.direct);
-		if (ptep_test_and_clear_young(pte))
+		if (pte_young(*pte) && ptep_test_and_clear_young(pte))
 			referenced++;
 
 		mm = ptep_to_mm(pte);
-		if (mm->rss < mm->rlimit_rss)
-			under_rsslimit++;
+		if (mm && mm->rss > mm->rlimit_rss)
+			over_rsslimit = 1;
 		rmap_ptep_unmap(pte);
-	} else {
+	} else if (page->pte.chain) {
 		int nr_chains = 0;
+		over_rsslimit = 1;
 
 		/* Check all the page tables mapping this page. */
-		for (pc = page->pte.chain; pc; pc = pc->next) {
+		for (pc = page->pte.chain; pc; pc = pte_chain_next(pc)) {
 			int i;
 
 			for (i = NRPTE-1; i >= 0; i--) {
@@ -160,8 +182,8 @@ int page_referenced(struct page * page, 
 				if (ptep_test_and_clear_young(pte))
 					referenced++;
 				mm = ptep_to_mm(pte);
-				if (mm->rss < mm->rlimit_rss)
-					under_rsslimit++;
+				if (mm && mm->rss <= mm->rlimit_rss)
+					over_rsslimit = 0;
 				rmap_ptep_unmap(pte);
 				nr_chains++;
 			}
@@ -179,7 +201,7 @@ int page_referenced(struct page * page, 
 	 * We're only over the RSS limit if all the processes sharing the
 	 * page are.
 	 */
-	*rsslimit = !under_rsslimit;
+	*rsslimit = over_rsslimit;
 
 	return referenced;
 }
@@ -197,7 +219,6 @@ page_add_rmap(struct page * page, pte_t 
 {
 	pte_addr_t pte_paddr = ptep_to_paddr(ptep);
 	struct pte_chain * cur_pte_chain;
-	int i;
 
 #ifdef DEBUG_RMAP
 	if (!page || !ptep)
@@ -219,6 +240,7 @@ page_add_rmap(struct page * page, pte_t 
 	 */
 	{
 		struct pte_chain * pc;
+		int i;
 		if (PageDirect(page)) {
 			if (page->pte.direct == pte_paddr)
 				BUG();
@@ -246,6 +268,7 @@ page_add_rmap(struct page * page, pte_t 
 		ClearPageDirect(page);
 		pte_chain->ptes[NRPTE-1] = page->pte.direct;
 		pte_chain->ptes[NRPTE-2] = pte_paddr;
+		pte_chain->next_and_idx = pte_chain_encode(NULL, NRPTE-2);
 		page->pte.direct = 0;
 		page->pte.chain = pte_chain;
 		pte_chain = NULL;	/* We consumed it */
@@ -254,22 +277,15 @@ page_add_rmap(struct page * page, pte_t 
 
 	cur_pte_chain = page->pte.chain;
 	if (cur_pte_chain->ptes[0]) {	/* It's full */
-		pte_chain->next = cur_pte_chain;
+		pte_chain->next_and_idx = pte_chain_encode(cur_pte_chain,
+								NRPTE - 1);
 		page->pte.chain = pte_chain;
 		pte_chain->ptes[NRPTE-1] = pte_paddr;
 		pte_chain = NULL;	/* We consumed it */
 		goto out;
 	}
-
-	BUG_ON(!cur_pte_chain->ptes[NRPTE-1]);
-
-	for (i = NRPTE-2; i >= 0; i--) {
-		if (!cur_pte_chain->ptes[i]) {
-			cur_pte_chain->ptes[i] = pte_paddr;
-			goto out;
-		}
-	}
-	BUG();
+	cur_pte_chain->ptes[pte_chain_idx(cur_pte_chain) - 1] = pte_paddr;
+	cur_pte_chain->next_and_idx--;
 out:
 	pte_chain_unlock(page);
 	return pte_chain;
@@ -311,18 +327,18 @@ void page_remove_rmap(struct page * page
 		}
 	} else {
 		struct pte_chain *start = page->pte.chain;
+		struct pte_chain *next;
 		int victim_i = -1;
 
-		for (pc = start; pc; pc = pc->next) {
+		for (pc = start; pc; pc = next) {
 			int i;
 
-			if (pc->next)
-				prefetch(pc->next);
-			for (i = 0; i < NRPTE; i++) {
+			next = pte_chain_next(pc);
+			if (next)
+				prefetch(next);
+			for (i = pte_chain_idx(pc); i < NRPTE; i++) {
 				pte_addr_t pa = pc->ptes[i];
 
-				if (!pa)
-					continue;
 				if (victim_i == -1)
 					victim_i = i;
 				if (pa != pte_paddr)
@@ -331,8 +347,10 @@ void page_remove_rmap(struct page * page
 				start->ptes[victim_i] = 0;
 				if (victim_i == NRPTE-1) {
 					/* Emptied a pte_chain */
-					page->pte.chain = start->next;
+					page->pte.chain = pte_chain_next(start);
 					__pte_chain_free(start);
+				} else {
+					start->next_and_idx++;
 				}
 				goto out;
 			}
@@ -441,8 +459,8 @@ out_unlock:
  * @page: the page to get unmapped
  *
  * Tries to remove all the page table entries which are mapping this
- * page, used in the pageout path.  Caller must hold the zone lru lock
- * and the page lock.  Return values are:
+ * page, used in the pageout path.  Caller must hold the page lock
+ * and its pte chain lock.  Return values are:
  *
  * SWAP_SUCCESS	- we succeeded in removing all mappings
  * SWAP_AGAIN	- we missed a trylock, try again later
@@ -477,10 +495,10 @@ int try_to_unmap(struct page * page)
 	for (pc = start; pc; pc = next_pc) {
 		int i;
 
-		next_pc = pc->next;
+		next_pc = pte_chain_next(pc);
 		if (next_pc)
 			prefetch(next_pc);
-		for (i = 0; i < NRPTE; i++) {
+		for (i = pte_chain_idx(pc); i < NRPTE; i++) {
 			pte_addr_t pte_paddr = pc->ptes[i];
 
 			if (!pte_paddr)
@@ -500,10 +518,12 @@ int try_to_unmap(struct page * page)
 				start->ptes[victim_i] = 0;
 				victim_i++;
 				if (victim_i == NRPTE) {
-					page->pte.chain = start->next;
+					page->pte.chain = pte_chain_next(start);
 					__pte_chain_free(start);
 					start = page->pte.chain;
 					victim_i = 0;
+				} else {
+					start->next_and_idx++;
 				}
 				break;
 			case SWAP_AGAIN:
@@ -540,7 +560,7 @@ void __init pte_chain_init(void)
 	pte_chain_cache = kmem_cache_create(	"pte_chain",
 						sizeof(struct pte_chain),
 						0,
-						0,
+						SLAB_MUST_HWCACHE_ALIGN,
 						pte_chain_ctor,
 						NULL);
 
diff -urNp linux-1000/mm/shmem.c linux-1020/mm/shmem.c
--- linux-1000/mm/shmem.c
+++ linux-1020/mm/shmem.c
@@ -489,7 +489,7 @@ void shmem_unuse(swp_entry_t entry, stru
  * once.  We still need to guard against racing with
  * shmem_getpage_locked().  
  */
-static int shmem_writepage(struct page * page)
+int shmem_writepage(struct page * page)
 {
 	struct shmem_inode_info *info;
 	swp_entry_t *entry, swap;
@@ -589,8 +589,6 @@ repeat:
 	}
 	
 	if (entry->val) {
-		unsigned long flags;
-
 		/* Look it up and read it in.. */
 		page = lookup_swap_cache(*entry);
 		if (!page) {
@@ -622,8 +620,12 @@ repeat:
 		swap_free(*entry);
 		*entry = (swp_entry_t) {0};
 		delete_from_swap_cache(page);
-		flags = page->flags & ~((1 << PG_uptodate) | (1 << PG_error) | (1 << PG_referenced) | (1 << PG_arch_1));
-		page->flags = flags | (1 << PG_dirty);
+
+		ClearPageUptodate(page);
+		ClearPageError(page);
+		ClearPageReferenced(page);
+		ClearPageArch1(page);
+		SetPageDirty(page);
 		add_to_page_cache_locked(page, mapping, idx);
 		info->swapped--;
 		spin_unlock (&info->lock);
diff -urNp linux-1000/mm/vmscan.c linux-1020/mm/vmscan.c
--- linux-1000/mm/vmscan.c
+++ linux-1020/mm/vmscan.c
@@ -152,10 +152,16 @@ struct page * reclaim_page(zone_t * zone
 			goto found_page;
 		}
 
-		/* We should never ever get here. */
-		printk(KERN_ERR "VM: reclaim_page, found unknown page\n");
-		list_del(page_lru);
-		zone->inactive_clean_pages--;
+		/* You might think that we should never ever get here.
+		 * But you'd be wrong. 
+		 * 
+		 * The VM can grab temporary page references while
+		 * scanning pages.  If the page gets unlinked while such
+		 * a reference is held, we end up here with the
+		 * page->count == 1 (the temp ref), but no mapping.
+		 */
+		del_page_from_inactive_clean_list(page);
+		add_page_to_inactive_dirty_list(page);
 		pte_chain_unlock(page);
 		UnlockPage(page);
 	}
@@ -208,6 +214,46 @@ static inline int need_rebalance_laundry
 	return 0;
 }
 
+static int slab_usable_pages(zone_t * inzone)
+{
+	pg_data_t *pgdat;
+	zonelist_t *zonelist;
+	zone_t **zone;
+
+	/* fast path to prevent looking at other zones */
+#if defined(CONFIG_IA64) || !defined(CONFIG_HIGHMEM)
+	if (inzone->free_pages)
+		return 1;
+#else
+	if (inzone->zone_pgdat->node_zones[ZONE_NORMAL].free_pages)
+		return 1;
+#endif
+	if (inzone - inzone->zone_pgdat->node_zones <= ZONE_NORMAL &&
+	    inzone->free_pages)
+		return 1;
+
+	/* slow path */
+	for_each_pgdat(pgdat) {
+		zonelist = pgdat->node_zonelists +
+#if defined(CONFIG_IA64)
+			ZONE_HIGHMEM;
+#else
+			ZONE_NORMAL;
+#endif
+		zone = zonelist->zones;
+		if (*zone) {
+			for (;;) {
+				zone_t *z = *(zone++);
+				if (!z)
+					break;
+				if (z->free_pages)
+					return 1;
+			}
+		}
+	}
+	return 0;
+}
+
 /**
  * launder_page - clean dirty page, move to inactive_laundry list
  * @zone: zone to free pages in
@@ -336,7 +382,8 @@ int launder_page(zone_t * zone, int gfp_
 		int (*writepage)(struct page *);
 
 		writepage = page->mapping->a_ops->writepage;
-		if ((gfp_mask & __GFP_FS) && writepage) {
+		if ((gfp_mask & __GFP_FS) && writepage &&
+				(page->buffers || slab_usable_pages(zone))) {
 			ClearPageDirty(page);
 			SetPageLaunder(page);
 			lru_unlock(zone);
@@ -347,9 +394,9 @@ int launder_page(zone_t * zone, int gfp_
 			lru_lock(zone);
 			return 1;
 		} else {
+			/* We cannot write, somebody else can. */
 			del_page_from_inactive_laundry_list(page);
 			add_page_to_inactive_dirty_list(page);
-			/* FIXME: this is wrong for !__GFP_FS !!! */
 			UnlockPage(page);
 			lru_unlock(zone);
 			page_cache_release(page);
@@ -364,10 +411,11 @@ int launder_page(zone_t * zone, int gfp_
 	 * the page as well.
 	 */
 	if (page->buffers) {
+		int ret;
 		/* To avoid freeing our page before we're done. */
 		lru_unlock(zone);
 
-		try_to_release_page(page, gfp_mask);
+		ret = try_to_release_page(page, gfp_mask);
 		UnlockPage(page);
 
 		/* 
@@ -378,7 +426,7 @@ int launder_page(zone_t * zone, int gfp_
 		page_cache_release(page);
 
 		lru_lock(zone);
-		return 1;
+		return ret;
 	}
 
 	/*
@@ -703,7 +751,13 @@ int rebalance_laundry_zone(struct zone_s
 			if ((gfp_mask & __GFP_WAIT) && (work_done < max_work)) {
 				int timed_out;
 				
-				page_cache_get(page);
+				/* Page is being freed, waiting on lru lock */
+				if (!atomic_inc_if_nonzero(&page->count)) {
+					lru_unlock(zone);
+					cpu_relax();
+					lru_lock(zone);
+					continue;
+				}
 				lru_unlock(zone);
 				run_task_queue(&tq_disk);
 				timed_out = wait_on_page_timeout(page, 5 * HZ);
@@ -715,10 +769,10 @@ int rebalance_laundry_zone(struct zone_s
 				 * be the best page to wait on; move it to
 				 * the head of the dirty list.
 				 */
-				if (timed_out & PageInactiveLaundry(page)) {
+				if (timed_out && PageInactiveLaundry(page)) {
 					unsigned char now;
 					now = (jiffies/HZ)&255;
-					if (now - page->age > 30) {
+					if ((unsigned char)(now - page->age) > 30) {
 						del_page_from_inactive_laundry_list(page);
 						add_page_to_inactive_dirty_list(page);
 					}
@@ -735,16 +789,36 @@ int rebalance_laundry_zone(struct zone_s
 				/* No dice, we can't wait for IO */
 				break;
 		}
-		UnlockPage(page);
+
+		if (page->buffers) {
+			page_cache_get(page);
+			lru_unlock(zone);
+			try_to_release_page(page, 0);
+			UnlockPage(page);
+			page_cache_release(page);
+			lru_lock(zone);
+			if (unlikely((page->buffers != NULL)) &&
+				       	PageInactiveLaundry(page)) {
+				del_page_from_inactive_laundry_list(page);
+				add_page_to_inactive_dirty_list(page);
+				max_loop++;
+				/* Eventually IO will complete. Prevent OOM. */
+				work_done++;
+			}
+			continue;
+		}
 
 		/* Check if the page is still clean or is accessed. */
 		if (unlikely(page->pte.direct || page->buffers ||
 				PageReferenced(page) || PageDirty(page) ||
 				page_count(page) > 1)) {
 			del_page_from_inactive_laundry_list(page);
-			add_page_to_active_list(page, INITIAL_AGE);
+			add_page_to_inactive_dirty_list(page);
+			UnlockPage(page);
+			max_loop++;
 			continue;
 		}
+		UnlockPage(page);
 
 		/*
 		 * If we get here either the IO on the page is done or
@@ -764,7 +838,8 @@ int rebalance_laundry_zone(struct zone_s
 	}
 
 	lru_unlock(zone);
-	return work_done;
+	/* The number of pages freed and those still freeable. */
+	return work_done + zone->inactive_laundry_pages;
 }
 
 /*
@@ -778,7 +853,7 @@ int rebalance_dirty_zone(struct zone_str
 	int work_done = 0;
 	struct page * page;
 
-	max_loop = max_work;
+	max_loop = zone->inactive_dirty_pages;
 	if (max_loop < BATCH_WORK_AMOUNT)
 		max_loop = BATCH_WORK_AMOUNT;
 	/* Take the lock while messing with the list... */
@@ -801,8 +876,8 @@ int rebalance_dirty_zone(struct zone_str
 		if (!launder_page(zone, gfp_mask, page))
 			continue;
 
-		work_done++;
-
+		if (++work_done > max_work)
+			break;
 		/*
 		 * If we've done the minimal batch of work and there's
 		 * no longer any need to rebalance, abort now.
@@ -885,8 +960,12 @@ static int do_try_to_free_pages(unsigned
 	/*
 	 * Eat memory from filesystem page cache, buffer cache,
 	 * dentry, inode and filesystem quota caches.
+	 *
+	 * Because the inactive list might be filled with pages that
+	 * are freeable in principle but not freeable at the moment,
+	 * make sure to always move some pages to the inactive list.
 	 */
-	ret += rebalance_inactive(25);
+	rebalance_inactive(25);
 	for_each_zone(zone) {
 		if (need_rebalance_dirty(zone))
 			ret += rebalance_dirty_zone(zone, BATCH_WORK_AMOUNT,  gfp_mask);
@@ -906,6 +985,12 @@ static int do_try_to_free_pages(unsigned
 	 */
 	ret += kmem_cache_reap(gfp_mask);
 
+	/*
+	 * Mhwahahhaha! This is the part I really like. Giggle.
+	 */
+	if (!ret && free_min(ANY_ZONE) > 0 && (gfp_mask & __GFP_FS))
+		out_of_memory();
+
 	return ret;
 }
 
@@ -955,12 +1040,6 @@ static int do_try_to_free_pages_kswapd(u
 
 	refill_freelist();
 
-	/*
-	 * Mhwahahhaha! This is the part I really like. Giggle.
-	 */
-	if (ret < free_min(ANY_ZONE))
-		out_of_memory();
-
 	return ret;
 }
 
@@ -1032,7 +1111,7 @@ int kswapd(void *unused)
 	 * Kswapd main loop.
 	 */
 	for (;;) {
-		static long recalc = 0;
+		static unsigned long recalc = 0;
 
 		/*
 		 * We try to rebalance the VM either when we have a
@@ -1075,7 +1154,7 @@ void wakeup_kswapd(unsigned int gfp_mask
 	/* If we're in the memory freeing business ourself, don't sleep
 	 * but just wake kswapd and go back to businesss.
 	 */
-	if (current->flags & PF_MEMALLOC) {
+	if (current->flags & (PF_MEMALLOC|PF_MEMDIE)) {
 		wake_up_interruptible(&kswapd_wait);
 		return;
 	}
@@ -1223,7 +1302,7 @@ int kscand(void *unused)
 				continue;
 
 			if (need_active_anon_scan(zone)) {
-				for (age = 0; age < MAX_AGE; age++)  {
+				for (age = MAX_AGE-1; age >= 0; age--)  {
 					scan_active_list(zone, age,
 						&zone->active_anon_list[age]);
 					if (current->need_resched)
@@ -1232,7 +1311,7 @@ int kscand(void *unused)
 			}
 
 			if (need_active_cache_scan(zone)) {
-				for (age = 0; age < MAX_AGE; age++)  {
+				for (age = MAX_AGE-1; age >= 0; age--)  {
 					scan_active_list(zone, age,
 						&zone->active_cache_list[age]);
 					if (current->need_resched)
