diff -urNp linux-7090/arch/i386/kernel/Makefile linux-7100/arch/i386/kernel/Makefile
--- linux-7090/arch/i386/kernel/Makefile
+++ linux-7100/arch/i386/kernel/Makefile
@@ -19,7 +19,7 @@ export-objs     := mca.o mtrr.o msr.o cp
 obj-y	:= process.o semaphore.o signal.o entry.o traps.o irq.o vm86.o \
 		ptrace.o i8259.o ioport.o ldt.o setup.o time.o sys_i386.o \
 		pci-dma.o i386_ksyms.o i387.o bluesmoke.o dmi_scan.o \
-		entry_trampoline.o doublefault.o
+		entry_trampoline.o doublefault.o process_timing.o
 
 
 ifdef CONFIG_PCI
diff -urNp linux-7090/arch/i386/kernel/apic.c linux-7100/arch/i386/kernel/apic.c
--- linux-7090/arch/i386/kernel/apic.c
+++ linux-7100/arch/i386/kernel/apic.c
@@ -1046,6 +1046,7 @@ unsigned int apic_timer_irqs [NR_CPUS];
 void smp_apic_timer_interrupt(struct pt_regs * regs)
 {
 	int cpu = smp_processor_id();
+	process_timing_time_type prev_type;
 
 	/*
 	 * the NMI deadlock-detector uses this.
@@ -1063,11 +1064,22 @@ void smp_apic_timer_interrupt(struct pt_
 	 * interrupt lock, which is the WrongThing (tm) to do.
 	 */
 	irq_enter(cpu, 0);
-	smp_local_timer_interrupt(regs);
+	if (process_timing.flags & PT_FLAGS_IRQ) {
+		prev_type = process_timing.intr_enter(PROCESS_TIMING_IRQ);
+		smp_local_timer_interrupt(regs);
+		process_timing.intr_exit(PROCESS_TIMING_IRQ, prev_type);
+	} else
+		smp_local_timer_interrupt(regs);
 	irq_exit(cpu, 0);
 
-	if (softirq_pending(cpu))
-		do_softirq();
+	if (softirq_pending(cpu)) {
+		if (process_timing.flags & PT_FLAGS_SOFTIRQ) {
+			prev_type = process_timing.intr_enter(PROCESS_TIMING_SOFTIRQ);
+			do_softirq();
+			process_timing.intr_exit(PROCESS_TIMING_SOFTIRQ, prev_type);
+		} else
+			do_softirq();
+	}
 }
 
 /*
diff -urNp linux-7090/arch/i386/kernel/apm.c linux-7100/arch/i386/kernel/apm.c
--- linux-7090/arch/i386/kernel/apm.c
+++ linux-7100/arch/i386/kernel/apm.c
@@ -871,18 +871,19 @@ recalc:
 	if (jiffies_since_last_check > IDLE_CALC_LIMIT) {
 		use_apm_idle = 0;
 		last_jiffies = jiffies;
-		last_stime = current->times.tms_stime;
+		last_stime = kernel_timeval_to_jiffies(&current->stime);
 	} else if (jiffies_since_last_check > idle_period) {
 		unsigned int idle_percentage;
 
-		idle_percentage = current->times.tms_stime - last_stime;
+		idle_percentage = kernel_timeval_to_jiffies(&current->stime) -
+				  last_stime;
 		idle_percentage *= 100;
 		idle_percentage /= jiffies_since_last_check;
 		use_apm_idle = (idle_percentage > idle_threshold);
 		if (apm_info.forbid_idle)
 			use_apm_idle = 0;
 		last_jiffies = jiffies;
-		last_stime = current->times.tms_stime;
+		last_stime = kernel_timeval_to_jiffies(&current->stime);
 	}
 
 	bucket = IDLE_LEAKY_MAX;
diff -urNp linux-7090/arch/i386/kernel/entry.S linux-7100/arch/i386/kernel/entry.S
--- linux-7090/arch/i386/kernel/entry.S
+++ linux-7100/arch/i386/kernel/entry.S
@@ -47,7 +47,7 @@
 #include <asm/page.h>
 #include <asm/smp.h>
 #include <asm/unistd.h>
-
+	
 EBX		= 0x00
 ECX		= 0x04
 EDX		= 0x08
@@ -86,6 +86,12 @@ real_stack	= 36
 virtual_stack	= 40
 user_pgd	= 44
 
+/*
+ * these are offsets in the process_timing_entry struct.
+ */
+pt_sys_enter	= 4
+pt_sys_exit	= 8
+
 ENOSYS = 38
 
 #define GET_CURRENT(reg) \
@@ -307,7 +313,18 @@ ENTRY(system_call)
 	jne tracesys
 	cmpl $(NR_syscalls),%eax
 	jae badsys
-
+	testb $0x08,flags(%ebx)		# PF_TIMING
+	je no_timing
+	movl $(pt_sys_enter),%edx
+	call *SYMBOL_NAME(process_timing)(,%edx,)
+	movl ORIG_EAX(%esp),%eax
+	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
+	movl %eax,EAX(%esp)		# save the return value
+	movl $(pt_sys_exit),%edx
+	call *SYMBOL_NAME(process_timing)(,%edx,)
+	movl EAX(%esp),%eax
+	jmp ret_from_sys_call
+no_timing:
 	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
 	movl %eax,EAX(%esp)		# save the return value
 ENTRY(ret_from_sys_call)
@@ -344,6 +361,18 @@ tracesys:
 	movl ORIG_EAX(%esp),%eax
 	cmpl $(NR_syscalls),%eax
 	jae tracesys_exit
+	testb $0x08,flags(%ebx)		# PF_TIMING
+	je no_timing2
+	movl $(pt_sys_enter),%edx
+	call *SYMBOL_NAME(process_timing)(,%edx,)
+	movl ORIG_EAX(%esp),%eax
+	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
+	movl %eax,EAX(%esp)		# save the return value
+	movl $(pt_sys_exit),%edx
+	call *SYMBOL_NAME(process_timing)(,%edx,)
+	movl EAX(%esp),%eax
+	jmp tracesys_exit
+no_timing2:
 	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
 	movl %eax,EAX(%esp)		# save the return value
 tracesys_exit:
diff -urNp linux-7090/arch/i386/kernel/irq.c linux-7100/arch/i386/kernel/irq.c
--- linux-7090/arch/i386/kernel/irq.c
+++ linux-7100/arch/i386/kernel/irq.c
@@ -33,6 +33,7 @@
 #include <linux/irq.h>
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
+#include <linux/process_timing.h>
 
 #include <asm/atomic.h>
 #include <asm/io.h>
@@ -555,6 +556,7 @@ void enable_irq(unsigned int irq)
 	spin_unlock_irqrestore(&desc->lock, flags);
 }
 
+
 /*
  * do_IRQ handles all normal device IRQ's (the special
  * SMP cross-CPU interrupts have their own specific
@@ -577,6 +579,8 @@ asmlinkage unsigned int do_IRQ(struct pt
 	irq_desc_t *desc = irq_desc + irq;
 	struct irqaction * action;
 	unsigned int status;
+	process_timing_time_type prev_type;
+
 #ifdef CONFIG_DEBUG_STACKOVERFLOW
 	long esp;
 
@@ -592,6 +596,9 @@ asmlinkage unsigned int do_IRQ(struct pt
 	}
 #endif
 
+	if (process_timing.flags & PT_FLAGS_IRQ)
+		prev_type = process_timing.intr_enter(PROCESS_TIMING_IRQ);
+
 	kstat_percpu[cpu].irqs[irq]++;
 	spin_lock(&desc->lock);
 	desc->handler->ack(irq);
@@ -651,8 +658,18 @@ out:
 	desc->handler->end(irq);
 	spin_unlock(&desc->lock);
 
-	if (softirq_pending(cpu))
-		do_softirq();
+	if (process_timing.flags & PT_FLAGS_IRQ)
+		process_timing.intr_exit(PROCESS_TIMING_IRQ, prev_type);
+
+	if (softirq_pending(cpu)) {
+		if (process_timing.flags & PT_FLAGS_SOFTIRQ) {
+			prev_type = process_timing.intr_enter(PROCESS_TIMING_SOFTIRQ);
+			do_softirq();
+			process_timing.intr_exit(PROCESS_TIMING_SOFTIRQ, prev_type);
+		} else
+			do_softirq();
+	}
+
 	return 1;
 }
 
diff -urNp linux-7090/arch/i386/kernel/process_timing.c linux-7100/arch/i386/kernel/process_timing.c
--- linux-7090/arch/i386/kernel/process_timing.c
+++ linux-7100/arch/i386/kernel/process_timing.c
@@ -0,0 +1,289 @@
+/*
+ *  Process Accounting
+ *
+ *  Author: Doug Ledford <dledford@redhat.com>
+ *
+ *  This file includes the x86 arch dependant setup routines and the various
+ *  accounting method functions.
+ *
+ *  Copyright (C) 2003 Red Hat Software
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/sched.h> /* for struct task_struct */
+#include <linux/time.h> /* for struct timeval and timeval functions */
+#include <linux/irq.h> /* for local_irq_save/local_irq_restore */
+#include <linux/kernel_stat.h> /* for kernel_stat_tick_times */
+#include <asm/msr.h> /* for rdtscl() */
+#include <linux/process_timing.h>
+#include <asm/processor.h> /* for cpu capability info */
+
+/* These are in arch/i386/kernel/time.c and are setup during time_init */
+extern int use_tsc;
+extern unsigned long fast_gettimeoffset_quotient;
+
+static void tsc_sysenter(void);
+static void tsc_sysexit(void);
+static void tsc_task_switch(task_t *prev, task_t *next);
+static process_timing_time_type tsc_intr_enter(process_timing_time_type);
+static void tsc_intr_exit(process_timing_time_type, process_timing_time_type);
+static void tsc_tick(void);
+
+static struct tsc_state_cpu tsc_state[NR_CPUS] __cacheline_aligned;
+
+static inline unsigned long tsc_to_usecs(unsigned long cycles)
+{
+	register unsigned long eax = cycles, edx;
+	__asm__("mull %2"
+		:"=a" (eax), "=d" (edx)
+		:"rm" (fast_gettimeoffset_quotient),
+		 "0" (eax));
+	return edx;
+}
+
+void __init process_timing_init(void)
+{
+	int i;
+	task_t *p;
+	
+	printk("Process timing init...");
+	if (process_timing_setup_flags && use_tsc) {
+		printk("tsc timing selected...");
+		for (i=0; i < smp_num_cpus; i++) {
+			memset(&tsc_state[cpu_logical_map(i)], 0,
+			       sizeof(struct tsc_state_cpu));
+			p = cpu_idle_ptr(i);
+			kstat_percpu[cpu_logical_map(i)].unaccounted_task = p;
+			if (process_timing_setup_flags & PT_FLAGS_ALL_PROCESS)
+				p->flags |= PF_TIMING;
+			memset(&p->timing_state, 0, sizeof(p->timing_state));
+			p->timing_state.type = PROCESS_TIMING_SYSTEM;
+			rdtscl(p->timing_state.method.tsc.timestamp);
+		}
+		if (process_timing_setup_flags & PT_FLAGS_ALL_PROCESS) {
+			current->flags |= PF_TIMING;
+			current->timing_state.type = PROCESS_TIMING_USER;
+			rdtscl(current->timing_state.method.tsc.timestamp);
+		}
+		process_timing.sys_enter = tsc_sysenter;
+		process_timing.sys_exit = tsc_sysexit;
+		process_timing.task_switch = tsc_task_switch;
+		process_timing.intr_enter = tsc_intr_enter;
+		process_timing.intr_exit = tsc_intr_exit;
+		process_timing.tick = tsc_tick;
+		process_timing.flags = process_timing_setup_flags;
+	}
+	printk("done.\n");
+}
+
+asmlinkage static void tsc_sysenter(void)
+{
+	task_t *p = current;
+	unsigned long flags, timestamp, delta;
+
+	local_irq_save(flags);
+	rdtscl(timestamp);
+	delta = timestamp - p->timing_state.method.tsc.timestamp;
+	p->timing_state.method.tsc.timestamp = timestamp;
+	if (task_nice(p) > 0)
+		p->timing_state.method.tsc.n_cycles += delta;
+	else
+		p->timing_state.method.tsc.u_cycles += delta;
+	p->timing_state.type = PROCESS_TIMING_SYSTEM;
+	local_irq_restore(flags);
+}
+
+asmlinkage static void tsc_sysexit(void)
+{
+	task_t *p = current;
+	unsigned long flags, timestamp, delta;
+
+	local_irq_save(flags);
+	rdtscl(timestamp);
+	delta = timestamp - p->timing_state.method.tsc.timestamp;
+	p->timing_state.method.tsc.timestamp = timestamp;
+	p->timing_state.method.tsc.s_cycles += delta;
+	p->timing_state.type = PROCESS_TIMING_USER;
+	local_irq_restore(flags);
+}
+
+static void tsc_task_switch(task_t *prev, task_t *next)
+{
+	unsigned long flags, timestamp, delta;
+	struct kernel_stat_tick_times time;
+	int cpu = smp_processor_id();
+
+	if (!tsc_state[cpu].in_intr) {
+		rdtscl(timestamp);
+		next->timing_state.method.tsc.timestamp = timestamp;
+
+		if (!(prev->flags & PF_TIMING)) {
+			kstat_percpu[cpu].unaccounted_task = prev;
+			return;
+		} else {
+			delta = timestamp -
+				prev->timing_state.method.tsc.timestamp;
+			if (prev->timing_state.type == PROCESS_TIMING_SYSTEM)
+				prev->timing_state.method.tsc.s_cycles += delta;
+			else if (task_nice(prev) > 0)
+				prev->timing_state.method.tsc.n_cycles += delta;
+			else
+				prev->timing_state.method.tsc.u_cycles += delta;
+		}
+	}
+
+	memset(&time, 0, sizeof(time));
+	time.u_usec = tsc_to_usecs(prev->timing_state.method.tsc.u_cycles);
+	time.n_usec = tsc_to_usecs(prev->timing_state.method.tsc.n_cycles);
+	time.s_usec = tsc_to_usecs(prev->timing_state.method.tsc.s_cycles);
+	prev->timing_state.method.tsc.u_cycles = 0;
+	prev->timing_state.method.tsc.n_cycles = 0;
+	prev->timing_state.method.tsc.s_cycles = 0;
+	update_process_time_intertick(prev, &time);
+}
+
+static process_timing_time_type tsc_intr_enter(process_timing_time_type type)
+{
+	unsigned long flags, timestamp, delta;
+	int cpu = smp_processor_id();
+	process_timing_time_type old_type;
+
+	local_irq_save(flags);
+	tsc_state[cpu].in_intr++;
+	if (type == tsc_state[cpu].intr_type) {
+		local_irq_restore(flags);
+		return type;
+	}
+	rdtscl(timestamp);
+	if (tsc_state[cpu].in_intr == 1) {
+		task_t *p = current;
+		if (p->flags & PF_TIMING) {
+			delta = timestamp - p->timing_state.method.tsc.timestamp;
+			if (p->timing_state.type == PROCESS_TIMING_SYSTEM)
+				p->timing_state.method.tsc.s_cycles += delta;
+			else if (task_nice(p) > 0)
+				p->timing_state.method.tsc.n_cycles += delta;
+			else
+				p->timing_state.method.tsc.u_cycles += delta;
+		}
+		old_type = p->timing_state.type;
+	} else {
+		delta = timestamp - tsc_state[cpu].timestamp;
+		switch(tsc_state[cpu].intr_type) {
+			default:
+			case PROCESS_TIMING_IRQ:
+				tsc_state[cpu].irq_cycles += delta;
+				break;
+			case PROCESS_TIMING_SOFTIRQ:
+				tsc_state[cpu].softirq_cycles += delta;
+				break;
+		}
+		old_type = tsc_state[cpu].intr_type;
+	}
+	tsc_state[cpu].timestamp = timestamp;
+	tsc_state[cpu].intr_type = type;
+	local_irq_restore(flags);
+	return old_type;
+}
+
+static void tsc_intr_exit(process_timing_time_type type,
+			  process_timing_time_type prev_type)
+{
+	unsigned long flags, timestamp, delta;
+	int cpu = smp_processor_id();
+
+	local_irq_save(flags);
+	tsc_state[cpu].in_intr--;
+	if (type == prev_type) {
+		local_irq_restore(flags);
+		return;
+	}
+	rdtscl(timestamp);
+	delta = timestamp - tsc_state[cpu].timestamp;
+	switch(type) {
+		default:
+		case PROCESS_TIMING_IRQ:
+			tsc_state[cpu].irq_cycles += delta;
+			break;
+		case PROCESS_TIMING_SOFTIRQ:
+			tsc_state[cpu].softirq_cycles += delta;
+			break;
+	}
+	if (tsc_state[cpu].in_intr) {
+		tsc_state[cpu].timestamp = timestamp;
+		tsc_state[cpu].intr_type = prev_type;
+	} else {
+		task_t *p = current;
+		p->timing_state.method.tsc.timestamp = timestamp;
+		tsc_state[cpu].intr_type = PROCESS_TIMING_USER;
+	}
+	local_irq_restore(flags);
+}
+
+/* Ticks are a bit special.  We want to close out the currently running
+ * process times and update the process.  If the system is all accounted,
+ * not mixed, then we can just let the intr stack be and use the timestamp
+ * in current->timing_state.last->method.tsc.tsc_low for all the calculations.
+ * But, if the system is in mixed mode, then we *have* to close out the
+ * entire intr stack and reset all of the intr states to use the timestamp
+ * from right now so that the "remainder" of unaccounted time won't get off
+ * in the update_process_times_mixed() calculations.
+ */
+
+static void tsc_tick(void)
+{
+	unsigned long flags, timestamp, delta;
+	task_t *p = current;
+	struct kernel_stat_tick_times time;
+	int cpu = smp_processor_id();
+
+	local_irq_save(flags);
+	if (tsc_state[cpu].in_intr) {
+		rdtscl(timestamp);
+		delta = timestamp - tsc_state[cpu].timestamp;
+		tsc_state[cpu].timestamp = timestamp;
+		switch (tsc_state[cpu].intr_type) {
+			default:
+			case PROCESS_TIMING_IRQ:
+				tsc_state[cpu].irq_cycles += delta;
+				break;
+			case PROCESS_TIMING_SOFTIRQ:
+				tsc_state[cpu].softirq_cycles += delta;
+				break;
+		}
+	} else if (p->flags & PF_TIMING) {
+		rdtscl(timestamp);
+		delta = timestamp - p->timing_state.method.tsc.timestamp;
+		p->timing_state.method.tsc.timestamp = timestamp;
+		switch (p->timing_state.type) {
+			default:
+			case PROCESS_TIMING_USER:
+				if (task_nice(p) > 0)
+					p->timing_state.method.tsc.n_cycles +=
+						delta;
+				else
+					p->timing_state.method.tsc.u_cycles +=
+						delta;
+				break;
+			case PROCESS_TIMING_SYSTEM:
+				p->timing_state.method.tsc.s_cycles += delta;
+				break;
+		}
+	}
+	memset(&time, 0, sizeof(time));
+	time.u_usec = tsc_to_usecs(p->timing_state.method.tsc.u_cycles);
+	time.n_usec = tsc_to_usecs(p->timing_state.method.tsc.n_cycles);
+	time.s_usec = tsc_to_usecs(p->timing_state.method.tsc.s_cycles);
+	time.irq_usec = tsc_to_usecs(tsc_state[cpu].irq_cycles);
+	time.softirq_usec = tsc_to_usecs(tsc_state[cpu].softirq_cycles);
+	p->timing_state.method.tsc.u_cycles = 0;
+	p->timing_state.method.tsc.n_cycles = 0;
+	p->timing_state.method.tsc.s_cycles = 0;
+	tsc_state[cpu].irq_cycles = 0;
+	tsc_state[cpu].softirq_cycles = 0;
+	update_process_time_intertick(p, &time);
+	local_irq_restore(flags);
+}
+
+
diff -urNp linux-7090/arch/i386/kernel/time.c linux-7100/arch/i386/kernel/time.c
--- linux-7090/arch/i386/kernel/time.c
+++ linux-7100/arch/i386/kernel/time.c
@@ -633,7 +633,7 @@ static inline void do_timer_interrupt(in
 #endif
 }
 
-static int use_tsc;
+int use_tsc;
 
 /*
  * This is the same as the above, except we _also_ save the current
diff -urNp linux-7090/fs/binfmt_elf.c linux-7100/fs/binfmt_elf.c
--- linux-7090/fs/binfmt_elf.c
+++ linux-7100/fs/binfmt_elf.c
@@ -1268,14 +1268,14 @@ static inline void fill_prstatus(struct 
 	prstatus->pr_ppid = p->parent->pid;
 	prstatus->pr_pgrp = p->pgrp;
 	prstatus->pr_sid = p->session;
-	prstatus->pr_utime.tv_sec = CT_TO_SECS(p->times.tms_utime);
-	prstatus->pr_utime.tv_usec = CT_TO_USECS(p->times.tms_utime);
-	prstatus->pr_stime.tv_sec = CT_TO_SECS(p->times.tms_stime);
-	prstatus->pr_stime.tv_usec = CT_TO_USECS(p->times.tms_stime);
-	prstatus->pr_cutime.tv_sec = CT_TO_SECS(p->times.tms_cutime);
-	prstatus->pr_cutime.tv_usec = CT_TO_USECS(p->times.tms_cutime);
-	prstatus->pr_cstime.tv_sec = CT_TO_SECS(p->times.tms_cstime);
-	prstatus->pr_cstime.tv_usec = CT_TO_USECS(p->times.tms_cstime);
+	prstatus->pr_utime.tv_sec = p->utime.tv_sec;
+	prstatus->pr_utime.tv_usec = p->utime.tv_usec;
+	prstatus->pr_stime.tv_sec = p->stime.tv_sec;
+	prstatus->pr_stime.tv_usec = p->stime.tv_usec;
+	prstatus->pr_cutime.tv_sec = p->cutime.tv_sec;
+	prstatus->pr_cutime.tv_usec = p->cutime.tv_usec;
+	prstatus->pr_cstime.tv_sec = p->cstime.tv_sec;
+	prstatus->pr_cstime.tv_usec = p->cstime.tv_usec;
 }
 
 static inline void fill_psinfo(struct elf_prpsinfo *psinfo, struct task_struct *p)
diff -urNp linux-7090/fs/proc/array.c linux-7100/fs/proc/array.c
--- linux-7090/fs/proc/array.c
+++ linux-7100/fs/proc/array.c
@@ -375,10 +375,10 @@ int proc_pid_stat(struct task_struct *ta
 		task->cmin_flt,
 		task->maj_flt,
 		task->cmaj_flt,
-		task->times.tms_utime,
-		task->times.tms_stime,
-		task->times.tms_cutime,
-		task->times.tms_cstime,
+		kernel_timeval_to_clock_t(&task->utime),
+		kernel_timeval_to_clock_t(&task->stime),
+		kernel_timeval_to_clock_t(&task->cutime),
+		kernel_timeval_to_clock_t(&task->cstime),
 		priority,
 		nice,
 		0UL /* removed */,
@@ -407,10 +407,10 @@ int proc_pid_stat(struct task_struct *ta
 		task_cpu(task),
 		task->rt_priority,
 		task->policy,
-		task->group_times.tms_utime,
-		task->group_times.tms_stime,
-		task->group_times.tms_cutime,
-		task->group_times.tms_cstime
+		kernel_timeval_to_clock_t(&task->group_utime),
+		kernel_timeval_to_clock_t(&task->group_stime),
+		kernel_timeval_to_clock_t(&task->group_cutime),
+		kernel_timeval_to_clock_t(&task->group_cstime)
 		);
 	if(mm)
 		mmput(mm);
@@ -745,14 +745,14 @@ int proc_pid_cpu(struct task_struct *tas
 
 	len = sprintf(buffer,
 		"cpu  %lu %lu\n",
-		task->times.tms_utime,
-		task->times.tms_stime);
+		kernel_timeval_to_clock_t(&task->utime),
+		kernel_timeval_to_clock_t(&task->stime));
 		
 	for (i = 0 ; i < smp_num_cpus; i++)
 		len += sprintf(buffer + len, "cpu%d %lu %lu\n",
 			i,
-			task->per_cpu_utime[cpu_logical_map(i)],
-			task->per_cpu_stime[cpu_logical_map(i)]);
+			kernel_timeval_to_clock_t(&task->per_cpu_utime[cpu_logical_map(i)]),
+			kernel_timeval_to_clock_t(&task->per_cpu_stime[cpu_logical_map(i)]));
 
 	return len;
 }
diff -urNp linux-7090/fs/proc/proc_misc.c linux-7100/fs/proc/proc_misc.c
--- linux-7090/fs/proc/proc_misc.c
+++ linux-7100/fs/proc/proc_misc.c
@@ -122,11 +122,11 @@ static int uptime_read_proc(char *page, 
 				 int count, int *eof, void *data)
 {
 	unsigned long uptime;
-	unsigned long idle;
+	struct kernel_timeval idle;
 	int len;
 
 	uptime = jiffies;
-	idle = init_task.times.tms_utime + init_task.times.tms_stime;
+	kernel_timeval_add(&idle, &init_task.utime, &init_task.stime);
 
 	/* The formula for the fraction parts really is ((t * 100) / HZ) % 100, but
 	   that would overflow about every five days at HZ == 100.
@@ -138,17 +138,17 @@ static int uptime_read_proc(char *page, 
 	   format is adapted to the same number of digits as zeroes in HZ.
 	 */
 #if HZ!=100
-	len = sprintf(page,"%lu.%02lu %lu.%02lu\n",
+	len = sprintf(page,"%lu.%02lu %u.%02u\n",
 		uptime / HZ,
 		(((uptime % HZ) * 100) / HZ) % 100,
-		idle / HZ,
-		(((idle % HZ) * 100) / HZ) % 100);
+		idle.tv_sec,
+		idle.tv_usec / 10000);
 #else
-	len = sprintf(page,"%lu.%02lu %lu.%02lu\n",
+	len = sprintf(page,"%lu.%02lu %u.%02u\n",
 		uptime / HZ,
 		uptime % HZ,
-		idle / HZ,
-		idle % HZ);
+		idle.tv_sec,
+		idle.tv_usec / 10000);
 #endif
 	return proc_calc_metrics(page, start, off, count, eof, len);
 }
@@ -328,38 +328,45 @@ static struct file_operations proc_slabi
 static int kstat_read_proc(char *page, char **start, off_t off,
 				 int count, int *eof, void *data)
 {
-	int i, len = 0;
+	int i, j, cpu, len = 0;
 	extern unsigned long total_forks;
 	unsigned long jif = jiffies;
-	unsigned int sum = 0, user = 0, nice = 0, system = 0, idle = 0, iowait = 0;
+	unsigned int intr_sum = 0, user = 0, nice = 0, system = 0, idle = 0, iowait = 0, irq = 0, softirq = 0;
 	int major, disk;
 
 	for (i = 0 ; i < smp_num_cpus; i++) {
-		int cpu = cpu_logical_map(i), j;
+		int j;
 
-		user += kstat_percpu[cpu].user;
-		nice += kstat_percpu[cpu].nice;
-		system += kstat_percpu[cpu].system;
-		idle += kstat_percpu[cpu].idle;
-		iowait += kstat_percpu[cpu].iowait;
+		cpu = cpu_logical_map(i);
+		user += kernel_timeval_to_clock_t(&kstat_percpu[cpu].user);
+		nice += kernel_timeval_to_clock_t(&kstat_percpu[cpu].nice);
+		system += kernel_timeval_to_clock_t(&kstat_percpu[cpu].system);
+		idle += kernel_timeval_to_clock_t(&kstat_percpu[cpu].idle);
+		iowait += kernel_timeval_to_clock_t(&kstat_percpu[cpu].iowait);
+		irq += kernel_timeval_to_clock_t(&kstat_percpu[cpu].irq);
+		softirq += kernel_timeval_to_clock_t(&kstat_percpu[cpu].softirq);
 #if !defined(CONFIG_ARCH_S390)
 		for (j = 0 ; j < NR_IRQS ; j++)
-			sum += kstat_percpu[cpu].irqs[j];
+			intr_sum += kstat_percpu[cpu].irqs[j];
 #endif
 	}
 
 	proc_sprintf(page, &off, &len,
-		      "cpu  %u %u %u %u %u\n", user, nice, system,
-		      idle, iowait);
-	for (i = 0 ; i < smp_num_cpus; i++)
+		      "cpu  %u %u %u %u %u %u %u\n", user, nice, system,
+		      idle, iowait, irq, softirq);
+	for (i = 0; i < smp_num_cpus; i++) {
+		cpu = cpu_logical_map(i);
 		proc_sprintf(page, &off, &len,
-			"cpu%d %u %u %u %u %u\n",
+			"cpu%d %u %u %u %u %u %u %u\n",
 			i,
-			kstat_percpu[cpu_logical_map(i)].user,
-			kstat_percpu[cpu_logical_map(i)].nice,
-			kstat_percpu[cpu_logical_map(i)].system,
-			kstat_percpu[cpu_logical_map(i)].idle,
-			kstat_percpu[cpu_logical_map(i)].iowait);
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].user),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].nice),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].system),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].idle),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].iowait),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].irq),
+			kernel_timeval_to_clock_t(&kstat_percpu[cpu].softirq));
+	}
 	proc_sprintf(page, &off, &len,
 		"page %u %u\n"
 		"swap %u %u\n"
@@ -368,12 +375,12 @@ static int kstat_read_proc(char *page, c
 			kstat_sum(pgpgout) >> 1,
 			kstat_sum(pswpin),
 			kstat_sum(pswpout),
-			sum
+			intr_sum
 	);
 #if !defined(CONFIG_ARCH_S390) && !defined(CONFIG_ALPHA) && !defined(CONFIG_PPC64)
-	for (i = 0 ; i < NR_IRQS ; i++)
+	for (j = 0 ; j < NR_IRQS ; j++)
 		proc_sprintf(page, &off, &len,
-			     " %u", kstat_irqs(i));
+			     " %u", kstat_irqs(j));
 #endif
 
 	proc_sprintf(page, &off, &len, "\ndisk_io: ");
diff -urNp linux-7090/include/asm-alpha/process_timing.h linux-7100/include/asm-alpha/process_timing.h
--- linux-7090/include/asm-alpha/process_timing.h
+++ linux-7100/include/asm-alpha/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-arm/process_timing.h linux-7100/include/asm-arm/process_timing.h
--- linux-7090/include/asm-arm/process_timing.h
+++ linux-7100/include/asm-arm/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-cris/process_timing.h linux-7100/include/asm-cris/process_timing.h
--- linux-7090/include/asm-cris/process_timing.h
+++ linux-7100/include/asm-cris/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-generic/process_timing.h linux-7100/include/asm-generic/process_timing.h
--- linux-7090/include/asm-generic/process_timing.h
+++ linux-7100/include/asm-generic/process_timing.h
@@ -0,0 +1,23 @@
+/*
+ *  Process Timing 
+ *
+ *  Author: Norm Murray (nmurray@redhat.com)
+ *
+ *  This header file contains the definitions needed to implement
+ *  absolute, clock based, process timestamp accounting rather than the
+ *  old style statistical time accounting. 
+ *  
+ *  Copyright (C) 2003 Red Hat Software
+ *
+ */
+
+#ifndef _LINUX_PROCESS_TIMING_ASM_H
+#define _LINUX_PROCESS_TIMING_ASM_H
+
+union process_timing_state_method {
+	int dummy;
+};
+
+#define process_timing_init()	do { } while (0)
+
+#endif	/* _LINUX_PROCESS_TIMING_ASM_H */
diff -urNp linux-7090/include/asm-i386/process_timing.h linux-7100/include/asm-i386/process_timing.h
--- linux-7090/include/asm-i386/process_timing.h
+++ linux-7100/include/asm-i386/process_timing.h
@@ -0,0 +1,39 @@
+/*
+ *  Process Timing 
+ *
+ *  Author: Norm Murray (nmurray@redhat.com)
+ *
+ *  This header file contains the definitions needed to implement
+ *  absolute, clock based, process timestamp accounting rather than the
+ *  old style statistical time accounting. 
+ *  
+ *  Copyright (C) 2003 Red Hat Software
+ *
+ */
+
+#ifndef _LINUX_PROCESS_TIMING_ASM_H
+#define _LINUX_PROCESS_TIMING_ASM_H
+
+extern void process_timing_init(void);
+
+struct tsc_state {
+	unsigned long timestamp;
+	unsigned long u_cycles;
+	unsigned long n_cycles;
+	unsigned long s_cycles;
+};
+
+struct tsc_state_cpu {
+	unsigned long timestamp;
+	unsigned long softirq_cycles;
+	unsigned long irq_cycles;
+	int in_intr;
+	process_timing_time_type intr_type;
+} ____cacheline_aligned;
+
+union process_timing_state_method {
+	struct tsc_state tsc;
+};
+
+
+#endif	/* _LINUX_PROCESS_TIMING_ASM_H */
diff -urNp linux-7090/include/asm-ia64/process_timing.h linux-7100/include/asm-ia64/process_timing.h
--- linux-7090/include/asm-ia64/process_timing.h
+++ linux-7100/include/asm-ia64/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-m68k/process_timing.h linux-7100/include/asm-m68k/process_timing.h
--- linux-7090/include/asm-m68k/process_timing.h
+++ linux-7100/include/asm-m68k/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-mips/process_timing.h linux-7100/include/asm-mips/process_timing.h
--- linux-7090/include/asm-mips/process_timing.h
+++ linux-7100/include/asm-mips/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-mips64/process_timing.h linux-7100/include/asm-mips64/process_timing.h
--- linux-7090/include/asm-mips64/process_timing.h
+++ linux-7100/include/asm-mips64/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-parisc/process_timing.h linux-7100/include/asm-parisc/process_timing.h
--- linux-7090/include/asm-parisc/process_timing.h
+++ linux-7100/include/asm-parisc/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-ppc/process_timing.h linux-7100/include/asm-ppc/process_timing.h
--- linux-7090/include/asm-ppc/process_timing.h
+++ linux-7100/include/asm-ppc/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-ppc64/process_timing.h linux-7100/include/asm-ppc64/process_timing.h
--- linux-7090/include/asm-ppc64/process_timing.h
+++ linux-7100/include/asm-ppc64/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-s390/process_timing.h linux-7100/include/asm-s390/process_timing.h
--- linux-7090/include/asm-s390/process_timing.h
+++ linux-7100/include/asm-s390/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-s390x/process_timing.h linux-7100/include/asm-s390x/process_timing.h
--- linux-7090/include/asm-s390x/process_timing.h
+++ linux-7100/include/asm-s390x/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-sh/process_timing.h linux-7100/include/asm-sh/process_timing.h
--- linux-7090/include/asm-sh/process_timing.h
+++ linux-7100/include/asm-sh/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-sparc/process_timing.h linux-7100/include/asm-sparc/process_timing.h
--- linux-7090/include/asm-sparc/process_timing.h
+++ linux-7100/include/asm-sparc/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-sparc64/process_timing.h linux-7100/include/asm-sparc64/process_timing.h
--- linux-7090/include/asm-sparc64/process_timing.h
+++ linux-7100/include/asm-sparc64/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/asm-x86_64/process_timing.h linux-7100/include/asm-x86_64/process_timing.h
--- linux-7090/include/asm-x86_64/process_timing.h
+++ linux-7100/include/asm-x86_64/process_timing.h
@@ -0,0 +1,2 @@
+/* No process accounting on this arch yet */
+#include <asm-generic/process_timing.h>
diff -urNp linux-7090/include/linux/kernel_stat.h linux-7100/include/linux/kernel_stat.h
--- linux-7090/include/linux/kernel_stat.h
+++ linux-7100/include/linux/kernel_stat.h
@@ -5,6 +5,7 @@
 #include <asm/irq.h>
 #include <linux/smp.h>
 #include <linux/threads.h>
+#include <linux/process_timing.h>
 
 /*
  * 'kernel_stat.h' contains the definitions needed for doing
@@ -15,8 +16,27 @@
 #define DK_MAX_MAJOR 16
 #define DK_MAX_DISK 16
 
+/* We only roll the accounted times into the kstat struct at each timer tick,
+ * this way we can handle both processes with and without accounting enabled
+ * on them.  We use our accumulated accounting time at each tick to subtract
+ * from the number of usecs that *would* be handed out by the timer tick
+ * accounting, that way we don't get accounting errors.
+ */
+struct kernel_stat_tick_times {
+	unsigned long u_usec;		/* user time */
+	unsigned long n_usec;		/* nice user time */
+	unsigned long s_usec;		/* system time */
+	unsigned long irq_usec;
+	unsigned long softirq_usec;
+	unsigned long iowait_usec;
+	unsigned long idle_usec;
+};
+
 struct kernel_stat_percpu {
-	unsigned int user, nice, system, iowait, idle;
+	struct kernel_timeval user, nice, system;
+	struct kernel_timeval irq, softirq, iowait, idle;
+	struct kernel_stat_tick_times accumulated_time;
+	struct task_struct *unaccounted_task;
 	unsigned int dk_drive[DK_MAX_MAJOR][DK_MAX_DISK];
 	unsigned int dk_drive_rio[DK_MAX_MAJOR][DK_MAX_DISK];
 	unsigned int dk_drive_wio[DK_MAX_MAJOR][DK_MAX_DISK];
diff -urNp linux-7090/include/linux/prctl.h linux-7100/include/linux/prctl.h
--- linux-7090/include/linux/prctl.h
+++ linux-7100/include/linux/prctl.h
@@ -34,4 +34,13 @@
 # define PR_FP_EXC_ASYNC	2	/* async recoverable exception mode */
 # define PR_FP_EXC_PRECISE	3	/* precise exception mode */
 
+/* Get/set whether we use statistical process timing or accurate timestamp
+ * based process timing */
+#define PR_GET_TIMING	13
+#define PR_SET_TIMING	14
+# define PR_TIMING_STATISTICAL	0	/* Normal, traditional,
+						   statistical process timing */
+# define PR_TIMING_TIMESTAMP	1	/* Accurate timestamp based
+						   process timing */
+
 #endif /* _LINUX_PRCTL_H */
diff -urNp linux-7090/include/linux/process_timing.h linux-7100/include/linux/process_timing.h
--- linux-7090/include/linux/process_timing.h
+++ linux-7100/include/linux/process_timing.h
@@ -0,0 +1,128 @@
+/*
+ *  Process Timing 
+ *
+ *  Author: Norm Murray (nmurray@redhat.com)
+ *
+ *  This header file contains the definitions needed to implement
+ *  absolute, clock based, process timestamp accounting rather than the
+ *  old style statistical time accounting. 
+ *  
+ *  Copyright (C) 2003 Red Hat Software
+ *
+ */
+
+#ifndef _LINUX_PROCESS_TIMING_H
+#define _LINUX_PROCESS_TIMING_H
+
+struct kernel_timeval {
+	unsigned int	tv_sec;
+	unsigned int	tv_usec;
+};
+
+/*
+ * The process timing code treats the values in timeval as unsigned long.
+ * We also normalize the usec field each time we add usecs, so we don't have
+ * to worry about it overflowing.  On the second field we don't worry about
+ * overflow either because it's just a wrap and will only happen after over
+ * 4 billion seconds of CPU time have been accounted to the process.  So,
+ * since we don't really do timeval math on anything but internal structures,
+ * we use these simple timeval functions.  If you need to deal with user
+ * supplied timeval structs like in itimer.c, then you will need to use more
+ * robust timeval functions.
+ */
+static inline void kernel_timeval_add_usec(struct kernel_timeval *time,
+                                           unsigned long usecs)
+{
+        time->tv_usec += usecs;
+        /* This loop is faster than a divide/mod sequence for typical cases */
+        while(time->tv_usec >= 1000000) {
+                time->tv_sec++;
+                time->tv_usec -= 1000000;
+        }
+}
+        
+static inline void kernel_timeval_addto(struct kernel_timeval *dest,
+					struct kernel_timeval *add)
+{
+        dest->tv_sec += add->tv_sec;    
+        kernel_timeval_add_usec(dest, add->tv_usec);
+}
+                
+static inline void kernel_timeval_add(struct kernel_timeval *sum,
+				      struct kernel_timeval *time1,
+                                      struct kernel_timeval *time2)
+{       
+        sum->tv_sec = time1->tv_sec + time2->tv_sec;
+        sum->tv_usec = time1->tv_usec + time2->tv_usec;
+        while(sum->tv_usec >= 1000000) {
+                sum->tv_sec++;
+                sum->tv_usec -= 1000000;
+        }
+}
+
+/* Used in almost all internal calculations where we want the time spent on
+ * this process in jiffie increments */
+static inline unsigned long kernel_timeval_to_jiffies(struct kernel_timeval *time)
+{
+        return ((unsigned long)time->tv_sec * HZ + (time->tv_usec / (1000000/HZ)));
+}       
+
+/* Used in the implementation of the times() syscall, returns user time and
+ * sys time with a normalized HZ of CLOCKS_PER_SEC */
+static inline unsigned long kernel_timeval_to_clock_t(struct kernel_timeval *time)
+{
+        return ((unsigned long)time->tv_sec * CLOCKS_PER_SEC + (time->tv_usec / (1000000/CLOCKS_PER_SEC)));
+}
+
+
+
+
+typedef enum {
+	PROCESS_TIMING_USER,
+	PROCESS_TIMING_SYSTEM,
+	PROCESS_TIMING_IRQ,
+	PROCESS_TIMING_SOFTIRQ
+} process_timing_time_type;
+
+/* Need the definition of union process_timing_state_method from the asm header */
+#include <asm/process_timing.h>
+
+/*
+ * We could have used a struct list_head in here, but we don't need full list
+ * functions, only head and tail of our interrupt state stack.  So, we do it
+ * by hand instead for speed/ease.
+ */
+struct process_timing_state {
+	process_timing_time_type		type;
+	union process_timing_state_method	method;
+};
+
+/*
+ * enabled, sys_enter, and sys_exit have hardcoded offsets in
+ * arch/i386/kernel/entry.S.  Don't move them around.
+ */
+struct process_timing_entry {
+    	int    flags;
+	void (*sys_enter)(void);
+	void (*sys_exit)(void);
+	void (*task_switch)(struct task_struct *prev, struct task_struct *next);
+	process_timing_time_type (*intr_enter)(process_timing_time_type now);
+	void (*intr_exit)(process_timing_time_type now,
+			  process_timing_time_type prev);
+	void (*tick)(void);
+};
+
+extern struct process_timing_entry process_timing;
+extern int process_timing_setup_flags;
+
+/*
+ * These are the various flags present in pa_flags that control how the system
+ * uses accurate accounting code.
+ */
+#define PT_FLAGS_IRQ		0x00000001	/* Account around interrupts */
+#define PT_FLAGS_SOFTIRQ	0x00000002	/* Account around softirqs */
+#define PT_FLAGS_PROCESS	0x00000004	/* Enabled process accounting */
+#define PT_FLAGS_ALL_PROCESS	0x00000008	/* Account all processes */
+#define PT_FLAGS_EVERYTHING	0x0000000f	/* Everything on */
+
+#endif	/* _LINUX_PROCESS_TIMING_H */
diff -urNp linux-7090/include/linux/sched.h linux-7100/include/linux/sched.h
--- linux-7090/include/linux/sched.h
+++ linux-7100/include/linux/sched.h
@@ -28,6 +28,7 @@ extern unsigned long event;
 #include <linux/securebits.h>
 #include <linux/fs_struct.h>
 #include <linux/pid.h>
+#include <linux/kernel_stat.h>
 
 struct exec_domain;
 extern int exec_shield;
@@ -165,9 +166,12 @@ extern void show_stack(unsigned long * e
 extern void cpu_init (void);
 extern void trap_init(void);
 extern void update_process_times(int user);
-extern void update_one_process(task_t *p, unsigned long user,
-			       unsigned long system, int cpu);
-extern void scheduler_tick(int user_tick, int system);
+extern void update_process_time_intertick(task_t *,
+					  struct kernel_stat_tick_times *);
+extern void update_one_process(task_t *p, struct kernel_stat_tick_times *,
+			       int cpu);
+extern void scheduler_tick(int timer_tick);
+extern void update_kstatpercpu(task_t *, struct kernel_stat_tick_times *time);
 extern int migration_init(void);
 extern unsigned long cache_decay_ticks;
 
@@ -443,6 +447,7 @@ struct task_struct {
 	/* ------- end of hardcoded fields ---------------- */
 
 	int prio, static_prio;
+
 	struct list_head run_list;
 	prio_array_t *array;
 
@@ -499,10 +504,15 @@ struct task_struct {
 	unsigned long it_real_value, it_prof_value, it_virt_value;
 	unsigned long it_real_incr, it_prof_incr, it_virt_incr;
 	struct timer_list real_timer;
-	struct tms times;
-	struct tms group_times;
+	struct kernel_timeval utime, stime, cutime, cstime;
+	struct kernel_timeval group_utime, group_stime;
+	struct kernel_timeval group_cutime, group_cstime;
+	struct process_timing_state timing_state;
+	unsigned int last_sigxcpu;	/* second of cpu time at which we sent
+					   our last sigxcpu signal */
+
 	unsigned long start_time;
-	long per_cpu_utime[NR_CPUS], per_cpu_stime[NR_CPUS];
+	struct kernel_timeval per_cpu_utime[NR_CPUS], per_cpu_stime[NR_CPUS];
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt, nswap, cmin_flt, cmaj_flt, cnswap;
 	int swappable:1;
@@ -579,6 +589,7 @@ struct task_struct {
 					/* Not implemented yet, only for 486*/
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
+#define PF_TIMING	0x00000008	/* enable accurate process accounting */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */
@@ -626,6 +637,8 @@ extern int task_prio(task_t *p);
 extern int task_nice(task_t *p);
 extern int task_curr(task_t *p);
 extern int idle_cpu(int cpu);
+extern task_t *cpu_idle_ptr(int cpu);
+extern task_t *cpu_curr_ptr(int cpu);
 
 int set_user(uid_t new_ruid, int dumpclear);
 
diff -urNp linux-7090/init/main.c linux-7100/init/main.c
--- linux-7090/init/main.c
+++ linux-7100/init/main.c
@@ -29,6 +29,7 @@
 #include <linux/file.h>
 #include <linux/tty.h>
 #include <linux/profile.h>
+#include <linux/process_timing.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -557,6 +558,11 @@ static int init(void * unused)
 	lock_kernel();
 
 	smp_init();
+	/* We want the process accouting init to happen after the smp init
+	 * so all cpu idle tasks already exist, but before the migration
+	 * init so we don't have to hunt down the migration tasks in case
+	 * the ALL_PROCESS flag was passed to the accounting setup function */
+	process_timing_init();
 #if CONFIG_SMP
 	migration_init();
 #endif
diff -urNp linux-7090/kernel/acct.c linux-7100/kernel/acct.c
--- linux-7090/kernel/acct.c
+++ linux-7100/kernel/acct.c
@@ -298,8 +298,8 @@ static void do_acct_process(long exitcod
 
 	ac.ac_btime = CT_TO_SECS(current->start_time) + (xtime.tv_sec - (jiffies / HZ));
 	ac.ac_etime = encode_comp_t(jiffies - current->start_time);
-	ac.ac_utime = encode_comp_t(current->times.tms_utime);
-	ac.ac_stime = encode_comp_t(current->times.tms_stime);
+	ac.ac_utime = encode_comp_t(kernel_timeval_to_clock_t(&current->utime));
+	ac.ac_stime = encode_comp_t(kernel_timeval_to_clock_t(&current->stime));
 	ac.ac_uid = fs_high2lowuid(current->uid);
 	ac.ac_gid = fs_high2lowgid(current->gid);
 	ac.ac_uid32 = current->uid;
diff -urNp linux-7090/kernel/exit.c linux-7100/kernel/exit.c
--- linux-7090/kernel/exit.c
+++ linux-7100/kernel/exit.c
@@ -19,7 +19,7 @@
 #include <linux/binfmts.h>
 #include <linux/ptrace.h>
 #include <linux/mount.h>
-
+#include <linux/process_timing.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <linux/profile.h>
@@ -28,6 +28,9 @@
 extern void sem_exit (void);
 extern struct task_struct *child_reaper;
 
+
+
+
 int getrusage(struct task_struct *, int, struct rusage *);
 
 static void __unhash_process(struct task_struct *p)
@@ -48,6 +51,7 @@ void release_task(struct task_struct * p
 {
 	int zap_leader;
 	task_t *leader;
+	struct kernel_timeval sum;
  
 	BUG_ON(p->state < TASK_ZOMBIE);
  
@@ -82,11 +86,12 @@ void release_task(struct task_struct * p
 			zap_leader = 1;
 	}
 
-	p->parent->times.tms_cutime += p->times.tms_utime + p->times.tms_cutime;
-	p->parent->times.tms_cstime += p->times.tms_stime + p->times.tms_cstime;
-	p->parent->group_leader->group_times.tms_cutime += p->times.tms_utime + p->times.tms_cutime;
-	p->parent->group_leader->group_times.tms_cstime += p->times.tms_stime + p->times.tms_cstime;
-
+	kernel_timeval_add(&sum, &p->utime, &p->cutime);
+	kernel_timeval_addto(&p->parent->cutime, &sum);
+	kernel_timeval_addto(&p->parent->group_leader->group_cutime, &sum);
+	kernel_timeval_add(&sum, &p->stime, &p->cstime);
+	kernel_timeval_addto(&p->parent->cstime, &sum);
+	kernel_timeval_addto(&p->parent->group_leader->group_cstime, &sum);
 	p->parent->cmin_flt += p->min_flt + p->cmin_flt;
 	p->parent->cmaj_flt += p->maj_flt + p->cmaj_flt;
 	p->parent->cnswap += p->nswap + p->cnswap;
diff -urNp linux-7090/kernel/fork.c linux-7100/kernel/fork.c
--- linux-7090/kernel/fork.c
+++ linux-7100/kernel/fork.c
@@ -28,6 +28,7 @@
 #include <linux/fs.h>
 #include <linux/futex.h>
 #include <linux/ptrace.h>
+#include <linux/process_timing.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -841,19 +842,23 @@ struct task_struct *copy_process(unsigne
 
 	p->leader = 0;		/* session leadership doesn't inherit */
 	p->tty_old_pgrp = 0;
-	p->times.tms_utime = p->times.tms_stime = 0;
-	p->times.tms_cutime = p->times.tms_cstime = 0;
-	p->group_times.tms_utime = p->group_times.tms_stime = 0;
-	p->group_times.tms_cutime = p->group_times.tms_cstime = 0;
-#ifdef CONFIG_SMP
-	{
-		int i;
+	memset(&p->utime, 0, sizeof(p->utime));
+	memset(&p->stime, 0, sizeof(p->stime));
+	memset(&p->cutime, 0, sizeof(p->cutime));
+	memset(&p->cstime, 0, sizeof(p->cstime));
+	memset(&p->group_utime, 0, sizeof(p->group_utime));
+	memset(&p->group_stime, 0, sizeof(p->group_stime));
+	memset(&p->group_cutime, 0, sizeof(p->group_cutime));
+	memset(&p->group_cstime, 0, sizeof(p->group_cstime));
 
-		/* ?? should we just memset this ?? */
-		for(i = 0; i < NR_CPUS; i++)
-			p->per_cpu_utime[i] = p->per_cpu_stime[i] = 0;
-	}
+#ifdef CONFIG_SMP
+	memset(&p->per_cpu_utime, 0, sizeof(p->per_cpu_utime));
+	memset(&p->per_cpu_stime, 0, sizeof(p->per_cpu_stime));
 #endif
+
+	memset(&p->timing_state, 0, sizeof(p->timing_state));
+	p->timing_state.type = PROCESS_TIMING_USER;
+	p->last_sigxcpu = 0;
 	p->array = NULL;
 	p->lock_depth = -1;		/* -1 = no lock */
 	p->start_time = jiffies;
@@ -919,7 +924,7 @@ struct task_struct *copy_process(unsigne
 		 * runqueue lock is not a problem.
 		 */
 		current->time_slice = 1;
-		scheduler_tick(0,0);
+		scheduler_tick(0 /* don't update the time stats */);
 	}
 	local_irq_enable();
 
diff -urNp linux-7090/kernel/sched.c linux-7100/kernel/sched.c
--- linux-7090/kernel/sched.c
+++ linux-7100/kernel/sched.c
@@ -26,11 +26,13 @@
 #include <linux/interrupt.h>
 #include <linux/completion.h>
 #include <linux/kernel_stat.h>
+#include <linux/process_timing.h>
 #include <linux/notifier.h>
 #include <linux/blkdev.h>
 #include <linux/delay.h>
 #include <linux/timer.h>
 
+
 #define cpu_to_node_mask(cpu) (cpu_online_map)
 
 /*
@@ -208,8 +210,8 @@ static struct runqueue runqueues[NR_CPUS
 
 #define cpu_rq(cpu)		(runqueues + (rq_idx(cpu)))
 #define cpu_int(c)		((cpu_rq(c))->cpu + cpu_idx(c))
-#define cpu_curr_ptr(cpu)	(cpu_int(cpu)->curr)
-#define cpu_idle_ptr(cpu)	(cpu_int(cpu)->idle)
+#define CPU_CURR_PTR(cpu)	(cpu_int(cpu)->curr)
+#define CPU_IDLE_PTR(cpu)	(cpu_int(cpu)->idle)
 
 #define this_rq()		cpu_rq(smp_processor_id())
 #define task_rq(p)		cpu_rq(task_cpu(p))
@@ -230,7 +232,7 @@ static struct runqueue runqueues[NR_CPUS
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(rq, next)	do { } while(0)
 # define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
-# define task_running(p)		(cpu_curr_ptr(task_cpu(p)) == (p))
+# define task_running(p)		(CPU_CURR_PTR(task_cpu(p)) == (p))
 #endif
 
 # define nr_running_init(rq)   do { } while (0)
@@ -419,7 +421,7 @@ static inline void resched_task(task_t *
 
 static inline void resched_cpu(int cpu)
 {
-	resched_task(cpu_curr_ptr(cpu));
+	resched_task(CPU_CURR_PTR(cpu));
 }
 
 #ifdef CONFIG_SMP
@@ -482,11 +484,11 @@ static void wake_up_cpu(runqueue_t *rq, 
 			return resched_cpu(curr_cpu->cpu);
 	}
 
-	if (p->prio < cpu_curr_ptr(cpu)->prio)
-		return resched_task(cpu_curr_ptr(cpu));
-	if (p->prio == cpu_curr_ptr(cpu)->prio &&
-			p->time_slice > cpu_curr_ptr(cpu)->time_slice)
-		return resched_task(cpu_curr_ptr(cpu));
+	if (p->prio < CPU_CURR_PTR(cpu)->prio)
+		return resched_task(CPU_CURR_PTR(cpu));
+	if (p->prio == CPU_CURR_PTR(cpu)->prio &&
+			p->time_slice > CPU_CURR_PTR(cpu)->time_slice)
+		return resched_task(CPU_CURR_PTR(cpu));
 
 	for_each_sibling(idx, rq) {
 		curr_cpu = rq->cpu + idx;
@@ -1208,6 +1210,31 @@ static void rebalance_tick(runqueue_t *t
 }
 #endif
 
+/*
+ * Update the per cpu kstat structs based upon ticks expired.
+ */
+void update_kstatpercpu(task_t *p, struct kernel_stat_tick_times *time)
+{
+	int cpu = smp_processor_id();
+	runqueue_t *rq = this_rq();
+
+	kstat_percpu[cpu].accumulated_time.irq_usec += time->irq_usec;
+	kstat_percpu[cpu].accumulated_time.softirq_usec += time->softirq_usec;
+	if (p == CPU_IDLE_PTR(cpu)) {
+		if (atomic_read(&rq->nr_iowait) > 0)
+			kstat_percpu[cpu].accumulated_time.iowait_usec +=
+				time->u_usec + time->n_usec + 
+				time->s_usec;
+		else
+			kstat_percpu[cpu].accumulated_time.idle_usec +=
+				time->u_usec + time->n_usec + 
+				time->s_usec;
+		return;
+	}
+	kstat_percpu[cpu].accumulated_time.u_usec += time->u_usec;
+	kstat_percpu[cpu].accumulated_time.n_usec += time->n_usec;
+	kstat_percpu[cpu].accumulated_time.s_usec += time->s_usec;
+}
 
 /*
  * We place interactive tasks back into the active array, if possible.
@@ -1230,28 +1257,38 @@ static void rebalance_tick(runqueue_t *t
  * It also gets called by the fork code, when changing the parent's
  * timeslices.
  */
-void scheduler_tick(int user_ticks, int sys_ticks)
+void scheduler_tick(int timer_tick)
 {
 	int cpu = smp_processor_id();
 	runqueue_t *rq = this_rq();
 	unsigned long j = jiffies;
 	task_t *p = current;
 
-	if (p == cpu_idle_ptr(cpu)) {
-		if (local_bh_count(cpu) || local_irq_count(cpu) > 1)
-			kstat_percpu[cpu].system += sys_ticks;
-		else if (atomic_read(&rq->nr_iowait) > 0)
-			kstat_percpu[cpu].iowait += sys_ticks;
-		else
-			kstat_percpu[cpu].idle += sys_ticks;
-		rebalance_tick(rq, cpu, 1);
-		return;
+	if (timer_tick) {
+		kernel_timeval_add_usec(&kstat_percpu[cpu].user,
+				  kstat_percpu[cpu].accumulated_time.u_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].nice,
+				  kstat_percpu[cpu].accumulated_time.n_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].system,
+				  kstat_percpu[cpu].accumulated_time.s_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].irq,
+				  kstat_percpu[cpu].accumulated_time.irq_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].softirq,
+				  kstat_percpu[cpu].accumulated_time.softirq_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].iowait,
+				  kstat_percpu[cpu].accumulated_time.iowait_usec);
+		kernel_timeval_add_usec(&kstat_percpu[cpu].idle,
+				  kstat_percpu[cpu].accumulated_time.idle_usec);
+		kstat_percpu[cpu].accumulated_time.u_usec = 0;
+		kstat_percpu[cpu].accumulated_time.n_usec = 0;
+		kstat_percpu[cpu].accumulated_time.s_usec = 0;
+		kstat_percpu[cpu].accumulated_time.irq_usec = 0;
+		kstat_percpu[cpu].accumulated_time.softirq_usec = 0;
+		kstat_percpu[cpu].accumulated_time.iowait_usec = 0;
+		kstat_percpu[cpu].accumulated_time.idle_usec = 0;
+		if (p == CPU_IDLE_PTR(cpu))
+			rebalance_tick(rq, cpu, 1);
 	}
-	if (TASK_NICE(p) > 0)
-		kstat_percpu[cpu].nice += user_ticks;
-	else
-		kstat_percpu[cpu].user += user_ticks;
-	kstat_percpu[cpu].system += sys_ticks;
 
 	/* Task might have expired already, but not scheduled off yet */
 	spin_lock(&rq->lock);
@@ -1350,7 +1387,7 @@ pick_next_task:
 		if (rq->nr_running)
 			goto pick_next_task;
 pick_idle:
-		next = cpu_idle_ptr(this_cpu);
+		next = CPU_IDLE_PTR(this_cpu);
 		rq->expired_timestamp = 0;
 		goto switch_tasks;
 	}
@@ -1406,7 +1443,9 @@ switch_tasks:
 
 	if (likely(prev != next)) {
 		rq->nr_switches++;
-		cpu_curr_ptr(this_cpu) = next;
+		if (process_timing.task_switch)
+			process_timing.task_switch(prev, next);
+		CPU_CURR_PTR(this_cpu) = next;
 		set_task_cpu(next, this_cpu);
 
 		prepare_arch_switch(rq, next);
@@ -1670,7 +1709,7 @@ void set_user_nice(task_t *p, long nice)
 		 * or increased its priority then reschedule its CPU:
 		 */
 		if ((NICE_TO_PRIO(nice) < p->static_prio) || task_running(p))
-			resched_task(cpu_curr_ptr(task_cpu(p)));
+			resched_task(CPU_CURR_PTR(task_cpu(p)));
 	}
 out_unlock:
 	task_rq_unlock(rq, &flags);
@@ -1743,7 +1782,7 @@ int task_nice(task_t *p)
  */
 int task_curr(task_t *p)
 {
-	return cpu_curr_ptr(task_cpu(p)) == p;
+	return CPU_CURR_PTR(task_cpu(p)) == p;
 }
 
 /**
@@ -1752,7 +1791,25 @@ int task_curr(task_t *p)
  */
 int idle_cpu(int cpu)
 {
-	return cpu_curr_ptr(cpu) == cpu_idle_ptr(cpu);
+	return CPU_CURR_PTR(cpu) == CPU_IDLE_PTR(cpu);
+}
+
+/**
+ * cpu_idle_ptr - return the idle task ptr for this cpu
+ * @cpu: the processor in question
+ */
+task_t *cpu_idle_ptr(int cpu)
+{
+	return CPU_IDLE_PTR(cpu);
+}
+
+/**
+ * cpu_curr_ptr - return the current task ptr for this cpu
+ * @cpu: the processor in question
+ */
+task_t *cpu_curr_ptr(int cpu)
+{
+	return CPU_CURR_PTR(cpu);
 }
 
 /**
@@ -2302,7 +2359,7 @@ void __init init_idle(task_t *idle, int 
 	local_irq_save(flags);
 	double_rq_lock(idle_rq, rq);
 
-	cpu_curr_ptr(cpu) = cpu_idle_ptr(cpu) = idle;
+	CPU_CURR_PTR(cpu) = CPU_IDLE_PTR(cpu) = idle;
 	deactivate_task(idle, rq);
 	idle->array = NULL;
 	idle->prio = MAX_PRIO;
@@ -2560,8 +2617,8 @@ void __init sched_init(void)
 	 * We have to do a little magic to get the first
 	 * thread right in SMP mode.
 	 */
-	cpu_curr_ptr(smp_processor_id()) = current;
-	cpu_idle_ptr(smp_processor_id()) = current;
+	CPU_CURR_PTR(smp_processor_id()) = current;
+	CPU_IDLE_PTR(smp_processor_id()) = current;
 
 	set_task_cpu(current, smp_processor_id());
 	wake_up_forked_process(current);
diff -urNp linux-7090/kernel/signal.c linux-7100/kernel/signal.c
--- linux-7090/kernel/signal.c
+++ linux-7100/kernel/signal.c
@@ -1228,8 +1228,8 @@ void do_notify_parent(struct task_struct
 	info.si_uid = tsk->uid;
 
 	/* FIXME: find out whether or not this is supposed to be c*time. */
-	info.si_utime = tsk->times.tms_utime;
-	info.si_stime = tsk->times.tms_stime;
+	info.si_utime = kernel_timeval_to_clock_t(&tsk->utime);
+	info.si_stime = kernel_timeval_to_clock_t(&tsk->stime);
 
 	status = tsk->exit_code & 0x7f;
 	why = SI_KERNEL;	/* shouldn't happen */
@@ -1317,8 +1317,8 @@ do_notify_parent_cldstop(struct task_str
 	info.si_uid = tsk->uid;
 
 	/* FIXME: find out whether or not this is supposed to be c*time. */
-	info.si_utime = tsk->times.tms_utime;
-	info.si_stime = tsk->times.tms_stime;
+	info.si_utime = kernel_timeval_to_clock_t(&tsk->utime);
+	info.si_stime = kernel_timeval_to_clock_t(&tsk->stime);
 
 	info.si_status = tsk->exit_code & 0x7f;
 	info.si_code = CLD_STOPPED;
diff -urNp linux-7090/kernel/sys.c linux-7100/kernel/sys.c
--- linux-7090/kernel/sys.c
+++ linux-7100/kernel/sys.c
@@ -18,6 +18,7 @@
 #include <linux/dcookies.h>
 #include <linux/fs.h>
 #include <linux/times.h>
+#include <linux/process_timing.h>
 
 #include <asm/uaccess.h>
 #include <asm/io.h>
@@ -871,15 +872,21 @@ asmlinkage long sys_setfsgid(gid_t gid)
 
 asmlinkage long sys_times(struct tms * tbuf)
 {
+	struct tms times;
 	/*
 	 *	In the SMP world we might just be unlucky and have one of
 	 *	the times increment as we use it. Since the value is an
 	 *	atomically safe type this is just fine. Conceptually its
 	 *	as if the syscall took an instant longer to occur.
 	 */
-	if (tbuf)
-		if (copy_to_user(tbuf, &current->times, sizeof(struct tms)))
+	if (tbuf) {
+		times.tms_utime = kernel_timeval_to_clock_t(&current->utime);
+		times.tms_stime = kernel_timeval_to_clock_t(&current->stime);
+		times.tms_cutime = kernel_timeval_to_clock_t(&current->cutime);
+		times.tms_cstime = kernel_timeval_to_clock_t(&current->cstime);
+		if (copy_to_user(tbuf, &times, sizeof(struct tms)))
 			return -EFAULT;
+	}
 	return jiffies;
 }
 
@@ -1262,32 +1269,34 @@ asmlinkage long sys_setrlimit(unsigned i
 int getrusage(struct task_struct *p, int who, struct rusage *ru)
 {
 	struct rusage r;
-
+	struct kernel_timeval sum;
 	memset((char *) &r, 0, sizeof(r));
 	switch (who) {
 		case RUSAGE_SELF:
-			r.ru_utime.tv_sec = CT_TO_SECS(p->times.tms_utime);
-			r.ru_utime.tv_usec = CT_TO_USECS(p->times.tms_utime);
-			r.ru_stime.tv_sec = CT_TO_SECS(p->times.tms_stime);
-			r.ru_stime.tv_usec = CT_TO_USECS(p->times.tms_stime);
+			r.ru_utime.tv_sec = p->utime.tv_sec;
+			r.ru_utime.tv_usec = p->utime.tv_usec;
+			r.ru_stime.tv_sec = p->stime.tv_sec;
+			r.ru_stime.tv_usec = p->stime.tv_usec;
 			r.ru_minflt = p->min_flt;
 			r.ru_majflt = p->maj_flt;
 			r.ru_nswap = p->nswap;
 			break;
 		case RUSAGE_CHILDREN:
-			r.ru_utime.tv_sec = CT_TO_SECS(p->times.tms_cutime);
-			r.ru_utime.tv_usec = CT_TO_USECS(p->times.tms_cutime);
-			r.ru_stime.tv_sec = CT_TO_SECS(p->times.tms_cstime);
-			r.ru_stime.tv_usec = CT_TO_USECS(p->times.tms_cstime);
+			r.ru_utime.tv_sec = p->cutime.tv_sec;
+			r.ru_utime.tv_usec = p->cutime.tv_usec;
+			r.ru_stime.tv_sec = p->cstime.tv_sec;
+			r.ru_stime.tv_usec = p->cstime.tv_usec;
 			r.ru_minflt = p->cmin_flt;
 			r.ru_majflt = p->cmaj_flt;
 			r.ru_nswap = p->cnswap;
 			break;
 		default:
-			r.ru_utime.tv_sec = CT_TO_SECS(p->times.tms_utime + p->times.tms_cutime);
-			r.ru_utime.tv_usec = CT_TO_USECS(p->times.tms_utime + p->times.tms_cutime);
-			r.ru_stime.tv_sec = CT_TO_SECS(p->times.tms_stime + p->times.tms_cstime);
-			r.ru_stime.tv_usec = CT_TO_USECS(p->times.tms_stime + p->times.tms_cstime);
+			kernel_timeval_add(&sum, &p->utime, &p->cutime);
+			r.ru_utime.tv_sec = sum.tv_sec;
+			r.ru_utime.tv_usec = sum.tv_usec;
+			kernel_timeval_add(&sum, &p->stime, &p->cstime);
+			r.ru_stime.tv_sec = sum.tv_sec;
+			r.ru_stime.tv_usec = sum.tv_usec;
 			r.ru_minflt = p->min_flt + p->cmin_flt;
 			r.ru_majflt = p->maj_flt + p->cmaj_flt;
 			r.ru_nswap = p->nswap + p->cnswap;
@@ -1357,6 +1366,25 @@ asmlinkage long sys_prctl(int option, un
 		case PR_GET_FPEXC:
 			error = GET_FPEXC_CTL(current, arg2);
 			break;
+		case PR_GET_TIMING:
+			error = (current->flags & PF_TIMING) ? 
+				PR_TIMING_TIMESTAMP : 
+				PR_TIMING_STATISTICAL;
+			break;
+		case PR_SET_TIMING:
+			if (arg2 == PR_TIMING_TIMESTAMP) {
+				if (process_timing.flags & PT_FLAGS_PROCESS)
+					current->flags |= PF_TIMING;
+				else
+					error = -EINVAL;
+			} else {
+				if (process_timing.flags & PT_FLAGS_ALL_PROCESS)
+					error = -EINVAL;
+				else
+					current->flags &= ~PF_TIMING;
+			}
+			break;
+			
 
 
 		case PR_GET_KEEPCAPS:
diff -urNp linux-7090/kernel/timer.c linux-7100/kernel/timer.c
--- linux-7090/kernel/timer.c
+++ linux-7100/kernel/timer.c
@@ -21,6 +21,8 @@
 #include <linux/config.h>
 #include <linux/mm.h>
 #include <linux/timex.h>
+#include <linux/process_timing.h>
+#include <linux/linkage.h>
 #include <linux/delay.h>
 #include <linux/smp_lock.h>
 #include <linux/interrupt.h>
@@ -43,6 +45,9 @@ struct timeval xtime __cacheline_aligned
 /* Don't completely fail for HZ > 500.  */
 int tickadj = 500/HZ ? : 1;		/* microsecs */
 
+/* The function pointers in case we are using the accurate process accounting */
+struct process_timing_entry process_timing __cacheline_aligned;
+
 DECLARE_TASK_QUEUE(tq_timer);
 DECLARE_TASK_QUEUE(tq_immediate);
 
@@ -117,6 +122,35 @@ static inline void set_running_timer(tve
 static tvec_base_t tvec_bases[NR_CPUS] =
 	{ [ 0 ... NR_CPUS-1] = { .lock = SPIN_LOCK_UNLOCKED} };
 
+/*
+ * Either enable or disable the accurate process timing code.
+ * The real work for the accounting code is handled almost entirely
+ * in the arch/ subdirectory since making this go fast is *very*
+ * hardware dependant.
+ */
+int process_timing_setup_flags;
+
+static int __init  setup_process_timing(char *str) {
+	while(str != NULL) {
+		if (strncmp(str, "irq", 3) == 0)
+			process_timing_setup_flags |= PT_FLAGS_IRQ;
+		if (strncmp(str, "softirq", 7) == 0)
+			process_timing_setup_flags |= PT_FLAGS_SOFTIRQ;
+		if (strncmp(str, "process", 7) == 0)
+			process_timing_setup_flags |= PT_FLAGS_PROCESS;
+		if (strncmp(str, "all_processes", 13) == 0)
+			process_timing_setup_flags |= PT_FLAGS_PROCESS |
+				PT_FLAGS_ALL_PROCESS;
+		if (strncmp(str, "everything", 10) == 0)
+			process_timing_setup_flags = PT_FLAGS_EVERYTHING;
+		str = strchr(str, ',');
+		if (str != NULL)
+			str += strspn(str, ", \t");
+	}
+	return 1;
+}
+__setup("process_timing=", setup_process_timing);
+
 static inline void check_kernel_timer(struct timer_list *timer)
 {
 	tvec_base_t *base = timer->base;
@@ -678,20 +712,24 @@ static void update_wall_time(unsigned lo
 }
 
 static inline void do_process_times(struct task_struct *p,
-	unsigned long user, unsigned long system)
+				    struct kernel_stat_tick_times *time)
 {
-	unsigned long psecs;
+	struct kernel_timeval psecs;
+
+	kernel_timeval_add_usec(&p->utime, time->u_usec + time->n_usec);
+	kernel_timeval_add_usec(&p->group_leader->group_utime, time->u_usec +
+							       time->n_usec);
+	kernel_timeval_add_usec(&p->stime, time->s_usec);
+	kernel_timeval_add_usec(&p->group_leader->group_stime, time->s_usec);
+	kernel_timeval_add(&psecs, &p->utime, &p->stime);
 
-	psecs = (p->times.tms_utime += user);
-	p->group_leader->group_times.tms_utime += user;
-	psecs += (p->times.tms_stime += system);
-	p->group_leader->group_times.tms_stime += system;
-	if (psecs / HZ > p->rlim[RLIMIT_CPU].rlim_cur) {
+	if (psecs.tv_sec > p->rlim[RLIMIT_CPU].rlim_cur &&
+	    psecs.tv_sec != p->last_sigxcpu) {
 		/* Send SIGXCPU every second.. */
-		if (!(psecs % HZ))
-			send_sig(SIGXCPU, p, 1);
+		send_sig(SIGXCPU, p, 1);
+		p->last_sigxcpu = psecs.tv_sec;
 		/* and SIGKILL when we go over max.. */
-		if (psecs / HZ > p->rlim[RLIMIT_CPU].rlim_max)
+		if (psecs.tv_sec > p->rlim[RLIMIT_CPU].rlim_max)
 			send_sig(SIGKILL, p, 1);
 	}
 }
@@ -723,27 +761,167 @@ static inline void do_it_prof(struct tas
 	}
 }
 
-void update_one_process(struct task_struct *p, unsigned long user,
-			unsigned long system, int cpu)
+
+void update_one_process(struct task_struct *p,
+			struct kernel_stat_tick_times *time, int cpu)
 {
-	p->per_cpu_utime[cpu] += user;
-	p->per_cpu_stime[cpu] += system;
-	do_process_times(p, user, system);
-	do_it_virt(p, user);
-	do_it_prof(p);
+	kernel_timeval_add_usec(&p->per_cpu_utime[cpu], time->u_usec + 
+							time->n_usec);
+	kernel_timeval_add_usec(&p->per_cpu_stime[cpu], time->s_usec);
+	do_process_times(p, time);
 }	
 
+void update_process_time_intertick(struct task_struct *p,
+				   struct kernel_stat_tick_times *time)
+{
+	int u_ticks;
+	int s_ticks;
+	int cpu = smp_processor_id();
+
+	u_ticks = kernel_timeval_to_jiffies(&p->utime);
+	s_ticks = kernel_timeval_to_jiffies(&p->stime);
+	update_one_process(p, time, cpu);
+	u_ticks = kernel_timeval_to_jiffies(&p->utime) - u_ticks;
+	s_ticks = kernel_timeval_to_jiffies(&p->stime) - s_ticks;
+
+	if (u_ticks) {
+		do_it_virt(p, u_ticks);
+		do_it_prof(p);
+	}
+
+	if (s_ticks)
+		do_it_prof(p);
+
+	update_kstatpercpu(p, time);
+}
+
 /*
  * Called from the timer interrupt handler to charge one tick to the current 
- * process.  user_tick is 1 if the tick is user time, 0 for system.
+ * process.  user_tick is 1 if the tick is user time, 0 for system.  This
+ * variant is called when we are mixing both statistical, tick based accounting
+ * with more accurate accounting methods, it keeps our numbers from getting
+ * off.
  */
-void update_process_times(int user_tick)
+static void update_process_times_mixed(int user_mode)
+{
+	struct task_struct *p, *unaccounted_task;
+	int cpu = smp_processor_id();
+	static unsigned long remainder = 0;
+	struct kernel_stat_tick_times time;
+	long unaccounted_usecs, accounted_usec_sum;
+
+	memset(&time, 0, sizeof(time));
+	if(process_timing.tick)
+		process_timing.tick();
+	accounted_usec_sum = kstat_percpu[cpu].accumulated_time.u_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.n_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.s_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.irq_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.softirq_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.iowait_usec;
+	accounted_usec_sum += kstat_percpu[cpu].accumulated_time.idle_usec;
+	unaccounted_usecs = (1000000/HZ);
+	remainder += (1000000%HZ);
+	while (remainder >= 1000000) {
+		unaccounted_usecs++;
+		remainder -= 1000000;
+	}
+	unaccounted_usecs -= accounted_usec_sum;
+	if (unaccounted_usecs < 0)
+		unaccounted_usecs = 0;
+
+	if (unaccounted_usecs) {
+		p = current;
+		memset(&time, 0, sizeof(time));
+		/* If this is an accounted task, add the extra to the
+		 * last unaccounted task instead of blowing this tasks
+		 * accounting.
+		 */
+		if (p->flags & PF_TIMING) {
+			/*
+			 * We move the idle ptr back to the unaccounted task
+			 * each time we use it, the context switch code is
+			 * responsible for setting it back to a real process
+			 * whenever it is run, but this makes sure that if
+			 * we have only a few unaccounted processes on the
+			 * system that we don't end up putting all unaccounted
+			 * time on those tasks even if they aren't being run
+			 * currently.
+			 */
+			unaccounted_task = kstat_percpu[cpu].unaccounted_task;
+			kstat_percpu[cpu].unaccounted_task = cpu_idle_ptr(cpu);
+		} else
+			unaccounted_task = p;
+
+		if (user_mode) {
+			if (task_nice(unaccounted_task) > 0)
+				time.n_usec = unaccounted_usecs;
+			else
+				time.u_usec = unaccounted_usecs;
+		} else {
+			if (!(process_timing.flags & PT_FLAGS_IRQ) &&
+			    local_irq_count(cpu) > 1)
+				time.irq_usec = unaccounted_usecs;
+			else if (!(process_timing.flags & PT_FLAGS_SOFTIRQ) &&
+				 local_bh_count(cpu))
+				time.softirq_usec = unaccounted_usecs;
+			else
+				time.s_usec = unaccounted_usecs;
+		}
+		update_process_time_intertick(unaccounted_task, &time);
+	}
+}
+
+/*
+ * Update process times based upon the traditional timer tick statistical
+ * method.  No accounting is currently enabled, so no magic need be done to
+ * keep accounted time and statistical time in sync.
+ */
+static void update_process_times_statistical(int user_mode)
 {
 	struct task_struct *p = current;
-	int cpu = smp_processor_id(), system = user_tick ^ 1;
+	struct kernel_stat_tick_times time;
+	unsigned long usecs;
+	static unsigned long remainder=0;
+	int cpu = smp_processor_id();
+
+	memset(&time, 0, sizeof(time));
+	usecs = (1000000/HZ);
+	remainder += (1000000%HZ);
+	while (remainder >= 1000000) {
+		usecs++;
+		remainder -= 1000000;
+	}
+	if (user_mode) {
+		if (task_nice(p) > 0)
+			time.n_usec = usecs;
+		else
+			time.u_usec = usecs;
+	} else {
+		if (local_irq_count(cpu) > 1)
+			time.irq_usec = usecs;
+		else if (local_bh_count(cpu))
+			time.softirq_usec = usecs;
+		else
+			time.s_usec = usecs;
+	}
+	update_process_time_intertick(p, &time);
+}
 
-	update_one_process(p, user_tick, system, cpu);
-	scheduler_tick(user_tick, system);
+/*
+ * Figure out which method of accounting to use and call the right function.
+ */
+void update_process_times(int user_mode)
+{
+	if (process_timing.flags == 0)
+		/* No process accounting is enabled, use straight statistical
+		 * accounting */
+		update_process_times_statistical(user_mode);
+	else if (process_timing.flags == PT_FLAGS_EVERYTHING)
+		process_timing.tick();
+	else
+		update_process_times_mixed(user_mode);
+	scheduler_tick(1 /* update time stats */);
 }
 
 /*
@@ -1040,6 +1218,8 @@ asmlinkage long sys_nanosleep(struct tim
 	return 0;
 }
 
+
+
 static inline void update_times(void)
 {
 	unsigned long ticks;
@@ -1099,3 +1279,4 @@ void __init init_timervecs(void)
 		base->timer_jiffies = jiffies;
 	}
 }
+
diff -urNp linux-7090/mm/oom_kill.c linux-7100/mm/oom_kill.c
--- linux-7090/mm/oom_kill.c
+++ linux-7100/mm/oom_kill.c
@@ -74,7 +74,7 @@ static int badness(struct task_struct *p
 	 * very well in practice. This is not safe against jiffie wraps
 	 * but we don't care _that_ much...
 	 */
-	cpu_time = (p->times.tms_utime + p->times.tms_stime) >> (SHIFT_HZ + 3);
+	cpu_time = p->utime.tv_sec + p->stime.tv_sec;
 	run_time = (jiffies - p->start_time) >> (SHIFT_HZ + 10);
 
 	points /= int_sqrt(cpu_time);
