diff -urNp linux-1110/arch/i386/kernel/process.c linux-1120/arch/i386/kernel/process.c
--- linux-1110/arch/i386/kernel/process.c
+++ linux-1120/arch/i386/kernel/process.c
@@ -1044,6 +1044,90 @@ unsigned long arch_align_stack(unsigned 
 # error SHLIB_BASE must be under 16MB!
 #endif
 
+static unsigned long
+arch_get_unmapped_nonexecutable_area(struct mm_struct *mm, unsigned long addr, unsigned long len)
+{
+	struct vm_area_struct *vma, *prev_vma;
+	unsigned long stack_limit, mmap_top, reserved;
+	int first_time = 1;
+
+	/* requested length too big for entire address space */
+	if (len > TASK_SIZE) 
+		return -ENOMEM;
+
+	/* don't allow allocations above current stack limit */
+	mmap_top = (arch_align_stack(mm->start_stack) & PAGE_MASK);
+	reserved = current->rlim[RLIMIT_STACK].rlim_cur;
+	if (mmap_top == 0UL || mmap_top > TASK_SIZE)
+		mmap_top = TASK_SIZE;
+	if (reserved < mmap_top - STACK_BUFFER_SPACE)
+		reserved += STACK_BUFFER_SPACE;
+	else
+		reserved = mmap_top;
+
+	/* now convert "reserved" to be distance down from TASK_SIZE */
+	reserved += (TASK_SIZE - mmap_top);
+	if (reserved > current->rlim[RLIMIT_STACK].rlim_max)
+		reserved = current->rlim[RLIMIT_STACK].rlim_max;
+	if (reserved < TASK_SIZE - STACK_BUFFER_SPACE - mm->brk)
+		stack_limit = TASK_SIZE - PAGE_ALIGN(reserved);
+	else
+		stack_limit = 0UL;
+	if (mm->non_executable_cache > stack_limit)
+		mm->non_executable_cache = stack_limit;
+
+	/* requesting a specific address */
+        if (addr) {
+                addr = PAGE_ALIGN(addr);
+                vma = find_vma(mm, addr);
+		if (TASK_SIZE - len >= addr && 
+		    (!vma || addr + len <= vma->vm_start))
+			return addr;
+        }
+
+	/* make sure it can fit in the remaining address space */
+	if (len > mm->non_executable_cache) {
+		if (len > stack_limit)
+			return -ENOMEM;
+		first_time = 0;
+		mm->non_executable_cache = stack_limit;
+	}
+
+	/* either no address requested or cant fit in requested address hole */
+try_again:
+        addr = (mm->non_executable_cache - len)&PAGE_MASK;
+	do {
+       	 	if (!(vma = find_vma_prev(mm, addr, &prev_vma)))
+                        return -ENOMEM;
+
+		/* Do not allocate non-MAP_FIXED all the way down to zero. */
+		if (addr < SHLIB_BASE)
+			goto fail;
+
+		/* new region fits between prev_vma->vm_end and vma->vm_start, use it */
+		if (addr+len <= vma->vm_start && (!prev_vma || (addr >= prev_vma->vm_end))) {
+			/* remember the address as a hint for next time */
+			mm->non_executable_cache = addr;
+			return addr;
+
+		/* pull non_executable_cache down to the first hole */
+		} else if (mm->non_executable_cache == vma->vm_end)
+				mm->non_executable_cache = vma->vm_start;	
+
+		/* try just below the current vma->vm_start */
+		addr = vma->vm_start-len;
+        } while (len <= vma->vm_start);
+
+fail:
+	/* if hint left us with no space for the requested mapping try again */
+	if (first_time && len <= stack_limit) {
+		first_time = 0;
+		mm->non_executable_cache = stack_limit;
+		goto try_again;
+	}
+	return -ENOMEM;
+}
+
 static unsigned long randomize_range(unsigned long start, unsigned long end, unsigned long len)
 {
 	unsigned long range = end - len - start;
@@ -1081,12 +1165,21 @@ unsigned long arch_get_unmapped_area(str
 		    (!vma || addr + len <= vma->vm_start))
 			return addr;
 	}
+
 	if (prot & PROT_EXEC) {
 		ascii_shield = 1;
 		addr = SHLIB_BASE;
-	} else
-search_upper:
+	} else {
+		/* this can fail if the stack was unlimited */
+search_all:
+		if ((tmp = arch_get_unmapped_nonexecutable_area(mm, addr, len)) != -ENOMEM)
+			return tmp;
+
+		/* the following is like arch_align_stack(), but upwards */
 		addr = TASK_UNMAPPED_BASE;
+		if (current->flags & PF_RELOCEXEC)
+			addr += (((get_random_int() % 1024) << 4) & PAGE_MASK);
+	}
 
 	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
 		/* At this point:  (!vma || addr < vma->vm_end). */
@@ -1099,7 +1192,7 @@ search_upper:
 			 */
 			if (ascii_shield && (addr + len > mm->brk)) {
 				ascii_shield = 0;
-				goto search_upper;
+				goto search_all;
 			}
 			/*
 			 * Up until the brk area we randomize addresses
diff -urNp linux-1110/fs/binfmt_aout.c linux-1120/fs/binfmt_aout.c
--- linux-1110/fs/binfmt_aout.c
+++ linux-1120/fs/binfmt_aout.c
@@ -306,7 +306,8 @@ static int load_aout_binary(struct linux
 	current->mm->brk = ex.a_bss +
 		(current->mm->start_brk = N_BSSADDR(ex));
 	current->mm->free_area_cache = TASK_UNMAPPED_BASE;
-
+	/* unlimited stack is larger than TASK_SIZE */
+	current->mm->non_executable_cache = NON_EXECUTABLE_CACHE(current);
 	current->mm->rss = 0;
 	current->mm->mmap = NULL;
 	compute_creds(bprm);
diff -urNp linux-1110/fs/binfmt_elf.c linux-1120/fs/binfmt_elf.c
--- linux-1110/fs/binfmt_elf.c
+++ linux-1120/fs/binfmt_elf.c
@@ -722,7 +722,8 @@ static int load_elf_binary(struct linux_
 	   change some of these later */
 	current->mm->rss = 0;
 	current->mm->free_area_cache = TASK_UNMAPPED_BASE;
-
+	/* unlimited stack is larger than TASK_SIZE */
+	current->mm->non_executable_cache = NON_EXECUTABLE_CACHE(current);
 	retval = setup_arg_pages(bprm, exec_stack);
 	current->mm->start_stack = bprm->p;
 
diff -urNp linux-1110/include/linux/sched.h linux-1120/include/linux/sched.h
--- linux-1110/include/linux/sched.h
+++ linux-1120/include/linux/sched.h
@@ -275,6 +275,7 @@ struct mm_struct {
 	rb_root_t mm_rb;
 	struct vm_area_struct * mmap_cache;	/* last find_vma result */
 	unsigned long free_area_cache;		/* first hole */
+	unsigned long non_executable_cache;	/* last hole top */
 	pgd_t * pgd;
 	atomic_t mm_users;			/* How many users with user space? */
 	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
@@ -322,6 +323,13 @@ extern int mmlist_nr;
 	rlimit_rss:	RLIM_INFINITY,			\
 }
 
+#define NON_EXECUTABLE_CACHE(task)     (((task)->rlim[RLIMIT_STACK].rlim_cur>TASK_SIZE?0: \
+                                       TASK_SIZE-(task)->rlim[RLIMIT_STACK].rlim_cur)&PAGE_MASK)
+
+#ifndef STACK_BUFFER_SPACE
+#define STACK_BUFFER_SPACE	((unsigned long)(128 * 1024 * 1024))
+#endif
+
 extern void show_stack(unsigned long *esp);
 
 extern int __broadcast_thread_group(struct task_struct *p, int sig);
diff -urNp linux-1110/kernel/fork.c linux-1120/kernel/fork.c
--- linux-1110/kernel/fork.c
+++ linux-1120/kernel/fork.c
@@ -195,6 +195,8 @@ static inline int dup_mmap(struct mm_str
 	mm->mmap_cache = NULL;
 	mm->map_count = 0;
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
+	/* unlimited stack is larger than TASK_SIZE */
+	mm->non_executable_cache = NON_EXECUTABLE_CACHE(current);
 	mm->rss = 0;
 	mm->cpu_vm_mask = 0;
 	pprev = &mm->mmap;
@@ -288,6 +290,8 @@ static struct mm_struct * mm_init(struct
 	mm->core_waiters = 0;
 	mm->page_table_lock = SPIN_LOCK_UNLOCKED;
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
+	/* unlimited stack is larger than TASK_SIZE */
+	mm->non_executable_cache = NON_EXECUTABLE_CACHE(current);
 	mm->pgd = pgd_alloc(mm);
 	mm->def_flags = 0;
 	if (current->mm)
diff -urNp linux-1110/mm/mmap.c linux-1120/mm/mmap.c
--- linux-1110/mm/mmap.c
+++ linux-1120/mm/mmap.c
@@ -995,6 +995,11 @@ static struct vm_area_struct * unmap_fix
 	if (area->vm_start >= TASK_UNMAPPED_BASE &&
 			area->vm_start < area->vm_mm->free_area_cache)
 		area->vm_mm->free_area_cache = area->vm_start;
+	/*
+	 * Is this a new hole at the highest possible address?
+	 */
+	if (addr+len > area->vm_mm->non_executable_cache)
+		area->vm_mm->non_executable_cache = addr+len;
 
 	/* Unmapping the whole area. */
 	if (addr == area->vm_start && end == area->vm_end) {
