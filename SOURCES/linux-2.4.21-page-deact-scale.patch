diff -urNp linux-1224/include/linux/sysctl.h linux-1225/include/linux/sysctl.h
--- linux-1224/include/linux/sysctl.h
+++ linux-1225/include/linux/sysctl.h
@@ -152,6 +152,9 @@ enum
 	VM_OVERCOMMIT_RATIO=16,	/* percent of RAM to allow overcommit in */
 	VM_PAGEBUF=22,		/* struct: Control pagebuf parameters */
 	VM_HUGETLB_POOL=23,	/* int: size of the hugetlb pool, in MB */
+	VM_INACTIVE_CLEAN_PERCENT=25, /* int: percent of inactive thats clean */
+	VM_SKIP_MAPPED_PAGES=27,/* int: don't reclaim pages w/active mappings */
+	VM_KSCAND_WORK_PERCENT=29, /* int: % of work on each kscand iteration */
 };
 
 
diff -urNp linux-1224/kernel/sysctl.c linux-1225/kernel/sysctl.c
--- linux-1224/kernel/sysctl.c
+++ linux-1225/kernel/sysctl.c
@@ -327,6 +327,10 @@ static ctl_table kern_table[] = {
 	{0}
 };
 
+extern int inactive_clean_percent;
+extern int skip_mapped_pages;
+extern int kscand_work_percent;
+
 static ctl_table vm_table[] = {
 	{VM_BDFLUSH, "bdflush", &bdf_prm, 9*sizeof(int), 0644, NULL,
 	 &proc_dointvec_minmax, &sysctl_intvec, NULL,
@@ -353,6 +357,14 @@ static ctl_table vm_table[] = {
 	{VM_HUGETLB_POOL, "hugetlb_pool", &htlbpool_max, sizeof(int), 0644,
 			NULL, &hugetlb_sysctl_handler},
 #endif
+	{VM_INACTIVE_CLEAN_PERCENT, "inactive_clean_percent",
+		&inactive_clean_percent, sizeof(inactive_clean_percent),
+		0644, NULL, &proc_dointvec},
+	{VM_SKIP_MAPPED_PAGES, "skip_mapped_pages",
+		&skip_mapped_pages, sizeof(skip_mapped_pages),
+		0644, NULL, &proc_dointvec},
+	{VM_KSCAND_WORK_PERCENT, "kscand_work_percent", &kscand_work_percent,
+		sizeof(kscand_work_percent), 0644, NULL, &proc_dointvec},
 	{0}
 };
 
diff -urNp linux-1224/mm/vmscan.c linux-1225/mm/vmscan.c
--- linux-1224/mm/vmscan.c
+++ linux-1225/mm/vmscan.c
@@ -31,6 +31,8 @@
 
 static void refill_freelist(void);
 static void wakeup_memwaiters(void);
+
+int kscand_work_percent = 100;
 /*
  * The "priority" of VM scanning is how much of the queues we
  * will scan in one go. A value of 6 for DEF_PRIORITY implies
@@ -185,17 +187,24 @@ found_page:
 	return page;
 }
 
+int inactive_clean_percent = 30;
+
 /**
  * need_rebalance_dirty - do we need to write inactive stuff to disk?
  * @zone: the zone in question
  *
- * Returns true if the zone in question has an inbalance between inactive
- * dirty on one side and inactive laundry + inactive clean on the other
- * Right now set the balance at 50%; may need tuning later on
+ * Returns true if the zone in question has too few inactive laundry
+ * and inactive clean pages. 
+ *
  */
 static inline int need_rebalance_dirty(zone_t * zone)
 {
-	if (zone->inactive_dirty_pages > zone->inactive_laundry_pages + zone->inactive_clean_pages)
+	if ((zone->inactive_dirty_pages*inactive_clean_percent/100) > 
+		(zone->inactive_laundry_pages + zone->inactive_clean_pages))
+		return 1;
+
+	if (zone->inactive_laundry_pages / 2 + zone->inactive_clean_pages +
+			zone->free_pages < zone->pages_high)
 		return 1;
 
 	return 0;
@@ -205,16 +214,35 @@ static inline int need_rebalance_dirty(z
  * need_rebalance_laundry - does the zone have too few inactive_clean pages?
  * @zone: the zone in question
  *
- * Returns true if the zone in question has too few pages in inactive clean
- * + free
+ * Returns true if the zone in question has too few pages in inactive clean pages.
+ * 
  */
 static inline int need_rebalance_laundry(zone_t * zone)
 {
-	if (free_high(zone) >= 0)
+	if (zone->inactive_laundry_pages > zone->inactive_clean_pages)
 		return 1;
+
 	return 0;
 }
 
+/*
+ * returns the active cache ratio relative to the total active list
+ * times 100 (eg. 30% cache returns 30)
+ */
+static inline int cache_ratio(struct zone_struct * zone)
+{
+        if (!zone->size)
+                return 0;
+        return 100 * zone->active_cache_pages / (zone->active_cache_pages +
+                        zone->active_anon_pages + 1);
+}
+
+struct cache_limits cache_limits = {
+        .min = 1,
+        .borrow = 15,
+        .max = 30,
+};
+
 static int slab_usable_pages(zone_t * inzone)
 {
 	pg_data_t *pgdat;
@@ -278,6 +306,17 @@ int launder_page(zone_t * zone, int gfp_
 
 	BUG_ON(!PageInactiveDirty(page));
 	del_page_from_inactive_dirty_list(page);
+
+	/* if pagecache is over max, don't reclaim anonymous pages */	
+	if (cache_ratio(zone) > cache_limits.max && page_anon(page) &&
+			free_min(zone) < 0) {
+		add_page_to_active_list(page, INITIAL_AGE);
+		lru_unlock(zone);
+		page_cache_release(page);
+		lru_lock(zone);
+		return 0;
+	}
+
 	add_page_to_inactive_laundry_list(page);
 	/* store the time we start IO */
 	page->age = (jiffies/HZ)&255;
@@ -527,24 +566,15 @@ static inline void kachunk_cache(struct 
 
 #define BATCH_WORK_AMOUNT	64
 
-/*
- * returns the active cache ratio relative to the total active list
- * times 100 (eg. 30% cache returns 30)
- */
-static inline int cache_ratio(struct zone_struct * zone)
+int skip_mapped_pages = 0;
+
+static inline int check_mapping_inuse(struct page *page)
 {
-	if (!zone->size)
-		return 0;
-	return 100 * zone->active_cache_pages / (zone->active_cache_pages +
-			zone->active_anon_pages + 1);
+	if (!skip_mapped_pages)
+		return 1;
+	return page_mapping_inuse(page);
 }
 
-struct cache_limits cache_limits = {
-	.min = 1,
-	.borrow = 15,
-	.max = 100,
-};
-
 /**
  * refill_inactive_zone - scan the active list and find pages to deactivate
  * @priority: how much are we allowed to scan
@@ -624,7 +654,9 @@ int refill_inactive_zone(struct zone_str
 		 * Do aging on the pages.
 		 */
 		pte_chain_lock(page);
-		if (page_referenced(page, &over_rsslimit) && !over_rsslimit) {
+		if (page_referenced(page, &over_rsslimit)
+				&& !over_rsslimit
+				&& check_mapping_inuse(page)) {
 			pte_chain_unlock(page);
 			age_page_up_nolock(page, 0);
 			UnlockPage(page);
@@ -666,7 +698,9 @@ int refill_inactive_zone(struct zone_str
 		 * Do aging on the pages.
 		 */
 		pte_chain_lock(page);
-		if (page_referenced(page, &over_rsslimit) && !over_rsslimit) {
+		if (page_referenced(page, &over_rsslimit)
+				&& !over_rsslimit
+				&& check_mapping_inuse(page)) {
 			pte_chain_unlock(page);
 			age_page_up_nolock(page, 0);
 			UnlockPage(page);
@@ -715,19 +749,26 @@ static int need_active_cache_scan(struct
 }
 
 static int scan_active_list(struct zone_struct * zone, int age,
-		struct list_head * list)
+		struct list_head * list, int count)
 {
 	struct list_head *page_lru , *next;
 	struct page * page;
 	int over_rsslimit;
 
+	count = count * kscand_work_percent / 100;
 	/* Take the lock while messing with the list... */
 	lru_lock(zone);
-	list_for_each_safe(page_lru, next, list) {
-		page = list_entry(page_lru, struct page, lru);
+	while (count-- > 0 && !list_empty(list)) {
+		page = list_entry(list->prev, struct page, lru);
 		pte_chain_lock(page);
-		if (page_referenced(page, &over_rsslimit) && !over_rsslimit)
+		if (page_referenced(page, &over_rsslimit)
+				&& !over_rsslimit
+				&& check_mapping_inuse(page))
 			age_page_up_nolock(page, age);
+		else {
+			list_del(&page->lru);
+			list_add(&page->lru, list);
+		}
 		pte_chain_unlock(page);
 	}
 	lru_unlock(zone);
@@ -956,7 +997,7 @@ static inline void background_aging(int 
 		if (inactive_low(zone) > 0)
 			refill_inactive_zone(zone, priority, BATCH_WORK_AMOUNT);
 	for_each_zone(zone)
-		if (free_plenty(zone) > 0)
+		if (need_rebalance_dirty(zone))
 			rebalance_dirty_zone(zone, BATCH_WORK_AMOUNT, GFP_KSWAPD);
 }
 
@@ -1037,9 +1078,9 @@ static int do_try_to_free_pages_kswapd(u
 
 		if (need_rebalance_dirty(zone))
 			rebalance_dirty_zone(zone, 4 * worktodo,  gfp_mask);
-	}
 
-	rebalance_inactive(20);
+		rebalance_inactive_zone(zone, max(worktodo, 4*BATCH_WORK_AMOUNT), 20);
+	}
 
 	for_each_pgdat(pgdat) {
 		zone_t *zone = pgdat->node_zones;
@@ -1322,7 +1363,8 @@ int kscand(void *unused)
 			if (need_active_anon_scan(zone)) {
 				for (age = MAX_AGE-1; age >= 0; age--)  {
 					scan_active_list(zone, age,
-						&zone->active_anon_list[age]);
+						&zone->active_anon_list[age],
+						zone->active_anon_count[age]);
 					if (current->need_resched)
 						schedule();
 				}
@@ -1331,7 +1373,8 @@ int kscand(void *unused)
 			if (need_active_cache_scan(zone)) {
 				for (age = MAX_AGE-1; age >= 0; age--)  {
 					scan_active_list(zone, age,
-						&zone->active_cache_list[age]);
+						&zone->active_cache_list[age],
+						zone->active_cache_count[age]);
 					if (current->need_resched)
 						schedule();
 				}
