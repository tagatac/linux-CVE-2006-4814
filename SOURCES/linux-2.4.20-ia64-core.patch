diff -urNp linux-260/Documentation/Configure.help linux-262/Documentation/Configure.help
--- linux-260/Documentation/Configure.help
+++ linux-262/Documentation/Configure.help
@@ -26384,6 +26384,14 @@ CONFIG_IA64_MCA
   Say Y here to enable machine check support for IA-64.  If you're
   unsure, answer Y.
 
+Use PAL_HALT_LIGHT in idle loop
+CONFIG_IA64_PAL_IDLE
+  Say Y here to enable use of PAL_HALT_LIGHT in the cpu_idle loop.
+  This allows the CPU to enter a low power state when idle.  You
+  can enable CONFIG_IA64_PALINFO and check /proc/pal/cpu0/power_info
+  to see the power consumption and latency for this state.  If you're
+  unsure your firmware supports it, answer N.
+
 Disable IA-64 Virtual Hash Page Table
 CONFIG_DISABLE_VHPT
   The Virtual Hash Page Table (VHPT) enhances virtual address
diff -urNp linux-260/arch/ia64/Makefile linux-262/arch/ia64/Makefile
--- linux-260/arch/ia64/Makefile
+++ linux-262/arch/ia64/Makefile
@@ -110,8 +110,9 @@ FORCE: ;
 
 compressed: vmlinux
 	$(OBJCOPY) --strip-all vmlinux vmlinux-tmp
-	gzip vmlinux-tmp
-	mv vmlinux-tmp.gz vmlinux.gz
+	gzip -cfv vmlinux-tmp > arch/$(ARCH)/boot/vmlinuz
+	ln -sf vmlinuz arch/ia64/boot/kernel_image
+	rm -f vmlinux-tmp
 
 rawboot:
 	@$(MAKEBOOT) rawboot
diff -urNp linux-260/arch/ia64/boot/Makefile linux-262/arch/ia64/boot/Makefile
--- linux-260/arch/ia64/boot/Makefile
+++ linux-262/arch/ia64/boot/Makefile
@@ -18,7 +18,6 @@ LINKFLAGS = -static -T bootloader.lds
 OBJECTS	= bootloader.o
 
 targets-$(CONFIG_IA64_HP_SIM) += bootloader
-targets-$(CONFIG_IA64_GENERIC) += bootloader
 
 all:	$(targets-y)
 
diff -urNp linux-260/arch/ia64/config.in linux-262/arch/ia64/config.in
--- linux-260/arch/ia64/config.in
+++ linux-262/arch/ia64/config.in
@@ -18,6 +18,8 @@ mainmenu_option next_comment
 comment 'General setup'
 
 define_bool CONFIG_IA64 y
+define_bool CONFIG_HIGHPTE n
+define_bool CONFIG_HIGHMEM n
 
 define_bool CONFIG_ISA n
 define_bool CONFIG_EISA n
@@ -86,6 +88,7 @@ fi
 
 define_bool CONFIG_KCORE_ELF y	# On IA-64, we always want an ELF /proc/kcore.
 
+bool 'Use PAL_HALT_LIGHT in idle loop' CONFIG_IA64_PAL_IDLE
 bool 'SMP support' CONFIG_SMP
 tristate 'Support running of Linux/x86 binaries' CONFIG_IA32_SUPPORT
 bool 'Performance monitor support' CONFIG_PERFMON
diff -urNp linux-260/arch/ia64/hp/common/sba_iommu.c linux-262/arch/ia64/hp/common/sba_iommu.c
--- linux-260/arch/ia64/hp/common/sba_iommu.c
+++ linux-262/arch/ia64/hp/common/sba_iommu.c
@@ -35,6 +35,7 @@
 #include <asm/delay.h>		/* ia64_get_itc() */
 #include <asm/io.h>
 #include <asm/page.h>		/* PAGE_OFFSET */
+#include <asm/system.h>		/* wmb() */
 
 
 #define PFX "IOC: "
@@ -130,6 +131,7 @@
 
 #define ZX1_IOC_ID	((PCI_DEVICE_ID_HP_ZX1_IOC << 16) | PCI_VENDOR_ID_HP)
 #define REO_IOC_ID	((PCI_DEVICE_ID_HP_REO_IOC << 16) | PCI_VENDOR_ID_HP)
+#define SX1000_IOC_ID	((PCI_DEVICE_ID_HP_SX1000_IOC << 16) | PCI_VENDOR_ID_HP)
 
 #define ZX1_IOC_OFFSET	0x1000	/* ACPI reports SBA, we want IOC */
 
@@ -425,6 +427,10 @@ sba_search_bitmap(struct ioc *ioc, unsig
 
 	ASSERT(((unsigned long) ioc->res_hint & (sizeof(unsigned long) - 1UL)) == 0);
 	ASSERT(res_ptr < res_end);
+
+	/* Do power of 2 allocations to avoid AR2305 */
+	if (ioc->func_id != ZX1_IOC_ID)
+		bits_wanted = 1 << get_order(bits_wanted << PAGE_SHIFT);
 	if (bits_wanted > (BITS_PER_LONG/2)) {
 		/* Search word at a time - no mask needed */
 		for(; res_ptr < res_end; ++res_ptr) {
@@ -569,6 +575,9 @@ sba_free_range(struct ioc *ioc, dma_addr
 	unsigned long *res_ptr = (unsigned long *) &((ioc)->res_map[ridx & ~RESMAP_IDX_MASK]);
 
 	int bits_not_wanted = size >> IOVP_SHIFT;
+	/* Do power of 2 allocations to avoid AR2305 */
+	if (ioc->func_id != ZX1_IOC_ID)
+		bits_not_wanted = 1 << get_order(bits_not_wanted << PAGE_SHIFT);
 
 	/* 3-bits "bit" address plus 2 (or 3) bits for "byte" == bit in word */
 	unsigned long m = RESMAP_MASK(bits_not_wanted) << (pide & (BITS_PER_LONG - 1));
@@ -578,7 +587,10 @@ sba_free_range(struct ioc *ioc, dma_addr
 		bits_not_wanted, m, pide, res_ptr, *res_ptr);
 
 #ifdef CONFIG_PROC_FS
-	ioc->used_pages -= bits_not_wanted;
+	if (ioc->func_id != ZX1_IOC_ID)
+		ioc->used_pages -= (size >> IOVP_SHIFT);
+	else
+		ioc->used_pages -= bits_not_wanted;
 #endif
 
 	ASSERT(m != 0);
@@ -816,6 +828,9 @@ sba_map_single(struct pci_dev *dev, void
 		size -= IOVP_SIZE;
 		pdir_start++;
 	}
+	/* force pdir update */
+	wmb();
+
 	/* form complete address */
 #ifdef ASSERT_PDIR_SANITY
 	sba_check_pdir(ioc,"Check after sba_map_single()");
@@ -957,7 +972,7 @@ sba_alloc_consistent(struct pci_dev *hwd
 		return 0;
 	}
 
-        ret = (void *) __get_free_pages(GFP_ATOMIC, get_order(size));
+	ret = (void *) __get_free_pages(GFP_ATOMIC|GFP_DMA, get_order(size));
 
 	if (ret) {
 		memset(ret, 0, size);
@@ -1078,6 +1093,9 @@ sba_fill_pdir(
 		}
 		startsg++;
 	}
+	/* force pdir update */
+	wmb();
+
 #ifdef DEBUG_LARGE_SG_ENTRIES
 	dump_run_sg = 0;
 #endif
@@ -1635,6 +1653,7 @@ struct ioc_iommu {
 static struct ioc_iommu ioc_iommu_info[] __initdata = {
 	{ ZX1_IOC_ID, "zx1", ioc_zx1_init },
 	{ REO_IOC_ID, "REO" },
+	{ SX1000_IOC_ID, "sx1000" },
 };
 
 static struct ioc * __init
@@ -1700,112 +1719,6 @@ ioc_init(u64 hpa, void *handle)
 **
 **************************************************************************/
 
-#ifdef CONFIG_PROC_FS
-static int
-sba_proc_info_one(char *buf, struct ioc *ioc)
-{
-	int total_pages = (int) (ioc->res_size << 3); /* 8 bits per byte */
-	unsigned long i = 0, avg = 0, min, max;
-
-	sprintf(buf, "Hewlett Packard %s IOC rev %d.%d\n",
-		ioc->name, ((ioc->rev >> 4) & 0xF), (ioc->rev & 0xF));
-	sprintf(buf, "%sIO PDIR size    : %d bytes (%d entries)\n",
-		buf,
-		(int) ((ioc->res_size << 3) * sizeof(u64)), /* 8 bits/byte */
-		total_pages);
-
-	sprintf(buf, "%sIO PDIR entries : %ld free  %ld used (%d%%)\n", buf,
-		total_pages - ioc->used_pages, ioc->used_pages,
-		(int) (ioc->used_pages * 100 / total_pages));
-	
-	sprintf(buf, "%sResource bitmap : %d bytes (%d pages)\n", 
-		buf, ioc->res_size, ioc->res_size << 3);   /* 8 bits per byte */
-
-	min = max = ioc->avg_search[0];
-	for (i = 0; i < SBA_SEARCH_SAMPLE; i++) {
-		avg += ioc->avg_search[i];
-		if (ioc->avg_search[i] > max) max = ioc->avg_search[i];
-		if (ioc->avg_search[i] < min) min = ioc->avg_search[i];
-	}
-	avg /= SBA_SEARCH_SAMPLE;
-	sprintf(buf, "%s  Bitmap search : %ld/%ld/%ld (min/avg/max CPU Cycles)\n",
-		buf, min, avg, max);
-
-	sprintf(buf, "%spci_map_single(): %12ld calls  %12ld pages (avg %d/1000)\n",
-		buf, ioc->msingle_calls, ioc->msingle_pages,
-		(int) ((ioc->msingle_pages * 1000)/ioc->msingle_calls));
-#ifdef ALLOW_IOV_BYPASS
-	sprintf(buf, "%spci_map_single(): %12ld bypasses\n",
-	        buf, ioc->msingle_bypass);
-#endif
-
-	sprintf(buf, "%spci_unmap_single: %12ld calls  %12ld pages (avg %d/1000)\n",
-		buf, ioc->usingle_calls, ioc->usingle_pages,
-		(int) ((ioc->usingle_pages * 1000)/ioc->usingle_calls));
-#ifdef ALLOW_IOV_BYPASS
-	sprintf(buf, "%spci_unmap_single: %12ld bypasses\n",
-	        buf, ioc->usingle_bypass);
-#endif
-
-	sprintf(buf, "%spci_map_sg()    : %12ld calls  %12ld pages (avg %d/1000)\n",
-		buf, ioc->msg_calls, ioc->msg_pages,
-		(int) ((ioc->msg_pages * 1000)/ioc->msg_calls));
-#ifdef ALLOW_IOV_BYPASS
-	sprintf(buf, "%spci_map_sg()    : %12ld bypasses\n",
-	        buf, ioc->msg_bypass);
-#endif
-
-	sprintf(buf, "%spci_unmap_sg()  : %12ld calls  %12ld pages (avg %d/1000)\n",
-		buf, ioc->usg_calls, ioc->usg_pages,
-		(int) ((ioc->usg_pages * 1000)/ioc->usg_calls));
-
-	return strlen(buf);
-}
-
-static int
-sba_proc_info(char *buf, char **start, off_t offset, int len)
-{
-	struct ioc *ioc;
-	char *base = buf;
-
-	for (ioc = ioc_list; ioc; ioc = ioc->next) {
-		buf += sba_proc_info_one(buf, ioc);
-	}
-
-	return strlen(base);
-}
-
-static int
-sba_resource_map_one(char *buf, struct ioc *ioc)
-{
-	unsigned int *res_ptr = (unsigned int *)ioc->res_map;
-	int i;
-
-	buf[0] = '\0';
-	for(i = 0; i < (ioc->res_size / sizeof(unsigned int)); ++i, ++res_ptr) {
-		if ((i & 7) == 0)
-		    strcat(buf,"\n   ");
-		sprintf(buf, "%s %08x", buf, *res_ptr);
-	}
-	strcat(buf, "\n");
-
-	return strlen(buf);
-}
-
-static int
-sba_resource_map(char *buf, char **start, off_t offset, int len)
-{
-	struct ioc *ioc;
-	char *base = buf;
-
-	for (ioc = ioc_list; ioc; ioc = ioc->next) {
-		buf += sba_resource_map_one(buf, ioc);
-	}
-
-	return strlen(base);
-}
-#endif
-
 void
 sba_enable_device(struct pci_dev *dev)
 {
@@ -1897,19 +1810,18 @@ ioc_acpi_init(void)
 	acpi_bus_register_driver(&acpi_ioc_driver);
 }
 
+#ifdef CONFIG_PROC_FS
+static void __init ioc_proc_init(void);
+#endif
+
 void __init
 sba_init(void)
 {
 	ioc_acpi_init();
 
 #ifdef CONFIG_PROC_FS
-	if (ioc_list) {
-		struct proc_dir_entry * proc_mckinley_root;
-
-		proc_mckinley_root = proc_mkdir("bus/mckinley",0);
-		create_proc_info_entry(ioc_list->name, 0, proc_mckinley_root, sba_proc_info);
-		create_proc_info_entry("bitmap", 0, proc_mckinley_root, sba_resource_map);
-	}
+	if (ioc_list)
+		ioc_proc_init();
 #endif
 }
 
@@ -1930,3 +1842,112 @@ EXPORT_SYMBOL(sba_unmap_sg);
 EXPORT_SYMBOL(sba_dma_supported);
 EXPORT_SYMBOL(sba_alloc_consistent);
 EXPORT_SYMBOL(sba_free_consistent);
+
+#ifdef CONFIG_PROC_FS
+
+/*
+ * The following includes must occur *after* the EXPORT_SYMBOL(sba_*)
+ * invocations above to prevent an obscure KMI compatibility problem!
+ */
+#include <linux/seq_file.h>
+#include <asm/bitops.h>
+
+static void *
+ioc_start(struct seq_file *s, loff_t *pos)
+{
+	struct ioc *ioc;
+	loff_t n = *pos;
+
+	for (ioc = ioc_list; ioc; ioc = ioc->next)
+		if (!n--)
+			return ioc;
+
+	return NULL;
+}
+
+static void *
+ioc_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct ioc *ioc = v;
+
+	++*pos;
+	return ioc->next;
+}
+
+static void
+ioc_stop(struct seq_file *s, void *v)
+{
+}
+
+static int
+ioc_show(struct seq_file *s, void *v)
+{
+	struct ioc *ioc = v;
+	unsigned long *res_ptr = (unsigned long *)ioc->res_map;
+	int i, used = 0;
+
+	seq_printf(s, "Hewlett Packard %s IOC rev %d.%d\n",
+		ioc->name, ((ioc->rev >> 4) & 0xF), (ioc->rev & 0xF));
+	seq_printf(s, "IOVA size       : %d MB\n", ioc->iov_size/(1024*1024));
+	seq_printf(s, "IOVA page size  : %ld kb\n", PAGE_SIZE/1024);
+
+	for (i = 0; i < (ioc->res_size / sizeof(unsigned long)); ++i, ++res_ptr)
+		used += hweight64(*res_ptr);
+
+	seq_printf(s, "PDIR size       : %d entries\n", ioc->res_size << 3);
+	seq_printf(s, "PDIR used       : %d entries\n", used);
+
+	{
+	  	unsigned long i = 0, avg = 0, min, max;
+		min = max = ioc->avg_search[0];
+		for (i = 0; i < SBA_SEARCH_SAMPLE; i++) {
+			avg += ioc->avg_search[i];
+			if (ioc->avg_search[i] > max) max = ioc->avg_search[i];
+			if (ioc->avg_search[i] < min) min = ioc->avg_search[i];
+		}
+		avg /= SBA_SEARCH_SAMPLE;
+		seq_printf(s, "Bitmap search   : %ld/%ld/%ld (min/avg/max CPU Cycles)\n",
+			   min, avg, max);
+	}
+
+#ifndef ALLOW_IOV_BYPASS
+	 seq_printf(s, "IOVA bypass disabled\n");
+#endif
+	return 0;
+}
+
+static struct seq_operations ioc_seq_ops = {
+	.start = ioc_start,
+	.next  = ioc_next,
+	.stop  = ioc_stop,
+	.show  = ioc_show
+};
+
+static int
+ioc_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &ioc_seq_ops);
+}
+
+static struct file_operations ioc_fops = {
+	.open    = ioc_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release
+};
+
+static void __init
+ioc_proc_init(void)
+{
+	struct proc_dir_entry *dir, *entry;
+
+	dir = proc_mkdir("bus/mckinley", 0);
+	if (!dir)
+		return;
+
+	entry = create_proc_entry(ioc_list->name, 0, dir);
+	if (entry)
+		entry->proc_fops = &ioc_fops;
+}
+
+#endif /* CONFIG_PROC_FS */
diff -urNp linux-260/arch/ia64/hp/zx1/hpzx1_misc.c linux-262/arch/ia64/hp/zx1/hpzx1_misc.c
--- linux-260/arch/ia64/hp/zx1/hpzx1_misc.c
+++ linux-262/arch/ia64/hp/zx1/hpzx1_misc.c
@@ -50,7 +50,16 @@ static int hp_cfg_read##sz (struct pci_d
 		fake_dev->sizing = 0; \
 		return PCIBIOS_SUCCESSFUL; \
 	} \
-	*value = read##sz(fake_dev->mapped_csrs + where); \
+	switch (where & ~0x7) { \
+		case 0x48: \
+			/* reading this register can initiate config cycles */ \
+		case 0x78: \
+			/* reading this register can put elroys in susepnd mode */ \
+			*value = 0; \
+			break; \
+		default: \
+			*value = read##sz(fake_dev->mapped_csrs + where); \
+	} \
 	if (where == PCI_COMMAND) \
 		*value |= PCI_COMMAND_MEMORY; /* SBA omits this */ \
 	return PCIBIOS_SUCCESSFUL; \
@@ -68,8 +77,16 @@ static int hp_cfg_write##sz (struct pci_
 		if (value == (u##bits) ~0) \
 			fake_dev->sizing = 1; \
 		return PCIBIOS_SUCCESSFUL; \
-	} else \
-		write##sz(value, fake_dev->mapped_csrs + where); \
+	} \
+	switch (where & ~0x7) { \
+		case 0x48: \
+			/* writing this register can initiate config cycles */ \
+		case 0x78: \
+			/* writing this register plays with elroy suspend mode */ \
+			break; \
+		default: \
+			write##sz(value, fake_dev->mapped_csrs + where); \
+	} \
 	return PCIBIOS_SUCCESSFUL; \
 }
 
diff -urNp linux-260/arch/ia64/kernel/brl_emu.c linux-262/arch/ia64/kernel/brl_emu.c
--- linux-260/arch/ia64/kernel/brl_emu.c
+++ linux-262/arch/ia64/kernel/brl_emu.c
@@ -59,7 +59,7 @@ ia64_emulate_brl (struct pt_regs *regs, 
 	unsigned long next_ip;
 	struct siginfo siginfo;
 	struct illegal_op_return rv;
-	int tmp_taken, unimplemented_address;
+	long tmp_taken, unimplemented_address;
 
 	rv.fkt = (unsigned long) -1;
 
diff -urNp linux-260/arch/ia64/kernel/efi.c linux-262/arch/ia64/kernel/efi.c
--- linux-260/arch/ia64/kernel/efi.c
+++ linux-262/arch/ia64/kernel/efi.c
@@ -324,7 +324,7 @@ efi_memmap_walk (efi_freemem_callback_t 
 				check_md = q;
 
 				if (check_md->attribute & EFI_MEMORY_WB)
-					trim_bottom(md, granule_addr);
+					trim_bottom(check_md, granule_addr);
 
 				if (check_md->phys_addr < granule_addr)
 					continue;
diff -urNp linux-260/arch/ia64/kernel/efivars.c linux-262/arch/ia64/kernel/efivars.c
--- linux-260/arch/ia64/kernel/efivars.c
+++ linux-262/arch/ia64/kernel/efivars.c
@@ -194,7 +194,6 @@ efivar_create_proc_entry(unsigned long v
 	kfree(short_name); short_name = NULL;
 	if (!new_efivar->entry) return 1;
 
-
 	new_efivar->entry->data = new_efivar;
 	new_efivar->entry->read_proc = efivar_read;
 	new_efivar->entry->write_proc = efivar_write;
@@ -340,7 +339,66 @@ efivar_write(struct file *file, const ch
 	return size;
 }
 
+ 
+ /* Since the EFI SYSTEM TABLE is a structure composed of many pointers to
+    valuable system management tables. Let's expose those pointers in proc
+    so we can follow the pointers to the SAL system table, hcdp, ACPI, SMBIOS,
+    and  DMI tables in /dev/mem
+ */
+
+static struct proc_dir_entry *efi_sys_table_entry;
+
+static ssize_t
+efi_sys_table_read(struct file *file, char *buffer, size_t count, loff_t *ppos)
+{
+       void *data;
+       
+       ssize_t size;
+       int max_nr_entries = 7; 	/* num ptrs to tables we could expose */
+       int max_line_len = 40;
+       char *proc_buffer= kmalloc((max_nr_entries*max_line_len*sizeof(char)), GFP_KERNEL);
+
+
+       ssize_t length;
+
+       if (!efi.systab )
+               return 0;
+
+       length = 0;
+
+       if( efi.mps > 0 )
+	       length += sprintf(proc_buffer, "MPS=0x%lx\n", __pa(efi.mps));
+       if( efi.acpi20 > 0 )
+	       length += sprintf(proc_buffer+length, "ACPI 2.0=0x%lx\n", __pa(efi.acpi20));
+       if( efi.acpi > 0 )
+	       length += sprintf(proc_buffer+length, "ACPI=0x%lx\n", __pa(efi.acpi));
+       if( efi.smbios > 0 )
+	       length += sprintf(proc_buffer+length, "SMBIOS=0x%lx\n", __pa(efi.smbios));
+       if( efi.sal_systab > 0 )
+	       length += sprintf(proc_buffer+length, "SALsystab=0x%lx\n", __pa(efi.sal_systab));
+       if( efi.hcdp > 0 )
+	       length += sprintf(proc_buffer+length, "HCDP=0x%lx\n", __pa(efi.hcdp));
+       if( efi.boot_info > 0 )
+	       length += sprintf(proc_buffer+length, "BootInfo=0x%lx\n", __pa(efi.boot_info));
+
+       if( *ppos >= length )
+	       return 0;
+
+       data = (u8 *) proc_buffer + file->f_pos;
+       size = length - file->f_pos;
+       if (size > count)
+               size = count;
+       if (copy_to_user(buffer, data, size))
+               return -EFAULT;
+
+       kfree(proc_buffer);
+       *ppos += size;
+       return size;
+}
 
+static struct file_operations efi_sys_table_fops = {
+       .read = efi_sys_table_read
+};
 
 static int __init
 efivars_init(void)
@@ -363,6 +421,11 @@ efivars_init(void)
         if (!efi_dir)
                 efi_dir = proc_mkdir("efi", NULL);
 
+	efi_sys_table_entry = create_proc_entry("systab", S_IRUSR | S_IRGRP, efi_dir);
+	if (efi_sys_table_entry)
+		efi_sys_table_entry->proc_fops = &efi_sys_table_fops;
+
+
 	efi_vars_dir = proc_mkdir("vars", efi_dir);
 
 
@@ -410,6 +473,7 @@ efivars_exit(void)
 
 	spin_lock(&efivars_lock);
 
+	remove_proc_entry("systab", efi_dir);
 	list_for_each_safe(pos, n, &efivar_list) {
 		efivar = efivar_entry(pos);
 		remove_proc_entry(efivar->entry->name, efi_vars_dir);
diff -urNp linux-260/arch/ia64/kernel/entry.S linux-262/arch/ia64/kernel/entry.S
--- linux-260/arch/ia64/kernel/entry.S
+++ linux-262/arch/ia64/kernel/entry.S
@@ -46,8 +46,11 @@
 	 * setup a null register window frame.
 	 */
 ENTRY(ia64_execve)
-	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(3)
-	alloc loc1=ar.pfs,3,2,4,0
+	/*
+	 * Allocate 8 input registers since ptrace() may clobber them
+	 */
+	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
+	alloc loc1=ar.pfs,8,2,4,0
 	mov loc0=rp
 	.body
 	mov out0=in0			// filename
@@ -89,8 +92,11 @@ ENTRY(ia64_execve)
 END(ia64_execve)
 
 GLOBAL_ENTRY(sys_clone2)
-	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(2)
-	alloc r16=ar.pfs,3,2,4,0
+	/*
+	 * Allocate 8 input registers since ptrace() may clobber them
+	 */
+	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
+	alloc r16=ar.pfs,8,2,4,0
 	DO_SAVE_SWITCH_STACK
 	mov loc0=rp
 	mov loc1=r16				// save ar.pfs across do_fork
@@ -108,8 +114,11 @@ GLOBAL_ENTRY(sys_clone2)
 END(sys_clone2)
 
 GLOBAL_ENTRY(sys_clone)
-	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(2)
-	alloc r16=ar.pfs,2,2,4,0
+	/*
+	 * Allocate 8 input registers since ptrace() may clobber them
+	 */
+	.prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
+	alloc r16=ar.pfs,8,2,4,0
 	DO_SAVE_SWITCH_STACK
 	mov loc0=rp
 	mov loc1=r16				// save ar.pfs across do_fork
@@ -268,8 +277,12 @@ GLOBAL_ENTRY(save_switch_stack)
 	;;
 	st8 [r14]=r16		// save ar.pfs
 	st8 [r15]=r21		// save ar.lc
+	add r22=IA64_TASK_THREAD_CSD_OFFSET, r13
 	stf.spill [r2]=f16,32
 	stf.spill [r3]=f17,32
+	add r23=IA64_TASK_THREAD_SSD_OFFSET, r13
+	mov r24 = ar.csd
+	mov r25 = ar.ssd
 	;;
 	stf.spill [r2]=f18,32
 	stf.spill [r3]=f19,32
@@ -299,6 +312,8 @@ GLOBAL_ENTRY(save_switch_stack)
 	;;
 	st8 [r2]=r29,16		// save ar.unat
 	st8 [r3]=r19,16		// save ar.rnat
+	st8 [r22] = r24		// save csd
+	st8 [r23] = r25		// save ssd
 	;;
 	st8 [r2]=r20		// save ar.bspstore
 	st8 [r3]=r21		// save predicate registers
@@ -341,12 +356,16 @@ ENTRY(load_switch_stack)
 	;;
 	ld8 r28=[r2]		// restore pr
 	ld8 r30=[r3]		// restore rnat
+	add r2=IA64_TASK_THREAD_CSD_OFFSET, r13
 	;;
 	ld8 r18=[r14],16	// restore caller's unat
 	ld8 r19=[r15],24	// restore fpsr
+	add r3=IA64_TASK_THREAD_SSD_OFFSET, r13
 	;;
 	ldf.fill f2=[r14],32
 	ldf.fill f3=[r15],32
+	ld8 r20=[r2]		// load ar.csd
+	ld8 r31=[r3]		// load ar.ssd
 	;;
 	ldf.fill f4=[r14],32
 	ldf.fill f5=[r15],32
@@ -362,6 +381,8 @@ ENTRY(load_switch_stack)
 	;;
 	ldf.fill f16=[r14],32
 	ldf.fill f17=[r15],32
+	mov ar.csd = r20
+	mov ar.ssd = r31
 	;;
 	ldf.fill f18=[r14],32
 	ldf.fill f19=[r15],32
@@ -454,7 +475,14 @@ END(invoke_syscall_trace)
 
 GLOBAL_ENTRY(ia64_trace_syscall)
 	PT_REGS_UNWIND_INFO(0)
+{	/*
+	 * Some versions of gas generate bad unwind info if the first instruction of a
+	 * procedure doesn't go into the first slot of a bundle.  This is a workaround.
+	 */
+	nop.m 0
+	nop.i 0
 	br.call.sptk.many rp=invoke_syscall_trace // give parent a chance to catch syscall args
+}
 .ret6:	br.call.sptk.many rp=b6			// do the syscall
 strace_check_retval:
 	cmp.lt p6,p0=r8,r0			// syscall failed?
@@ -484,12 +512,19 @@ END(ia64_trace_syscall)
 
 GLOBAL_ENTRY(ia64_ret_from_clone)
 	PT_REGS_UNWIND_INFO(0)
+{	/*
+	 * Some versions of gas generate bad unwind info if the first instruction of a
+	 * procedure doesn't go into the first slot of a bundle.  This is a workaround.
+	 */
+	nop.m 0
+	nop.i 0
 	/*
 	 * We need to call schedule_tail() to complete the scheduling process.
 	 * Called by ia64_switch_to after do_fork()->copy_thread().  r8 contains the
 	 * address of the previously executing task.
 	 */
 	br.call.sptk.many rp=ia64_invoke_schedule_tail
+}
 .ret8:
 	adds r2=IA64_TASK_PTRACE_OFFSET,r13
 	;;
@@ -652,7 +687,7 @@ GLOBAL_ENTRY(ia64_leave_kernel)
 	 * NOTE: alloc, loadrs, and cover can't be predicated.
 	 */
 (pNonSys) br.cond.dpnt dont_preserve_current_frame
-	cover				// add current frame into dirty partition
+	cover				// add current frame into dirty partition and set cr.ifs
 	;;
 	mov r19=ar.bsp			// get new backing store pointer
 	sub r16=r16,r18			// krbs = old bsp - size of dirty partition
@@ -678,24 +713,12 @@ dont_preserve_current_frame:
 #	define Nregs	14
 #endif
 	alloc loc0=ar.pfs,2,Nregs-2,2,0
-	shr.u loc1=r18,9		// RNaTslots <= dirtySize / (64*8) + 1
+	shr.u loc1=r18,9		// RNaTslots <= floor(dirtySize / (64*8))
 	sub r17=r17,r18			// r17 = (physStackedSize + 8) - dirtySize
 	;;
-#if 1
-	.align 32		// see comment below about gas bug...
-#endif
 	mov ar.rsc=r19			// load ar.rsc to be used for "loadrs"
 	shladd in0=loc1,3,r17
 	mov in1=0
-#if 0
-	// gas-2.11.90 is unable to generate a stop bit after .align, which is bad,
-	// because alloc must be at the beginning of an insn-group.
-	.align 32
-#else
-	nop 0
-	nop 0
-	nop 0
-#endif
 	;;
 rse_clear_invalid:
 #ifdef CONFIG_ITANIUM
@@ -737,13 +760,13 @@ rse_clear_invalid:
 	;;
 	mov loc3=0
 	mov loc4=0
-	mov loc9=0
 	mov loc5=0
 	mov loc6=0
+	mov loc7=0
 (pRecurse) br.call.sptk.many b6=rse_clear_invalid
 	;;
-	mov loc7=0
 	mov loc8=0
+	mov loc9=0
 	cmp.ne pReturn,p0=r0,in1	// if recursion count != 0, we need to do a br.ret
 	mov loc10=0
 	mov loc11=0
@@ -848,13 +871,14 @@ ENTRY(handle_signal_delivery)
 	mov r9=ar.unat
 	mov loc0=rp				// save return address
 	mov out0=0				// there is no "oldset"
-	adds out1=0,sp				// out1=&sigscratch
+	adds out1=8,sp				// out1=&sigscratch->ar_pfs
 (pSys)	mov out2=1				// out2==1 => we're in a syscall
 	;;
 (pNonSys) mov out2=0				// out2==0 => not a syscall
 	.fframe 16
 	.spillpsp ar.unat, 16			// (note that offset is relative to psp+0x10!)
 	st8 [sp]=r9,-16				// allocate space for ar.unat and save it
+	st8 [out1]=loc1,-8			// save ar.pfs, out1=&sigscratch
 	.body
 	br.call.sptk.many rp=ia64_do_signal
 .ret15:	.restore sp
@@ -875,11 +899,12 @@ GLOBAL_ENTRY(sys_rt_sigsuspend)
 	mov loc0=rp				// save return address
 	mov out0=in0				// mask
 	mov out1=in1				// sigsetsize
-	adds out2=0,sp				// out2=&sigscratch
+	adds out2=8,sp				// out2=&sigscratch->ar_pfs
 	;;
 	.fframe 16
 	.spillpsp ar.unat, 16			// (note that offset is relative to psp+0x10!)
 	st8 [sp]=r9,-16				// allocate space for ar.unat and save it
+	st8 [out2]=loc1,-8			// save ar.pfs, out2=&sigscratch
 	.body
 	br.call.sptk.many rp=ia64_rt_sigsuspend
 .ret17:	.restore sp
@@ -895,7 +920,10 @@ END(sys_rt_sigsuspend)
 
 ENTRY(sys_rt_sigreturn)
 	PT_REGS_UNWIND_INFO(0)
-	alloc r2=ar.pfs,0,0,1,0
+	/*
+	 * Allocate 8 input registers since ptrace() may clobber them
+	 */
+	alloc r2=ar.pfs,8,0,1,0
 	.prologue
 	PT_REGS_SAVES(16)
 	adds sp=-16,sp
@@ -915,17 +943,16 @@ ENTRY(sys_rt_sigreturn)
 END(sys_rt_sigreturn)
 
 GLOBAL_ENTRY(ia64_prepare_handle_unaligned)
-	//
-	// r16 = fake ar.pfs, we simply need to make sure
-	// privilege is still 0
-	//
-	mov r16=r0
 	.prologue
+	/*
+	 * r16 = fake ar.pfs, we simply need to make sure privilege is still 0
+	 */
+	mov r16=r0
 	DO_SAVE_SWITCH_STACK
-	br.call.sptk.many rp=ia64_handle_unaligned // stack frame setup in ivt
+	br.call.sptk.many rp=ia64_handle_unaligned	// stack frame setup in ivt
 .ret21:	.body
 	DO_LOAD_SWITCH_STACK
-	br.cond.sptk.many rp			  // goes to ia64_leave_kernel
+	br.cond.sptk.many rp				// goes to ia64_leave_kernel
 END(ia64_prepare_handle_unaligned)
 
 	//
diff -urNp linux-260/arch/ia64/kernel/gate.S linux-262/arch/ia64/kernel/gate.S
--- linux-260/arch/ia64/kernel/gate.S
+++ linux-262/arch/ia64/kernel/gate.S
@@ -63,15 +63,19 @@
 	 * call stack.
 	 */
 
+#define SIGTRAMP_SAVES										\
+	.unwabi @svr4, 's';	/* mark this as a sigtramp handler (saves scratch regs) */	\
+	.savesp ar.unat, UNAT_OFF+SIGCONTEXT_OFF;						\
+	.savesp ar.fpsr, FPSR_OFF+SIGCONTEXT_OFF;						\
+	.savesp pr, PR_OFF+SIGCONTEXT_OFF;     							\
+	.savesp rp, RP_OFF+SIGCONTEXT_OFF;							\
+	.savesp ar.pfs, CFM_OFF+SIGCONTEXT_OFF;							\
+	.vframesp SP_OFF+SIGCONTEXT_OFF
+
 GLOBAL_ENTRY(ia64_sigtramp)
 	// describe the state that is active when we get here:
 	.prologue
-	.unwabi @svr4, 's'		// mark this as a sigtramp handler (saves scratch regs)
-	.savesp ar.unat, UNAT_OFF+SIGCONTEXT_OFF
-	.savesp ar.fpsr, FPSR_OFF+SIGCONTEXT_OFF
-	.savesp pr, PR_OFF+SIGCONTEXT_OFF
-	.savesp rp, RP_OFF+SIGCONTEXT_OFF
-	.vframesp SP_OFF+SIGCONTEXT_OFF
+	SIGTRAMP_SAVES
 	.body
 
 	.label_state 1
@@ -84,14 +88,12 @@ GLOBAL_ENTRY(ia64_sigtramp)
 	ld8 r15=[base1]					// get address of new RBS base (or NULL)
 	cover				// push args in interrupted frame onto backing store
 	;;
-	cmp.ne p8,p0=r15,r0		// do we need to switch the rbs?
+	cmp.ne p1,p0=r15,r0		// do we need to switch rbs? (note: pr is saved by kernel)
 	mov.m r9=ar.bsp			// fetch ar.bsp
-	.spillsp.p p8, ar.rnat, RNAT_OFF+SIGCONTEXT_OFF
-(p8)	br.cond.spnt setup_rbs		// yup -> (clobbers r14, r15, and r16)
+	.spillsp.p p1, ar.rnat, RNAT_OFF+SIGCONTEXT_OFF
+(p1)	br.cond.spnt setup_rbs		// yup -> (clobbers p8, r14-r16, and r18-r20)
 back_from_setup_rbs:
-
-	.spillreg ar.pfs, r8
-	alloc r8=ar.pfs,0,0,3,0		// get CFM0, EC0, and CPL0 into r8
+	alloc r8=ar.pfs,0,0,3,0
 	ld8 out0=[base0],16		// load arg0 (signum)
 	adds base1=(ARG1_OFF-(RBS_BASE_OFF+SIGCONTEXT_OFF)),base1
 	;;
@@ -100,17 +102,12 @@ back_from_setup_rbs:
 	;;
 	ld8 out2=[base0]		// load arg2 (sigcontextp)
 	ld8 gp=[r17]			// get signal handler's global pointer
-
 	adds base0=(BSP_OFF+SIGCONTEXT_OFF),sp
 	;;
 	.spillsp ar.bsp, BSP_OFF+SIGCONTEXT_OFF
-	st8 [base0]=r9,(CFM_OFF-BSP_OFF)	// save sc_ar_bsp
-	dep r8=0,r8,38,26			// clear EC0, CPL0 and reserved bits
-	adds base1=(FR6_OFF+16+SIGCONTEXT_OFF),sp
-	;;
-	.spillsp ar.pfs, CFM_OFF+SIGCONTEXT_OFF
-	st8 [base0]=r8				// save CFM0
+	st8 [base0]=r9			// save sc_ar_bsp
 	adds base0=(FR6_OFF+SIGCONTEXT_OFF),sp
+	adds base1=(FR6_OFF+16+SIGCONTEXT_OFF),sp
 	;;
 	stf.spill [base0]=f6,32
 	stf.spill [base1]=f7,32
@@ -133,9 +130,8 @@ back_from_setup_rbs:
 	ld8 r15=[base0],(CFM_OFF-BSP_OFF)	// fetch sc_ar_bsp and advance to CFM_OFF
 	mov r14=ar.bsp
 	;;
-	ld8 r8=[base0]				// restore (perhaps modified) CFM0, EC0, and CPL0
-	cmp.ne p8,p0=r14,r15			// do we need to restore the rbs?
-(p8)	br.cond.spnt restore_rbs		// yup -> (clobbers r14-r18, f6 & f7)
+	cmp.ne p1,p0=r14,r15			// do we need to restore the rbs?
+(p1)	br.cond.spnt restore_rbs		// yup -> (clobbers p8, r14-r18, f6 & f7)
 	;;
 back_from_restore_rbs:
 	adds base0=(FR6_OFF+SIGCONTEXT_OFF),sp
@@ -156,36 +152,43 @@ back_from_restore_rbs:
 	ldf.fill f14=[base0],32
 	ldf.fill f15=[base1],32
 	mov r15=__NR_rt_sigreturn
+	.restore sp				// pop .prologue
 	break __BREAK_SYSCALL
 
-	.body
-	.copy_state 1
+	.prologue
+	SIGTRAMP_SAVES
 setup_rbs:
 	mov ar.rsc=0				// put RSE into enforced lazy mode
 	;;
-	.save ar.rnat, r16
-	mov r16=ar.rnat				// save RNaT before switching backing store area
+	.save ar.rnat, r19
+	mov r19=ar.rnat				// save RNaT before switching backing store area
 	adds r14=(RNAT_OFF+SIGCONTEXT_OFF),sp
 
+	mov r18=ar.bspstore
 	mov ar.bspstore=r15			// switch over to new register backing store area
 	;;
-	.spillsp ar.rnat, RNAT_OFF+SIGCONTEXT_OFF
-	st8 [r14]=r16				// save sc_ar_rnat
-	adds r14=(LOADRS_OFF+SIGCONTEXT_OFF),sp
 
+	.spillsp ar.rnat, RNAT_OFF+SIGCONTEXT_OFF
+	st8 [r14]=r19				// save sc_ar_rnat
+	.body
 	mov.m r16=ar.bsp			// sc_loadrs <- (new bsp - new bspstore) << 16
+	adds r14=(LOADRS_OFF+SIGCONTEXT_OFF),sp
 	;;
 	invala
 	sub r15=r16,r15
+	extr.u r20=r18,3,6
 	;;
+	mov ar.rsc=0xf				// set RSE into eager mode, pl 3
+	cmp.eq p8,p0=63,r20
 	shl r15=r15,16
 	;;
 	st8 [r14]=r15				// save sc_loadrs
-	mov ar.rsc=0xf				// set RSE into eager mode, pl 3
+(p8)	st8 [r18]=r19		// if bspstore points at RNaT slot, store RNaT there now
+	.restore sp				// pop .prologue
 	br.cond.sptk back_from_setup_rbs
 
 	.prologue
-	.copy_state 1
+	SIGTRAMP_SAVES
 	.spillsp ar.rnat, RNAT_OFF+SIGCONTEXT_OFF
 	.body
 restore_rbs:
diff -urNp linux-260/arch/ia64/kernel/head.S linux-262/arch/ia64/kernel/head.S
--- linux-260/arch/ia64/kernel/head.S
+++ linux-262/arch/ia64/kernel/head.S
@@ -88,7 +88,7 @@ start_ap:
 	/*
 	 * Switch into virtual mode:
 	 */
-	movl r16=(IA64_PSR_IT|IA64_PSR_IC|IA64_PSR_DT|IA64_PSR_RT|IA64_PSR_DFH|IA64_PSR_BN \
+	movl r16=(IA64_PSR_IT|IA64_PSR_DT|IA64_PSR_RT|IA64_PSR_DFH|IA64_PSR_BN \
 		  |IA64_PSR_DI)
 	;;
 	mov cr.ipsr=r16
@@ -147,9 +147,31 @@ start_ap:
 	cmp4.ne isAP,isBP=r3,r0
 	;;			// RAW on r2
 	extr r3=r2,0,61		// r3 == phys addr of task struct
-	mov r16=KERNEL_TR_PAGE_NUM
 	;;
 
+	shr.u r17=r3,KERNEL_TR_PAGE_SHIFT
+	;;
+	cmp.eq p6,p7=KERNEL_TR_PAGE_NUM,r17
+(p6)	br.cond.dpnt	skip_stack_dtr
+	;;
+	movl r17=PAGE_KERNEL
+	movl r18=IA64_GRANULE_SHIFT<<2
+	;;
+	mov cr.itir=r18
+	mov cr.ifa=r2
+	dep r20=0,r3,0,12
+	;;
+	or r20=r17,r20
+	mov r19=IA64_TR_CURRENT_STACK
+	;;
+	itr.d dtr[r19]=r20
+	;;
+skip_stack_dtr:
+	ssm psr.ic
+	;;
+	srlz.d
+	shr.u r16=r3,IA64_GRANULE_SHIFT
+	;;
 	// load the "current" pointer (r13) and ar.k6 with the current task
 	mov r13=r2
 	mov IA64_KR(CURRENT)=r3		// Physical address
@@ -782,6 +804,7 @@ GLOBAL_ENTRY(ia64_spinlock_contention)
 	;;
 	// delay a little...
 .wait:	sub tmp=tmp,timeout
+	hint @pause
 	or delay=0xf,delay	// make sure delay is non-zero (otherwise we get stuck with 0)
 	;;
 	cmp.lt p15,p0=tmp,r0
diff -urNp linux-260/arch/ia64/kernel/ia64_ksyms.c linux-262/arch/ia64/kernel/ia64_ksyms.c
--- linux-260/arch/ia64/kernel/ia64_ksyms.c
+++ linux-262/arch/ia64/kernel/ia64_ksyms.c
@@ -42,11 +42,14 @@ EXPORT_SYMBOL(csum_partial_copy_nocheck)
 EXPORT_SYMBOL(csum_tcpudp_magic);
 EXPORT_SYMBOL(ip_compute_csum);
 EXPORT_SYMBOL(ip_fast_csum);
+EXPORT_SYMBOL(csum_tcpudp_nofold);
 
 #include <asm/io.h>
 EXPORT_SYMBOL(__ia64_memcpy_fromio);
 EXPORT_SYMBOL(__ia64_memcpy_toio);
 EXPORT_SYMBOL(__ia64_memset_c_io);
+EXPORT_SYMBOL(io_space);
+EXPORT_SYMBOL(screen_info);
 
 #include <asm/semaphore.h>
 EXPORT_SYMBOL_NOVERS(__down);
@@ -93,6 +96,9 @@ EXPORT_SYMBOL(ia64_cpu_to_sapicid);
 #include <linux/smp.h>
 EXPORT_SYMBOL(smp_num_cpus);
 
+#include <linux/pm.h>
+EXPORT_SYMBOL(pm_idle);
+
 #include <asm/smplock.h>
 EXPORT_SYMBOL(kernel_flag);
 
@@ -165,3 +171,12 @@ EXPORT_SYMBOL(machvec_noop);
 EXPORT_SYMBOL(pfm_install_alternate_syswide_subsystem);
 EXPORT_SYMBOL(pfm_remove_alternate_syswide_subsystem);
 #endif
+
+#include <asm/dma.h>
+EXPORT_SYMBOL(MAX_DMA_ADDRESS);
+
+#include <asm/iosapic.h>
+EXPORT_SYMBOL_GPL(iosapic_fixup_pci_interrupt);
+
+#include <linux/efi.h>
+EXPORT_SYMBOL(efi_mem_type);
diff -urNp linux-260/arch/ia64/kernel/init_task.c linux-262/arch/ia64/kernel/init_task.c
--- linux-260/arch/ia64/kernel/init_task.c
+++ linux-262/arch/ia64/kernel/init_task.c
@@ -15,7 +15,8 @@
 
 static struct fs_struct init_fs = INIT_FS;
 static struct files_struct init_files = INIT_FILES;
-static struct signal_struct init_signals = INIT_SIGNALS;
+static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
+static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
 struct mm_struct init_mm = INIT_MM(init_mm);
 
 /*
@@ -25,6 +26,61 @@ struct mm_struct init_mm = INIT_MM(init_
  * process stacks are handled. This is done by having a special
  * "init_task" linker map entry..
  */
+
+
+/*
+ *  INIT_TASK is used to set up the first task table, touch at
+ * your own risk!. Base=0, limit=0x1fffff (=2MB)
+ */
+#define INIT_IA64_TASK(tsk)	\
+{									\
+    state:              0,                                              \
+    flags:              0,                                              \
+    sigpending:         0,                                              \
+    addr_limit:         KERNEL_DS,                                      \
+    exec_domain:        &default_exec_domain,                           \
+    lock_depth:         -1,                                             \
+    prio:               MAX_PRIO-20,                                    \
+    static_prio:        MAX_PRIO-20,                                    \
+    policy:             SCHED_NORMAL,                                   \
+    cpus_allowed:       ~0UL,                                           \
+    mm:                 NULL,                                           \
+    active_mm:          &init_mm,                                       \
+    run_list:           LIST_HEAD_INIT(tsk.run_list),                   \
+    time_slice:         HZ,                                             \
+    tasks:              LIST_HEAD_INIT(tsk.tasks),                      \
+    ptrace_children:    LIST_HEAD_INIT(tsk.ptrace_children),            \
+    ptrace_list:        LIST_HEAD_INIT(tsk.ptrace_list),                \
+    real_parent:        &tsk,                                           \
+    parent:             &tsk,                                           \
+    children:           LIST_HEAD_INIT(tsk.children),                   \
+    sibling:            LIST_HEAD_INIT(tsk.sibling),                    \
+    group_leader:       &tsk,                                           \
+    wait_chldexit:      __WAIT_QUEUE_HEAD_INITIALIZER(tsk.wait_chldexit),\
+    real_timer:         {                                               \
+        function:               it_real_fn                              \
+    },                                                                  \
+    cap_effective:      CAP_INIT_EFF_SET,                               \
+    cap_inheritable:    CAP_INIT_INH_SET,                               \
+    cap_permitted:      CAP_FULL_SET,                                   \
+   keep_capabilities:  0,                                              \
+    rlim:               INIT_RLIMITS,                                   \
+    user:               INIT_USER,                                      \
+    comm:               "swapper",                                      \
+    thread:             INIT_THREAD,                                    \
+    fs:                 &init_fs,                                       \
+    files:              &init_files,                                    \
+    signal:             &init_signals,                                  \
+    sighand:            &init_sighand,                                  \
+    pending:            { NULL, &tsk.pending.head, {{0}}},              \
+    blocked:            {{0}},                                          \
+    alloc_lock:         SPIN_LOCK_UNLOCKED,                             \
+    switch_lock:        SPIN_LOCK_UNLOCKED,                             \
+    journal_info:       NULL                                            \
+}
+
 union task_union init_task_union
 	__attribute__((section("init_task"))) =
-		{ INIT_TASK(init_task_union.task) };
+		{ INIT_IA64_TASK(init_task_union.task) };
+
+
diff -urNp linux-260/arch/ia64/kernel/ivt.S linux-262/arch/ia64/kernel/ivt.S
--- linux-260/arch/ia64/kernel/ivt.S
+++ linux-262/arch/ia64/kernel/ivt.S
@@ -44,6 +44,7 @@
 #include <asm/ptrace.h>
 #include <asm/system.h>
 #include <asm/unistd.h>
+#include <asm/errno.h>
 
 #if 1
 # define PSR_DEFAULT_BITS	psr.ac
@@ -539,6 +540,13 @@ ENTRY(iaccess_bit)
 	;;
 1:	ld8 r18=[r17]
 	;;
+	tbit.z p6,p0=r18,_PAGE_P_BIT		// check the present bit in the PTE
+	;;
+(p6)	mov r24=PAGE_SHIFT<<2
+	;;
+(p6)	ptc.l r16,r24
+(p6)	br.cond.spnt page_fault
+	;;					// 
 	mov ar.ccv=r18				// set compare value for cmpxchg
 	or r25=_PAGE_A,r18			// set the accessed bit
 	;;
@@ -586,6 +594,13 @@ ENTRY(daccess_bit)
 	mov r28=ar.ccv				// save ar.ccv
 	;;
 1:	ld8 r18=[r17]
+	;;
+	tbit.z p6,p0=r18,_PAGE_P_BIT		// check the present bit in the PTE.
+	;;
+(p6)	mov r24=PAGE_SHIFT<<2
+	;;
+(p6)	ptc.l r16,r24
+(p6)	br.cond.spnt page_fault
 	;;					// avoid RAW on r18
 	mov ar.ccv=r18				// set compare value for cmpxchg
 	or r25=_PAGE_A,r18			// set the dirty bit
@@ -656,11 +671,11 @@ ENTRY(break_fault)
 	movl r15=ia64_ret_from_syscall
 (p7)	adds r16=(__NR_ni_syscall-1024)*8,r16	// force __NR_ni_syscall
 	;;
-	ld8 r16=[r16]				// load address of syscall entry point
+(p9)	ld8 r16=[r16]				// load address of syscall entry point
 	mov rp=r15				// set the real return addr
 	;;
 	ld8 r2=[r2]				// r2 = current->ptrace
-	mov b6=r16
+(p9)	mov b6=r16
 
 	// arrange things so we skip over break instruction when returning:
 
@@ -683,41 +698,12 @@ ENTRY(break_fault)
 	dep r18=r20,r18,41,2			// insert new ei into cr.isr
 	;;
 	st8 [r16]=r18				// store new value for cr.isr
-
+(p10)	br.cond.spnt.many ia64_ret_from_syscall
 (p8)	br.call.sptk.many b6=b6			// ignore this return addr
 	br.cond.sptk ia64_trace_syscall
 	// NOT REACHED
 END(break_fault)
 
-ENTRY(demine_args)
-	alloc r2=ar.pfs,8,0,0,0
-	tnat.nz p8,p0=in0
-	tnat.nz p9,p0=in1
-	;;
-(p8)	mov in0=-1
-	tnat.nz p10,p0=in2
-	tnat.nz p11,p0=in3
-
-(p9)	mov in1=-1
-	tnat.nz p12,p0=in4
-	tnat.nz p13,p0=in5
-	;;
-(p10)	mov in2=-1
-	tnat.nz p14,p0=in6
-	tnat.nz p15,p0=in7
-
-(p11)	mov in3=-1
-	tnat.nz p8,p0=r15	// demining r15 is not a must, but it is safer
-
-(p12)	mov in4=-1
-(p13)	mov in5=-1
-	;;
-(p14)	mov in6=-1
-(p15)	mov in7=-1
-(p8)	mov r15=-1
-	br.ret.sptk.many rp
-END(demine_args)
-
 	.align 1024
 /////////////////////////////////////////////////////////////////////////////////////////
 // 0x3000 Entry 12 (size 64 bundles) External Interrupt (4)
@@ -751,6 +737,46 @@ END(interrupt)
 	DBG_FAULT(13)
 	FAULT(13)
 
+// Ensure that the syscall arguments plus r15 (syscall number) are valid.
+// Exit with r2 containing the frame size when the syscall was issued.
+// This function belongs to break_fault and can live anywhere (even outside
+// the IVT); it's being placed here just to save a little space.
+// On exit:
+//	- p10: TRUE if syscall is invoked with more than 8 out
+//	       Registers or r15's Nat is true
+//	- p9:  !(p10)
+ENTRY(demine_args)
+	alloc r2=ar.pfs,8,0,0,0
+	tnat.nz p8,p0=in0
+	tnat.nz p9,p0=in1
+	;;
+	and r18=0x7f,r2				// get sof of issuer's cfm
+	extr.u r17=r2,7,7			// get sol of issuer's cfm
+	tnat.nz p10,p0=in2
+(p8)	mov in0=-1
+	tnat.nz p11,p0=in3
+	tnat.nz p12,p0=in4
+	;;
+(p9)	mov in1=-1
+	tnat.nz p13,p0=in5
+	add r17=8,r17				// sol + 8
+(p10)	mov in2=-1
+	tnat.nz p14,p0=in6
+	tnat.nz p15,p0=in7
+	;;
+	cmp.lt p10,p9=r17,r18			// frame size can't be more than local+8
+(p11)	mov in3=-1
+(p12)	mov in4=-1
+	;;
+(p13)	mov in5=-1
+(p9)	tnat.nz p10,p9=r15	// demining r15 is not a must, but it is safer
+(p14)	mov in6=-1
+	;;
+(p15)	mov in7=-1
+(p10)	mov r8=-EINVAL
+	br.ret.sptk.many rp
+END(demine_args)
+
 	.align 1024
 /////////////////////////////////////////////////////////////////////////////////////////
 // 0x3800 Entry 14 (size 64 bundles) Reserved
diff -urNp linux-260/arch/ia64/kernel/mca.c linux-262/arch/ia64/kernel/mca.c
--- linux-260/arch/ia64/kernel/mca.c
+++ linux-262/arch/ia64/kernel/mca.c
@@ -45,6 +45,7 @@
 #include <linux/timer.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
+#include <linux/smp.h>
 
 #include <asm/machvec.h>
 #include <asm/page.h>
@@ -53,6 +54,7 @@
 #include <asm/sal.h>
 #include <asm/mca.h>
 
+#include <asm/processor.h>
 #include <asm/irq.h>
 #include <asm/hw_irq.h>
 
@@ -110,8 +112,16 @@ static struct irqaction mca_cpe_irqactio
 
 #define MAX_CPE_POLL_INTERVAL (15*60*HZ) /* 15 minutes */
 #define MIN_CPE_POLL_INTERVAL (2*60*HZ)  /* 2 minutes */
+#define CMC_POLL_INTERVAL     (1*60*HZ)  /* 1 minute */
+#define CMC_HISTORY_LENGTH    5
 
 static struct timer_list cpe_poll_timer;
+static struct timer_list cmc_poll_timer;
+/*
+ * Start with this in the wrong state so we won't play w/ timers
+ * before the system is ready.
+ */
+static int cmc_polling_enabled = 1;
 
 /*
  *  ia64_mca_log_sal_error_record
@@ -160,7 +170,7 @@ mca_handler_platform (void)
 void
 ia64_mca_cpe_int_handler (int cpe_irq, void *arg, struct pt_regs *ptregs)
 {
-	IA64_MCA_DEBUG("ia64_mca_cpe_int_handler: received interrupt. vector = %#x\n", cpe_irq);
+	IA64_MCA_DEBUG("ia64_mca_cpe_int_handler: received interrupt. CPU:%d vector = %#x\n", smp_processor_id(), cpe_irq);
 
 	/* Get the CMC error record and log it */
 	ia64_mca_log_sal_error_record(SAL_INFO_TYPE_CPE, 0);
@@ -331,6 +341,60 @@ ia64_mca_cmc_vector_setup (void)
 		       smp_processor_id(), ia64_get_cmcv());
 }
 
+/*
+ * ia64_mca_cmc_vector_disable
+ *
+ *  Mask the corrected machine check vector register in the processor.
+ *  This function is invoked on a per-processor basis.
+ *
+ * Inputs
+ *      dummy(unused)
+ *
+ * Outputs
+ *	None
+ */
+void
+ia64_mca_cmc_vector_disable (void *dummy)
+{
+	cmcv_reg_t	cmcv;
+	
+	cmcv = (cmcv_reg_t)ia64_get_cmcv();
+
+	cmcv.cmcv_mask = 1; /* Mask/disable interrupt */
+	ia64_set_cmcv(cmcv.cmcv_regval);
+
+	IA64_MCA_DEBUG("ia64_mca_cmc_vector_disable: CPU %d corrected "
+		       "machine check vector %#x disabled.\n",
+		       smp_processor_id(), cmcv.cmcv_vector);
+}
+
+/*
+ * ia64_mca_cmc_vector_enable
+ *
+ *  Unmask the corrected machine check vector register in the processor.
+ *  This function is invoked on a per-processor basis.
+ *
+ * Inputs
+ *      dummy(unused)
+ *
+ * Outputs
+ *	None
+ */
+void
+ia64_mca_cmc_vector_enable (void *dummy)
+{
+	cmcv_reg_t	cmcv;
+	
+	cmcv = (cmcv_reg_t)ia64_get_cmcv();
+
+	cmcv.cmcv_mask = 0; /* Unmask/enable interrupt */
+	ia64_set_cmcv(cmcv.cmcv_regval);
+
+	IA64_MCA_DEBUG("ia64_mca_cmc_vector_enable: CPU %d corrected "
+		       "machine check vector %#x enabled.\n",
+		       smp_processor_id(), cmcv.cmcv_vector);
+}
+
 
 #if defined(MCA_TEST)
 
@@ -780,11 +844,68 @@ ia64_mca_ucmc_handler(void)
 void
 ia64_mca_cmc_int_handler(int cmc_irq, void *arg, struct pt_regs *ptregs)
 {
+	static unsigned long	cmc_history[CMC_HISTORY_LENGTH];
+	static int		index;
+	static spinlock_t	cmc_history_lock = SPIN_LOCK_UNLOCKED;
+
 	IA64_MCA_DEBUG("ia64_mca_cmc_int_handler: received interrupt vector = %#x on CPU %d\n",
 		       cmc_irq, smp_processor_id());
 
 	/* Get the CMC error record and log it */
 	ia64_mca_log_sal_error_record(SAL_INFO_TYPE_CMC, 0);
+
+	spin_lock(&cmc_history_lock);
+	if (!cmc_polling_enabled) {
+		int i, count = 1; /* we know 1 happened now */
+		unsigned long now = jiffies;
+		
+		for (i = 0; i < CMC_HISTORY_LENGTH; i++) {
+			if (now - cmc_history[i] <= HZ)
+				count++;
+		}
+
+		IA64_MCA_DEBUG(KERN_INFO "CMC threshold %d/%d\n", count, CMC_HISTORY_LENGTH);
+		if (count >= CMC_HISTORY_LENGTH) {
+			/*
+			 * CMC threshold exceeded, clear the history
+			 * so we have a fresh start when we return
+			 */
+			for (index = 0 ; index < CMC_HISTORY_LENGTH; index++)
+				cmc_history[index] = 0;
+			index = 0;
+
+			/* Switch to polling mode */
+			cmc_polling_enabled = 1;
+
+			/*
+			 * Unlock & enable interrupts  before 
+			 * smp_call_function or risk deadlock
+			 */
+			spin_unlock(&cmc_history_lock);
+			ia64_mca_cmc_vector_disable(NULL);
+
+			local_irq_enable();
+			smp_call_function(ia64_mca_cmc_vector_disable, NULL, 1, 1);
+
+			/*
+			 * Corrected errors will still be corrected, but
+			 * make sure there's a log somewhere that indicates
+			 * something is generating more than we can handle.
+			 */
+			printk(KERN_WARNING "ia64_mca_cmc_int_handler: WARNING: Switching to polling CMC handler, error records may be lost\n");
+
+			mod_timer(&cmc_poll_timer, jiffies + CMC_POLL_INTERVAL);
+			local_irq_disable();
+
+			/* lock already released, get out now */
+			return;
+		} else {
+			cmc_history[index++] = now;
+			if (index == CMC_HISTORY_LENGTH)
+				index = 0;
+		}
+	}
+	spin_unlock(&cmc_history_lock);
 }
 
 /*
@@ -797,6 +918,7 @@ typedef struct ia64_state_log_s
 {
 	spinlock_t	isl_lock;
 	int		isl_index;
+	unsigned long	isl_count;
 	ia64_err_rec_t  *isl_log[IA64_MAX_LOGS]; /* need space to store header + error log */
 } ia64_state_log_t;
 
@@ -813,11 +935,78 @@ static ia64_state_log_t ia64_state_log[I
 #define IA64_LOG_NEXT_INDEX(it)    ia64_state_log[it].isl_index
 #define IA64_LOG_CURR_INDEX(it)    1 - ia64_state_log[it].isl_index
 #define IA64_LOG_INDEX_INC(it) \
-    ia64_state_log[it].isl_index = 1 - ia64_state_log[it].isl_index
+    {ia64_state_log[it].isl_index = 1 - ia64_state_log[it].isl_index; \
+    ia64_state_log[it].isl_count++;}
 #define IA64_LOG_INDEX_DEC(it) \
     ia64_state_log[it].isl_index = 1 - ia64_state_log[it].isl_index
 #define IA64_LOG_NEXT_BUFFER(it)   (void *)((ia64_state_log[it].isl_log[IA64_LOG_NEXT_INDEX(it)]))
 #define IA64_LOG_CURR_BUFFER(it)   (void *)((ia64_state_log[it].isl_log[IA64_LOG_CURR_INDEX(it)]))
+#define IA64_LOG_COUNT(it)         ia64_state_log[it].isl_count
+
+/*
+ *  ia64_mca_cmc_int_caller
+ *
+ * 	Call CMC interrupt handler, only purpose is to have a
+ * 	smp_call_function callable entry.
+ *
+ * Inputs   :	dummy(unused)
+ * Outputs  :	None
+ * */
+static void
+ia64_mca_cmc_int_caller(void *dummy)
+{
+	ia64_mca_cmc_int_handler(0, NULL, NULL);
+}
+
+/*
+ *  ia64_mca_cmc_poll
+ *
+ *	Poll for Corrected Machine Checks (CMCs)
+ *
+ * Inputs   :   dummy(unused)
+ * Outputs  :   None
+ *
+ */
+static void
+ia64_mca_cmc_poll (unsigned long dummy)
+{
+	int start_count;
+
+	start_count = IA64_LOG_COUNT(SAL_INFO_TYPE_CMC);
+
+	/* Call the interrupt handler */
+	smp_call_function(ia64_mca_cmc_int_caller, NULL, 1, 1);
+	local_irq_disable();
+	ia64_mca_cmc_int_caller(NULL);
+	local_irq_enable();
+
+	/*
+	 * If no log recored, switch out of polling mode.
+	 */
+	if (start_count == IA64_LOG_COUNT(SAL_INFO_TYPE_CMC)) {
+		printk(KERN_WARNING "ia64_mca_cmc_poll: Returning to interrupt driven CMC handler\n");
+		cmc_polling_enabled = 0;
+		smp_call_function(ia64_mca_cmc_vector_enable, NULL, 1, 1);
+		ia64_mca_cmc_vector_enable(NULL);
+	} else {
+		mod_timer(&cmc_poll_timer, jiffies + CMC_POLL_INTERVAL);
+	}
+}
+
+/*
+ *  ia64_mca_cpe_int_caller
+ *
+ * 	Call CPE interrupt handler, only purpose is to have a
+ * 	smp_call_function callable entry.
+ *
+ * Inputs   :	dummy(unused)
+ * Outputs  :	None
+ * */
+static void
+ia64_mca_cpe_int_caller(void *dummy)
+{
+	ia64_mca_cpe_int_handler(0, NULL, NULL);
+}
 
 /*
  *  ia64_mca_cpe_poll
@@ -832,19 +1021,22 @@ static ia64_state_log_t ia64_state_log[I
 static void
 ia64_mca_cpe_poll (unsigned long dummy)
 {
-	int start_index;
+	int start_count;
 	static int poll_time = MAX_CPE_POLL_INTERVAL;
 
-	start_index = IA64_LOG_CURR_INDEX(SAL_INFO_TYPE_CPE);
+	start_count = IA64_LOG_COUNT(SAL_INFO_TYPE_CPE);
 
 	/* Call the interrupt handler */
-	ia64_mca_cpe_int_handler(0, NULL, NULL);
+	smp_call_function(ia64_mca_cpe_int_caller, NULL, 1, 1);
+	local_irq_disable();
+	ia64_mca_cpe_int_caller(NULL);
+	local_irq_enable();
 
 	/*
 	 * If a log was recorded, increase our polling frequency,
 	 * otherwise, backoff.
 	 */
-	if (start_index != IA64_LOG_CURR_INDEX(SAL_INFO_TYPE_CPE)) {
+	if (start_count != IA64_LOG_COUNT(SAL_INFO_TYPE_CPE)) {
 		poll_time = max(MIN_CPE_POLL_INTERVAL, poll_time/2);
 	} else {
 		poll_time = min(MAX_CPE_POLL_INTERVAL, poll_time * 2);
@@ -865,11 +1057,19 @@ ia64_mca_cpe_poll (unsigned long dummy)
 static int __init
 ia64_mca_late_init(void)
 {
-	if (acpi_request_vector(ACPI_INTERRUPT_CPEI) < 0) {
-		init_timer(&cpe_poll_timer);
-		cpe_poll_timer.function = ia64_mca_cpe_poll;
-		ia64_mca_cpe_poll(0);
-	}
+	init_timer(&cmc_poll_timer);
+	cmc_poll_timer.function = ia64_mca_cmc_poll;
+
+	/* Reset to the correct state */
+	cmc_polling_enabled = 0;
+
+	init_timer(&cpe_poll_timer);
+	cpe_poll_timer.function = ia64_mca_cpe_poll;
+
+	/* If platform doesn't support CPEI, get the timer going. */
+	if (acpi_request_vector(ACPI_INTERRUPT_CPEI) < 0)
+		ia64_mca_cpe_poll(0UL);
+
 	return 0;
 }
 
@@ -1077,7 +1277,7 @@ ia64_log_rec_header_print (sal_log_recor
 {
 	prfunc("+Err Record ID: %d    SAL Rev: %2x.%02x\n", lh->id,
 		lh->revision.major, lh->revision.minor);
-	prfunc("+Time: %02x/%02x/%02x%02x %02d:%02d:%02d    Severity %d\n",
+	prfunc("+Time: %02x/%02x/%02x%02x %02x:%02x:%02x    Severity %d\n",
 		lh->timestamp.slh_month, lh->timestamp.slh_day,
 		lh->timestamp.slh_century, lh->timestamp.slh_year,
 		lh->timestamp.slh_hour, lh->timestamp.slh_minute,
diff -urNp linux-260/arch/ia64/kernel/pci.c linux-262/arch/ia64/kernel/pci.c
--- linux-260/arch/ia64/kernel/pci.c
+++ linux-262/arch/ia64/kernel/pci.c
@@ -169,7 +169,7 @@ struct pci_ops pci_sal_ops = {
  */
 
 static struct pci_controller *
-alloc_pci_controller (int seg)
+alloc_pci_controller(int seg)
 {
 	struct pci_controller *controller;
 
@@ -182,57 +182,8 @@ alloc_pci_controller (int seg)
 	return controller;
 }
 
-static struct pci_bus *
-scan_root_bus (int bus, struct pci_ops *ops, void *sysdata)
-{
-	struct pci_bus *b;
-
-	/*
-	 * We know this is a new root bus we haven't seen before, so
-	 * scan it, even if we've seen the same bus number in a different
-	 * segment.
-	 */
-	b = kmalloc(sizeof(*b), GFP_KERNEL);
-	if (!b)
-		return NULL;
-
-	memset(b, 0, sizeof(*b));
-	INIT_LIST_HEAD(&b->children);
-	INIT_LIST_HEAD(&b->devices);
-
-	list_add_tail(&b->node, &pci_root_buses);
-
-	b->number = b->secondary = bus;
-	b->resource[0] = &ioport_resource;
-	b->resource[1] = &iomem_resource;
-
-	b->sysdata = sysdata;
-	b->ops = ops;
-	b->subordinate = pci_do_scan_bus(b);
-
-	return b;
-}
-
-static void
-alloc_resource (char *name, struct resource *root, unsigned long start, unsigned long end, unsigned long flags)
-{
-	struct resource *res;
-
-	res = kmalloc(sizeof(*res), GFP_KERNEL);
-	if (!res)
-		return;
-
-	memset(res, 0, sizeof(*res));
-	res->name = name;
-	res->start = start;
-	res->end = end;
-	res->flags = flags;
-
-	request_resource(root, res);
-}
-
 static u64
-add_io_space (acpi_resource_address64 *addr)
+add_io_space (struct acpi_resource_address64 *addr)
 {
 	u64 offset;
 	int sparse = 0;
@@ -263,10 +214,10 @@ add_io_space (acpi_resource_address64 *a
 }
 
 static acpi_status
-count_window (acpi_resource *resource, void *data)
+count_window (struct acpi_resource *resource, void *data)
 {
 	unsigned int *windows = (unsigned int *) data;
-	acpi_resource_address64 addr;
+	struct acpi_resource_address64 addr;
 	acpi_status status;
 
 	status = acpi_resource_to_address64(resource, &addr);
@@ -284,24 +235,24 @@ struct pci_root_info {
 };
 
 static acpi_status
-add_window (acpi_resource *res, void *data)
+add_window (struct acpi_resource *res, void *data)
 {
 	struct pci_root_info *info = (struct pci_root_info *) data;
 	struct pci_window *window;
-	acpi_resource_address64 addr;
+	struct acpi_resource_address64 addr;
 	acpi_status status;
 	unsigned long flags, offset = 0;
-	struct resource *root;
 
 	status = acpi_resource_to_address64(res, &addr);
 	if (ACPI_SUCCESS(status)) {
+		if (!addr.address_length)
+			return AE_OK;
+
 		if (addr.resource_type == ACPI_MEMORY_RANGE) {
 			flags = IORESOURCE_MEM;
-			root = &iomem_resource;
 			offset = addr.address_translation_offset;
 		} else if (addr.resource_type == ACPI_IO_RANGE) {
 			flags = IORESOURCE_IO;
-			root = &ioport_resource;
 			offset = add_io_space(&addr);
 			if (offset == ~0)
 				return AE_OK;
@@ -313,16 +264,44 @@ add_window (acpi_resource *res, void *da
 		window->resource.start  = addr.min_address_range;
 		window->resource.end    = addr.max_address_range;
 		window->offset		= offset;
-
-		alloc_resource(info->name, root, addr.min_address_range + offset,
-			addr.max_address_range + offset, flags);
 	}
 
 	return AE_OK;
 }
 
+static struct pci_bus *
+scan_root_bus(int bus, struct pci_ops *ops, void *sysdata)
+{
+	struct pci_bus *b;
+
+	/*
+	 * We know this is a new root bus we haven't seen before, so
+	 * scan it, even if we've seen the same bus number in a different
+	 * segment.
+	 */
+	b = kmalloc(sizeof(*b), GFP_KERNEL);
+	if (!b)
+		return NULL;
+
+	memset(b, 0, sizeof(*b));
+	INIT_LIST_HEAD(&b->children);
+	INIT_LIST_HEAD(&b->devices);
+
+	list_add_tail(&b->node, &pci_root_buses);
+
+	b->number = b->secondary = bus;
+	b->resource[0] = &ioport_resource;
+	b->resource[1] = &iomem_resource;
+
+	b->sysdata = sysdata;
+	b->ops = ops;
+	b->subordinate = pci_do_scan_bus(b);
+
+	return b;
+}
+
 struct pci_bus *
-pcibios_scan_root (void *handle, int seg, int bus)
+pcibios_scan_root(void *handle, int seg, int bus)
 {
 	struct pci_root_info info;
 	struct pci_controller *controller;
@@ -333,7 +312,7 @@ pcibios_scan_root (void *handle, int seg
 
 	controller = alloc_pci_controller(seg);
 	if (!controller)
-		goto out1;
+		return NULL;
 
 	controller->acpi_handle = handle;
 
@@ -346,7 +325,7 @@ pcibios_scan_root (void *handle, int seg
 	if (!name)
 		goto out3;
 
-	sprintf(name, "PCI Bus %02x:%02x", seg, bus);
+	sprintf(name, "PCI Bus %04x:%02x", seg, bus);
 	info.controller = controller;
 	info.name = name;
 	acpi_walk_resources(handle, METHOD_NAME__CRS, add_window, &info);
@@ -357,7 +336,6 @@ out3:
 	kfree(controller->window);
 out2:
 	kfree(controller);
-out1:
 	return NULL;
 }
 
@@ -406,7 +384,7 @@ pcibios_init (void)
 }
 
 void __init
-pcibios_fixup_device_resources (struct pci_dev *dev, struct pci_bus *bus)
+pcibios_fixup_device_resources(struct pci_dev *dev, struct pci_bus *bus)
 {
 	struct pci_controller *controller = PCI_CONTROLLER(dev);
 	struct pci_window *window;
diff -urNp linux-260/arch/ia64/kernel/perfmon.c linux-262/arch/ia64/kernel/perfmon.c
--- linux-260/arch/ia64/kernel/perfmon.c
+++ linux-262/arch/ia64/kernel/perfmon.c
@@ -2524,7 +2524,7 @@ check_task_state(struct task_struct *tas
 
 		task_lock(task);
 		DBprintk((" [%d] state=%ld\n", task->pid, task->state));
-		if (!task_has_cpu(task)) break;
+//		if (!task_has_cpu(task)) break;
 		task_unlock(task);
 
 		do {
@@ -2534,7 +2534,7 @@ check_task_state(struct task_struct *tas
 			}
 			barrier();
 			cpu_relax();
-		} while (task_has_cpu(task));
+		} while (/*task_has_cpu(task)*/0);
 	}
 	task_unlock(task);
 #else
@@ -2561,6 +2561,10 @@ sys_perfmonctl (pid_t pid, int cmd, void
 	 * reject any call if perfmon was disabled at initialization time
 	 */
 	if (PFM_IS_DISABLED()) return -ENOSYS;
+	
+	if (!capable(CAP_SYS_ADMIN)) {
+		return -EPERM;
+	}
 
 	DBprintk(("cmd=%d idx=%d valid=%d narg=0x%x\n", cmd, PFM_CMD_IDX(cmd), 
 		  PFM_CMD_IS_VALID(cmd), PFM_CMD_NARG(cmd)));
diff -urNp linux-260/arch/ia64/kernel/process.c linux-262/arch/ia64/kernel/process.c
--- linux-260/arch/ia64/kernel/process.c
+++ linux-262/arch/ia64/kernel/process.c
@@ -33,6 +33,19 @@
 #include <asm/sn/idle.h>
 #endif
 
+#ifdef CONFIG_IA64_PAL_IDLE
+static int use_pal_halt;
+
+static int __init
+pal_halt (char *str)
+{
+	use_pal_halt = 1;
+	return 1;
+}
+
+__setup("palhalt", pal_halt);
+#endif
+
 static void
 do_show_stack (struct unw_frame_info *info, void *arg)
 {
@@ -60,12 +73,13 @@ show_trace_task (struct task_struct *tas
 }
 
 void
-show_stack (struct task_struct *task)
+show_stack (unsigned long * sp)
 {
-	if (!task)
+	if (!sp)
 		unw_init_running(do_show_stack, 0);
 	else {
 		struct unw_frame_info info;
+		struct task_struct *task = (struct task_struct *)sp;
 
 		unw_init_from_blocked_task(&info, task);
 		do_show_stack(&info, 0);
@@ -122,16 +136,27 @@ show_regs (struct pt_regs *regs)
 		show_stack(0);
 }
 
+/*
+ * We use this if we don't have any better idle routine..
+ */
+void
+default_idle (void)
+{
+#ifdef CONFIG_IA64_PAL_IDLE
+	if (use_pal_halt && !current->need_resched)
+		safe_halt();
+#endif
+}
+
 void __attribute__((noreturn))
 cpu_idle (void *unused)
 {
 	/* endless idle loop with no priority at all */
-	init_idle();
-	current->nice = 20;
-	current->counter = -100;
-
-
 	while (1) {
+		void (*idle)(void) = pm_idle;
+		if (!idle)
+			idle = default_idle;
+
 #ifdef CONFIG_SMP
 		if (!current->need_resched)
 			min_xtp();
@@ -141,7 +166,7 @@ cpu_idle (void *unused)
 #ifdef CONFIG_IA64_SGI_SN
 			snidle();
 #endif
-			continue;
+			(*idle)();
 		}
 
 #ifdef CONFIG_IA64_SGI_SN
@@ -153,8 +178,6 @@ cpu_idle (void *unused)
 #endif
 		schedule();
 		check_pgt_cache();
-		if (pm_idle)
-			(*pm_idle)();
 	}
 }
 
@@ -331,6 +354,7 @@ copy_thread (int nr, unsigned long clone
 #	define THREAD_FLAGS_TO_SET	0
 	p->thread.flags = ((current->thread.flags & ~THREAD_FLAGS_TO_CLEAR)
 			   | THREAD_FLAGS_TO_SET);
+	ia64_drop_fpu(p);	/* don't pick up stale state from a CPU's fph */
 #ifdef CONFIG_IA32_SUPPORT
 	/*
 	 * If we're cloning an IA32 task then save the IA32 extra
@@ -459,7 +483,7 @@ dump_fpu (struct pt_regs *pt, elf_fpregs
 	return 1;	/* f0-f31 are always valid so we always return 1 */
 }
 
-asmlinkage long
+long
 sys_execve (char *filename, char **argv, char **envp, struct pt_regs *regs)
 {
 	int error;
@@ -506,11 +530,7 @@ flush_thread (void)
 {
 	/* drop floating-point and debug-register state if it exists: */
 	current->thread.flags &= ~(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID);
-
-#ifndef CONFIG_SMP
-	if (ia64_get_fpu_owner() == current)
-		ia64_set_fpu_owner(0);
-#endif
+	ia64_drop_fpu(current);
 }
 
 #ifdef CONFIG_PERFMON
@@ -548,10 +568,7 @@ release_thread (struct task_struct *task
 void
 exit_thread (void)
 {
-#ifndef CONFIG_SMP
-	if (ia64_get_fpu_owner() == current)
-		ia64_set_fpu_owner(0);
-#endif
+	ia64_drop_fpu(current);
 #ifdef CONFIG_PERFMON
        /* stop monitoring */
 	if (current->thread.pfm_context)
diff -urNp linux-260/arch/ia64/kernel/ptrace.c linux-262/arch/ia64/kernel/ptrace.c
--- linux-260/arch/ia64/kernel/ptrace.c
+++ linux-262/arch/ia64/kernel/ptrace.c
@@ -578,16 +578,11 @@ inline void
 ia64_flush_fph (struct task_struct *task)
 {
 	struct ia64_psr *psr = ia64_psr(ia64_task_regs(task));
-#ifdef CONFIG_SMP
-	struct task_struct *fpu_owner = current;
-#else
-	struct task_struct *fpu_owner = ia64_get_fpu_owner();
-#endif
 
-	if (task == fpu_owner && psr->mfh) {
+	if (ia64_is_local_fpu_owner(task) && psr->mfh) {
 		psr->mfh = 0;
-		ia64_save_fpu(&task->thread.fph[0]);
 		task->thread.flags |= IA64_THREAD_FPH_VALID;
+		ia64_save_fpu(&task->thread.fph[0]);
 	}
 }
 
@@ -609,10 +604,7 @@ ia64_sync_fph (struct task_struct *task)
 		task->thread.flags |= IA64_THREAD_FPH_VALID;
 		memset(&task->thread.fph, 0, sizeof(task->thread.fph));
 	}
-#ifndef CONFIG_SMP
-	if (ia64_get_fpu_owner() == task)
-		ia64_set_fpu_owner(0);
-#endif
+	ia64_drop_fpu(task);
 	psr->dfh = 1;
 }
 
@@ -784,6 +776,13 @@ access_uarea (struct task_struct *child,
 			else
 				*data = (pt->cr_ipsr & IPSR_READ_MASK);
 			return 0;
+		
+		      case PT_AR_RSC:
+			if (write_access)
+				pt->ar_rsc = *data | (3 << 2); /* force PL3 */
+			else
+				*data = pt->ar_rsc;
+			return 0;
 
 		      case PT_AR_RNAT:
 			urbs_end = ia64_get_user_rbs_end(child, pt, NULL);
@@ -803,8 +802,7 @@ access_uarea (struct task_struct *child,
 		      case PT_B0:  case PT_B6:  case PT_B7:
 		      case PT_F6:  case PT_F6+8: case PT_F7: case PT_F7+8:
 		      case PT_F8:  case PT_F8+8: case PT_F9: case PT_F9+8:
-		      case PT_AR_BSPSTORE:
-		      case PT_AR_RSC: case PT_AR_UNAT: case PT_AR_PFS:
+		      case PT_AR_BSPSTORE: case PT_AR_UNAT: case PT_AR_PFS:
 		      case PT_AR_CCV: case PT_AR_FPSR: case PT_CR_IIP: case PT_PR:
 			/* scratch register */
 			ptr = (unsigned long *) ((long) pt + addr - PT_CR_IPSR);
@@ -1000,6 +998,7 @@ ptrace_getregs (struct task_struct *chil
 static long
 ptrace_setregs (struct task_struct *child, struct pt_all_user_regs *ppr)
 {
+	unsigned long rsc;
 	struct switch_stack *sw;
 	struct pt_regs *pt;
 	long ret, retval;
@@ -1034,7 +1033,7 @@ ptrace_setregs (struct task_struct *chil
 	/* app regs */
 
 	retval |= __get_user(pt->ar_pfs, &ppr->ar[PT_AUR_PFS]);
-	retval |= __get_user(pt->ar_rsc, &ppr->ar[PT_AUR_RSC]);
+	retval |= __get_user(rsc, &ppr->ar[PT_AUR_RSC]);
 	retval |= __get_user(pt->ar_bspstore, &ppr->ar[PT_AUR_BSPSTORE]);
 	retval |= __get_user(pt->ar_unat, &ppr->ar[PT_AUR_UNAT]);
 	retval |= __get_user(pt->ar_ccv, &ppr->ar[PT_AUR_CCV]);
@@ -1122,6 +1121,8 @@ ptrace_setregs (struct task_struct *chil
 
 	retval |= access_uarea(child, PT_NAT_BITS, &ppr->nat, 1);
 
+	retval |= access_uarea(child, PT_AR_RSC, &rsc, 1);
+
 	ret = retval ? -EIO : 0;
 	return ret;
 }
diff -urNp linux-260/arch/ia64/kernel/setup.c linux-262/arch/ia64/kernel/setup.c
--- linux-260/arch/ia64/kernel/setup.c
+++ linux-262/arch/ia64/kernel/setup.c
@@ -397,10 +397,10 @@ show_cpuinfo (struct seq_file *m, void *
 {
 #ifdef CONFIG_SMP
 #	define lpj	c->loops_per_jiffy
-#	define cpu	c->processor
+#	define cpum	c->processor
 #else
 #	define lpj	loops_per_jiffy
-#	define cpu	0
+#	define cpum	0
 #endif
 	char family[32], features[128], *cp;
 	struct cpuinfo_ia64 *c = v;
@@ -422,6 +422,11 @@ show_cpuinfo (struct seq_file *m, void *
 		cp = strchr(cp, '\0');
 		mask &= ~1UL;
 	}
+	if (mask & 4) {
+		strcpy(cp, " 16-byte atomic ops");
+		cp = strchr(cp, '\0');
+		mask &= ~4UL;
+	}
 	if (mask)
 		sprintf(cp, " 0x%lx", mask);
 
@@ -439,7 +444,7 @@ show_cpuinfo (struct seq_file *m, void *
 		   "cpu MHz    : %lu.%06lu\n"
 		   "itc MHz    : %lu.%06lu\n"
 		   "BogoMIPS   : %lu.%02lu\n\n",
-		   cpu, c->vendor, family, c->model, c->revision, c->archrev,
+		   cpum, c->vendor, family, c->model, c->revision, c->archrev,
 		   features, c->ppn, c->number,
 		   c->proc_freq / 1000000, c->proc_freq % 1000000,
 		   c->itc_freq / 1000000, c->itc_freq % 1000000,
@@ -609,6 +614,8 @@ cpu_init (void)
 	/* Clear the stack memory reserved for pt_regs: */
 	memset(ia64_task_regs(current), 0, sizeof(struct pt_regs));
 
+	ia64_set_kr(IA64_KR_FPU_OWNER, 0);
+
 	/*
 	 * Initialize default control register to defer all speculative faults.  The
 	 * kernel MUST NOT depend on a particular setting of these bits (in other words,
@@ -619,10 +626,6 @@ cpu_init (void)
 	 */
 	ia64_set_dcr(  IA64_DCR_DP | IA64_DCR_DK | IA64_DCR_DX | IA64_DCR_DR
 		     | IA64_DCR_DA | IA64_DCR_DD | IA64_DCR_LC);
-#ifndef CONFIG_SMP
-	ia64_set_fpu_owner(0);
-#endif
-
 	atomic_inc(&init_mm.mm_count);
 	current->active_mm = &init_mm;
 
diff -urNp linux-260/arch/ia64/kernel/signal.c linux-262/arch/ia64/kernel/signal.c
--- linux-260/arch/ia64/kernel/signal.c
+++ linux-262/arch/ia64/kernel/signal.c
@@ -104,7 +104,7 @@ sys_sigaltstack (const stack_t *uss, sta
 static long
 restore_sigcontext (struct sigcontext *sc, struct sigscratch *scr)
 {
-	unsigned long ip, flags, nat, um, cfm;
+	unsigned long ip, flags, nat, um, cfm, rsc;
 	long err;
 
 	/* restore scratch that always needs gets updated during signal delivery: */
@@ -114,7 +114,7 @@ restore_sigcontext (struct sigcontext *s
 	err |= __get_user(ip, &sc->sc_ip);			/* instruction pointer */
 	err |= __get_user(cfm, &sc->sc_cfm);
 	err |= __get_user(um, &sc->sc_um);			/* user mask */
-	err |= __get_user(scr->pt.ar_rsc, &sc->sc_ar_rsc);
+	err |= __get_user(rsc, &sc->sc_ar_rsc);
 	err |= __get_user(scr->pt.ar_ccv, &sc->sc_ar_ccv);
 	err |= __get_user(scr->pt.ar_unat, &sc->sc_ar_unat);
 	err |= __get_user(scr->pt.ar_fpsr, &sc->sc_ar_fpsr);
@@ -129,6 +129,7 @@ restore_sigcontext (struct sigcontext *s
 	err |= __copy_from_user(&scr->pt.r16, &sc->sc_gr[16], 16*8);	/* r16-r31 */
 
 	scr->pt.cr_ifs = cfm | (1UL << 63);
+	scr->pt.ar_rsc = rsc | (3 << 2); /* force PL3 */
 
 	/* establish new instruction pointer: */
 	scr->pt.cr_iip = ip & ~0x3UL;
@@ -142,8 +143,13 @@ restore_sigcontext (struct sigcontext *s
 
 		__copy_from_user(current->thread.fph, &sc->sc_fr[32], 96*16);
 		psr->mfh = 0;	/* drop signal handler's fph contents... */
-		if (!psr->dfh)
+		if (psr->dfh)
+			ia64_drop_fpu(current);
+		else {
+			/* We already own the local fph, otherwise psr->dfh wouldn't be 0.  */
 			__ia64_load_fpu(current->thread.fph);
+			ia64_set_local_fpu_owner(current);
+		}
 	}
 	return err;
 }
diff -urNp linux-260/arch/ia64/kernel/sys_ia64.c linux-262/arch/ia64/kernel/sys_ia64.c
--- linux-260/arch/ia64/kernel/sys_ia64.c
+++ linux-262/arch/ia64/kernel/sys_ia64.c
@@ -120,7 +120,7 @@ ia64_brk (unsigned long brk, long arg1, 
 
 	/* Always allow shrinking brk. */
 	if (brk <= mm->brk) {
-		if (!do_munmap(mm, newbrk, oldbrk-newbrk))
+		if (!do_munmap(mm, newbrk, oldbrk-newbrk,1))
 			goto set_brk;
 		goto out;
 	}
@@ -138,10 +138,6 @@ ia64_brk (unsigned long brk, long arg1, 
 	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))
 		goto out;
 
-	/* Check if we have enough memory.. */
-	if (!vm_enough_memory((newbrk-oldbrk) >> PAGE_SHIFT))
-		goto out;
-
 	/* Ok, looks good - let it rip. */
 	if (do_brk(oldbrk, newbrk-oldbrk) != oldbrk)
 		goto out;
diff -urNp linux-260/arch/ia64/kernel/time.c linux-262/arch/ia64/kernel/time.c
--- linux-260/arch/ia64/kernel/time.c
+++ linux-262/arch/ia64/kernel/time.c
@@ -25,8 +25,8 @@
 
 extern rwlock_t xtime_lock;
 extern unsigned long wall_jiffies;
-extern unsigned long last_time_offset;
 
+unsigned long last_time_offset;
 #ifdef CONFIG_IA64_DEBUG_IRQ
 
 unsigned long last_cli_ip;
diff -urNp linux-260/arch/ia64/kernel/traps.c linux-262/arch/ia64/kernel/traps.c
--- linux-260/arch/ia64/kernel/traps.c
+++ linux-262/arch/ia64/kernel/traps.c
@@ -134,6 +134,8 @@ ia64_bad_break (unsigned long break_num,
 {
 	siginfo_t siginfo;
 	int sig, code;
+	unsigned long bundle[2];
+	unsigned long instruction;
 
 	/* SIGILL, SIGFPE, SIGSEGV, and SIGBUS want these field initialized: */
 	siginfo.si_addr = (void *) (regs->cr_iip + ia64_psr(regs)->ri);
@@ -144,6 +146,29 @@ ia64_bad_break (unsigned long break_num,
 	switch (break_num) {
 	      case 0: /* unknown error (used by GCC for __builtin_abort()) */
 		die_if_kernel("Bad break", regs, break_num);
+		/* 
+		 *  IA64 replaces gdb's "break.b 0xccccc" immediate value 
+		 *  with an immediate value of zero, so check the user code 
+		 *  for this special case.
+		 */
+        	if (copy_from_user(bundle, (void *)(regs->cr_iip), 
+		    sizeof(bundle)) == 0) {
+			instruction = 0;
+			switch (ia64_psr(regs)->ri)
+			{
+			case 0: instruction = (bundle[0] & 0x3fffffffffff) >> 5;
+				break;
+			case 1: instruction = (bundle[0] >> 46) ||
+				((bundle[1] & 0x3ffff) << 46);
+				break;
+			case 2: instruction = bundle[1] >> 23;
+				break;
+			}
+			if ((instruction >> 6) == 0xccccc) {  
+				sig = SIGTRAP; code = TRAP_BRKPT;
+				break;
+			}
+		}
 		sig = SIGILL; code = ILL_ILLOPC;
 		break;
 
@@ -211,6 +236,17 @@ ia64_bad_break (unsigned long break_num,
 	force_sig_info(sig, &siginfo, current);
 }
 
+#define throttled_printk(args ...)              \
+do {                                            \
+	static int count=0, limit=1;            \
+	if(++count > limit) {                   \
+		count=0;                        \
+		limit<<=1;                      \
+		printk(KERN_DEBUG ## args);     \
+	}                                       \
+}while(0)
+
+
 /*
  * Unimplemented system calls.  This is called only for stuff that
  * we're supposed to implement but haven't done so yet.  Everything
@@ -223,7 +259,7 @@ ia64_ni_syscall (unsigned long arg0, uns
 {
 	struct pt_regs *regs = (struct pt_regs *) &stack;
 
-	printk("%s(%d): <sc%ld(%lx,%lx,%lx,%lx)>\n", current->comm, current->pid,
+	throttled_printk("%s(%d): <sc%ld(%lx,%lx,%lx,%lx)>\n", current->comm, current->pid,
 	       regs->r15, arg0, arg1, arg2, arg3);
 	return -ENOSYS;
 }
@@ -245,17 +281,17 @@ disabled_fph_fault (struct pt_regs *regs
 	psr->dfh = 0;
 #ifndef CONFIG_SMP
 	{
-		struct task_struct *fpu_owner = ia64_get_fpu_owner();
+		struct task_struct *fpu_owner
+			= (struct task_struct *)ia64_get_kr(IA64_KR_FPU_OWNER);
 
-		if (fpu_owner == current)
+		if (ia64_is_local_fpu_owner(current))
 			return;
 
 		if (fpu_owner)
 			ia64_flush_fph(fpu_owner);
-
 	}
 #endif /* !CONFIG_SMP */
-	ia64_set_fpu_owner(current);
+	ia64_set_local_fpu_owner(current);
 	if ((current->thread.flags & IA64_THREAD_FPH_VALID) != 0) {
 		__ia64_load_fpu(current->thread.fph);
 		psr->mfh = 0;
@@ -361,6 +397,10 @@ handle_fpu_swa (int fp_fault, struct pt_
 			siginfo.si_addr = (void *) (regs->cr_iip + ia64_psr(regs)->ri);
 			if (isr & 0x11) {
 				siginfo.si_code = FPE_FLTINV;
+			} else if (isr & 0x22) {
+				/* denormal operand gets the same si_code as underflow 
+				* see arch/i386/kernel/traps.c:math_error()  */
+				siginfo.si_code = FPE_FLTUND;
 			} else if (isr & 0x44) {
 				siginfo.si_code = FPE_FLTDIV;
 			}
@@ -485,19 +525,23 @@ ia64_fault (unsigned long vector, unsign
 
 	      case 26: /* NaT Consumption */
 		if (user_mode(regs)) {
+			void *addr;
+
 			if (((isr >> 4) & 0xf) == 2) {
 				/* NaT page consumption */
 				sig = SIGSEGV;
 				code = SEGV_ACCERR;
+				addr = (void *) ifa;
 			} else {
 				/* register NaT consumption */
 				sig = SIGILL;
 				code = ILL_ILLOPN;
+				addr = (void *) (regs->cr_iip + ia64_psr(regs)->ri);
 			}
 			siginfo.si_signo = sig;
 			siginfo.si_code = code;
 			siginfo.si_errno = 0;
-			siginfo.si_addr = (void *) (regs->cr_iip + ia64_psr(regs)->ri);
+			siginfo.si_addr = addr;
 			siginfo.si_imm = vector;
 			siginfo.si_flags = __ISR_VALID;
 			siginfo.si_isr = isr;
diff -urNp linux-260/arch/ia64/kernel/unaligned.c linux-262/arch/ia64/kernel/unaligned.c
--- linux-260/arch/ia64/kernel/unaligned.c
+++ linux-262/arch/ia64/kernel/unaligned.c
@@ -22,7 +22,7 @@
 #include <asm/processor.h>
 #include <asm/unaligned.h>
 
-extern void die_if_kernel(char *str, struct pt_regs *regs, long err) __attribute__ ((noreturn));
+extern void die_if_kernel(char *str, struct pt_regs *regs, long err);
 
 #undef DEBUG_UNALIGNED_TRAP
 
@@ -51,6 +51,14 @@ dump (const char *str, void *vp, size_t 
 #define SIGN_EXT9		0xffffffffffffff00ul
 
 /*
+ *  sysctl settable hook which tells the kernel whether to honor the
+ *  IA64_THREAD_UAC_NOPRINT prctl.  Because this is user settable, we want
+ *  to allow the super user to enable/disable this for security reasons
+ *  (i.e. don't allow attacker to fill up logs with unaligned accesses).
+ */
+int honor_uac_noprint = 0;
+
+/*
  * For M-unit:
  *
  *  opcode |   m  |   x6    |
@@ -1331,7 +1339,8 @@ ia64_handle_unaligned (unsigned long ifa
 		if ((current->thread.flags & IA64_THREAD_UAC_SIGBUS) != 0)
 			goto force_sigbus;
 
-		if (!(current->thread.flags & IA64_THREAD_UAC_NOPRINT)
+		if (honor_uac_noprint > 0 &&
+		    !(current->thread.flags & IA64_THREAD_UAC_NOPRINT)
 		    && within_logging_rate_limit())
 		{
 			char buf[200];	/* comm[] is at most 16 bytes... */
@@ -1340,17 +1349,11 @@ ia64_handle_unaligned (unsigned long ifa
 			len = sprintf(buf, "%s(%d): unaligned access to 0x%016lx, "
 				      "ip=0x%016lx\n\r", current->comm, current->pid,
 				      ifa, regs->cr_iip + ipsr->ri);
-			/*
-			 * Don't call tty_write_message() if we're in the kernel; we might
-			 * be holding locks...
-			 */
-			if (user_mode(regs))
-				tty_write_message(current->tty, buf);
 			buf[len-1] = '\0';	/* drop '\r' */
 			printk(KERN_WARNING "%s", buf);	/* watch for command names containing %s */
 		}
 	} else {
-		if (within_logging_rate_limit())
+		if (honor_uac_noprint >= 0 && within_logging_rate_limit())
 			printk(KERN_WARNING "kernel unaligned access to 0x%016lx, ip=0x%016lx\n",
 			       ifa, regs->cr_iip + ipsr->ri);
 		set_fs(KERNEL_DS);
diff -urNp linux-260/arch/ia64/kernel/unwind.c linux-262/arch/ia64/kernel/unwind.c
--- linux-260/arch/ia64/kernel/unwind.c
+++ linux-262/arch/ia64/kernel/unwind.c
@@ -1658,7 +1658,7 @@ run_script (struct unw_script *script, s
 			if (!state->pri_unat_loc)
 				state->pri_unat_loc = &state->sw->ar_unat;
 			/* register off. is a multiple of 8, so the least 3 bits (type) are 0 */
-			s[dst+1] = (*state->pri_unat_loc - s[dst]) | UNW_NAT_MEMSTK;
+			s[dst+1] = ((unsigned long) state->pri_unat_loc - s[dst]) | UNW_NAT_MEMSTK;
 			break;
 
 		      case UNW_INSN_SETNAT_TYPE:
@@ -1816,7 +1816,7 @@ unw_unwind (struct unw_frame_info *info)
 int
 unw_unwind_to_user (struct unw_frame_info *info)
 {
-	unsigned long ip;
+	unsigned long ip, sp;
 
 	while (unw_unwind(info) >= 0) {
 		if (unw_get_rp(info, &ip) < 0) {
@@ -1828,6 +1828,9 @@ unw_unwind_to_user (struct unw_frame_inf
 		 * We don't have unwind info for the gate page, so we consider that part
 		 * of user-space for the purpose of unwinding.
 		 */
+		unw_get_sp(info, &sp);
+		if (sp >= (unsigned long)info->task + IA64_STK_OFFSET)
+			break;
 		if (ip < GATE_ADDR + PAGE_SIZE)
 			return 0;
 	}
diff -urNp linux-260/arch/ia64/lib/memcpy_mck.S linux-262/arch/ia64/lib/memcpy_mck.S
--- linux-260/arch/ia64/lib/memcpy_mck.S
+++ linux-262/arch/ia64/lib/memcpy_mck.S
@@ -163,7 +163,7 @@ GLOBAL_ENTRY(__copy_user)
 	mov	ar.ec=2
 (p10)	br.dpnt.few .aligned_src_tail
 	;;
-	.align 32
+//	.align 32
 1:
 EX(.ex_handler, (p16)	ld8	r34=[src0],16)
 EK(.ex_handler, (p16)	ld8	r38=[src1],16)
@@ -320,7 +320,7 @@ EK(.ex_handler,	(p[D])	st8 [dst1] = t15,
 (p7)	mov	ar.lc = r21
 (p8)	mov	ar.lc = r0
 	;;
-	.align 32
+//	.align 32
 1:	lfetch	  [src_pre_mem], 128
 	lfetch.excl [dst_pre_mem], 128
 	br.cloop.dptk.few 1b
@@ -526,7 +526,7 @@ EK(.ex_handler,  (p17)	st8	[dst1]=r39,8)
 		 shrp	r21=r22,r38,shift;	/* speculative work */			\
 		 br.sptk.few .unaligned_src_tail /* branch out of jump table */		\
 		 ;;
-	.align 32
+//	.align 32
 .jump_table:
 	COPYU(8)	// unaligned cases
 .jmp1:
diff -urNp linux-260/arch/ia64/lib/memset.S linux-262/arch/ia64/lib/memset.S
--- linux-260/arch/ia64/lib/memset.S
+++ linux-262/arch/ia64/lib/memset.S
@@ -125,7 +125,7 @@ GLOBAL_ENTRY(memset)
 (p_zr)	br.cond.dptk.many .l1b			// Jump to use stf.spill
 ;; }
 
-	.align 32 // -------------------------- //  L1A: store ahead into cache lines; fill later
+//	.align 32 // -------------------------- //  L1A: store ahead into cache lines; fill later
 { .mmi
 	and	tmp = -(LINE_SIZE), cnt		// compute end of range
 	mov	ptr9 = ptr1			// used for prefetching
@@ -194,7 +194,7 @@ GLOBAL_ENTRY(memset)
 	br.cond.dpnt.many  .move_bytes_from_alignment	// Branch no. 3
 ;; }
 
-	.align 32
+//	.align 32
 .l1b:	// ------------------------------------ //  L1B: store ahead into cache lines; fill later
 { .mmi
 	and	tmp = -(LINE_SIZE), cnt		// compute end of range
@@ -261,7 +261,7 @@ GLOBAL_ENTRY(memset)
 	and	cnt = 0x1f, cnt			// compute the remaining cnt
 	mov.i   ar.lc = loopcnt
 ;; }
-	.align 32
+//	.align 32
 .l2:	// ------------------------------------ //  L2A:  store 32B in 2 cycles
 { .mmb
 	stf8	[ptr1] = fvalue, 8
diff -urNp linux-260/arch/ia64/mm/extable.c linux-262/arch/ia64/mm/extable.c
--- linux-260/arch/ia64/mm/extable.c
+++ linux-262/arch/ia64/mm/extable.c
@@ -38,6 +38,8 @@ search_one_table (const struct exception
 register unsigned long main_gp __asm__("gp");
 #endif
 
+extern spinlock_t modlist_lock;
+
 struct exception_fixup
 search_exception_table (unsigned long addr)
 {
@@ -53,8 +55,10 @@ search_exception_table (unsigned long ad
 #else
 	struct archdata *archdata;
 	struct module *mp;
-
+	unsigned long flags;
+	
 	/* The kernel is the last "module" -- no need to treat it special. */
+	spin_lock_irqsave(&modlist_lock, flags);
 	for (mp = module_list; mp; mp = mp->next) {
 		if (!mp->ex_table_start)
 			continue;
@@ -65,9 +69,10 @@ search_exception_table (unsigned long ad
 					 addr, (unsigned long) archdata->gp);
 		if (entry) {
 			fix.cont = entry->cont + (unsigned long) archdata->gp;
-			return fix;
+			break;
 		}
 	}
+	spin_unlock_irqrestore(&modlist_lock, flags);
 #endif
 	return fix;
 }
diff -urNp linux-260/arch/ia64/mm/fault.c linux-262/arch/ia64/mm/fault.c
--- linux-260/arch/ia64/mm/fault.c
+++ linux-262/arch/ia64/mm/fault.c
@@ -43,6 +43,33 @@ expand_backing_store (struct vm_area_str
 	return 0;
 }
 
+/*
+ * Return TRUE if ADDRESS points at a page in the kernel's mapped segment
+ * (inside region 5, on ia64) and that page is present.
+ */
+static int
+mapped_kernel_page_is_present (unsigned long address)
+{
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	pgd = pgd_offset_k(address);
+	if (pgd_none(*pgd) || pgd_bad(*pgd))
+		return 0;
+
+	pmd = pmd_offset(pgd, address);
+	if (pmd_none(*pmd) || pmd_bad(*pmd))
+		return 0;
+
+	ptep = pte_offset_kernel(pmd, address);
+	if (!ptep)
+		return 0;
+
+	pte = *ptep;
+	return pte_present(pte);
+}
+
 void
 ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *regs)
 {
@@ -177,6 +204,16 @@ ia64_do_page_fault (unsigned long addres
 		return;
 	}
 
+	/*
+	 * Since we have no vma's for region 5, we might get here even if the address is
+	 * valid, due to the VHPT walker inserting a non present translation that becomes
+	 * stale. If that happens, the non present fault handler already purged the stale
+	 * translation, which fixed the problem. So, we check to see if the translation is
+	 * valid, and return if it is.
+	 */
+	if (REGION_NUMBER(address) == 5 && mapped_kernel_page_is_present(address))
+		return;
+
 	if (done_with_exception(regs))
 		return;
 
diff -urNp linux-260/arch/ia64/mm/init.c linux-262/arch/ia64/mm/init.c
--- linux-260/arch/ia64/mm/init.c
+++ linux-262/arch/ia64/mm/init.c
@@ -186,11 +186,39 @@ si_meminfo (struct sysinfo *val)
 	return;
 }
 
+struct show_mem_stats {
+	int total;
+	int reserved;
+	int shared;
+	int cached;
+};
+
+static int
+get_show_mem_stats (u64 start, u64 end, void *arg)
+{
+	struct show_mem_stats *s = arg;
+	struct page *pg;
+
+	for (pg = virt_to_page((void *)start);
+	     pg < virt_to_page((void *)end); ++pg) {
+		s->total++;
+		if (PageReserved(pg))
+			s->reserved++;
+		else if (PageSwapCache(pg))
+			s->cached++;
+		else if (page_count(pg))
+			s->shared += page_count(pg) - 1;
+	}
+
+	return 0;
+}
+
 void
 show_mem(void)
 {
 	int i, total = 0, reserved = 0;
 	int shared = 0, cached = 0;
+	struct show_mem_stats stats;
 
 	printk("Mem-info:\n");
 	show_free_areas();
@@ -222,22 +250,12 @@ show_mem(void)
 	}
 #else /* !CONFIG_DISCONTIGMEM */
 	printk("Free swap:       %6dkB\n", nr_swap_pages<<(PAGE_SHIFT-10));
-	i = max_mapnr;
-	while (i-- > 0) {
-		if (!VALID_PAGE(mem_map + i))
-			continue;
-		total++;
-		if (PageReserved(mem_map+i))
-			reserved++;
-		else if (PageSwapCache(mem_map+i))
-			cached++;
-		else if (page_count(mem_map + i))
-			shared += page_count(mem_map + i) - 1;
-	}
-	printk("%d pages of RAM\n", total);
-	printk("%d reserved pages\n", reserved);
-	printk("%d pages shared\n", shared);
-	printk("%d pages swap cached\n", cached);
+	memset(&stats, 0, sizeof(struct show_mem_stats));
+	efi_memmap_walk(get_show_mem_stats, &stats);
+	printk("%d pages of RAM\n", stats.total);
+	printk("%d reserved pages\n", stats.reserved);
+	printk("%d pages shared\n", stats.shared);
+	printk("%d pages swap cached\n", stats.cached);
 	printk("%ld pages in page table cache\n", pgtable_cache_size);
 	show_buffers();
 #endif /* !CONFIG_DISCONTIGMEM */
@@ -365,7 +383,9 @@ create_mem_map_page_table (u64 start, u6
 	pgd_t *pgd;
 	pmd_t *pmd;
 	pte_t *pte;
+	static void *lastpage;
 
+	lastpage = __pa(MAX_DMA_ADDRESS);
 	/* should we use platform_map_nr here? */
 
 	map_start = vmem_map + MAP_NR_DENSE(start);
@@ -384,9 +404,10 @@ create_mem_map_page_table (u64 start, u6
 			pmd_populate(&init_mm, pmd, alloc_bootmem_pages(PAGE_SIZE));
 		pte = pte_offset(pmd, address);
 
-		if (pte_none(*pte))
-			set_pte(pte, mk_pte_phys(__pa(alloc_bootmem_pages(PAGE_SIZE)),
-						 PAGE_KERNEL));
+		if (pte_none(*pte)) {
+			lastpage = __pa(__alloc_bootmem(PAGE_SIZE, PAGE_SIZE, lastpage));	
+			set_pte(pte, mk_pte_phys(lastpage, PAGE_KERNEL));
+		}
  	}
  	return 0;
 }
diff -urNp linux-260/arch/ia64/tools/print_offsets.c linux-262/arch/ia64/tools/print_offsets.c
--- linux-260/arch/ia64/tools/print_offsets.c
+++ linux-262/arch/ia64/tools/print_offsets.c
@@ -57,6 +57,8 @@ tab[] =
     { "IA64_TASK_PROCESSOR_OFFSET",	offsetof (struct task_struct, processor) },
     { "IA64_TASK_THREAD_OFFSET",	offsetof (struct task_struct, thread) },
     { "IA64_TASK_THREAD_KSP_OFFSET",	offsetof (struct task_struct, thread.ksp) },
+    { "IA64_TASK_THREAD_CSD_OFFSET",	offsetof (struct task_struct, thread.csd) },
+    { "IA64_TASK_THREAD_SSD_OFFSET",	offsetof (struct task_struct, thread.ssd) },
 #ifdef CONFIG_PERFMON
     { "IA64_TASK_PFM_OVFL_BLOCK_RESET_OFFSET",
 					offsetof(struct task_struct, thread.pfm_ovfl_block_reset) },
diff -urNp linux-260/arch/ia64/vmlinux.lds.S linux-262/arch/ia64/vmlinux.lds.S
--- linux-260/arch/ia64/vmlinux.lds.S
+++ linux-262/arch/ia64/vmlinux.lds.S
@@ -7,6 +7,10 @@
 OUTPUT_FORMAT("elf64-ia64-little")
 OUTPUT_ARCH(ia64)
 ENTRY(phys_start)
+PHDRS
+{
+  rwe_segment PT_LOAD;
+}
 SECTIONS
 {
   /* Sections to be discarded */
@@ -31,7 +35,7 @@ SECTIONS
     {
 	*(.text.ivt)
 	*(.text)
-    }
+    } :rwe_segment
   .text2 : AT(ADDR(.text2) - PAGE_OFFSET)
 	{ *(.text2) }
 #ifdef CONFIG_SMP
diff -urNp linux-260/drivers/char/agp/agpgart_be.c linux-262/drivers/char/agp/agpgart_be.c
--- linux-260/drivers/char/agp/agpgart_be.c
+++ linux-262/drivers/char/agp/agpgart_be.c
@@ -4715,7 +4715,6 @@ static int __init hp_zx1_setup (struct p
 	agp_bridge.cleanup = hp_zx1_cleanup;
 	agp_bridge.tlb_flush = hp_zx1_tlbflush;
 	agp_bridge.mask_memory = hp_zx1_mask_memory;
-	agp_bridge.unmask_memory = hp_zx1_unmask_memory;
 	agp_bridge.agp_enable = agp_generic_agp_enable;
 	agp_bridge.cache_flush = global_cache_flush;
 	agp_bridge.create_gatt_table = hp_zx1_create_gatt_table;
diff -urNp linux-260/drivers/char/mem.c linux-262/drivers/char/mem.c
--- linux-260/drivers/char/mem.c
+++ linux-262/drivers/char/mem.c
@@ -177,6 +177,11 @@ static inline int noncached_address(unsi
 		  test_bit(X86_FEATURE_CYRIX_ARR, &boot_cpu_data.x86_capability) ||
 		  test_bit(X86_FEATURE_CENTAUR_MCR, &boot_cpu_data.x86_capability) )
 	  && addr >= __pa(high_memory);
+#elif defined(__ia64__)
+	struct page *page;
+
+	page = virt_to_page(__va(addr));
+	return !VALID_PAGE(page) || PageReserved(page);
 #else
 	return addr >= __pa(high_memory);
 #endif
@@ -191,7 +196,11 @@ static int mmap_mem(struct file * file, 
 	 * through a file pointer that was marked O_SYNC will be
 	 * done non-cached.
 	 */
+#ifdef __ia64__
+	if (noncached_address(offset))
+#else
 	if (noncached_address(offset) || (file->f_flags & O_SYNC))
+#endif
 		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	/* Don't try to swap out physical pages.. */
diff -urNp linux-260/drivers/video/vga16fb.c linux-262/drivers/video/vga16fb.c
--- linux-260/drivers/video/vga16fb.c
+++ linux-262/drivers/video/vga16fb.c
@@ -22,6 +22,9 @@
 #include <linux/selection.h>
 #include <linux/ioport.h>
 #include <linux/init.h>
+#ifdef CONFIG_IA64
+#include <linux/efi.h>
+#endif
 
 #include <asm/io.h>
 
@@ -894,6 +897,13 @@ int __init vga16fb_init(void)
 
 	printk(KERN_DEBUG "vga16fb: initializing\n");
 
+#ifdef CONFIG_IA64
+	if (efi_mem_type(VGA_FB_PHYS ) == EFI_CONVENTIONAL_MEMORY) {
+		printk(KERN_INFO "vga16fb: no VGA device\n");
+		return -ENODEV;
+	}
+#endif
+
 	/* XXX share VGA_FB_PHYS region with vgacon */
 
         vga16fb.video_vbase = ioremap(VGA_FB_PHYS, VGA_FB_PHYS_LEN);
diff -urNp linux-260/include/asm-generic/xor.h linux-262/include/asm-generic/xor.h
--- linux-260/include/asm-generic/xor.h
+++ linux-262/include/asm-generic/xor.h
@@ -13,6 +13,8 @@
  * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  */
 
+#include <asm/processor.h>
+
 static void
 xor_8regs_2(unsigned long bytes, unsigned long *p1, unsigned long *p2)
 {
@@ -299,6 +301,364 @@ xor_32regs_5(unsigned long bytes, unsign
 	} while (--lines > 0);
 }
 
+static void
+xor_8regs_p_2(unsigned long bytes, unsigned long *p1, unsigned long *p2)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+	prefetchw(p1);
+	prefetch(p2);
+
+	do {
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		p1[0] ^= p2[0];
+		p1[1] ^= p2[1];
+		p1[2] ^= p2[2];
+		p1[3] ^= p2[3];
+		p1[4] ^= p2[4];
+		p1[5] ^= p2[5];
+		p1[6] ^= p2[6];
+		p1[7] ^= p2[7];
+		p1 += 8;
+		p2 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_8regs_p_3(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+
+	do {
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+		p1[0] ^= p2[0] ^ p3[0];
+		p1[1] ^= p2[1] ^ p3[1];
+		p1[2] ^= p2[2] ^ p3[2];
+		p1[3] ^= p2[3] ^ p3[3];
+		p1[4] ^= p2[4] ^ p3[4];
+		p1[5] ^= p2[5] ^ p3[5];
+		p1[6] ^= p2[6] ^ p3[6];
+		p1[7] ^= p2[7] ^ p3[7];
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_8regs_p_4(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3, unsigned long *p4)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+	prefetch(p4);
+
+	do {
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+		prefetch(p4+8);
+
+		p1[0] ^= p2[0] ^ p3[0] ^ p4[0];
+		p1[1] ^= p2[1] ^ p3[1] ^ p4[1];
+		p1[2] ^= p2[2] ^ p3[2] ^ p4[2];
+		p1[3] ^= p2[3] ^ p3[3] ^ p4[3];
+		p1[4] ^= p2[4] ^ p3[4] ^ p4[4];
+		p1[5] ^= p2[5] ^ p3[5] ^ p4[5];
+		p1[6] ^= p2[6] ^ p3[6] ^ p4[6];
+		p1[7] ^= p2[7] ^ p3[7] ^ p4[7];
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+		p4 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_8regs_p_5(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3, unsigned long *p4, unsigned long *p5)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+	prefetch(p4);
+	prefetch(p5);
+
+	do {
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+		prefetch(p4+8);
+		prefetch(p5+8);
+
+		p1[0] ^= p2[0] ^ p3[0] ^ p4[0] ^ p5[0];
+		p1[1] ^= p2[1] ^ p3[1] ^ p4[1] ^ p5[1];
+		p1[2] ^= p2[2] ^ p3[2] ^ p4[2] ^ p5[2];
+		p1[3] ^= p2[3] ^ p3[3] ^ p4[3] ^ p5[3];
+		p1[4] ^= p2[4] ^ p3[4] ^ p4[4] ^ p5[4];
+		p1[5] ^= p2[5] ^ p3[5] ^ p4[5] ^ p5[5];
+		p1[6] ^= p2[6] ^ p3[6] ^ p4[6] ^ p5[6];
+		p1[7] ^= p2[7] ^ p3[7] ^ p4[7] ^ p5[7];
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+		p4 += 8;
+		p5 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_32regs_p_2(unsigned long bytes, unsigned long *p1, unsigned long *p2)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+
+	do {
+		register long d0, d1, d2, d3, d4, d5, d6, d7;
+
+		prefetchw(p1+8);
+		prefetch(p2+8);
+
+		d0 = p1[0];	/* Pull the stuff into registers	*/
+		d1 = p1[1];	/*  ... in bursts, if possible.		*/
+		d2 = p1[2];
+		d3 = p1[3];
+		d4 = p1[4];
+		d5 = p1[5];
+		d6 = p1[6];
+		d7 = p1[7];
+		d0 ^= p2[0];
+		d1 ^= p2[1];
+		d2 ^= p2[2];
+		d3 ^= p2[3];
+		d4 ^= p2[4];
+		d5 ^= p2[5];
+		d6 ^= p2[6];
+		d7 ^= p2[7];
+		p1[0] = d0;	/* Store the result (in burts)		*/
+		p1[1] = d1;
+		p1[2] = d2;
+		p1[3] = d3;
+		p1[4] = d4;
+		p1[5] = d5;
+		p1[6] = d6;
+		p1[7] = d7;
+		p1 += 8;
+		p2 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_32regs_p_3(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+
+	do {
+		register long d0, d1, d2, d3, d4, d5, d6, d7;
+
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+
+		d0 = p1[0];	/* Pull the stuff into registers	*/
+		d1 = p1[1];	/*  ... in bursts, if possible.		*/
+		d2 = p1[2];
+		d3 = p1[3];
+		d4 = p1[4];
+		d5 = p1[5];
+		d6 = p1[6];
+		d7 = p1[7];
+		d0 ^= p2[0];
+		d1 ^= p2[1];
+		d2 ^= p2[2];
+		d3 ^= p2[3];
+		d4 ^= p2[4];
+		d5 ^= p2[5];
+		d6 ^= p2[6];
+		d7 ^= p2[7];
+		d0 ^= p3[0];
+		d1 ^= p3[1];
+		d2 ^= p3[2];
+		d3 ^= p3[3];
+		d4 ^= p3[4];
+		d5 ^= p3[5];
+		d6 ^= p3[6];
+		d7 ^= p3[7];
+		p1[0] = d0;	/* Store the result (in burts)		*/
+		p1[1] = d1;
+		p1[2] = d2;
+		p1[3] = d3;
+		p1[4] = d4;
+		p1[5] = d5;
+		p1[6] = d6;
+		p1[7] = d7;
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_32regs_p_4(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3, unsigned long *p4)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+	prefetch(p4);
+
+	do {
+		register long d0, d1, d2, d3, d4, d5, d6, d7;
+
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+		prefetch(p4+8);
+
+		d0 = p1[0];	/* Pull the stuff into registers	*/
+		d1 = p1[1];	/*  ... in bursts, if possible.		*/
+		d2 = p1[2];
+		d3 = p1[3];
+		d4 = p1[4];
+		d5 = p1[5];
+		d6 = p1[6];
+		d7 = p1[7];
+		d0 ^= p2[0];
+		d1 ^= p2[1];
+		d2 ^= p2[2];
+		d3 ^= p2[3];
+		d4 ^= p2[4];
+		d5 ^= p2[5];
+		d6 ^= p2[6];
+		d7 ^= p2[7];
+		d0 ^= p3[0];
+		d1 ^= p3[1];
+		d2 ^= p3[2];
+		d3 ^= p3[3];
+		d4 ^= p3[4];
+		d5 ^= p3[5];
+		d6 ^= p3[6];
+		d7 ^= p3[7];
+		d0 ^= p4[0];
+		d1 ^= p4[1];
+		d2 ^= p4[2];
+		d3 ^= p4[3];
+		d4 ^= p4[4];
+		d5 ^= p4[5];
+		d6 ^= p4[6];
+		d7 ^= p4[7];
+		p1[0] = d0;	/* Store the result (in burts)		*/
+		p1[1] = d1;
+		p1[2] = d2;
+		p1[3] = d3;
+		p1[4] = d4;
+		p1[5] = d5;
+		p1[6] = d6;
+		p1[7] = d7;
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+		p4 += 8;
+	} while (--lines > 0);
+}
+
+static void
+xor_32regs_p_5(unsigned long bytes, unsigned long *p1, unsigned long *p2,
+	    unsigned long *p3, unsigned long *p4, unsigned long *p5)
+{
+	long lines = bytes / (sizeof (long)) / 8;
+
+	prefetchw(p1);
+	prefetch(p2);
+	prefetch(p3);
+	prefetch(p4);
+	prefetch(p5);
+
+	do {
+		register long d0, d1, d2, d3, d4, d5, d6, d7;
+
+		prefetchw(p1+8);
+		prefetch(p2+8);
+		prefetch(p3+8);
+		prefetch(p4+8);
+		prefetch(p5+8);
+
+		d0 = p1[0];	/* Pull the stuff into registers	*/
+		d1 = p1[1];	/*  ... in bursts, if possible.		*/
+		d2 = p1[2];
+		d3 = p1[3];
+		d4 = p1[4];
+		d5 = p1[5];
+		d6 = p1[6];
+		d7 = p1[7];
+		d0 ^= p2[0];
+		d1 ^= p2[1];
+		d2 ^= p2[2];
+		d3 ^= p2[3];
+		d4 ^= p2[4];
+		d5 ^= p2[5];
+		d6 ^= p2[6];
+		d7 ^= p2[7];
+		d0 ^= p3[0];
+		d1 ^= p3[1];
+		d2 ^= p3[2];
+		d3 ^= p3[3];
+		d4 ^= p3[4];
+		d5 ^= p3[5];
+		d6 ^= p3[6];
+		d7 ^= p3[7];
+		d0 ^= p4[0];
+		d1 ^= p4[1];
+		d2 ^= p4[2];
+		d3 ^= p4[3];
+		d4 ^= p4[4];
+		d5 ^= p4[5];
+		d6 ^= p4[6];
+		d7 ^= p4[7];
+		d0 ^= p5[0];
+		d1 ^= p5[1];
+		d2 ^= p5[2];
+		d3 ^= p5[3];
+		d4 ^= p5[4];
+		d5 ^= p5[5];
+		d6 ^= p5[6];
+		d7 ^= p5[7];
+		p1[0] = d0;	/* Store the result (in burts)		*/
+		p1[1] = d1;
+		p1[2] = d2;
+		p1[3] = d3;
+		p1[4] = d4;
+		p1[5] = d5;
+		p1[6] = d6;
+		p1[7] = d7;
+		p1 += 8;
+		p2 += 8;
+		p3 += 8;
+		p4 += 8;
+		p5 += 8;
+	} while (--lines > 0);
+}
+
 static struct xor_block_template xor_block_8regs = {
 	name: "8regs",
 	do_2: xor_8regs_2,
@@ -315,8 +675,26 @@ static struct xor_block_template xor_blo
 	do_5: xor_32regs_5,
 };
 
+static struct xor_block_template xor_block_8regs_p = {
+	name: "8regs_prefetch",
+	do_2: xor_8regs_p_2,
+	do_3: xor_8regs_p_3,
+	do_4: xor_8regs_p_4,
+	do_5: xor_8regs_p_5,
+};
+
+static struct xor_block_template xor_block_32regs_p = {
+	name: "32regs_prefetch",
+	do_2: xor_32regs_p_2,
+	do_3: xor_32regs_p_3,
+	do_4: xor_32regs_p_4,
+	do_5: xor_32regs_p_5,
+};
+
 #define XOR_TRY_TEMPLATES			\
 	do {					\
 		xor_speed(&xor_block_8regs);	\
+		xor_speed(&xor_block_8regs_p);	\
 		xor_speed(&xor_block_32regs);	\
+		xor_speed(&xor_block_32regs_p);	\
 	} while (0)
diff -urNp linux-260/include/asm-ia64/bitops.h linux-262/include/asm-ia64/bitops.h
--- linux-260/include/asm-ia64/bitops.h
+++ linux-262/include/asm-ia64/bitops.h
@@ -6,7 +6,6 @@
  *	David Mosberger-Tang <davidm@hpl.hp.com>
  */
 
-#include <linux/types.h>
 #include <asm/system.h>
 
 /**
@@ -276,11 +275,12 @@ test_bit (int nr, volatile void *addr)
 }
 
 /**
- * ffz - find the first zero bit in a long word
- * @x: The long word to find the bit in
+ * ffz - find the first zero bit in a memory region
+ * @x: The address to start the search at
  *
- * Returns the bit-number (0..63) of the first (least significant) zero bit.  Undefined if
- * no zero exists, so code should check against ~0UL first...
+ * Returns the bit-number (0..63) of the first (least significant) zero bit, not
+ * the number of the byte containing a bit.  Undefined if no zero exists, so
+ * code should check against ~0UL first...
  */
 static inline unsigned long
 ffz (unsigned long x)
@@ -291,26 +291,27 @@ ffz (unsigned long x)
 	return result;
 }
 
-/**
+/*
  * __ffs - find first bit in word.
  * @x: The word to search
  *
  * Undefined if no bit exists, so code should check against 0 first.
  */
+
 static __inline__ unsigned long
 __ffs (unsigned long x)
 {
-	unsigned long result;
+        unsigned long result;
 
-	__asm__ ("popcnt %0=%1" : "=r" (result) : "r" ((x - 1) & ~x));
-	return result;
+        __asm__ ("popcnt %0=%1" : "=r" (result) : "r" ((x - 1) & ~x));
+	        return result;
 }
 
 #ifdef __KERNEL__
 
 /*
  * find_last_zero_bit - find the last zero bit in a 64 bit quantity
- * @x: The value to search
+ * The value to search
  */
 static inline unsigned long
 ia64_fls (unsigned long x)
@@ -322,10 +323,19 @@ ia64_fls (unsigned long x)
 	return exp - 0xffff;
 }
 
-static int
-fls (int x)
+/*
+ * Every architecture must define this function. It's the fastest
+ * way of searching a 140-bit bitmap where the first 100 bits are
+ * unlikely to be set. It's guaranteed that at least one of the 140
+ * bits is cleared.
+ */
+static inline int _sched_find_first_bit(unsigned long *b)
 {
-	return ia64_fls((unsigned int) x);
+	if (unlikely(b[0]))
+		return __ffs(b[0]);
+	if (unlikely(b[1]))
+		return __ffs(b[1]) + 64;
+	return __ffs(b[2]) + 128;
 }
 
 /*
@@ -337,6 +347,7 @@ fls (int x)
 #define ffs(x)	__builtin_ffs(x)
 
 /*
+ * 
  * hweightN: returns the hamming weight (i.e. the number
  * of bits set) of a N-bit word
  */
@@ -441,12 +452,8 @@ find_next_bit (void *addr, unsigned long
 	return result + __ffs(tmp);
 }
 
-#define find_first_bit(addr, size) find_next_bit((addr), (size), 0)
-
 #ifdef __KERNEL__
 
-#define __clear_bit(nr, addr)        clear_bit(nr, addr)
-
 #define ext2_set_bit                 test_and_set_bit
 #define ext2_clear_bit               test_and_clear_bit
 #define ext2_test_bit                test_bit
@@ -470,6 +477,7 @@ sched_find_first_bit (unsigned long *b)
 	return __ffs(b[2]) + 128;
 }
 
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_IA64_BITOPS_H */
diff -urNp linux-260/include/asm-ia64/dma.h linux-262/include/asm-ia64/dma.h
--- linux-260/include/asm-ia64/dma.h
+++ linux-262/include/asm-ia64/dma.h
@@ -7,15 +7,192 @@
  */
 
 #include <linux/config.h>
-
+#include <linux/spinlock.h>
 #include <asm/io.h>		/* need byte IO */
+#include <linux/delay.h>
 
 extern unsigned long MAX_DMA_ADDRESS;
 
+#define MAX_DMA_CHANNELS 8
+
 #ifdef CONFIG_PCI
   extern int isa_dma_bridge_buggy;
 #else
 # define isa_dma_bridge_buggy 	(0)
 #endif
 
+#ifdef HAVE_REALLY_SLOW_DMA_CONTROLLER
+#define dma_outb	outb_p
+#else
+#define dma_outb	outb
+#endif
+
+#define dma_inb		inb
+
+#define MAX_DMA_CHANNELS	8
+
+/* DMA controllers */
+#define IO_DMA1_BASE	0x00	/* 8 bit slave DMA, channels 0..3 */
+#define IO_DMA2_BASE	0xC0	/* 16 bit master DMA, ch 4(=slave input)..7 */
+
+/* DMA controller registers */
+#define DMA1_CMD_REG		0x08	/* command register (w) */
+#define DMA1_STAT_REG		0x08	/* status register (r) */
+#define DMA1_REQ_REG            0x09    /* request register (w) */
+#define DMA1_MASK_REG		0x0A	/* single-channel mask (w) */
+#define DMA1_MODE_REG		0x0B	/* mode register (w) */
+#define DMA1_CLEAR_FF_REG	0x0C	/* clear pointer flip-flop (w) */
+#define DMA1_TEMP_REG           0x0D    /* Temporary Register (r) */
+#define DMA1_RESET_REG		0x0D	/* Master Clear (w) */
+#define DMA1_CLR_MASK_REG       0x0E    /* Clear Mask */
+#define DMA1_MASK_ALL_REG       0x0F    /* all-channels mask (w) */
+
+#define DMA2_CMD_REG		0xD0	/* command register (w) */
+#define DMA2_STAT_REG		0xD0	/* status register (r) */
+#define DMA2_REQ_REG            0xD2    /* request register (w) */
+#define DMA2_MASK_REG		0xD4	/* single-channel mask (w) */
+#define DMA2_MODE_REG		0xD6	/* mode register (w) */
+#define DMA2_CLEAR_FF_REG	0xD8	/* clear pointer flip-flop (w) */
+#define DMA2_TEMP_REG           0xDA    /* Temporary Register (r) */
+#define DMA2_RESET_REG		0xDA	/* Master Clear (w) */
+#define DMA2_CLR_MASK_REG       0xDC    /* Clear Mask */
+#define DMA2_MASK_ALL_REG       0xDE    /* all-channels mask (w) */
+
+#define DMA_ADDR_0              0x00    /* DMA address registers */
+#define DMA_ADDR_1              0x02
+#define DMA_ADDR_2              0x04
+#define DMA_ADDR_3              0x06
+#define DMA_ADDR_4              0xC0
+#define DMA_ADDR_5              0xC4
+#define DMA_ADDR_6              0xC8
+#define DMA_ADDR_7              0xCC
+
+#define DMA_CNT_0               0x01    /* DMA count registers */
+#define DMA_CNT_1               0x03
+#define DMA_CNT_2               0x05
+#define DMA_CNT_3               0x07
+#define DMA_CNT_4               0xC2
+#define DMA_CNT_5               0xC6
+#define DMA_CNT_6               0xCA
+#define DMA_CNT_7               0xCE
+
+#define DMA_PAGE_0              0x87    /* DMA page registers */
+#define DMA_PAGE_1              0x83
+#define DMA_PAGE_2              0x81
+#define DMA_PAGE_3              0x82
+#define DMA_PAGE_5              0x8B
+#define DMA_PAGE_6              0x89
+#define DMA_PAGE_7              0x8A
+
+#define DMA_MODE_READ	0x44	/* I/O to memory, no autoinit, increment, single mode */
+#define DMA_MODE_WRITE	0x48	/* memory to I/O, no autoinit, increment, single mode */
+#define DMA_MODE_CASCADE 0xC0   /* pass thru DREQ->HRQ, DACK<-HLDA only */
+
+#define DMA_AUTOINIT	0x10
+
+extern spinlock_t  dma_spin_lock;
+
+static __inline__ unsigned long claim_dma_lock(void)
+{
+        unsigned long flags;
+        spin_lock_irqsave(&dma_spin_lock, flags);
+        return flags;
+}
+
+static __inline__ void release_dma_lock(unsigned long flags)
+{
+        spin_unlock_irqrestore(&dma_spin_lock, flags);
+}
+static __inline__ void enable_dma(unsigned int dmanr)
+{
+        unsigned char ucDmaCmd=0x00;
+
+        if (dmanr != 4)
+        {
+                dma_outb(0, DMA2_MASK_REG);  /* This may not be enabled */
+                dma_outb(ucDmaCmd, DMA2_CMD_REG);  /* Enable group */
+        }
+        if (dmanr<=3)
+        {
+                dma_outb(dmanr,  DMA1_MASK_REG);
+                dma_outb(ucDmaCmd, DMA1_CMD_REG);  /* Enable group */
+        } else
+        {
+                dma_outb(dmanr & 3,  DMA2_MASK_REG);
+        }
+}
+
+static __inline__ void disable_dma(unsigned int dmanr)
+{
+        if (dmanr<=3)
+                dma_outb(dmanr | 4,  DMA1_MASK_REG);
+        else
+                dma_outb((dmanr & 3) | 4,  DMA2_MASK_REG);
+}
+static __inline__ void clear_dma_ff(unsigned int dmanr)
+{
+        if (dmanr<=3)
+                dma_outb(0,  DMA1_CLEAR_FF_REG);
+        else
+                dma_outb(0,  DMA2_CLEAR_FF_REG);
+}
+
+static __inline__ void set_dma_mode(unsigned int dmanr, char mode)
+{
+        if (dmanr<=3)
+                dma_outb(mode | dmanr,  DMA1_MODE_REG);
+        else
+                dma_outb(mode | (dmanr&3),  DMA2_MODE_REG);
+}
+
+static __inline__ void set_dma_addr(unsigned int dmanr, unsigned int phys)
+{
+        if (dmanr <= 3)  {
+            dma_outb( phys & 0xff, ((dmanr&3)<<1) + IO_DMA1_BASE );
+            dma_outb( (phys>>8) & 0xff, ((dmanr&3)<<1) + IO_DMA1_BASE );
+        }  else  {
+            dma_outb( (phys>>1) & 0xff, ((dmanr&3)<<2) + IO_DMA2_BASE );
+            dma_outb( (phys>>9) & 0xff, ((dmanr&3)<<2) + IO_DMA2_BASE );
+        }
+}
+
+static __inline__ void set_dma_count(unsigned int dmanr, unsigned int count)
+{
+        count--;
+        if (dmanr <= 3)  {
+            dma_outb( count & 0xff, ((dmanr&3)<<1) + 1 + IO_DMA1_BASE );
+            dma_outb( (count>>8) & 0xff, ((dmanr&3)<<1) + 1 + IO_DMA1_BASE );
+        } else {
+            dma_outb( (count>>1) & 0xff, ((dmanr&3)<<2) + 2 + IO_DMA2_BASE );
+            dma_outb( (count>>9) & 0xff, ((dmanr&3)<<2) + 2 + IO_DMA2_BASE );
+        }
+}
+
+static __inline__ int get_dma_residue(unsigned int dmanr)
+{
+        unsigned int io_port = (dmanr<=3)? ((dmanr&3)<<1) + 1 + IO_DMA1_BASE
+                                         : ((dmanr&3)<<2) + 2 + IO_DMA2_BASE;
+
+        /* using short to get 16-bit wrap around */
+        unsigned short count;
+
+        count = 1 + dma_inb(io_port);
+        count += dma_inb(io_port) << 8;
+
+        return (dmanr <= 3)? count : (count<<1);
+}
+
+
+/* These are in kernel/dma.c: */
+extern int request_dma(unsigned int dmanr, const char * device_id);	/* reserve a DMA channel */
+extern void free_dma(unsigned int dmanr);	/* release it again */
+
+/* From PCI */
+
+#ifdef CONFIG_PCI
+extern int isa_dma_bridge_buggy;
+#else
+#define isa_dma_bridge_buggy 	(0)
+#endif
+
 #endif /* _ASM_IA64_DMA_H */
diff -urNp linux-260/include/asm-ia64/iosapic.h linux-262/include/asm-ia64/iosapic.h
--- linux-260/include/asm-ia64/iosapic.h
+++ linux-262/include/asm-ia64/iosapic.h
@@ -72,6 +72,7 @@ extern int iosapic_register_platform_int
 extern unsigned int iosapic_version (char *addr);
 
 extern void iosapic_pci_fixup (int);
+extern void iosapic_fixup_pci_interrupt (struct pci_dev *dev);
 
 # endif /* !__ASSEMBLY__ */
 #endif /* __ASM_IA64_IOSAPIC_H */
diff -urNp linux-260/include/asm-ia64/kmap_types.h linux-262/include/asm-ia64/kmap_types.h
--- linux-260/include/asm-ia64/kmap_types.h
+++ linux-262/include/asm-ia64/kmap_types.h
@@ -0,0 +1,22 @@
+#ifndef _ASM_KMAP_TYPES_H
+#define _ASM_KMAP_TYPES_H
+
+enum km_type {
+	KM_BOUNCE_READ,
+	KM_SKB_DATA,
+	KM_SKB_DATA_SOFTIRQ,
+	KM_USER0,
+	KM_USER1,
+	KM_IRQ0,
+	KM_IRQ1,
+	KM_BH_IRQ,
+	KM_PTE0,
+	KM_PTE1,
+	KM_PTE2,
+	KM_SOFTIRQ0,
+	KM_SOFTIRQ1,
+	KM_NETDUMP,
+	KM_TYPE_NR
+};
+
+#endif
diff -urNp linux-260/include/asm-ia64/mmu_context.h linux-262/include/asm-ia64/mmu_context.h
--- linux-260/include/asm-ia64/mmu_context.h
+++ linux-262/include/asm-ia64/mmu_context.h
@@ -47,12 +47,14 @@ enter_lazy_tlb (struct mm_struct *mm, st
 static inline mm_context_t
 get_mmu_context (struct mm_struct *mm)
 {
+	unsigned long flags;
+
 	mm_context_t context = mm->context;
 
 	if (context)
 		return context;
 
-	spin_lock(&ia64_ctx.lock);
+	spin_lock_irqsave(&ia64_ctx.lock, flags);
 	{
 		/* re-check, now that we've got the lock: */
 		context = mm->context;
@@ -62,7 +64,7 @@ get_mmu_context (struct mm_struct *mm)
 			mm->context = context = ia64_ctx.next++;
 		}
 	}
-	spin_unlock(&ia64_ctx.lock);
+	spin_unlock_irqrestore(&ia64_ctx.lock, flags);
 	return context;
 }
 
diff -urNp linux-260/include/asm-ia64/page.h linux-262/include/asm-ia64/page.h
--- linux-260/include/asm-ia64/page.h
+++ linux-262/include/asm-ia64/page.h
@@ -56,6 +56,8 @@ extern void copy_page (void *to, void *f
 # include <asm/machvec.h>
 # define virt_to_page(kaddr)	(mem_map + platform_map_nr(kaddr))
 # define page_to_phys(page)	((page - mem_map) << PAGE_SHIFT)
+# define page_to_pfn(page)      ((unsigned long)((page) - mem_map))
+# define pfn_valid(pfn)		(((pfn) < max_mapnr) && ia64_page_valid(mem_map + (pfn)))
 #elif defined (CONFIG_IA64_SGI_SN1)
 # ifndef CONFIG_DISCONTIGMEM
 #  define virt_to_page(kaddr)	(mem_map + MAP_NR_DENSE(kaddr))
diff -urNp linux-260/include/asm-ia64/param.h linux-262/include/asm-ia64/param.h
--- linux-260/include/asm-ia64/param.h
+++ linux-262/include/asm-ia64/param.h
@@ -20,6 +20,8 @@
 # define HZ	1024
 #endif
 
+#define hz_to_std(a) (a)
+
 #define EXEC_PAGESIZE	65536
 
 #ifndef NGROUPS
diff -urNp linux-260/include/asm-ia64/pci.h linux-262/include/asm-ia64/pci.h
--- linux-260/include/asm-ia64/pci.h
+++ linux-262/include/asm-ia64/pci.h
@@ -98,11 +98,10 @@ struct pci_controller {
 	void *acpi_handle;
 	void *iommu;
 	int segment;
-
+	u64 mem_offset;
+	void *platform_data;
 	unsigned int windows;
 	struct pci_window *window;
-
-	void *platform_data;
 };
 
 #define PCI_CONTROLLER(dev) ((struct pci_controller *) dev->sysdata)
diff -urNp linux-260/include/asm-ia64/pgalloc.h linux-262/include/asm-ia64/pgalloc.h
--- linux-260/include/asm-ia64/pgalloc.h
+++ linux-262/include/asm-ia64/pgalloc.h
@@ -259,17 +259,17 @@ do {												\
 } while (0)
 
 static inline void
-clear_user_page (void *addr, unsigned long vaddr, struct page *page)
+clear_user_page (void *addr, unsigned long vaddr)
 {
 	clear_page(addr);
-	flush_dcache_page(page);
+	flush_dcache_page(virt_to_page(addr));
 }
 
 static inline void
-copy_user_page (void *to, void *from, unsigned long vaddr, struct page *page)
+copy_user_page (void *to, void *from, unsigned long vaddr)
 {
 	copy_page(to, from);
-	flush_dcache_page(page);
+	flush_dcache_page(virt_to_page(to));
 }
 
 /*
diff -urNp linux-260/include/asm-ia64/processor.h linux-262/include/asm-ia64/processor.h
--- linux-260/include/asm-ia64/processor.h
+++ linux-262/include/asm-ia64/processor.h
@@ -300,7 +300,8 @@ struct thread_struct {
 	INIT_THREAD_PM					\
 	{0, },				/* dbr */	\
 	{0, },				/* ibr */	\
-	{{{{0}}}, }			/* fph */	\
+	{{{{0}}}, },			/* fph */	\
+	-1				/* last_fph_cpu*/	\
 }
 
 #define start_thread(regs,new_ip,new_sp) do {							\
@@ -423,18 +424,23 @@ ia64_set_kr (unsigned long regnum, unsig
 	      case 7: asm volatile ("mov ar.k7=%0" :: "r"(r)); break;
 	}
 }
+/* Return TRUE if task T owns the fph partition of the CPU we're running on. */
+#define ia64_is_local_fpu_owner(t)								\
+({												\
+	struct task_struct *__ia64_islfo_task = (t);						\
+	(__ia64_islfo_task->thread.last_fph_cpu == smp_processor_id()				\
+	 && __ia64_islfo_task == (struct task_struct *) ia64_get_kr(IA64_KR_FPU_OWNER));	\
+})
+
+/* Mark task T as owning the fph partition of the CPU we're running on. */
+#define ia64_set_local_fpu_owner(t) do {						\
+	struct task_struct *__ia64_slfo_task = (t);					\
+	__ia64_slfo_task->thread.last_fph_cpu = smp_processor_id();			\
+	ia64_set_kr(IA64_KR_FPU_OWNER, (unsigned long) __ia64_slfo_task);		\
+} while (0)
 
-static inline struct task_struct *
-ia64_get_fpu_owner (void)
-{
-	return (struct task_struct *) ia64_get_kr(IA64_KR_FPU_OWNER);
-}
-
-static inline void
-ia64_set_fpu_owner (struct task_struct *t)
-{
-	ia64_set_kr(IA64_KR_FPU_OWNER, (unsigned long) t);
-}
+/* Mark the fph partition of task T as being invalid on all CPUs.  */
+#define ia64_drop_fpu(t)	((t)->thread.last_fph_cpu = -1)
 
 extern void __ia64_init_fpu (void);
 extern void __ia64_save_fpu (struct ia64_fpreg *fph);
@@ -744,11 +750,10 @@ thread_saved_pc (struct thread_struct *t
 
 #define THREAD_SIZE	IA64_STK_OFFSET
 /* NOTE: The task struct and the stacks are allocated together.  */
-#define alloc_task_struct() \
+#define __alloc_task_struct() \
         ((struct task_struct *) __get_free_pages(GFP_KERNEL, IA64_TASK_STRUCT_LOG_NUM_PAGES))
-#define free_task_struct(p)	free_pages((unsigned long)(p), IA64_TASK_STRUCT_LOG_NUM_PAGES)
-#define get_task_struct(tsk)	atomic_inc(&virt_to_page(tsk)->count)
-
+#define __free_task_struct(p)     free_pages((unsigned long)(p),1)
+	
 #define init_task	(init_task_union.task)
 #define init_stack	(init_task_union.stack)
 
diff -urNp linux-260/include/asm-ia64/spinlock.h linux-262/include/asm-ia64/spinlock.h
--- linux-260/include/asm-ia64/spinlock.h
+++ linux-262/include/asm-ia64/spinlock.h
@@ -87,6 +87,8 @@ typedef struct {
 	"ld4 r2 = [%0]\n"					\
 	";;\n"							\
 	"cmp4.eq p0,p7 = r0,r2\n"				\
+	";;\n"							\
+	"(p7) hint @pause\n"					\
 	"(p7) br.cond.spnt.few 1b \n"				\
 	"cmpxchg4.acq r2 = [%0], r29, ar.ccv\n"			\
 	";;\n"							\
@@ -110,6 +112,8 @@ typedef struct {
 
 #define rwlock_init(x) do { *(x) = RW_LOCK_UNLOCKED; } while(0)
 
+#define rwlock_is_locked(x) (*(volatile int *) (x) != 0)
+
 #define read_lock(rw)								\
 do {										\
 	int tmp = 0;								\
@@ -123,6 +127,8 @@ do {										\
 			      "3:\tld4.acq %0 = [%1]\n"				\
 			      ";;\n"						\
 			      "tbit.nz p6,p0 = %0, 31\n"			\
+			      ";;\n"						\
+			      "(p6) hint @pause\n"				\
 			      "(p6) br.cond.sptk.few 3b\n"			\
 			      "br.cond.sptk.few 1b\n"				\
 			      ";;\n"						\
@@ -150,6 +156,8 @@ do {										\
 		"ld4 r2 = [%0]\n"						\
 		";;\n"								\
 		"cmp4.eq p0,p7 = r0,r2\n"					\
+		";;\n"								\
+		"(p7) hint @pause\n"						\
 		"(p7) br.cond.spnt.few 1b \n"					\
 		"cmpxchg4.acq r2 = [%0], r29, ar.ccv\n"				\
 		";;\n"								\
diff -urNp linux-260/include/asm-ia64/system.h linux-262/include/asm-ia64/system.h
--- linux-260/include/asm-ia64/system.h
+++ linux-262/include/asm-ia64/system.h
@@ -104,7 +104,7 @@ ia64_insn_group_barrier (void)
 #define set_mb(var, value)	do { (var) = (value); mb(); } while (0)
 #define set_wmb(var, value)	do { (var) = (value); mb(); } while (0)
 
-#define safe_halt()         ia64_pal_halt(1)                /* PAL_HALT */
+#define safe_halt()         ia64_pal_halt_light()    /* PAL_HALT_LIGHT */
 
 /*
  * The group barrier in front of the rsm & ssm are necessary to ensure
@@ -407,14 +407,17 @@ extern void ia64_load_extra (struct task
 # define PERFMON_IS_SYSWIDE() (0)
 #endif
 
-#define __switch_to(prev,next,last) do {						\
-	if (((prev)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID))	\
-	    || IS_IA32_PROCESS(ia64_task_regs(prev)) || PERFMON_IS_SYSWIDE())	\
-		ia64_save_extra(prev);							\
-	if (((next)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID))	\
-	    || IS_IA32_PROCESS(ia64_task_regs(next)) || PERFMON_IS_SYSWIDE())	\
-		ia64_load_extra(next);							\
-	(last) = ia64_switch_to((next));						\
+#define IA64_HAS_EXTRA_STATE(t)							\
+	((t)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID)	\
+	 || IS_IA32_PROCESS(ia64_task_regs(t)) || PERFMON_IS_SYSWIDE())
+
+#define __switch_to(prev,next,last) do {							 \
+	if (IA64_HAS_EXTRA_STATE(prev))								 \
+		ia64_save_extra(prev);								 \
+	if (IA64_HAS_EXTRA_STATE(next))								 \
+		ia64_load_extra(next);								 \
+	ia64_psr(ia64_task_regs(next))->dfh = !ia64_is_local_fpu_owner(next);			 \
+	(last) = ia64_switch_to((next));							 \
 } while (0)
 
 #ifdef CONFIG_SMP
@@ -423,37 +426,28 @@ extern void ia64_load_extra (struct task
 #define arch_consoles_callable() (cpu_online_map & (1UL << smp_processor_id()))
 
 /*
- * In the SMP case, we save the fph state when context-switching
- * away from a thread that modified fph.  This way, when the thread
- * gets scheduled on another CPU, the CPU can pick up the state from
- * task->thread.fph, avoiding the complication of having to fetch
- * the latest fph state from another CPU.
+ * In the SMP case, we save the fph state when context-switching away from a thread that
+ * modified fph.  This way, when the thread gets scheduled on another CPU, the CPU can
+ * pick up the state from task->thread.fph, avoiding the complication of having to fetch
+ * the latest fph state from another CPU.  In other words: eager save, lazy restore.
  */
-# define switch_to(prev,next,last) do {					\
-	if (ia64_psr(ia64_task_regs(prev))->mfh) {			\
-		ia64_psr(ia64_task_regs(prev))->mfh = 0;		\
-		(prev)->thread.flags |= IA64_THREAD_FPH_VALID;		\
-		__ia64_save_fpu((prev)->thread.fph);			\
-		(prev)->thread.last_fph_cpu = smp_processor_id();	\
-	}								\
-	if ((next)->thread.flags & IA64_THREAD_FPH_VALID) {		\
-		if (((next)->thread.last_fph_cpu == smp_processor_id()) \
-		    && (ia64_get_fpu_owner() == next)) {		\
-			ia64_psr(ia64_task_regs(next))->dfh = 0;	\
-			ia64_psr(ia64_task_regs(next))->mfh = 0;	\
-		} else {						\
-			ia64_psr(ia64_task_regs(next))->dfh = 1;	\
-		}							\
-	}								\
-	__switch_to(prev,next,last);					\
-  } while (0)
-#else
-# define switch_to(prev,next,last) do {					\
-	ia64_psr(ia64_task_regs(next))->dfh = (ia64_get_fpu_owner() != (next));	\
-	__switch_to(prev,next,last);					\
+# define switch_to(prev,next,last) do {						\
+	if (ia64_psr(ia64_task_regs(prev))->mfh 				\
+		&& ia64_is_local_fpu_owner(prev)) {				\
+		ia64_psr(ia64_task_regs(prev))->mfh = 0;			\
+		(prev)->thread.flags |= IA64_THREAD_FPH_VALID;			\
+		__ia64_save_fpu((prev)->thread.fph);				\
+	}									\
+	__switch_to(prev, next, last);						\
 } while (0)
+#else
+# define switch_to(prev,next,last)	__switch_to(prev, next, last)
 #endif
 
+#define prepare_arch_switch(rq, next)  do {spin_lock(&(next)->switch_lock); spin_unlock(&(rq)->lock);} while(0)
+#define finish_arch_switch(rq, prev)   do {spin_unlock_irq(&(prev)->switch_lock);} while(0)
+#define task_running(p)    ((CPU_CURR_PTR(task_cpu(p)) == (p)) || spin_is_locked(&(p)->switch_lock))
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */
diff -urNp linux-260/include/asm-ia64/types.h linux-262/include/asm-ia64/types.h
--- linux-260/include/asm-ia64/types.h
+++ linux-262/include/asm-ia64/types.h
@@ -63,6 +63,7 @@ typedef __u64 u64;
 /* DMA addresses are 64-bits wide, in general.  */
 
 typedef u64 dma_addr_t;
+typedef u64 dma64_addr_t;
 
 # endif /* __KERNEL__ */
 #endif /* !__ASSEMBLY__ */
diff -urNp linux-260/include/asm-ia64/uaccess.h linux-262/include/asm-ia64/uaccess.h
--- linux-260/include/asm-ia64/uaccess.h
+++ linux-262/include/asm-ia64/uaccess.h
@@ -165,12 +165,14 @@ extern void __put_user_unknown (void);
 
 #define __put_user_nocheck(x,ptr,size)		\
 ({						\
+	__typeof__(*(ptr)) *__pu_addr = (ptr);	\
+	__typeof__(x) __x = (x);		\
 	register long __pu_err asm ("r8") = 0;	\
 	switch (size) {				\
-	  case 1: __put_user_8(x,ptr); break;	\
-	  case 2: __put_user_16(x,ptr); break;	\
-	  case 4: __put_user_32(x,ptr); break;	\
-	  case 8: __put_user_64(x,ptr); break;	\
+	  case 1: __put_user_8(__x,__pu_addr); break;	\
+	  case 2: __put_user_16(__x,__pu_addr); break;	\
+	  case 4: __put_user_32(__x,__pu_addr); break;	\
+	  case 8: __put_user_64(__x,__pu_addr); break;	\
 	  default: __put_user_unknown(); break;	\
 	}					\
 	__pu_err;				\
@@ -178,15 +180,16 @@ extern void __put_user_unknown (void);
 
 #define __put_user_check(x,ptr,size,segment)			\
 ({								\
-	register long __pu_err asm ("r8") = -EFAULT;		\
 	__typeof__(*(ptr)) *__pu_addr = (ptr);			\
+	__typeof__(x) __x = (x);				\
+	register long __pu_err asm ("r8") = -EFAULT;		\
 	if (__access_ok((long)__pu_addr,size,segment)) {	\
 		__pu_err = 0;					\
 		switch (size) {					\
-		  case 1: __put_user_8(x,__pu_addr); break;	\
-		  case 2: __put_user_16(x,__pu_addr); break;	\
-		  case 4: __put_user_32(x,__pu_addr); break;	\
-		  case 8: __put_user_64(x,__pu_addr); break;	\
+		  case 1: __put_user_8(__x,__pu_addr); break;	\
+		  case 2: __put_user_16(__x,__pu_addr); break;	\
+		  case 4: __put_user_32(__x,__pu_addr); break;	\
+		  case 8: __put_user_64(__x,__pu_addr); break;	\
 		  default: __put_user_unknown(); break;		\
 		}						\
 	}							\
diff -urNp linux-260/include/asm-ia64/xor.h linux-262/include/asm-ia64/xor.h
--- linux-260/include/asm-ia64/xor.h
+++ linux-262/include/asm-ia64/xor.h
@@ -13,6 +13,7 @@
  * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  */
 
+#include <asm-generic/xor.h>
 
 extern void xor_ia64_2(unsigned long, unsigned long *, unsigned long *);
 extern void xor_ia64_3(unsigned long, unsigned long *, unsigned long *,
@@ -280,4 +281,11 @@ static struct xor_block_template xor_blo
 	do_5: xor_ia64_5,
 };
 
-#define XOR_TRY_TEMPLATES	xor_speed(&xor_block_ia64)
+#define XOR_TRY_TEMPLATES     do { \
+		xor_speed(&xor_block_8regs); \
+		xor_speed(&xor_block_8regs_p); \
+		xor_speed(&xor_block_32regs); \
+		xor_speed(&xor_block_32regs_p); \
+		xor_speed(&xor_block_ia64); \
+	} while(0)
+
diff -urNp linux-260/include/linux/pci_ids.h linux-262/include/linux/pci_ids.h
--- linux-260/include/linux/pci_ids.h
+++ linux-262/include/linux/pci_ids.h
@@ -545,6 +545,7 @@
 #define PCI_DEVICE_ID_HP_ZX1_SBA	0x1229
 #define PCI_DEVICE_ID_HP_ZX1_IOC	0x122a
 #define PCI_DEVICE_ID_HP_ZX1_LBA	0x122e
+#define PCI_DEVICE_ID_HP_SX1000_IOC	0x127c
 
 #define PCI_VENDOR_ID_PCTECH		0x1042
 #define PCI_DEVICE_ID_PCTECH_RZ1000	0x1000
diff -urNp linux-260/include/linux/sysctl.h linux-262/include/linux/sysctl.h
--- linux-260/include/linux/sysctl.h
+++ linux-262/include/linux/sysctl.h
@@ -127,6 +127,7 @@ enum
 	KERN_PID_MAX=55,	/* int: max PID value of processes */
  	KERN_CORE_PATTERN=56,	/* string: pattern for core-files */
  	KERN_CORE_SETUID=58,	/* int: set to allow core dumps of setuid apps */
+	KERN_HONOR_UAC_NOPRINT=61, /* int: allow access to UAC_NOPRINT prctl */
 };
 
 
diff -urNp linux-260/kernel/sysctl.c linux-262/kernel/sysctl.c
--- linux-260/kernel/sysctl.c
+++ linux-262/kernel/sysctl.c
@@ -95,6 +95,10 @@ extern int sysctl_ieee_emulation_warning
 extern int sysctl_userprocess_debug;
 #endif
 
+#ifdef CONFIG_IA64
+extern int honor_uac_noprint;
+#endif
+
 #ifdef CONFIG_PPC32
 extern unsigned long zero_paged_on, powersave_nap;
 int proc_dol2crvec(ctl_table *table, int write, struct file *filp,
@@ -283,6 +287,10 @@ static ctl_table kern_table[] = {
 	{KERN_S390_USER_DEBUG_LOGGING,"userprocess_debug",
 	 &sysctl_userprocess_debug,sizeof(int),0644,NULL,&proc_dointvec},
 #endif
+#ifdef CONFIG_IA64
+	{KERN_HONOR_UAC_NOPRINT, "honor_uac_noprint_prctl", &honor_uac_noprint,
+	 sizeof(int), 0644, NULL, &proc_dointvec},
+#endif
 	{0}
 };
 
