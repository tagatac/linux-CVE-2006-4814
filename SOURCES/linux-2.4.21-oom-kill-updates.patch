diff -urNp linux-1228/include/linux/mm.h linux-1229/include/linux/mm.h
--- linux-1228/include/linux/mm.h
+++ linux-1229/include/linux/mm.h
@@ -838,6 +838,7 @@ extern struct page *filemap_nopage(struc
 #define __GFP_HIGHIO	0x80	/* Can start high mem physical IO? */
 #define __GFP_FS	0x100	/* Can call down to low-level FS? */
 #define __GFP_WIRED	0x200   /* Highmem bias and wired */
+#define __GFP_NUMA	0x400	/* NUMA allocation */
 
 #define GFP_NOHIGHIO	(__GFP_HIGH | __GFP_WAIT | __GFP_IO)
 #define GFP_NOIO	(__GFP_HIGH | __GFP_WAIT)
diff -urNp linux-1228/include/linux/sysctl.h linux-1229/include/linux/sysctl.h
--- linux-1228/include/linux/sysctl.h
+++ linux-1229/include/linux/sysctl.h
@@ -154,7 +154,10 @@ enum
 	VM_HUGETLB_POOL=23,	/* int: size of the hugetlb pool, in MB */
 	VM_INACTIVE_CLEAN_PERCENT=25, /* int: percent of inactive thats clean */
 	VM_SKIP_MAPPED_PAGES=27,/* int: don't reclaim pages w/active mappings */
+	VM_OOM_KILL=28,		/* int: limit on concurrent OOM kills */
 	VM_KSCAND_WORK_PERCENT=29, /* int: % of work on each kscand iteration */
+	VM_NUMA_ALLOCATOR=30,	/* NUMA memory allocation policy */
+	VM_DEFRAGMENT=31,	/* Tunable defragmentation iterations */
 };
 
 
diff -urNp linux-1228/kernel/sysctl.c linux-1229/kernel/sysctl.c
--- linux-1228/kernel/sysctl.c
+++ linux-1229/kernel/sysctl.c
@@ -329,7 +329,12 @@ static ctl_table kern_table[] = {
 
 extern int inactive_clean_percent;
 extern int skip_mapped_pages;
+extern int oom_kill_limit;
 extern int kscand_work_percent;
+extern int vm_defragment;
+#if defined(CONFIG_DISCONTIGMEM) && defined(CONFIG_NUMA)
+extern int numa_memory_allocator;
+#endif
 
 static ctl_table vm_table[] = {
 	{VM_BDFLUSH, "bdflush", &bdf_prm, 9*sizeof(int), 0644, NULL,
@@ -363,8 +368,16 @@ static ctl_table vm_table[] = {
 	{VM_SKIP_MAPPED_PAGES, "skip_mapped_pages",
 		&skip_mapped_pages, sizeof(skip_mapped_pages),
 		0644, NULL, &proc_dointvec},
+	{VM_OOM_KILL, "oom-kill", &oom_kill_limit, sizeof(oom_kill_limit),
+		0644, NULL, &proc_dointvec},
 	{VM_KSCAND_WORK_PERCENT, "kscand_work_percent", &kscand_work_percent,
 		sizeof(kscand_work_percent), 0644, NULL, &proc_dointvec},
+#if defined(CONFIG_DISCONTIGMEM) && defined(CONFIG_NUMA)
+	{VM_NUMA_ALLOCATOR, "numa_memory_allocator", &numa_memory_allocator,
+		sizeof(numa_memory_allocator), 0644, NULL, &proc_dointvec},
+#endif
+	{VM_DEFRAGMENT, "vm-defragment", &vm_defragment,
+		sizeof(vm_defragment), 0644, NULL, &proc_dointvec},
 	{0}
 };
 
diff -urNp linux-1228/mm/numa.c linux-1229/mm/numa.c
--- linux-1228/mm/numa.c
+++ linux-1229/mm/numa.c
@@ -88,6 +88,10 @@ static struct page * alloc_pages_pgdat(p
 	return __alloc_pages(gfp_mask, order, pgdat->node_zonelists + (gfp_mask & GFP_ZONEMASK));
 }
 
+#ifdef CONFIG_NUMA
+int numa_memory_allocator = 0;
+#endif
+
 /*
  * This can be refined. Currently, tries to do round robin, instead
  * should do concentratic circle search, starting from current node.
@@ -104,6 +108,31 @@ struct page * _alloc_pages(unsigned int 
 	if (order >= MAX_ORDER)
 		return NULL;
 #ifdef CONFIG_NUMA
+	/* since the numa allocator is tunable we skip it if its off */
+	if (!numa_memory_allocator)
+		goto std_allocator;
+	/* The first two loops will allocate memory from every node
+	 * down to pages_low. This prevents the system from swapping
+	 * like crazy on one or more nodes while others still have
+	 * lots of free memory
+	 */
+	start = temp = NODE_DATA(numa_node_id());
+	while (temp) {
+		if ((ret = alloc_pages_pgdat(temp, gfp_mask|__GFP_NUMA, order)))
+			return(ret);
+		temp = temp->node_next;
+	}
+	temp = pgdat_list;
+	while (temp != start) {
+		if ((ret = alloc_pages_pgdat(temp, gfp_mask|__GFP_NUMA, order)))
+			return(ret);
+		temp = temp->node_next;
+	}
+	/* The second two loops will allocate memory from every node
+	 * all the way down to and below pages_min thereby causing
+	 * lots of swapping but not until every node is below pages_low.
+	 */
+std_allocator:
 	temp = NODE_DATA(numa_node_id());
 #else
 	spin_lock_irqsave(&node_lock, flags);
diff -urNp linux-1228/mm/oom_kill.c linux-1229/mm/oom_kill.c
--- linux-1228/mm/oom_kill.c
+++ linux-1229/mm/oom_kill.c
@@ -23,6 +23,8 @@
 
 /* #define DEBUG */
 
+int oom_kill_limit = 1; /* limit on concurrent OOM kills (-1 => unlimited) */
+
 /**
  * int_sqrt - oom_kill.c internal function, rough approximation to sqrt
  * @x: integer of which to calculate the sqrt
@@ -38,7 +40,7 @@ static unsigned int int_sqrt(unsigned in
 }	
 
 /**
- * oom_badness - calculate a numeric value for how bad this task has been
+ * badness - calculate a numeric value for how bad this task has been
  * @p: task struct of which task we should calculate
  *
  * The formula used is relatively simple and documented inline in the
@@ -114,15 +116,23 @@ static int badness(struct task_struct *p
  *
  * (not docbooked, we don't want this one cluttering up the manual)
  */
-static struct task_struct * select_bad_process(void)
+static struct task_struct *select_bad_process(int *pending_countp)
 {
 	int maxpoints = 0;
 	struct task_struct *g, *p;
 	struct task_struct *chosen = NULL;
+	int pending, points;
 
+	pending = 0;
 	do_each_thread(g, p) {
 		if (p->pid) {
-			int points = badness(p);
+			if (p->flags & PF_MEMDIE) {
+				if (p->state < TASK_ZOMBIE &&
+				    thread_group_leader(p))
+					pending++;
+				continue;
+			}
+			points = badness(p);
 			if (points > maxpoints) {
 				chosen = p;
 				maxpoints = points;
@@ -130,6 +140,7 @@ static struct task_struct * select_bad_p
 		}
 	} while_each_thread(g, p);
 
+	*pending_countp = pending;
 	return chosen;
 }
 
@@ -138,10 +149,8 @@ static struct task_struct * select_bad_p
  * CAP_SYS_RAW_IO set, send SIGTERM instead (but it's unlikely that
  * we select a process with CAP_SYS_RAW_IO set).
  */
-void oom_kill_task(struct task_struct *p)
+static void __oom_kill_task(struct task_struct *p)
 {
-	printk(KERN_ERR "Out of Memory: Killed process %d (%s).\n", p->pid, p->comm);
-
 	/*
 	 * We give our sacrificial lamb high priority and access to
 	 * all the memory it needs. That way it should be able to
@@ -158,6 +167,24 @@ void oom_kill_task(struct task_struct *p
 	}
 }
 
+static struct mm_struct *oom_kill_task(struct task_struct *p)
+{
+	struct mm_struct *mm = get_task_mm(p);
+
+	if (!mm)
+		return NULL;
+
+	if (mm == &init_mm) {
+		mmput(mm);
+		return NULL;
+	}
+
+	printk(KERN_ERR "Out of Memory: Killed process %d (%s).\n",
+		p->tgid, p->comm);
+	__oom_kill_task(p);
+	return mm;
+}
+
 /**
  * oom_kill - kill the "best" process when we run out of memory
  *
@@ -170,24 +197,39 @@ static void oom_kill(void)
 {
 	struct task_struct *g, *p, *q;
 	extern wait_queue_head_t kswapd_done;
+	struct mm_struct *mm;
+	int pending;
 
 	/* print the memory stats whenever we OOM kill */
 	show_mem();
-
+retry:
 	read_lock(&tasklist_lock);
-	p = select_bad_process();
+	p = select_bad_process(&pending);
+
+	if (pending >= oom_kill_limit && oom_kill_limit >= 0) {
+		read_unlock(&tasklist_lock);
+		printk("Avoiding OOM kill due to %d pending\n", pending);
+		return;
+	}
 
 	/* Found nothing?!?! Either we hang forever, or we panic. */
 	if (p == NULL)
 		panic("Out of memory and no killable processes...\n");
 
+	mm = oom_kill_task(p);
+	if (!mm) {
+		read_unlock(&tasklist_lock);
+		goto retry;
+	}
 	/* kill all processes that share the ->mm (i.e. all threads) */
 	do_each_thread(g, q)
-		if (q->mm == p->mm && q->tgid == p->tgid)
-			oom_kill_task(q);
+		if (q->mm == mm && !(q->flags & PF_MEMDIE))
+			__oom_kill_task(q);
 	while_each_thread(g, q);
-
+	if (!p->mm)
+		printk("Fixed up OOM kill of mm-less task\n");
 	read_unlock(&tasklist_lock);
+	mmput(mm);
 
 	/* Chances are by this time our victim is sleeping on kswapd. */
 	wake_up(&kswapd_done);
@@ -202,8 +244,9 @@ static void oom_kill(void)
 	{
 		extern int numa_nodes;
 		if (numa_nodes > 1) {
-			printk(KERN_INFO "OOM kill occurred on an x86_64 NUMA system!\n");
-			printk(KERN_INFO "The numa=off boot option might help avoid this.\n");
+			printk("OOM kill occurred on an x86_64 NUMA system!\n");
+			printk("Setting /proc/sys/vm/numa_memory_allocator or\n");
+			printk("using the numa=off boot option might help avoid this.\n");
 		}
 	}
 #endif
@@ -215,9 +258,18 @@ static void oom_kill(void)
  */
 void out_of_memory(void)
 {
+	/*
+	 * oom_lock protects out_of_memory()'s static variables.
+	 * It's a global lock; this is not performance-critical.
+	 */
+	static spinlock_t oom_lock = SPIN_LOCK_UNLOCKED;
 	static unsigned long first, last, count, lastkill;
 	unsigned long now, since;
 
+	if (oom_kill_limit == 0)
+		return;
+
+	spin_lock(&oom_lock);
 	now = jiffies;
 	since = now - last;
 	last = now;
@@ -236,14 +288,14 @@ void out_of_memory(void)
 	 */
 	since = now - first;
 	if (since < HZ)
-		return;
+		goto out_unlock;
 
 	/*
 	 * If we have gotten only a few failures,
 	 * we're not really oom. 
 	 */
 	if (++count < 10)
-		return;
+		goto out_unlock;
 
 	/*
 	 * If we just killed a process, wait a while
@@ -252,15 +304,23 @@ void out_of_memory(void)
 	 */
 	since = now - lastkill;
 	if (since < HZ*5)
-		return;
+		goto out_unlock;
 
 	/*
 	 * Ok, really out of memory. Kill something.
 	 */
 	lastkill = now;
+
+	/* oom_kill() can sleep */
+	spin_unlock(&oom_lock);
 	oom_kill();
+	spin_lock(&oom_lock);
 
 reset:
-	first = now;
+	if (time_after(now, first))
+		first = now;
 	count = 0;
+
+out_unlock:
+	spin_unlock(&oom_lock);
 }
diff -urNp linux-1228/mm/page_alloc.c linux-1229/mm/page_alloc.c
--- linux-1228/mm/page_alloc.c
+++ linux-1229/mm/page_alloc.c
@@ -28,6 +28,8 @@
 int nr_swap_pages;
 pg_data_t *pgdat_list;
 
+int vm_defragment;
+
 /*
  *
  * The zone_table array is used to look up the address of the
@@ -551,6 +553,10 @@ try_again:
 	 */
 	wakeup_kswapd(gfp_mask);
 
+	/* skip to the next node below pages_low */
+	if (gfp_mask & __GFP_NUMA)
+		return NULL;
+
 	/*
 	 * After waking up kswapd, we try to allocate a page
 	 * from any zone which isn't critical yet.
@@ -656,6 +662,21 @@ try_again:
 			if (page)
 				return page;
 		}
+		if (order > 0 && vm_defragment) {
+			int try_harder = vm_defragment * 64;
+			while (z->inactive_clean_pages && try_harder-- > 0) {
+				struct page *page;
+				/* Move one page to the free list. */
+				page = reclaim_page(z);
+				if (!page)
+					break;
+				__free_page(page);
+				/* Try if the allocation succeeds. */
+				page = rmqueue(z, order);
+				if (page)
+					return page;
+			}
+		}
 	}
 	goto out_failed;
 
@@ -692,7 +713,7 @@ defragment_again:
 			 */
 			numpages = z->inactive_laundry_pages;
 			if (try_harder) {
-				numpages = 64;
+				numpages = try_harder * 64;
 				rebalance_inactive(20);
 				freed += rebalance_dirty_zone(z, numpages, mask);
 			}
@@ -726,9 +747,14 @@ defragment_again:
 				try_harder = 1;
 				goto defragment_again;
 			}
-			/* Try smaller allocations indefinately. */
+			/* Try smaller allocations indefinitely. */
 			if (order <= 2 && freed)
 				goto defragment_again;
+			/* Try medium allocations a tunable number of times. */
+			if (order < 5 && freed && try_harder <= vm_defragment) {
+				try_harder++;
+				goto defragment_again;
+			}
 		}
 	}
 
diff -urNp linux-1228/mm/vmscan.c linux-1229/mm/vmscan.c
--- linux-1228/mm/vmscan.c
+++ linux-1229/mm/vmscan.c
@@ -785,6 +785,7 @@ int rebalance_laundry_zone(struct zone_s
 	int max_loop;
 	int work_done = 0;
 	struct page * page;
+	unsigned long local_count;
 
 	max_loop = max_work;
 	if (max_loop < BATCH_WORK_AMOUNT)
@@ -807,19 +808,28 @@ int rebalance_laundry_zone(struct zone_s
 			 */
 			if ((gfp_mask & __GFP_WAIT) && (work_done < max_work)) {
 				int timed_out;
-				
+
 				/* Page is being freed, waiting on lru lock */
+				local_count = zone->inactive_laundry_pages;
 				if (!atomic_inc_if_nonzero(&page->count)) {
 					lru_unlock(zone);
 					cpu_relax();
 					lru_lock(zone);
+					if (zone->inactive_laundry_pages <
+					    local_count)
+						work_done++;
 					continue;
 				}
+				/* move page to tail so every caller won't wait on it */
+				list_del(&page->lru);
+				list_add(&page->lru, &zone->inactive_laundry_list);
 				lru_unlock(zone);
 				run_task_queue(&tq_disk);
 				timed_out = wait_on_page_timeout(page, 5 * HZ);
 				page_cache_release(page);
 				lru_lock(zone);
+				if (zone->inactive_laundry_pages < local_count)
+					work_done++;
 				/*
 				 * If we timed out and the page has been in
 				 * flight for over 30 seconds, this might not
@@ -849,11 +859,14 @@ int rebalance_laundry_zone(struct zone_s
 
 		if (page->buffers) {
 			page_cache_get(page);
+			local_count = zone->inactive_laundry_pages;
 			lru_unlock(zone);
 			try_to_release_page(page, 0);
 			UnlockPage(page);
 			page_cache_release(page);
 			lru_lock(zone);
+			if (zone->inactive_laundry_pages < local_count)
+				work_done++;
 			if (unlikely((page->buffers != NULL)) &&
 				       	PageInactiveLaundry(page)) {
 				del_page_from_inactive_laundry_list(page);
@@ -1013,6 +1026,7 @@ static int do_try_to_free_pages(unsigned
 {
 	int ret = 0;
 	struct zone_struct * zone;
+	pg_data_t *pgdat;
 
 	/*
 	 * Eat memory from filesystem page cache, buffer cache,
@@ -1037,6 +1051,18 @@ static int do_try_to_free_pages(unsigned
 	ret += shrink_dqcache_memory(DEF_PRIORITY, gfp_mask);
 #endif
 
+#ifdef CONFIG_HIGHMEM
+	/* reclaim bufferheaders on highmem systems with lowmem exhaustion */
+	for_each_pgdat(pgdat) {
+		zone_t *zone = pgdat->node_zones;
+
+		if (free_low(zone + ZONE_DMA) > 0 || free_low(zone + ZONE_NORMAL) > 0) {
+			ret += try_to_reclaim_buffers(DEF_PRIORITY, gfp_mask);
+			break;
+		}
+	}
+#endif
+
 	/* 	
 	 * Reclaim unused slab cache memory.
 	 */
