diff -urNp linux-1100/include/linux/brlock.h linux-1110/include/linux/brlock.h
--- linux-1100/include/linux/brlock.h
+++ linux-1110/include/linux/brlock.h
@@ -37,6 +37,7 @@ enum brlock_indices {
 	BR_AIO_REQ_LOCK,
 	BR_SEMAPHORE_LOCK,
 	BR_LRU_LOCK,
+	BR_PAGECACHE_LOCK,
 
 	__BR_END
 };
diff -urNp linux-1100/include/linux/swap.h linux-1110/include/linux/swap.h
--- linux-1100/include/linux/swap.h
+++ linux-1110/include/linux/swap.h
@@ -3,6 +3,7 @@
 
 #include <linux/spinlock.h>
 #include <asm/page.h>
+#include <linux/brlock.h>
 
 #define SWAP_FLAG_PREFER	0x8000	/* set if swap priority specified */
 #define SWAP_FLAG_PRIO_MASK	0x7fff
@@ -94,8 +95,32 @@ extern unsigned int freeable_lowmem(void
 extern atomic_t page_cache_size;
 extern atomic_t buffermem_pages;
 
+#if 1
+
+static inline void
+	lock_pagecache(void) { br_write_lock(BR_PAGECACHE_LOCK); }
+static inline void
+	unlock_pagecache(void) { br_write_unlock(BR_PAGECACHE_LOCK); }
+static inline void
+	lock_pagecache_readonly(void) { br_read_lock(BR_PAGECACHE_LOCK); }
+static inline void
+	unlock_pagecache_readonly(void) { br_read_unlock(BR_PAGECACHE_LOCK); }
+
+#else
+
 extern spinlock_cacheline_t pagecache_lock_cacheline;
-#define pagecache_lock (pagecache_lock_cacheline.lock)
+#define __pagecache_lock (pagecache_lock_cacheline.lock)
+
+static inline void
+	lock_pagecache(void) { spin_lock(&__pagecache_lock); }
+static inline void
+	unlock_pagecache(void) { spin_unlock(&__pagecache_lock); }
+static inline void
+	lock_pagecache_readonly(void) { spin_lock(&__pagecache_lock); }
+static inline void
+	unlock_pagecache_readonly(void) { spin_unlock(&__pagecache_lock); }
+
+#endif
 
 extern void __remove_inode_page(struct page *);
 
@@ -109,6 +134,7 @@ struct zone_t;
 /* linux/mm/rmap.c */
 struct pte_chain;
 extern int FASTCALL(page_referenced(struct page *, int *));
+extern int FASTCALL(page_referenced_lock(struct page *, int *));
 extern struct pte_chain * FASTCALL(page_add_rmap(struct page *, pte_t *,
 					struct pte_chain *));
 extern void FASTCALL(page_remove_rmap(struct page *, pte_t *));
diff -urNp linux-1100/mm/filemap.c linux-1110/mm/filemap.c
--- linux-1100/mm/filemap.c
+++ linux-1110/mm/filemap.c
@@ -127,8 +127,6 @@ static inline void remove_page_from_hash
  */
 void __remove_inode_page(struct page *page)
 {
-	if (PageDirty(page) && !PageSwapCache(page))
-		BUG();
 	remove_page_from_inode_queue(page);
 	remove_page_from_hash_queue(page);
 }
@@ -138,9 +136,9 @@ void remove_inode_page(struct page *page
 	if (!PageLocked(page))
 		PAGE_BUG(page);
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	__remove_inode_page(page);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 }
 
 static inline int sync_page(struct page *page)
@@ -161,13 +159,13 @@ void set_page_dirty(struct page *page)
 		struct address_space *mapping = page->mapping;
 
 		if (mapping) {
-			spin_lock(&pagecache_lock);
+			lock_pagecache();
 			mapping = page->mapping;
 			if (mapping) {	/* may have been truncated */
 				list_del(&page->list);
 				list_add(&page->list, &mapping->dirty_pages);
 			}
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 
 			if (mapping && mapping->host)
 				mark_inode_dirty_pages(mapping->host);
@@ -191,7 +189,7 @@ void invalidate_inode_pages(struct inode
 	head = &inode->i_mapping->clean_pages;
 
 	lru_lock(ALL_ZONES);
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	curr = head->next;
 
 	while (curr != head) {
@@ -223,7 +221,7 @@ unlock:
 		continue;
 	}
 
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	lru_unlock(ALL_ZONES);
 }
 
@@ -295,7 +293,7 @@ static int truncate_list_pages(struct li
 				/* Restart on this page */
 				list_add(head, curr);
 
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 			unlocked = 1;
 
  			if (!failed) {
@@ -316,7 +314,7 @@ static int truncate_list_pages(struct li
 				schedule();
 			}
 
-			spin_lock(&pagecache_lock);
+			lock_pagecache();
 			goto restart;
 		}
 		curr = curr->prev;
@@ -340,14 +338,14 @@ void truncate_inode_pages(struct address
 	unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);
 	int unlocked;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	do {
 		unlocked = truncate_list_pages(&mapping->clean_pages, start, &partial);
 		unlocked |= truncate_list_pages(&mapping->dirty_pages, start, &partial);
 		unlocked |= truncate_list_pages(&mapping->locked_pages, start, &partial);
 	} while (unlocked);
 	/* Traversed all three lists without dropping the lock */
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 }
 
 static inline int invalidate_this_page2(struct page * page,
@@ -366,7 +364,7 @@ static inline int invalidate_this_page2(
 		list_add_tail(head, curr);
 
 		page_cache_get(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 		truncate_complete_page(page);
 	} else {
 		if (page->buffers) {
@@ -375,7 +373,7 @@ static inline int invalidate_this_page2(
 			list_add_tail(head, curr);
 
 			page_cache_get(page);
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 			block_invalidate_page(page);
 		} else
 			unlocked = 0;
@@ -415,7 +413,7 @@ static int invalidate_list_pages2(struct
 			list_add(head, curr);
 
 			page_cache_get(page);
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 			unlocked = 1;
 			wait_on_page(page);
 		}
@@ -426,7 +424,7 @@ static int invalidate_list_pages2(struct
 			schedule();
 		}
 
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 		goto restart;
 	}
 	return unlocked;
@@ -441,13 +439,13 @@ void invalidate_inode_pages2(struct addr
 {
 	int unlocked;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	do {
 		unlocked = invalidate_list_pages2(&mapping->clean_pages);
 		unlocked |= invalidate_list_pages2(&mapping->dirty_pages);
 		unlocked |= invalidate_list_pages2(&mapping->locked_pages);
 	} while (unlocked);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 }
 
 static inline struct page * __find_page_nolock(struct address_space *mapping, unsigned long offset, struct page *page)
@@ -459,6 +457,7 @@ static inline struct page * __find_page_
 inside:
 		if (!page)
 			goto not_found;
+		prefetch(page->next_hash);
 		if (page->mapping != mapping)
 			continue;
 		if (page->index == offset)
@@ -475,7 +474,7 @@ static int do_buffer_fdatasync(struct li
 	struct page *page;
 	int retval = 0;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	curr = head->next;
 	while (curr != head) {
 		page = list_entry(curr, struct page, list);
@@ -488,7 +487,7 @@ static int do_buffer_fdatasync(struct li
 			continue;
 
 		page_cache_get(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 		lock_page(page);
 
 		/* The buffers could have been free'd while we waited for the page lock */
@@ -496,11 +495,11 @@ static int do_buffer_fdatasync(struct li
 			retval |= fn(page);
 
 		UnlockPage(page);
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 		curr = page->list.next;
 		page_cache_release(page);
 	}
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 
 	return retval;
 }
@@ -567,7 +566,7 @@ int filemap_fdatasync(struct address_spa
 	int ret = 0;
 	int (*writepage)(struct page *) = mapping->a_ops->writepage;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 
         while (!list_empty(&mapping->dirty_pages)) {
 		struct page *page = list_entry(mapping->dirty_pages.prev, struct page, list);
@@ -579,7 +578,7 @@ int filemap_fdatasync(struct address_spa
 			continue;
 
 		page_cache_get(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 
 		lock_page(page);
 
@@ -593,9 +592,9 @@ int filemap_fdatasync(struct address_spa
 			UnlockPage(page);
 
 		page_cache_release(page);
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 	}
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	return ret;
 }
 
@@ -610,7 +609,7 @@ int filemap_fdatawait(struct address_spa
 {
 	int ret = 0;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 
         while (!list_empty(&mapping->locked_pages)) {
 		struct page *page = list_entry(mapping->locked_pages.next, struct page, list);
@@ -622,16 +621,16 @@ int filemap_fdatawait(struct address_spa
 			continue;
 
 		page_cache_get(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 
 		___wait_on_page(page);
 		if (PageError(page))
 			ret = -EIO;
 
 		page_cache_release(page);
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 	}
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	return ret;
 }
 
@@ -648,10 +647,10 @@ void add_to_page_cache_locked(struct pag
 
 	page->index = index;
 	page_cache_get(page);
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	add_page_to_inode_queue(mapping, page);
 	add_page_to_hash_queue(page, page_hash(mapping, index));
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 
 	lru_cache_add(page);
 }
@@ -679,9 +678,9 @@ static inline void __add_to_page_cache(s
 
 void add_to_page_cache(struct page * page, struct address_space * mapping, unsigned long offset)
 {
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	__add_to_page_cache(page, mapping, offset, page_hash(mapping, offset));
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	lru_cache_add(page);
 }
 
@@ -692,7 +691,7 @@ int add_to_page_cache_unique(struct page
 	int err;
 	struct page *alias;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	alias = __find_page_nolock(mapping, offset, *hash);
 
 	err = 1;
@@ -701,7 +700,7 @@ int add_to_page_cache_unique(struct page
 		err = 0;
 	}
 
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	if (!err)
 		lru_cache_add(page);
 	return err;
@@ -718,9 +717,9 @@ static int page_cache_read(struct file *
 	struct page **hash = page_hash(mapping, offset);
 	struct page *page; 
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache_readonly();
 	page = __find_page_nolock(mapping, offset, *hash);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache_readonly();
 	if (page)
 		return 0;
 
@@ -978,11 +977,11 @@ struct page * __find_get_page(struct add
 	 * We scan the hash list read-only. Addition to and removal from
 	 * the hash-list needs a held write-lock.
 	 */
-	spin_lock(&pagecache_lock);
+	lock_pagecache_readonly();
 	page = __find_page_nolock(mapping, offset, *hash);
 	if (page)
 		page_cache_get(page);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache_readonly();
 	return page;
 }
 
@@ -994,13 +993,13 @@ struct page *find_trylock_page(struct ad
 	struct page *page;
 	struct page **hash = page_hash(mapping, offset);
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache_readonly();
 	page = __find_page_nolock(mapping, offset, *hash);
 	if (page) {
 		if (TryLockPage(page))
 			page = NULL;
 	}
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache_readonly();
 	return page;
 }
 EXPORT_SYMBOL_GPL(find_trylock_page);
@@ -1025,9 +1024,38 @@ repeat:
 	if (page) {
 		page_cache_get(page);
 		if (TryLockPage(page)) {
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
+			lock_page(page);
+			lock_pagecache();
+
+			/* Has the page been re-allocated while we slept? */
+			if (page->mapping != mapping || page->index != offset) {
+				UnlockPage(page);
+				page_cache_release(page);
+				goto repeat;
+			}
+		}
+	}
+	return page;
+}
+static struct page * FASTCALL(__find_lock_page_helper_readonly(struct address_space *, unsigned long, struct page *));
+static struct page * __find_lock_page_helper_readonly(struct address_space *mapping,
+					unsigned long offset, struct page *hash)
+{
+	struct page *page;
+
+	/*
+	 * We scan the hash list read-only. Addition to and removal from
+	 * the hash-list needs a held write-lock.
+	 */
+repeat:
+	page = __find_page_nolock(mapping, offset, hash);
+	if (page) {
+		page_cache_get(page);
+		if (TryLockPage(page)) {
+			unlock_pagecache_readonly();
 			lock_page(page);
-			spin_lock(&pagecache_lock);
+			lock_pagecache_readonly();
 
 			/* Has the page been re-allocated while we slept? */
 			if (page->mapping != mapping || page->index != offset) {
@@ -1040,6 +1068,7 @@ repeat:
 	return page;
 }
 
+
 /*
  * Same as the above, but lock the page too, verifying that
  * it's still valid once we own it.
@@ -1049,9 +1078,9 @@ struct page * __find_lock_page (struct a
 {
 	struct page *page;
 
-	spin_lock(&pagecache_lock);
-	page = __find_lock_page_helper(mapping, offset, *hash);
-	spin_unlock(&pagecache_lock);
+	lock_pagecache_readonly();
+	page = __find_lock_page_helper_readonly(mapping, offset, *hash);
+	unlock_pagecache_readonly();
 	return page;
 }
 
@@ -1063,20 +1092,20 @@ struct page * find_or_create_page(struct
 	struct page *page;
 	struct page **hash = page_hash(mapping, index);
 
-	spin_lock(&pagecache_lock);
-	page = __find_lock_page_helper(mapping, index, *hash);
-	spin_unlock(&pagecache_lock);
+	lock_pagecache_readonly();
+	page = __find_lock_page_helper_readonly(mapping, index, *hash);
+	unlock_pagecache_readonly();
 	if (!page) {
 		struct page *newpage = alloc_page(gfp_mask);
 		if (newpage) {
-			spin_lock(&pagecache_lock);
+			lock_pagecache();
 			page = __find_lock_page_helper(mapping, index, *hash);
 			if (likely(!page)) {
 				page = newpage;
 				__add_to_page_cache(page, mapping, index, hash);
 				newpage = NULL;
 			}
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 			if (newpage == NULL)
 				lru_cache_add(page);
 			else 
@@ -1463,7 +1492,7 @@ void do_generic_file_read(struct file * 
 	}
 
 	for (;;) {
-		struct page *page, **hash;
+		struct page *page, **hash, **next_hash;
 		unsigned long end_index, nr, ret;
 
 		end_index = inode->i_size >> PAGE_CACHE_SHIFT;
@@ -1487,14 +1516,17 @@ void do_generic_file_read(struct file * 
 		 * Try to find the data in the page cache..
 		 */
 		hash = page_hash(mapping, index);
+		prefetch(*hash);
+		if (likely(hash + 1 != page_hash_table + PAGE_HASH_SIZE))
+			prefetch(*(hash + 1));
 
-		spin_lock(&pagecache_lock);
+		lock_pagecache_readonly();
 		page = __find_page_nolock(mapping, index, *hash);
 		if (!page)
 			goto no_cached_page;
-found_page:
 		page_cache_get(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache_readonly();
+found_page:
 
 		if (!Page_Uptodate(page))
 			goto page_not_up_to_date;
@@ -1522,17 +1554,10 @@ page_ok:
 		index += offset >> PAGE_CACHE_SHIFT;
 		offset &= ~PAGE_CACHE_MASK;
 
-		lru_lock(page_zone(page));
-		pte_chain_lock(page);
-		if (PageFresh(page) && page->age <= INITIAL_AGE &&
-					!page_referenced(page, &dummy)) {
-			pte_chain_unlock(page);
+		if (PageFresh(page) && page->age <= INITIAL_AGE && !page_referenced_lock(page, &dummy) ) {
 			ClearPageFresh(page);
-			deactivate_page_nolock(page);
-			lru_unlock(page_zone(page));
+			deactivate_page(page);
 		} else {
-			pte_chain_unlock(page);
-			lru_unlock(page_zone(page));
 			/*
 			 * Mark the page accessed if we read the beginning or
 			 * we just did an lseek, in order to prevent a series
@@ -1606,7 +1631,7 @@ readpage:
 
 no_cached_page:
 		if (flags & F_ATOMIC) {
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache_readonly();
 			desc->error = -EWOULDBLOCKIO;
 			break;
 		}
@@ -1617,21 +1642,25 @@ no_cached_page:
 		 * We get here with the page cache lock held.
 		 */
 		if (!cached_page) {
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache_readonly();
 			cached_page = page_cache_alloc(mapping);
 			if (!cached_page) {
 				desc->error = -ENOMEM;
 				break;
 			}
+		} else
+			unlock_pagecache_readonly();
 
-			/*
-			 * Somebody may have added the page while we
-			 * dropped the page cache lock. Check for that.
-			 */
-			spin_lock(&pagecache_lock);
-			page = __find_page_nolock(mapping, index, *hash);
-			if (page)
-				goto found_page;
+		/*
+		 * Somebody may have added the page while we
+		 * dropped the page cache lock. Check for that.
+		 */
+		lock_pagecache();
+		page = __find_page_nolock(mapping, index, *hash);
+		if (page) {
+			page_cache_get(page);
+			unlock_pagecache();
+			goto found_page;
 		}
 
 		/*
@@ -1640,7 +1669,7 @@ no_cached_page:
 		page = cached_page;
 		__add_to_page_cache(page, mapping, index, hash);
 		SetPageFresh(page);
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 		lru_cache_add(page);		
 		cached_page = NULL;
 
@@ -2867,11 +2896,11 @@ static unsigned char mincore_page(struct
 	struct address_space * as = vma->vm_file->f_dentry->d_inode->i_mapping;
 	struct page * page, ** hash = page_hash(as, pgoff);
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache_readonly();
 	page = __find_page_nolock(as, pgoff, *hash);
 	if ((page) && (Page_Uptodate(page)))
 		present = 1;
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache_readonly();
 
 	return present;
 }
@@ -3499,7 +3528,7 @@ static int address_space_map(struct addr
 
 	ret = 0;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 
 	while (nr > 0) {
 		struct page **hash = page_hash(as, index);
@@ -3517,15 +3546,15 @@ got_page:
 
 		if (cached_page) {
 			__add_to_page_cache(cached_page, as, index, hash);
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 			lru_cache_add(cached_page);
-			spin_lock(&pagecache_lock);
+			lock_pagecache();
 			nr_new++;
 			*new_pages++ = page = cached_page;
 			cached_page = NULL;
 			goto got_page;
 		}
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 
 		cached_page = page_cache_alloc(as);
 		if (!cached_page)
@@ -3533,10 +3562,10 @@ got_page:
 
 		/* Okay, we now have an allocated page.  Retry
 		 * the search and add. */
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 	}
 
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 
 out:
 	if (cached_page)
diff -urNp linux-1100/mm/rmap.c linux-1110/mm/rmap.c
--- linux-1100/mm/rmap.c
+++ linux-1110/mm/rmap.c
@@ -205,6 +205,17 @@ int page_referenced(struct page * page, 
 	return referenced;
 }
 
+int page_referenced_lock(struct page * page, int * rsslimit)
+{
+	int ret;
+
+	pte_chain_lock(page);
+	ret = page_referenced(page, rsslimit);
+	pte_chain_unlock(page);
+
+	return ret;
+}
+
 /**
  * page_add_rmap - add reverse mapping entry to a page
  * @page: the page to add the mapping to
diff -urNp linux-1100/mm/swap_state.c linux-1110/mm/swap_state.c
--- linux-1100/mm/swap_state.c
+++ linux-1110/mm/swap_state.c
@@ -156,9 +156,9 @@ void delete_from_swap_cache(struct page 
 
 	entry.val = page->index;
 
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	__delete_from_swap_cache(page);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 
 	swap_free(entry);
 	page_cache_release(page);
diff -urNp linux-1100/mm/swapfile.c linux-1110/mm/swapfile.c
--- linux-1100/mm/swapfile.c
+++ linux-1110/mm/swapfile.c
@@ -238,10 +238,10 @@ static int exclusive_swap_page(struct pa
 		/* Is the only swap cache user the cache itself? */
 		if (p->swap_map[SWP_OFFSET(entry)] == 1) {
 			/* Recheck the page count with the pagecache lock held.. */
-			spin_lock(&pagecache_lock);
+			lock_pagecache();
 			if (page_count(page) - !!page->buffers == 2)
 				retval = 1;
-			spin_unlock(&pagecache_lock);
+			unlock_pagecache();
 		}
 		swap_info_put(p);
 	}
@@ -306,13 +306,13 @@ int remove_exclusive_swap_page(struct pa
 	retval = 0;
 	if (p->swap_map[SWP_OFFSET(entry)] == 1) {
 		/* Recheck the page count with the pagecache lock held.. */
-		spin_lock(&pagecache_lock);
+		lock_pagecache();
 		if (page_count(page) - !!page->buffers == 2) {
 			__delete_from_swap_cache(page);
 			SetPageDirty(page);
 			retval = 1;
 		}
-		spin_unlock(&pagecache_lock);
+		unlock_pagecache();
 	}
 	swap_info_put(p);
 
diff -urNp linux-1100/mm/vmscan.c linux-1110/mm/vmscan.c
--- linux-1100/mm/vmscan.c
+++ linux-1110/mm/vmscan.c
@@ -108,7 +108,7 @@ struct page * reclaim_page(zone_t * zone
 	 * reclaim_page() doesn't race with other pagecache users
 	 */
 	lru_lock(zone);
-	spin_lock(&pagecache_lock);
+	lock_pagecache();
 	maxscan = zone->inactive_clean_pages;
 	while (maxscan-- && !list_empty(&zone->inactive_clean_list)) {
 		page_lru = zone->inactive_clean_list.prev;
@@ -165,7 +165,7 @@ struct page * reclaim_page(zone_t * zone
 		pte_chain_unlock(page);
 		UnlockPage(page);
 	}
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	lru_unlock(zone);
 	return NULL;
 
@@ -173,7 +173,7 @@ struct page * reclaim_page(zone_t * zone
 found_page:
 	__lru_cache_del(page);
 	pte_chain_unlock(page);
-	spin_unlock(&pagecache_lock);
+	unlock_pagecache();
 	lru_unlock(zone);
 	if (entry.val)
 		swap_free(entry);
