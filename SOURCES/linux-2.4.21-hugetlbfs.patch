diff -urNp linux-1141/Documentation/Configure.help linux-1150/Documentation/Configure.help
--- linux-1141/Documentation/Configure.help
+++ linux-1150/Documentation/Configure.help
@@ -114,6 +114,16 @@ CONFIG_PROFILING
   Say Y here to enable the extended profiling support mechanisms used
   by profilers such as OProfile.
 
+Huge TLB Page and Huge TLB FS Support
+CONFIG_HUGETLBFS
+  This enables support for huge pages.  User space applications
+  can make use of this support via mounting hugetlbfs, or using
+  the SHM_HUGETLB flag to shmget().  If your applications are
+  huge page aware and your processor (Pentium or later for x86)
+  supports this, then say Y here.
+
+  Otherwise, say N.
+
 Symmetric Multi-Processing support
 CONFIG_SMP
   This enables support for systems with more than one CPU. If you have
diff -urNp linux-1141/Documentation/vm/hugetlbpage.txt linux-1150/Documentation/vm/hugetlbpage.txt
--- linux-1141/Documentation/vm/hugetlbpage.txt
+++ linux-1150/Documentation/vm/hugetlbpage.txt
@@ -0,0 +1,203 @@
+
+The intent of this file is to give a brief summary of hugetlbpage support in
+the Linux kernel.  This support is built on top of multiple page size support
+that is provided by most of modern architectures.  For example, IA-32
+architecture supports 4K and 4M (2M in PAE mode) page sizes, IA-64
+architecture supports multiple page sizes 4K, 8K, 64K, 256K, 1M, 4M, 16M,
+256M.  A TLB is a cache of virtual-to-physical translations.  Typically this
+is a very scarce resource on processor.  Operating systems try to make best
+use of limited number of TLB resources.  This optimization is more critical
+now as bigger and bigger physical memories (several GBs) are more readily
+available.
+
+Users can use the huge page support in Linux kernel by either using the mmap
+system call or standard SYSv shared memory system calls (shmget, shmat).
+
+First the Linux kernel needs to be built with CONFIG_HUGETLB_PAGE (present
+under Processor types and feature)  and CONFIG_HUGETLBFS (present under file
+system option on config menu) config options.
+
+The kernel built with hugepage support should show the number of configured
+hugepages in the system by running the "cat /proc/meminfo" command.  
+
+/proc/meminfo also provides information about the total number of hugetlb
+pages configured in the kernel.  It also displays information about the
+number of free hugetlb pages at any time.  It also displays information about
+the configured hugepage size - this is needed for generating the proper
+alignment and size of the arguments to the above system calls.
+
+The output of "cat /proc/meminfo" will have output like:
+
+.....
+HugePages_Total: xxx
+HugePages_Free:  yyy
+Hugepagesize:    zzz KB
+
+/proc/filesystems should also show a filesystem of type "hugetlbfs" configured
+in the kernel.
+
+/proc/sys/vm/nr_hugepages indicates the current number of configured hugetlb
+pages in the kernel.  Super user can dynamically request more (or free some
+pre-configured) hugepages. 
+The allocation( or deallocation) of hugetlb pages is posible only if there are
+enough physically contiguous free pages in system (freeing of hugepages is
+possible only if there are enough hugetlb pages free that can be transfered 
+back to regular memory pool).
+
+Pages that are used as hugetlb pages are reserved inside the kernel and can
+not be used for other purposes. 
+
+Once the kernel with Hugetlb page support is built and running, a user can
+use either the mmap system call or shared memory system calls to start using
+the huge pages.  It is required that the system administrator preallocate
+enough memory for huge page purposes.  
+
+Use the following command to dynamically allocate/deallocate hugepages:
+
+	echo 20 > /proc/sys/vm/nr_hugepages
+
+This command will try to configure 20 hugepages in the system.  The success
+or failure of allocation depends on the amount of physically contiguous
+memory that is preset in system at this time.  System administrators may want
+to put this command in one of the local rc init file.  This will enable the
+kernel to request huge pages early in the boot process (when the possibility
+of getting physical contiguous pages is still very high).
+
+If the user applications are going to request hugepages using mmap system
+call, then it is required that system administrator mount a file system of
+type hugetlbfs:
+
+	mount none /mnt/huge -t hugetlbfs
+
+This command mounts a (pseudo) filesystem of type hugetlbfs on the directory
+/mnt/huge.  Any files created on /mnt/huge uses hugepages.  An example is
+given at the end of this document.
+
+read and write system calls are not supported on files that reside on hugetlb
+file systems.
+
+Also, it is important to note that no such mount command is required if the
+applications are going to use only shmat/shmget system calls.  It is possible
+for same or different applications to use any combination of mmaps and shm*
+calls.  Though the mount of filesystem will be required for using mmaps.
+
+/* Example of using hugepage in user application using Sys V shared memory
+ * system calls.  In this example, app is requesting memory of size 256MB that
+ * is backed by huge pages.  Application uses the flag SHM_HUGETLB in shmget
+ * system call to informt the kernel that it is requesting hugepages.  For
+ * IA-64 architecture, Linux kernel reserves Region number 4 for hugepages.
+ * That means the addresses starting with 0x800000....will need to be
+ * specified.
+ */
+#include <sys/types.h>
+#include <sys/shm.h>
+#include <sys/types.h>
+#include <sys/mman.h>
+
+extern int errno;
+#define SHM_HUGETLB 04000
+#define LPAGE_SIZE      (256UL*1024UL*1024UL)
+#define         dprintf(x)  printf(x)
+#define ADDR (0x8000000000000000UL)
+main()
+{
+        int shmid;
+        int     i, j, k;
+        volatile        char    *shmaddr;
+
+        if ((shmid =shmget(2, LPAGE_SIZE, SHM_HUGETLB|IPC_CREAT|SHM_R|SHM_W ))
+< 0) {
+                perror("Failure:");
+                exit(1);
+        }
+        printf("shmid: 0x%x\n", shmid);
+        shmaddr = shmat(shmid, (void *)ADDR, SHM_RND) ;
+        if (errno != 0) {
+                perror("Shared Memory Attach Failure:");
+                exit(2);
+        }
+        printf("shmaddr: %p\n", shmaddr);
+
+        dprintf("Starting the writes:\n");
+        for (i=0;i<LPAGE_SIZE;i++) {
+                shmaddr[i] = (char) (i);
+                if (!(i%(1024*1024))) dprintf(".");
+        }
+        dprintf("\n");
+        dprintf("Starting the Check...");
+        for (i=0; i<LPAGE_SIZE;i++)
+                if (shmaddr[i] != (char)i)
+                        printf("\nIndex %d mismatched.");
+        dprintf("Done.\n");
+        if (shmdt((const void *)shmaddr) != 0) {
+                perror("Detached Failure:");
+                exit (3);
+        }
+}
+*******************************************************************
+*******************************************************************
+
+
+/* Example of using hugepage in user application using mmap 
+ * system call.  Before running this application, make sure that
+ * administrator has mounted the hugetlbfs (on some directory like /mnt) using
+ * the command mount -t hugetlbfs nodev /mnt
+ * In this example, app is requesting memory of size 256MB that
+ * is backed by huge pages.  Application uses the flag SHM_HUGETLB in shmget
+ * system call to informt the kernel that it is requesting hugepages.  For
+ * IA-64 architecture, Linux kernel reserves Region number 4 for hugepages.
+ * That means the addresses starting with 0x800000....will need to be
+ * specified.
+ */
+#include <unistd.h>
+#include <stdio.h>
+#include <sys/mman.h>
+#include <fcntl.h>
+
+#define FILE_NAME "/mnt/hugepagefile"
+#define LENGTH (256*1024*1024)
+#define PROTECTION (PROT_READ | PROT_WRITE)
+#define FLAGS   MAP_SHARED |MAP_FIXED
+#define ADDRESS (char *)(0x60000000UL + 0x8000000000000000UL)
+
+extern errno;
+
+check_bytes(char *addr)
+{
+        printf("First hex is %x\n", *((unsigned int *)addr));
+}
+
+write_bytes(char *addr)
+{
+        int i;
+        for (i=0;i<LENGTH;i++)
+                *(addr+i)=(char)i;
+}
+read_bytes(char *addr)
+{
+        int i;
+        check_bytes(addr);
+        for (i=0;i<LENGTH;i++)
+                if (*(addr+i)!=(char)i) {
+                        printf("Mismatch at %d\n", i);
+                        break;
+                }
+}
+main()
+{
+        unsigned long addr = 0;
+        int fd ;
+
+        fd = open(FILE_NAME, O_CREAT|O_RDWR, 0755);
+        if (fd < 0) {
+                perror("Open failed");
+                exit(errno);
+        }
+        addr = (unsigned long)mmap(ADDRESS, LENGTH, PROTECTION, FLAGS, fd, 0);
+        if (errno != 0)
+                perror("mmap failed");
+        printf("Returned address is %p\n", addr);
+        check_bytes((char*)addr);
+        write_bytes((char*)addr);
+        read_bytes((char *)addr);
+}
diff -urNp linux-1141/arch/i386/config.in linux-1150/arch/i386/config.in
--- linux-1141/arch/i386/config.in
+++ linux-1150/arch/i386/config.in
@@ -260,6 +260,7 @@ bool 'Math emulation' CONFIG_MATH_EMULAT
 bool 'MTRR (Memory Type Range Register) support' CONFIG_MTRR
 bool 'Symmetric multi-processing support' CONFIG_SMP
 
+
 if [ "$CONFIG_SMP" = "y" ]; then
    choice 'Hyperthreading Support' \
        "off           CONFIG_NR_SIBLINGS_0 \
diff -urNp linux-1141/arch/i386/mm/Makefile linux-1150/arch/i386/mm/Makefile
--- linux-1141/arch/i386/mm/Makefile
+++ linux-1150/arch/i386/mm/Makefile
@@ -12,4 +12,6 @@ O_TARGET := mm.o
 obj-y	 := init.o fault.o ioremap.o extable.o pageattr.o pgtable.o
 export-objs := pageattr.o
 
+obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
+
 include $(TOPDIR)/Rules.make
diff -urNp linux-1141/arch/i386/mm/hugetlbpage.c linux-1150/arch/i386/mm/hugetlbpage.c
--- linux-1141/arch/i386/mm/hugetlbpage.c
+++ linux-1150/arch/i386/mm/hugetlbpage.c
@@ -0,0 +1,536 @@
+/*
+ * IA-32 Huge TLB Page Support for Kernel.
+ *
+ * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/sysctl.h>
+#include <asm/mman.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+
+#include <linux/sysctl.h>
+
+static long    htlbpagemem;
+int     htlbpool_max;
+int     htlbpage_max;
+static long    htlbzone_pages;
+
+static LIST_HEAD(htlbpage_freelist);
+static spinlock_t htlbpage_lock = SPIN_LOCK_UNLOCKED;
+
+void free_huge_page(struct page *page);
+
+#define CHECK_LINK(p) BUG_ON((p) != (struct page *)(p)->lru.next)
+
+static void check_huge_page(struct page *page0)
+{
+	int i;
+	struct page *tmp;
+
+	BUG_ON(page_count(page0) != 1);
+	CHECK_LINK(page0);
+
+	BUG_ON(page0->flags & (1 << PG_reserved));
+	tmp = page0;
+	for (i = 0; i < (HPAGE_SIZE / PAGE_SIZE); i++) {
+		if (i && (tmp->flags & (1 << PG_locked | 1 << PG_reserved))) {
+			printk("hm, tmp: %p (%d), ->flags: %08lx\n", tmp, i, tmp->flags);
+			BUG();
+		}
+		if (i && page_count(tmp)) {
+			printk("hm, tmp: %p (%d), page_count(): %d\n", tmp, i, page_count(tmp));
+			BUG();
+		}
+		if (tmp->mapping) {
+			printk("hm, tmp: %p (%d), ->mapping: %p\n", tmp, i, tmp->mapping);
+			BUG();
+		}
+		tmp++;
+	}
+}
+
+static struct page *alloc_hugetlb_page(void)
+{
+	int i;
+	struct page *page;
+
+	spin_lock(&htlbpage_lock);
+	if (list_empty(&htlbpage_freelist)) {
+		spin_unlock(&htlbpage_lock);
+		return NULL;
+	}
+
+	page = list_entry(htlbpage_freelist.next, struct page, list);
+	list_del(&page->list);
+	htlbpagemem--;
+	spin_unlock(&htlbpage_lock);
+	check_huge_page(page);
+	page->lru.prev = (void *)free_huge_page;
+	for (i = 0; i < (HPAGE_SIZE/PAGE_SIZE); ++i)
+		clear_highpage(&page[i]);
+
+	return page;
+}
+
+static pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pmd = pmd_alloc(mm, pgd, addr);
+
+	return (pte_t *) pmd;
+}
+
+static pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pmd = pmd_offset(pgd, addr);
+	return (pte_t *) pmd;
+}
+
+static void set_huge_pte(struct mm_struct *mm, struct vm_area_struct *vma, struct page *page, pte_t * page_table, int write_access)
+{
+	pte_t entry;
+
+	mm->rss += (HPAGE_SIZE / PAGE_SIZE);
+	if (write_access) {
+		entry =
+		    pte_mkwrite(pte_mkdirty(mk_pte(page, vma->vm_page_prot)));
+	} else
+		entry = pte_wrprotect(mk_pte(page, vma->vm_page_prot));
+	entry = pte_mkyoung(entry);
+	mk_pte_huge(entry);
+	vm_set_pte(vma, vma->vm_start, page_table, entry);
+}
+
+/*
+ * This function checks for proper alignment of input addr and len parameters.
+ */
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	return 0;
+}
+
+int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
+			struct vm_area_struct *vma)
+{
+	pte_t *src_pte, *dst_pte, entry;
+	struct page *ptepage;
+	unsigned long addr = vma->vm_start;
+	unsigned long end = vma->vm_end;
+
+	while (addr < end) {
+		dst_pte = huge_pte_alloc(dst, addr);
+		if (!dst_pte)
+			goto nomem;
+		src_pte = huge_pte_offset(src, addr);
+		entry = *src_pte;
+		ptepage = pte_page(entry);
+		get_page(ptepage);
+		vm_set_pte(vma, vma->vm_start, dst_pte, entry);
+		dst->rss += (HPAGE_SIZE / PAGE_SIZE);
+		addr += HPAGE_SIZE;
+	}
+	return 0;
+
+nomem:
+	return -ENOMEM;
+}
+
+int
+follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		    struct page **pages, struct vm_area_struct **vmas,
+		    unsigned long *position, int *length, int i)
+{
+	unsigned long vpfn, vaddr = *position;
+	int remainder = *length;
+
+	BUG_ON(!is_vm_hugetlb_page(vma));
+
+	vpfn = vaddr/PAGE_SIZE;
+	while (vaddr < vma->vm_end && remainder) {
+
+		if (pages) {
+			pte_t *pte;
+			struct page *page;
+
+			pte = huge_pte_offset(mm, vaddr);
+
+			/* hugetlb should be locked, and hence, prefaulted */
+			BUG_ON(!pte || pte_none(*pte));
+
+			page = &pte_page(*pte)[vpfn % (HPAGE_SIZE/PAGE_SIZE)];
+
+			BUG_ON(!PageCompound(page));
+
+			get_page(page);
+			pages[i] = page;
+		}
+
+		if (vmas)
+			vmas[i] = vma;
+
+		vaddr += PAGE_SIZE;
+		++vpfn;
+		--remainder;
+		++i;
+	}
+
+	*length = remainder;
+	*position = vaddr;
+
+	return i;
+}
+
+struct page *
+follow_huge_addr(struct mm_struct *mm,
+	struct vm_area_struct *vma, unsigned long address, int write)
+{
+	return NULL;
+}
+
+struct vm_area_struct *hugepage_vma(struct mm_struct *mm, unsigned long addr)
+{
+	return NULL;
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_PSE);
+}
+
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
+{
+	struct page *page;
+
+	page = pte_page(*(pte_t *)pmd);
+	if (page)
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+
+	return page;
+}
+
+
+void free_huge_page(struct page *page0)
+{
+	BUG_ON(page_count(page0));
+	set_page_count(page0, 1);
+
+	check_huge_page(page0);
+
+	INIT_LIST_HEAD(&page0->list);
+
+	spin_lock(&htlbpage_lock);
+	list_add(&page0->list, &htlbpage_freelist);
+	htlbpagemem++;
+	spin_unlock(&htlbpage_lock);
+}
+
+void huge_page_release(struct page *page)
+{
+	CHECK_LINK(page);
+
+	if (!put_page_testzero(page))
+		return;
+
+	free_huge_page(page);
+}
+
+void unmap_hugepage_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long end)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long address;
+	pte_t *pte, old_pte;
+	struct page *page;
+
+	BUG_ON(start & (HPAGE_SIZE - 1));
+	BUG_ON(end & (HPAGE_SIZE - 1));
+
+	for (address = start; address < end; address += HPAGE_SIZE) {
+		pte = huge_pte_offset(mm, address);
+		if (pte_none(*pte))
+			continue;
+		page = pte_page(*pte);	
+		old_pte = ptep_get_and_clear(pte);
+		vm_account(vma, old_pte, address, -1);
+		flush_tlb_range(vma, address, address + HPAGE_SIZE);
+		huge_page_release(page);
+	}
+	mm->rss -= (end - start) >> PAGE_SHIFT;
+}
+
+void
+zap_hugepage_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long length)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	spin_lock(&mm->page_table_lock);
+	unmap_hugepage_range(vma, start, start + length);
+	spin_unlock(&mm->page_table_lock);
+}
+
+int zap_one_hugepage(struct vm_area_struct *vma, unsigned long address, 
+		     unsigned long size)
+{
+	struct page *page;
+	pte_t *pte;
+
+	BUG_ON(is_aligned_hugepage_range(address, size) < 0);
+
+	pte = huge_pte_offset(vma->vm_mm, address);
+	if (pte_none(*pte))
+		BUG();
+	page = pte_page(*pte);	
+	vm_pte_clear(vma, address, pte);
+	flush_tlb_range(vma, address, address + HPAGE_SIZE);
+	huge_page_release(page);
+
+	return HPAGE_SIZE >> PAGE_SHIFT;
+}
+
+
+int hugetlb_prefault(struct address_space *mapping, struct vm_area_struct *vma)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long addr;
+	int ret = 0;
+	extern void free_one_pmd(pmd_t *);
+
+	BUG_ON(vma->vm_start & ~HPAGE_MASK);
+	BUG_ON(vma->vm_end & ~HPAGE_MASK);
+
+	spin_lock(&mm->page_table_lock);
+	for (addr = vma->vm_start; addr < vma->vm_end; addr += HPAGE_SIZE) {
+		unsigned long idx;
+		pte_t *pte = huge_pte_alloc(mm, addr);
+		struct page *page;
+
+		if (!pte) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		/* if a ptepage already exists from a previous mapping, get rid of it */
+		if (!pte_none(*pte))
+			free_one_pmd((pmd_t *)pte);
+
+		idx = ((addr - vma->vm_start) >> HPAGE_SHIFT)
+			+ (vma->vm_pgoff >> (HPAGE_SHIFT - PAGE_SHIFT));
+		page = find_get_page(mapping, idx);
+		if (!page) {
+			page = alloc_hugetlb_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			ret = add_to_page_cache_unique_nolru(page, mapping, idx, page_hash(mapping, idx));
+			unlock_page(page);
+			if (ret) {
+				free_huge_page(page);
+				goto out;
+			}
+		}
+		CHECK_LINK(page);
+		set_huge_pte(mm, vma, page, pte, vma->vm_flags & VM_WRITE);
+	}
+out:
+	spin_unlock(&mm->page_table_lock);
+	return ret;
+}
+
+void update_and_free_page(struct page *page)
+{
+	int j;
+	struct page *map;
+
+	map = page;
+	htlbzone_pages--;
+	for (j = 0; j < (HPAGE_SIZE / PAGE_SIZE); j++) {
+		map->flags &= ~(1 << PG_locked | 1 << PG_error | 1 << PG_referenced | 1 << PG_dirty | 1 << PG_reserved);
+		set_page_count(map, 0);
+		map++;
+	}
+	set_page_count(page, 1);
+	free_pages_ok(page, HUGETLB_PAGE_ORDER);
+}
+
+static int try_to_free_low(int count)
+{
+	struct list_head *p;
+	struct page *page, *map;
+
+	map = NULL;
+	spin_lock(&htlbpage_lock);
+	list_for_each(p, &htlbpage_freelist) {
+		if (map) {
+			list_del(&map->list);
+			update_and_free_page(map);
+			htlbpagemem--;
+			map = NULL;
+			if (++count == 0)
+				break;
+		}
+		page = list_entry(p, struct page, list);
+		if (!PageHighMem(page))
+			map = page;
+	}
+	if (map) {
+		list_del(&map->list);
+		update_and_free_page(map);
+		htlbpagemem--;
+		count++;
+	}
+	spin_unlock(&htlbpage_lock);
+	return count;
+}
+
+int set_hugetlb_mem_size(int count)
+{
+	int lcount;
+	struct page *page;
+
+	if (count < 0)
+		lcount = count;
+	else
+		lcount = count - htlbzone_pages;
+
+	if (lcount == 0)
+		return (int)htlbzone_pages;
+	if (lcount > 0) {	/* Increase the mem size. */
+		while (lcount--) {
+			page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+			if (page == NULL)
+				break;
+			spin_lock(&htlbpage_lock);
+			list_add(&page->list, &htlbpage_freelist);
+			htlbpagemem++;
+			htlbzone_pages++;
+			spin_unlock(&htlbpage_lock);
+		}
+		return (int) htlbzone_pages;
+	}
+	/* Shrink the memory size. */
+	lcount = try_to_free_low(lcount);
+	while (lcount++) {
+		page = alloc_hugetlb_page();
+		if (page == NULL)
+			break;
+		spin_lock(&htlbpage_lock);
+		update_and_free_page(page);
+		spin_unlock(&htlbpage_lock);
+	}
+	return (int) htlbzone_pages;
+}
+
+#define HPAGE_FACTOR (HPAGE_SIZE / 1024 / 1024)
+
+int hugetlb_sysctl_handler(ctl_table *table, int write,
+		struct file *file, void *buffer, size_t *length)
+{
+	int ret = proc_dointvec(table, write, file, buffer, length);
+	/*
+	 * htlbpool_max is in units of MB.
+	 * htlbpage_max is in units of hugepages.
+	 *
+	 * Be careful about 32-bit overflows:
+	 */
+	if (write) {
+		htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+		htlbpage_max = set_hugetlb_mem_size(htlbpage_max);
+		htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	}
+	return ret;
+}
+
+static int __init hugetlb_setup(char *s)
+{
+	if (sscanf(s, "%d", &htlbpage_max) <= 0)
+		htlbpage_max = 0;
+	htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	return 1;
+}
+__setup("hugepages=", hugetlb_setup);
+
+static int __init hugetlbpool_setup(char *s)
+{
+	if (sscanf(s, "%d", &htlbpool_max) <= 0)
+		htlbpool_max = 0;
+	htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+	return 1;
+}
+__setup("hugetlbpool=", hugetlbpool_setup);
+
+
+static int __init hugetlb_init(void)
+{
+	int i;
+	struct page *page;
+
+	for (i = 0; i < htlbpage_max; ++i) {
+		page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+		if (!page)
+			break;
+		spin_lock(&htlbpage_lock);
+		list_add(&page->list, &htlbpage_freelist);
+		spin_unlock(&htlbpage_lock);
+	}
+	htlbpage_max = htlbpagemem = htlbzone_pages = i;
+	printk("Total HugeTLB memory allocated, %ld\n", htlbpagemem);
+	return 0;
+}
+module_init(hugetlb_init);
+
+int hugetlb_report_meminfo(char *buf)
+{
+	return sprintf(buf,
+			"HugePages_Total: %5lu\n"
+			"HugePages_Free:  %5lu\n"
+			"Hugepagesize:    %5lu kB\n",
+			htlbzone_pages,
+			htlbpagemem,
+			HPAGE_SIZE/1024);
+}
+
+int is_hugepage_mem_enough(size_t size)
+{
+	return (size + ~HPAGE_MASK)/HPAGE_SIZE <= htlbpagemem;
+}
+
+/*
+ * We cannot handle pagefaults against hugetlb pages at all.  They cause
+ * handle_mm_fault() to try to instantiate regular-sized pages in the
+ * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get
+ * this far.
+ */
+static struct page *hugetlb_nopage(struct vm_area_struct *vma,
+				unsigned long address, int unused)
+{
+	BUG();
+	return NULL;
+}
+
+struct vm_operations_struct hugetlb_vm_ops = {
+	.nopage = hugetlb_nopage,
+};
diff -urNp linux-1141/arch/i386/mm/init.c linux-1150/arch/i386/mm/init.c
--- linux-1141/arch/i386/mm/init.c
+++ linux-1150/arch/i386/mm/init.c
@@ -15,6 +15,7 @@
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/mman.h>
+#include <linux/mm_inline.h>
 #include <linux/mm.h>
 #include <linux/swap.h>
 #include <linux/smp.h>
@@ -311,7 +312,7 @@ unsigned long long __PAGE_KERNEL = _PAGE
 unsigned long long __PAGE_KERNEL_EXEC = _PAGE_KERNEL_EXEC;
 
 #if CONFIG_HIGHMEM64G
-#define MIN_ZONE_DMA_PAGES 1024
+#define MIN_ZONE_DMA_PAGES 2048
 #else
 #define MIN_ZONE_DMA_PAGES 4096
 #endif
@@ -326,10 +327,10 @@ static void __init zone_sizes_init(void)
 	high = highend_pfn;
 	
 	/*
-	 * Make the DMA zone 5% of memory, with a minimum of 4Mb
+	 * Make the DMA zone 5% of memory, with a minimum of 8Mb
 	 * and a maximum of 16Mb.
 	 */
-	#if MAX_ORDER != 10
+	#if MAX_ORDER != 11
 	#error fix this
 	#endif
 	ratio_maxdma = (highend_pfn / 20) & ~((1 << MAX_ORDER)-1); 
diff -urNp linux-1141/arch/i386/mm/pgtable.c linux-1150/arch/i386/mm/pgtable.c
--- linux-1141/arch/i386/mm/pgtable.c
+++ linux-1150/arch/i386/mm/pgtable.c
@@ -6,6 +6,7 @@
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>
+#include <linux/mm_inline.h>
 #include <linux/mm.h>
 #include <linux/swap.h>
 #include <linux/smp.h>
diff -urNp linux-1141/arch/s390/mm/init.c linux-1150/arch/s390/mm/init.c
--- linux-1141/arch/s390/mm/init.c
+++ linux-1150/arch/s390/mm/init.c
@@ -18,6 +18,7 @@
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/mman.h>
+#include <linux/mm_inline.h>	/* set_page_count */
 #include <linux/mm.h>
 #include <linux/swap.h>
 #include <linux/smp.h>
diff -urNp linux-1141/arch/s390x/mm/init.c linux-1150/arch/s390x/mm/init.c
--- linux-1141/arch/s390x/mm/init.c
+++ linux-1150/arch/s390x/mm/init.c
@@ -18,6 +18,7 @@
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/mman.h>
+#include <linux/mm_inline.h>	/* set_page_count */
 #include <linux/mm.h>
 #include <linux/swap.h>
 #include <linux/smp.h>
diff -urNp linux-1141/arch/x86_64/kernel/pci-gart.c linux-1150/arch/x86_64/kernel/pci-gart.c
--- linux-1141/arch/x86_64/kernel/pci-gart.c
+++ linux-1150/arch/x86_64/kernel/pci-gart.c
@@ -208,20 +208,24 @@ void pci_free_consistent(struct pci_dev 
 	u64 pte;
 	unsigned long iommu_page;
 	int i;
+	int order = get_order(size);
 
 	size = round_up(size, PAGE_SIZE); 
 	if (bus < iommu_bus_base || bus >= iommu_bus_base + iommu_size) { 
-		free_pages((unsigned long)vaddr, get_order(size)); 		
+		free_pages((unsigned long)vaddr, order);
 		return;
 	} 
 	size >>= PAGE_SHIFT;
 	iommu_page = (bus - iommu_bus_base) / PAGE_SIZE;
 	for (i = 0; i < size; i++) {
+		void *mem = vaddr + i*PAGE_SIZE;
+		if (i > 0) 
+			atomic_dec(&virt_to_page(mem)->count); 
 		pte = iommu_gatt_base[iommu_page + i];
 		BUG_ON((pte & GPTE_VALID) == 0); 
 		iommu_gatt_base[iommu_page + i] = gart_unmapped_entry; 		
-		free_page((unsigned long) __va(GPTE_DECODE(pte)));
 	} 
+	free_pages((unsigned long)vaddr, order);
 	flush_gart(); 
 	free_iommu(iommu_page, size);
 }
diff -urNp linux-1141/arch/x86_64/mm/Makefile linux-1150/arch/x86_64/mm/Makefile
--- linux-1141/arch/x86_64/mm/Makefile
+++ linux-1150/arch/x86_64/mm/Makefile
@@ -11,6 +11,7 @@ O_TARGET := mm.o
 obj-y	 := init.o fault.o ioremap.o extable.o modutil.o pageattr.o
 obj-$(CONFIG_DISCONTIGMEM) += numa.o
 obj-$(CONFIG_K8_NUMA) += k8topology.o
+obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
 
 export-objs := pageattr.o numa.o
 
diff -urNp linux-1141/arch/x86_64/mm/hugetlbpage.c linux-1150/arch/x86_64/mm/hugetlbpage.c
--- linux-1141/arch/x86_64/mm/hugetlbpage.c
+++ linux-1150/arch/x86_64/mm/hugetlbpage.c
@@ -0,0 +1,533 @@
+/*
+ * X86_64 Huge TLB Page Support for Kernel, based on IA-32 work.
+ *
+ * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/sysctl.h>
+#include <asm/mman.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+
+#include <linux/sysctl.h>
+
+static long    htlbpagemem;
+int	htlbpool_max;
+int     htlbpage_max;
+static long    htlbzone_pages;
+
+static LIST_HEAD(htlbpage_freelist);
+static spinlock_t htlbpage_lock = SPIN_LOCK_UNLOCKED;
+
+void free_huge_page(struct page *page);
+
+#define CHECK_LINK(p) BUG_ON((p) != (struct page *)(p)->lru.next)
+
+static void check_huge_page(struct page *page0)
+{
+	int i;
+	struct page *tmp;
+
+	BUG_ON(page_count(page0) != 1);
+	CHECK_LINK(page0);
+
+	BUG_ON(page0->flags & (1 << PG_reserved));
+	tmp = page0;
+	for (i = 0; i < (HPAGE_SIZE / PAGE_SIZE); i++) {
+		if (i && (tmp->flags & (1 << PG_locked | 1 << PG_reserved))) {
+			printk("hm, tmp: %p (%d), ->flags: %08lx\n", tmp, i, tmp->flags);
+			BUG();
+		}
+		if (i && page_count(tmp)) {
+			printk("hm, tmp: %p (%d), page_count(): %d\n", tmp, i, page_count(tmp));
+			BUG();
+		}
+		if (tmp->mapping) {
+			printk("hm, tmp: %p (%d), ->mapping: %p\n", tmp, i, tmp->mapping);
+			BUG();
+		}
+		tmp++;
+	}
+}
+
+static struct page *alloc_hugetlb_page(void)
+{
+	int i;
+	struct page *page;
+
+	spin_lock(&htlbpage_lock);
+	if (list_empty(&htlbpage_freelist)) {
+		spin_unlock(&htlbpage_lock);
+		return NULL;
+	}
+
+	page = list_entry(htlbpage_freelist.next, struct page, list);
+	list_del(&page->list);
+	htlbpagemem--;
+	spin_unlock(&htlbpage_lock);
+	check_huge_page(page);
+	page->lru.prev = (void *)free_huge_page;
+	for (i = 0; i < (HPAGE_SIZE/PAGE_SIZE); ++i)
+		clear_highpage(&page[i]);
+
+	return page;
+}
+
+static pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pmd = pmd_alloc(mm, pgd, addr);
+	return (pte_t *) pmd;
+}
+
+static pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pmd = pmd_offset(pgd, addr);
+	return (pte_t *) pmd;
+}
+
+static void set_huge_pte(struct mm_struct *mm, struct vm_area_struct *vma, struct page *page, pte_t * page_table, int write_access)
+{
+	pte_t entry;
+
+	mm->rss += (HPAGE_SIZE / PAGE_SIZE);
+	if (write_access) {
+		entry =
+		    pte_mkwrite(pte_mkdirty(mk_pte(page, vma->vm_page_prot)));
+	} else
+		entry = pte_wrprotect(mk_pte(page, vma->vm_page_prot));
+	entry = pte_mkyoung(entry);
+	mk_pte_huge(entry);
+	vm_set_pte(vma, vma->vm_start, page_table, entry);
+}
+
+/*
+ * This function checks for proper alignment of input addr and len parameters.
+ */
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	return 0;
+}
+
+int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
+			struct vm_area_struct *vma)
+{
+	pte_t *src_pte, *dst_pte, entry;
+	struct page *ptepage;
+	unsigned long addr = vma->vm_start;
+	unsigned long end = vma->vm_end;
+
+	while (addr < end) {
+		dst_pte = huge_pte_alloc(dst, addr);
+		if (!dst_pte)
+			goto nomem;
+		src_pte = huge_pte_offset(src, addr);
+		entry = *src_pte;
+		ptepage = pte_page(entry);
+		get_page(ptepage);
+		vm_set_pte(vma, vma->vm_start, dst_pte, entry);
+		dst->rss += (HPAGE_SIZE / PAGE_SIZE);
+		addr += HPAGE_SIZE;
+	}
+	return 0;
+
+nomem:
+	return -ENOMEM;
+}
+
+int
+follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
+		    struct page **pages, struct vm_area_struct **vmas,
+		    unsigned long *position, int *length, int i)
+{
+	unsigned long vpfn, vaddr = *position;
+	int remainder = *length;
+
+	BUG_ON(!is_vm_hugetlb_page(vma));
+
+	vpfn = vaddr/PAGE_SIZE;
+	while (vaddr < vma->vm_end && remainder) {
+
+		if (pages) {
+			pte_t *pte;
+			struct page *page;
+
+			pte = huge_pte_offset(mm, vaddr);
+
+			/* hugetlb should be locked, and hence, prefaulted */
+			BUG_ON(!pte || pte_none(*pte));
+
+			page = &pte_page(*pte)[vpfn % (HPAGE_SIZE/PAGE_SIZE)];
+
+			BUG_ON(!PageCompound(page));
+
+			get_page(page);
+			pages[i] = page;
+		}
+
+		if (vmas)
+			vmas[i] = vma;
+
+		vaddr += PAGE_SIZE;
+		++vpfn;
+		--remainder;
+		++i;
+	}
+
+	*length = remainder;
+	*position = vaddr;
+
+	return i;
+}
+
+struct page *
+follow_huge_addr(struct mm_struct *mm,
+	struct vm_area_struct *vma, unsigned long address, int write)
+{
+	return NULL;
+}
+
+struct vm_area_struct *hugepage_vma(struct mm_struct *mm, unsigned long addr)
+{
+	return NULL;
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_PSE);
+}
+
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
+{
+	struct page *page;
+
+	page = pte_page(*(pte_t *)pmd);
+	if (page)
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+
+	return page;
+}
+
+
+void free_huge_page(struct page *page0)
+{
+	BUG_ON(page_count(page0));
+	set_page_count(page0, 1);
+
+	check_huge_page(page0);
+
+	INIT_LIST_HEAD(&page0->list);
+
+	spin_lock(&htlbpage_lock);
+	list_add(&page0->list, &htlbpage_freelist);
+	htlbpagemem++;
+	spin_unlock(&htlbpage_lock);
+}
+
+void huge_page_release(struct page *page)
+{
+	CHECK_LINK(page);
+
+	if (!put_page_testzero(page))
+		return;
+
+	free_huge_page(page);
+}
+
+void unmap_hugepage_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long end)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long address;
+	pte_t *pte;
+	struct page *page;
+
+	BUG_ON(start & (HPAGE_SIZE - 1));
+	BUG_ON(end & (HPAGE_SIZE - 1));
+
+	for (address = start; address < end; address += HPAGE_SIZE) {
+		pte = huge_pte_offset(mm, address);
+		if (pte_none(*pte))
+			continue;
+		page = pte_page(*pte);	
+		vm_pte_clear(vma, address, pte);
+		flush_tlb_range(vma, address, address + HPAGE_SIZE);
+		huge_page_release(page);
+	}
+	mm->rss -= (end - start) >> PAGE_SHIFT;
+}
+
+void
+zap_hugepage_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long length)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	spin_lock(&mm->page_table_lock);
+	unmap_hugepage_range(vma, start, start + length);
+	spin_unlock(&mm->page_table_lock);
+}
+
+int zap_one_hugepage(struct vm_area_struct *vma, unsigned long address, 
+		     unsigned long size)
+{
+	struct page *page;
+	pte_t *pte;
+
+	BUG_ON(is_aligned_hugepage_range(address, size) < 0);
+
+	pte = huge_pte_offset(vma->vm_mm, address);
+	if (pte_none(*pte))
+		BUG();
+	page = pte_page(*pte);	
+	vm_pte_clear(vma, address, pte);
+	flush_tlb_range(vma, address, address + HPAGE_SIZE);
+	huge_page_release(page);
+
+	return HPAGE_SIZE >> PAGE_SHIFT;
+}
+
+
+int hugetlb_prefault(struct address_space *mapping, struct vm_area_struct *vma)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long addr;
+	int ret = 0;
+	extern void free_one_pmd(pmd_t *);
+
+	BUG_ON(vma->vm_start & ~HPAGE_MASK);
+	BUG_ON(vma->vm_end & ~HPAGE_MASK);
+
+	spin_lock(&mm->page_table_lock);
+	for (addr = vma->vm_start; addr < vma->vm_end; addr += HPAGE_SIZE) {
+		unsigned long idx;
+		pte_t *pte = huge_pte_alloc(mm, addr);
+		struct page *page;
+
+		if (!pte) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		/* if a ptepage already exists from a previous mapping, get rid of it */
+		if (!pte_none(*pte))
+			free_one_pmd((pmd_t *)pte);
+
+		idx = ((addr - vma->vm_start) >> HPAGE_SHIFT)
+			+ (vma->vm_pgoff >> (HPAGE_SHIFT - PAGE_SHIFT));
+		page = find_get_page(mapping, idx);
+		if (!page) {
+			page = alloc_hugetlb_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			ret = add_to_page_cache_unique_nolru(page, mapping, idx, page_hash(mapping, idx));
+			unlock_page(page);
+			if (ret) {
+				free_huge_page(page);
+				goto out;
+			}
+		}
+		CHECK_LINK(page);
+		set_huge_pte(mm, vma, page, pte, vma->vm_flags & VM_WRITE);
+	}
+out:
+	spin_unlock(&mm->page_table_lock);
+	return ret;
+}
+
+void update_and_free_page(struct page *page)
+{
+	int j;
+	struct page *map;
+
+	map = page;
+	htlbzone_pages--;
+	for (j = 0; j < (HPAGE_SIZE / PAGE_SIZE); j++) {
+		map->flags &= ~(1 << PG_locked | 1 << PG_error | 1 << PG_referenced | 1 << PG_dirty | 1 << PG_reserved);
+		set_page_count(map, 0);
+		map++;
+	}
+	set_page_count(page, 1);
+	free_pages_ok(page, HUGETLB_PAGE_ORDER);
+}
+
+static int try_to_free_low(int count)
+{
+	struct list_head *p;
+	struct page *page, *map;
+
+	map = NULL;
+	spin_lock(&htlbpage_lock);
+	list_for_each(p, &htlbpage_freelist) {
+		if (map) {
+			list_del(&map->list);
+			update_and_free_page(map);
+			htlbpagemem--;
+			map = NULL;
+			if (++count == 0)
+				break;
+		}
+		page = list_entry(p, struct page, list);
+		if (!PageHighMem(page))
+			map = page;
+	}
+	if (map) {
+		list_del(&map->list);
+		update_and_free_page(map);
+		htlbpagemem--;
+		count++;
+	}
+	spin_unlock(&htlbpage_lock);
+	return count;
+}
+
+int set_hugetlb_mem_size(int count)
+{
+	int lcount;
+	struct page *page;
+
+	if (count < 0)
+		lcount = count;
+	else
+		lcount = count - htlbzone_pages;
+
+	if (lcount == 0)
+		return (int)htlbzone_pages;
+	if (lcount > 0) {	/* Increase the mem size. */
+		while (lcount--) {
+			page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+			if (page == NULL)
+				break;
+			spin_lock(&htlbpage_lock);
+			list_add(&page->list, &htlbpage_freelist);
+			htlbpagemem++;
+			htlbzone_pages++;
+			spin_unlock(&htlbpage_lock);
+		}
+		return (int) htlbzone_pages;
+	}
+	/* Shrink the memory size. */
+	lcount = try_to_free_low(lcount);
+	while (lcount++) {
+		page = alloc_hugetlb_page();
+		if (page == NULL)
+			break;
+		spin_lock(&htlbpage_lock);
+		update_and_free_page(page);
+		spin_unlock(&htlbpage_lock);
+	}
+	return (int) htlbzone_pages;
+}
+
+#define HPAGE_FACTOR (HPAGE_SIZE / 1024 / 1024)
+
+int hugetlb_sysctl_handler(ctl_table *table, int write,
+		struct file *file, void *buffer, size_t *length)
+{
+	int ret = proc_dointvec(table, write, file, buffer, length);
+	/*
+	 * htlbpool_max is in units of MB.
+	 * htlbpage_max is in units of hugepages.
+	 *
+	 * Be careful about 32-bit overflows:
+	 */
+	if (write) {
+		htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+		htlbpage_max = set_hugetlb_mem_size(htlbpage_max);
+		htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	}
+	return ret;
+}
+
+static int __init hugetlb_setup(char *s)
+{
+	if (sscanf(s, "%d", &htlbpage_max) <= 0)
+		htlbpage_max = 0;
+	htlbpool_max = htlbpage_max * HPAGE_FACTOR;
+	return 1;
+}
+__setup("hugepages=", hugetlb_setup);
+
+static int __init hugetlbpool_setup(char *s)
+{
+	if (sscanf(s, "%d", &htlbpool_max) <= 0)
+		htlbpool_max = 0;
+	htlbpage_max = htlbpool_max / HPAGE_FACTOR;
+	return 1;
+}
+__setup("hugetlbpool=", hugetlbpool_setup);
+
+
+static int __init hugetlb_init(void)
+{
+	int i;
+	struct page *page;
+
+	for (i = 0; i < htlbpage_max; ++i) {
+		page = alloc_pages(__GFP_HIGHMEM, HUGETLB_PAGE_ORDER);
+		if (!page)
+			break;
+		spin_lock(&htlbpage_lock);
+		list_add(&page->list, &htlbpage_freelist);
+		spin_unlock(&htlbpage_lock);
+	}
+	htlbpage_max = htlbpagemem = htlbzone_pages = i;
+	printk("Total HugeTLB memory allocated, %ld\n", htlbpagemem);
+	return 0;
+}
+module_init(hugetlb_init);
+
+int hugetlb_report_meminfo(char *buf)
+{
+	return sprintf(buf,
+			"HugePages_Total: %5lu\n"
+			"HugePages_Free:  %5lu\n"
+			"Hugepagesize:    %5lu kB\n",
+			htlbzone_pages,
+			htlbpagemem,
+			HPAGE_SIZE/1024);
+}
+
+int is_hugepage_mem_enough(size_t size)
+{
+	return (size + ~HPAGE_MASK)/HPAGE_SIZE <= htlbpagemem;
+}
+
+/*
+ * We cannot handle pagefaults against hugetlb pages at all.  They cause
+ * handle_mm_fault() to try to instantiate regular-sized pages in the
+ * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get
+ * this far.
+ */
+static struct page *hugetlb_nopage(struct vm_area_struct *vma,
+				unsigned long address, int unused)
+{
+	BUG();
+	return NULL;
+}
+
+struct vm_operations_struct hugetlb_vm_ops = {
+	.nopage = hugetlb_nopage,
+};
diff -urNp linux-1141/arch/x86_64/mm/init.c linux-1150/arch/x86_64/mm/init.c
--- linux-1141/arch/x86_64/mm/init.c
+++ linux-1150/arch/x86_64/mm/init.c
@@ -15,6 +15,7 @@
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/mman.h>
+#include <linux/mm_inline.h>
 #include <linux/mm.h>
 #include <linux/swap.h>
 #include <linux/smp.h>
diff -urNp linux-1141/drivers/scsi/sg.c linux-1150/drivers/scsi/sg.c
--- linux-1141/drivers/scsi/sg.c
+++ linux-1150/drivers/scsi/sg.c
@@ -55,6 +55,7 @@
 #include <linux/init.h>
 #include <linux/poll.h>
 #include <linux/smp_lock.h>
+#include <linux/mm_inline.h>
 
 #include <asm/io.h>
 #include <asm/uaccess.h>
diff -urNp linux-1141/fs/Config.in linux-1150/fs/Config.in
--- linux-1141/fs/Config.in
+++ linux-1150/fs/Config.in
@@ -56,6 +56,10 @@ dep_tristate 'Journalling Flash File Sys
 if [ "$CONFIG_JFFS2_FS" = "y" -o "$CONFIG_JFFS2_FS" = "m" ] ; then
    int 'JFFS2 debugging verbosity (0 = quiet, 2 = noisy)' CONFIG_JFFS2_FS_DEBUG 0
 fi
+bool 'HugeTLB page and HugeTLB file system support' CONFIG_HUGETLBFS
+if [ "$CONFIG_HUGETLBFS" != "n" ]; then
+ define_bool CONFIG_HUGETLB_PAGE y
+fi
 tristate 'Compressed ROM file system support' CONFIG_CRAMFS
 bool 'Virtual memory file system support (former shm fs)' CONFIG_TMPFS
 define_bool CONFIG_RAMFS y
diff -urNp linux-1141/fs/Makefile linux-1150/fs/Makefile
--- linux-1141/fs/Makefile
+++ linux-1150/fs/Makefile
@@ -35,6 +35,7 @@ subdir-$(CONFIG_JBD)		+= jbd
 subdir-$(CONFIG_EXT2_FS)	+= ext2
 subdir-$(CONFIG_CRAMFS)		+= cramfs
 subdir-$(CONFIG_RAMFS)		+= ramfs
+subdir-$(CONFIG_HUGETLBFS)	+= hugetlbfs
 subdir-$(CONFIG_CODA_FS)	+= coda
 subdir-$(CONFIG_INTERMEZZO_FS)	+= intermezzo
 subdir-$(CONFIG_MINIX_FS)	+= minix
diff -urNp linux-1141/fs/attr.c linux-1150/fs/attr.c
--- linux-1141/fs/attr.c
+++ linux-1150/fs/attr.c
@@ -89,7 +89,7 @@ out:
 	return error;
 }
 
-static int setattr_mask(unsigned int ia_valid)
+int setattr_mask(unsigned int ia_valid)
 {
 	unsigned long dn_mask = 0;
 
diff -urNp linux-1141/fs/exec.c linux-1150/fs/exec.c
--- linux-1141/fs/exec.c
+++ linux-1150/fs/exec.c
@@ -25,6 +25,7 @@
 #include <linux/config.h>
 #include <linux/slab.h>
 #include <linux/file.h>
+#include <linux/mm_inline.h>
 #include <linux/mman.h>
 #include <linux/a.out.h>
 #include <linux/stat.h>
diff -urNp linux-1141/fs/hugetlbfs/Makefile linux-1150/fs/hugetlbfs/Makefile
--- linux-1141/fs/hugetlbfs/Makefile
+++ linux-1150/fs/hugetlbfs/Makefile
@@ -0,0 +1,11 @@
+#
+# Makefile for the linux ramfs routines.
+#
+
+O_TARGET := hugetlbfs.o
+
+obj-y   := inode.o
+obj-m   := $(O_TARGET)
+
+include $(TOPDIR)/Rules.make
+
diff -urNp linux-1141/fs/hugetlbfs/inode.c linux-1150/fs/hugetlbfs/inode.c
--- linux-1141/fs/hugetlbfs/inode.c
+++ linux-1150/fs/hugetlbfs/inode.c
@@ -0,0 +1,654 @@
+/*
+ * hugetlbpage-backed filesystem.  Based on ramfs.
+ *
+ * William Irwin, 2002
+ *
+ * Copyright (C) 2002 Linus Torvalds.
+ * Backported from 2.5.48 11/19/2002 Rohit Seth <rohit.seth@intel.com>
+ */
+
+#include <linux/module.h>
+#include <linux/personality.h>
+#include <asm/current.h>
+#include <linux/sched.h>		/* remove ASAP */
+#include <linux/fs.h>
+#include <linux/mount.h>
+#include <linux/file.h>
+#include <linux/pagemap.h>
+#include <linux/highmem.h>
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/hugetlb.h>
+#include <linux/quotaops.h>
+#include <linux/dnotify.h>
+
+#include <asm/uaccess.h>
+
+#ifndef HUGE_TASK_SIZE
+#define HUGE_TASK_SIZE TASK_SIZE
+#endif
+
+extern struct list_head inode_unused;
+
+/* some random number */
+#define HUGETLBFS_MAGIC	0x958458f6
+
+static struct super_operations hugetlbfs_ops;
+static struct address_space_operations hugetlbfs_aops;
+struct file_operations hugetlbfs_file_operations;
+struct file_operations hugetlbfs_dir_operations;
+static struct inode_operations hugetlbfs_dir_inode_operations;
+
+static inline int hugetlbfs_positive(struct dentry *dentry)
+{
+	return dentry->d_inode && !d_unhashed(dentry);
+}
+
+static int hugetlbfs_empty(struct dentry *dentry)
+{
+	struct list_head *list;
+
+	spin_lock (&dcache_lock);
+	list = dentry->d_subdirs.next;
+
+	while (list != &dentry->d_subdirs) {
+		struct dentry *de = list_entry(list, struct dentry, d_child);
+
+		if (hugetlbfs_positive(de)) {
+			spin_unlock(&dcache_lock);
+			return 0;
+		}
+		list = list->next;
+	}
+	spin_unlock(&dcache_lock);
+	return 1;
+}
+
+int hugetlbfs_sync_file(struct file * file, struct dentry *dentry, int datasync)
+{
+	return 0;
+}
+
+static int hugetlbfs_statfs(struct super_block *sb, struct statfs *buf)
+{
+	buf->f_type = HUGETLBFS_MAGIC;
+	buf->f_bsize = PAGE_CACHE_SIZE;
+	buf->f_namelen = 255;
+	return 0;
+}
+
+static int hugetlbfs_rename(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry)
+{
+	int error = -ENOTEMPTY;
+
+	if (hugetlbfs_empty(new_dentry)) {
+		struct inode *inode = new_dentry->d_inode;
+		if (inode) {
+			inode->i_nlink--;
+			dput(new_dentry);
+		}
+		error = 0;
+	}
+	return error;
+}
+
+static int hugetlbfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	int retval = -ENOTEMPTY;
+
+	if (hugetlbfs_empty(dentry)) {
+		struct inode *inode = dentry->d_inode;
+
+		inode->i_nlink--;
+		dput(dentry);			/* Undo the count from "create" - this does all the work */
+		retval = 0;
+	}
+	return retval;
+}
+
+#define hugetlbfs_rmdir hugetlbfs_unlink
+
+static int hugetlbfs_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)
+{
+	struct inode *inode = old_dentry->d_inode;
+
+	if (S_ISDIR(inode->i_mode))
+		return -EPERM;
+
+	inode->i_nlink++;
+	atomic_inc(&inode->i_count);
+	dget(dentry);
+	d_instantiate(dentry, inode);
+	return 0;
+}
+
+static struct dentry *hugetlbfs_lookup(struct inode *dir, struct dentry *dentry)
+{
+	d_add(dentry, NULL);
+	return NULL;
+}
+
+static int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct inode *inode =file->f_dentry->d_inode;
+	struct address_space *mapping = inode->i_mapping;
+	loff_t len;
+	int ret;
+
+	if (vma->vm_start & ~HPAGE_MASK)
+		return -EINVAL;
+	if (vma->vm_end & ~HPAGE_MASK)
+		return -EINVAL;
+	if (vma->vm_end - vma->vm_start < HPAGE_SIZE)
+		return -EINVAL;
+#ifdef CONFIG_IA64
+	if (vma->vm_start < (REGION_HPAGE << REGION_SHIFT))
+		return -EINVAL;
+#endif
+	down(&inode->i_sem);
+
+	UPDATE_ATIME(inode);
+	vma->vm_flags |= VM_HUGETLB | VM_RESERVED;
+	vma->vm_ops = &hugetlb_vm_ops;
+	ret = hugetlb_prefault(mapping, vma);
+
+	len = (loff_t)(vma->vm_end - vma->vm_start) +
+			((loff_t)vma->vm_pgoff << PAGE_SHIFT);
+	if (!ret && inode->i_size < len)
+		inode->i_size = len;
+	up(&inode->i_sem);
+
+	return ret;
+}
+
+/*
+ * Called under down_write(mmap_sem), page_table_lock is not held
+ */
+
+#ifdef HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags);
+#else
+static unsigned long
+hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	int full_search = 1;
+
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (len > HUGE_TASK_SIZE)
+		return -ENOMEM;
+
+	if (addr) {
+		addr = ALIGN(addr, HPAGE_SIZE);
+		vma = find_vma(mm, addr);
+		if (HUGE_TASK_SIZE - len >= addr &&
+		    (!vma || addr + len <= vma->vm_start))
+			return addr;
+	}
+
+	addr = ALIGN(mm->free_area_cache, HPAGE_SIZE);
+
+repeat_loop:
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (HUGE_TASK_SIZE - len < addr) {
+			if (full_search) {
+				full_search = 0;
+				addr = 0;
+				goto repeat_loop;
+			}
+			return -ENOMEM;
+		}
+		if (!vma || addr + len <= vma->vm_start)
+			return addr;
+		addr = ALIGN(vma->vm_end, HPAGE_SIZE);
+	}
+}
+#endif
+
+/*
+ * Read a page. Again trivial. If it didn't already exist
+ * in the page cache, it is zero-filled.
+ */
+static int hugetlbfs_readpage(struct file *file, struct page * page)
+{
+	unlock_page(page);
+	return -EINVAL;
+}
+
+static int hugetlbfs_prepare_write(struct file *file, struct page *page, unsigned offset, unsigned to)
+{
+	return -EINVAL;
+}
+
+static int hugetlbfs_commit_write(struct file *file, struct page *page, unsigned offset, unsigned to)
+{
+	return -EINVAL;
+}
+
+void truncate_partial_hugepage(struct page *page, unsigned partial)
+{
+	int i;
+	const unsigned piece = partial & (PAGE_SIZE - 1);
+	const unsigned tailstart = PAGE_SIZE - piece;
+	const unsigned whole_pages = partial / PAGE_SIZE;
+	const unsigned last_page_offset = HPAGE_SIZE/PAGE_SIZE - whole_pages;
+
+	for (i = HPAGE_SIZE/PAGE_SIZE - 1; i >= last_page_offset; ++i)
+		memclear_highpage_flush(&page[i], 0, PAGE_SIZE);
+
+	if (!piece)
+		return;
+
+	memclear_highpage_flush(&page[last_page_offset - 1], tailstart, piece);
+}
+
+void truncate_huge_page(struct address_space *mapping, struct page *page)
+{
+	BUG_ON(page->mapping != mapping);
+
+	ClearPageDirty(page);
+	ClearPageUptodate(page);
+	remove_inode_page(page);
+	set_page_count(page, 1);
+
+	BUG_ON(page->mapping);
+
+	huge_page_release(page);
+}
+
+void truncate_hugepages(struct inode *inode, struct address_space *mapping, loff_t lstart)
+{
+	unsigned long  start = (lstart + HPAGE_SIZE - 1) >> HPAGE_SHIFT;
+	unsigned partial = lstart & (HPAGE_SIZE - 1);
+	unsigned long next;
+	unsigned long max_idx;
+	struct page *page;
+
+	max_idx = inode->i_size >> HPAGE_SHIFT;
+	next = start;
+	while (next < max_idx) {
+		page = find_lock_page(mapping, next);
+		next++;
+		if (!page)
+			continue;
+		truncate_huge_page(mapping, page);
+		unlock_page(page);
+	}
+
+	if (partial) {
+		struct page *page = find_lock_page(mapping, start - 1);
+		if (page) {
+			truncate_partial_hugepage(page, partial);
+			unlock_page(page);
+			huge_page_release(page);
+		}
+	}
+}
+
+static void hugetlbfs_drop_inode(struct inode *inode)
+{
+	if (inode->i_data.nrpages)
+		truncate_hugepages(inode, &inode->i_data, 0);
+}
+
+static void hugetlb_vmtruncate_list(struct vm_area_struct *mpnt, unsigned long pgoff)
+{
+
+	do {
+		unsigned long start = mpnt->vm_start;
+		unsigned long end = mpnt->vm_end;
+		unsigned long size = end - start;
+		unsigned long diff;
+		if (mpnt->vm_pgoff >= pgoff) {
+			zap_hugepage_range(mpnt, start, size);
+			continue;
+		}
+		size >>= PAGE_SHIFT;
+		diff = pgoff - mpnt->vm_pgoff;
+		if (diff >= size)
+			continue;
+		start += diff << PAGE_SHIFT;
+		size = (size - diff) << PAGE_SHIFT;
+		zap_hugepage_range(mpnt, start, size);
+	}while ((mpnt = mpnt->vm_next_share)!= NULL);
+}
+
+static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
+{
+	unsigned long pgoff;
+	struct address_space *mapping = inode->i_mapping;
+	unsigned long limit;
+
+	pgoff = (offset + HPAGE_SIZE - 1) >> HPAGE_SHIFT;
+
+	if (inode->i_size < offset)
+		goto do_expand;
+
+	inode->i_size = offset;
+	spin_lock(&mapping->i_shared_lock);
+	if (!mapping->i_mmap && !mapping->i_mmap_shared)
+		goto out_unlock;
+	if (mapping->i_mmap != NULL)
+		hugetlb_vmtruncate_list(mapping->i_mmap, pgoff);
+	if (mapping->i_mmap_shared != NULL)
+		hugetlb_vmtruncate_list(mapping->i_mmap_shared, pgoff);
+
+out_unlock:
+	spin_unlock(&mapping->i_shared_lock);
+	truncate_hugepages(inode, mapping, offset);
+	return 0;
+
+do_expand:
+	limit = current->rlim[RLIMIT_FSIZE].rlim_cur;
+	if (limit != RLIM_INFINITY && offset > limit)
+		goto out_sig;
+	if (offset > inode->i_sb->s_maxbytes)
+		goto out;
+	inode->i_size = offset;
+	return 0;
+
+out_sig:
+	send_sig(SIGXFSZ, current, 0);
+out:
+	return -EFBIG;
+}
+
+static int hugetlbfs_setattr(struct dentry *dentry, struct iattr *attr)
+{
+	struct inode *inode = dentry->d_inode;
+	int error;
+	unsigned int ia_valid = attr->ia_valid;
+
+	BUG_ON(!inode);
+
+	error = inode_change_ok(inode, attr);
+	if (error)
+		goto out;
+
+	if ((ia_valid & ATTR_UID && attr->ia_uid != inode->i_uid) ||
+	    (ia_valid & ATTR_GID && attr->ia_gid != inode->i_gid))
+		error = DQUOT_TRANSFER(inode, attr) ? -EDQUOT : 0;
+	if (error)
+		goto out;
+
+	if (ia_valid & ATTR_SIZE) {
+		error = -EINVAL;
+		if (!(attr->ia_size & ~HPAGE_MASK))
+			error = hugetlb_vmtruncate(inode, attr->ia_size);
+		if (error)
+			goto out;
+		attr->ia_valid &= ~ATTR_SIZE;
+	}
+	error = inode_setattr(inode, attr);
+out:
+	return error;
+}
+
+struct inode *hugetlbfs_get_inode(struct super_block *sb, uid_t uid, gid_t gid,
+				int mode, int dev)
+{
+	struct inode * inode = new_inode(sb);
+
+	if (inode) {
+		inode->i_mode = mode;
+		inode->i_uid = uid;
+		inode->i_gid = gid;
+		inode->i_blksize = PAGE_CACHE_SIZE;
+		inode->i_blocks = 0;
+		inode->i_rdev = NODEV;
+		inode->i_mapping->a_ops = &hugetlbfs_aops;
+		inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;
+		switch (mode & S_IFMT) {
+		default:
+			init_special_inode(inode, mode, dev);
+			break;
+		case S_IFREG:
+			inode->i_fop = &hugetlbfs_file_operations;
+			break;
+		case S_IFDIR:
+			inode->i_op = &hugetlbfs_dir_inode_operations;
+			inode->i_fop = &hugetlbfs_dir_operations;
+			break;
+		case S_IFLNK:
+			inode->i_op = &page_symlink_inode_operations;
+			break;
+		}
+	}
+	return inode;
+}
+
+/*
+ * File creation. Allocate an inode, and we're done..
+ */
+static int hugetlbfs_mknod(struct inode *dir, struct dentry *dentry, int mode, int dev)
+{
+	struct inode * inode = hugetlbfs_get_inode(dir->i_sb, current->fsuid,
+						current->fsgid, mode, dev);
+	int error = -ENOSPC;
+
+	if (inode) {
+		d_instantiate(dentry, inode);
+		dget(dentry);		/* Extra count - pin the dentry in core */
+		error = 0;
+	}
+	return error;
+}
+
+static int hugetlbfs_mkdir(struct inode * dir, struct dentry * dentry, int mode)
+{
+	return hugetlbfs_mknod(dir, dentry, mode | S_IFDIR, 0);
+}
+
+static int hugetlbfs_create(struct inode *dir, struct dentry *dentry, int mode)
+{
+	return hugetlbfs_mknod(dir, dentry, mode | S_IFREG, 0);
+}
+
+static int hugetlbfs_symlink(struct inode * dir, struct dentry *dentry, const char * symname)
+{
+	int error;
+
+	error = hugetlbfs_mknod(dir, dentry, S_IFLNK|S_IRWXUGO, 0);
+	if (!error) {
+		int l = strlen(symname)+1;
+		struct inode *inode = dentry->d_inode;
+		error = block_symlink(inode, symname, l);
+	}
+	return error;
+}
+
+static struct address_space_operations hugetlbfs_aops = {
+	readpage:	hugetlbfs_readpage,
+	writepage:	fail_writepage,
+	prepare_write:	hugetlbfs_prepare_write,
+	commit_write:	hugetlbfs_commit_write
+};
+
+struct file_operations hugetlbfs_file_operations = {
+	read:			generic_file_read,
+	write:			generic_file_write,
+	mmap:			hugetlbfs_file_mmap,
+	fsync:			hugetlbfs_sync_file,
+	get_unmapped_area:	hugetlb_get_unmapped_area
+};
+
+struct file_operations hugetlbfs_dir_operations = {
+	open:		dcache_dir_open,
+	release:	dcache_dir_close,
+	llseek:		dcache_dir_lseek,
+	read:		generic_read_dir,
+	readdir:	dcache_readdir,
+	fsync:		hugetlbfs_sync_file,
+};
+
+static struct inode_operations hugetlbfs_dir_inode_operations = {
+	create:		hugetlbfs_create,
+	lookup:		hugetlbfs_lookup,
+	link:		hugetlbfs_link,
+	unlink:		hugetlbfs_unlink,
+	symlink:	hugetlbfs_symlink,
+	mkdir:		hugetlbfs_mkdir,
+	rmdir:		hugetlbfs_rmdir,
+	mknod:		hugetlbfs_mknod,
+	rename:		hugetlbfs_rename,
+	setattr:	hugetlbfs_setattr,
+};
+
+static struct super_operations hugetlbfs_ops = {
+	statfs:		hugetlbfs_statfs,
+	put_inode:	hugetlbfs_drop_inode,
+};
+
+static int hugetlbfs_parse_options(char *options, struct hugetlbfs_config *pconfig)
+{
+	char *opt, *value, *rest;
+
+	if (!options)
+		return 0;
+	while ((opt = strsep(&options, ",")) != NULL) {
+		if (!*opt)
+			continue;
+
+		value = strchr(opt, '=');
+		if (!value || !*value)
+			return -EINVAL;
+		else
+			*value++ = '\0';
+
+		if (!strcmp(opt, "uid"))
+			pconfig->uid = simple_strtoul(value, &value, 0);
+		else if (!strcmp(opt, "gid"))
+			pconfig->gid = simple_strtoul(value, &value, 0);
+		else if (!strcmp(opt, "mode"))
+			pconfig->mode = simple_strtoul(value, &value, 0) & 0777U;
+		else 
+			return -EINVAL;
+
+		if (*value)
+			return -EINVAL;
+	}
+	return 0;
+}
+
+static struct super_block * hugetlbfs_fill_super(struct super_block * sb, void * data, int silent)
+{
+	struct inode * inode;
+	struct dentry * root;
+	struct hugetlbfs_config config;
+
+	config.uid = current->fsuid;
+	config.gid = current->fsgid;
+	config.mode = 0755;
+
+	if (hugetlbfs_parse_options(data, &config))
+		return NULL;
+
+	sb->s_blocksize = PAGE_CACHE_SIZE;
+	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_magic = HUGETLBFS_MAGIC;
+	sb->s_op = &hugetlbfs_ops;
+	inode = hugetlbfs_get_inode(sb, config.uid, config.gid, S_IFDIR | config.mode, 0);
+	if (!inode)
+		return NULL;
+
+	root = d_alloc_root(inode);
+	if (!root) {
+		iput(inode);
+		return NULL;
+	}
+	sb->s_root = root;
+	return sb;
+}
+
+static DECLARE_FSTYPE(hugetlbfs_fs_type, "hugetlbfs", hugetlbfs_fill_super, FS_LITTER);
+
+static struct vfsmount *hugetlbfs_vfsmount;
+
+static atomic_t hugetlbfs_counter = ATOMIC_INIT(0);
+
+struct file *hugetlb_zero_setup(size_t size)
+{
+	int error, n;
+	struct file *file;
+	struct inode *inode;
+	struct dentry *dentry, *root;
+	struct qstr quick_string;
+	char buf[16];
+
+	if (!is_hugepage_mem_enough(size))
+		return ERR_PTR(-ENOMEM);
+	n = atomic_read(&hugetlbfs_counter);
+	atomic_inc(&hugetlbfs_counter);
+
+	root = hugetlbfs_vfsmount->mnt_root;
+	snprintf(buf, 16, "%d", n);
+	quick_string.name = buf;
+	quick_string.len = strlen(quick_string.name);
+	quick_string.hash = 0;
+	dentry = d_alloc(root, &quick_string);
+	if (!dentry)
+		return ERR_PTR(-ENOMEM);
+
+	error = -ENFILE;
+	file = get_empty_filp();
+	if (!file)
+		goto out_dentry;
+
+	error = -ENOSPC;
+	inode = hugetlbfs_get_inode(root->d_sb, current->fsuid, current->fsgid,
+				S_IFREG | S_IRWXUGO, 0);
+	if (!inode)
+		goto out_file;
+
+	d_instantiate(dentry, inode);
+	inode->i_size = size;
+	inode->i_nlink = 0;
+	file->f_vfsmnt = mntget(hugetlbfs_vfsmount);
+	file->f_dentry = dentry;
+	file->f_op = &hugetlbfs_file_operations;
+	file->f_mode = FMODE_WRITE | FMODE_READ;
+	return file;
+
+out_file:
+	put_filp(file);
+out_dentry:
+	dput(dentry);
+	return ERR_PTR(error);
+}
+
+static int __init init_hugetlbfs_fs(void)
+{
+	int error;
+	struct vfsmount *vfsmount;
+
+	error = register_filesystem(&hugetlbfs_fs_type);
+	if (error)
+		return error;
+
+	vfsmount = kern_mount(&hugetlbfs_fs_type);
+
+	if (!IS_ERR(vfsmount)) {
+		printk("Hugetlbfs mounted.\n");
+		hugetlbfs_vfsmount = vfsmount;
+		return 0;
+	}
+
+	printk("Error in  mounting hugetlbfs.\n");
+	error = PTR_ERR(vfsmount);
+	return error;
+}
+
+static void __exit exit_hugetlbfs_fs(void)
+{
+	unregister_filesystem(&hugetlbfs_fs_type);
+}
+
+module_init(init_hugetlbfs_fs)
+module_exit(exit_hugetlbfs_fs)
+
+MODULE_LICENSE("GPL");
diff -urNp linux-1141/fs/inode.c linux-1150/fs/inode.c
--- linux-1141/fs/inode.c
+++ linux-1150/fs/inode.c
@@ -58,7 +58,7 @@ static unsigned int i_hash_shift;
  */
 
 static LIST_HEAD(inode_in_use);
-static LIST_HEAD(inode_unused);
+LIST_HEAD(inode_unused);
 static LIST_HEAD(inode_unused_pagecache);
 static struct list_head *inode_hashtable;
 static LIST_HEAD(anon_hash_chain); /* for inodes with NULL i_sb */
diff -urNp linux-1141/fs/proc/array.c linux-1150/fs/proc/array.c
--- linux-1141/fs/proc/array.c
+++ linux-1150/fs/proc/array.c
@@ -70,6 +70,7 @@
 #include <linux/smp.h>
 #include <linux/signal.h>
 #include <linux/highmem.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -511,6 +512,16 @@ int proc_pid_statm(struct task_struct *t
 			pgd_t *pgd = pgd_offset(mm, vma->vm_start);
 			int pages = 0, shared = 0, dirty = 0, total = 0;
 
+			if (is_vm_hugetlb_page(vma)) {
+				pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+				size += pages;
+				resident += pages;
+				drs += pages;
+				if (!(vma->vm_flags & VM_DONTCOPY))
+					share += pages;
+				vma = vma->vm_next;
+				continue;
+			}
 			statm_pgd_range(pgd, vma->vm_start, vma->vm_end, &pages, &shared, &dirty, &total);
 			resident += pages;
 			share += shared;
diff -urNp linux-1141/fs/proc/proc_misc.c linux-1150/fs/proc/proc_misc.c
--- linux-1141/fs/proc/proc_misc.c
+++ linux-1150/fs/proc/proc_misc.c
@@ -39,6 +39,7 @@
 #include <linux/seq_file.h>
 #include <linux/sysrq.h>
 #include <linux/mm_inline.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -225,6 +226,8 @@ static int meminfo_read_proc(char *page,
 		K(i.totalswap),
 		K(i.freeswap));
 
+	len += hugetlb_report_meminfo(page + len);
+
 	return proc_calc_metrics(page, start, off, count, eof, len);
 #undef B
 #undef K
diff -urNp linux-1141/include/asm-generic/tlb.h linux-1150/include/asm-generic/tlb.h
--- linux-1141/include/asm-generic/tlb.h
+++ linux-1150/include/asm-generic/tlb.h
@@ -93,6 +93,8 @@ static inline void tlb_finish_mmu(struct
 	}
 }
 
+#define tlb_to_mm(tlb)		(tlb)->mm
+
 #else
 
 /* The uniprocessor functions are quite simple and are inline macros in an
@@ -109,6 +111,7 @@ typedef struct vm_area_struct mmu_gather
 		vm_pte_clear((tlb), (addr), (ptep));\
 		__free_pte(__pte);\
 	} while (0)
+#define tlb_to_mm(tlb)		(tlb)
 
 #endif
 
diff -urNp linux-1141/include/asm-i386/page.h linux-1150/include/asm-i386/page.h
--- linux-1141/include/asm-i386/page.h
+++ linux-1150/include/asm-i386/page.h
@@ -44,15 +44,23 @@ typedef struct { unsigned long long pmd;
 typedef struct { unsigned long long pgd; } pgd_t;
 typedef struct { unsigned long pgprot; } pgprot_t;
 #define pte_val(x)	((x).pte_low | ((unsigned long long)(x).pte_high << 32))
+#define HPAGE_SHIFT	21
 #else
 typedef struct { unsigned long pte_low; } pte_t;
 typedef struct { unsigned long pmd; } pmd_t;
 typedef struct { unsigned long pgd; } pgd_t;
 typedef struct { unsigned long pgprot; } pgprot_t;
 #define pte_val(x)	((x).pte_low)
+#define HPAGE_SHIFT	22
 #endif
 #define PTE_MASK	PAGE_MASK
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HPAGE_SIZE	((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK	(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#endif
+
 #define pmd_val(x)	((x).pmd)
 #define pgd_val(x)	((x).pgd)
 #define pgprot_val(x)	((x).pgprot)
diff -urNp linux-1141/include/asm-i386/pgtable.h linux-1150/include/asm-i386/pgtable.h
--- linux-1141/include/asm-i386/pgtable.h
+++ linux-1150/include/asm-i386/pgtable.h
@@ -367,6 +367,7 @@ static inline void ptep_mkdirty(pte_t *p
  */
 
 #define mk_pte(page, pgprot)	__mk_pte((page) - mem_map, (pgprot))
+#define mk_pte_huge(entry) ((entry).pte_low |= _PAGE_PRESENT | _PAGE_PSE)
 
 /* This takes a physical page address that is used by the remapping functions */
 #define mk_pte_phys(physpage, pgprot)	__mk_pte((physpage) >> PAGE_SHIFT, pgprot)
@@ -398,6 +399,9 @@ static inline pte_t pte_modify(pte_t pte
 #define pmd_page(pmd) (pfn_to_page(pmd_val(pmd) >> PAGE_SHIFT))
 #endif /* !CONFIG_DISCONTIGMEM */
 
+#define pmd_large(pmd) \
+	((pmd_val(pmd) & (_PAGE_PSE|_PAGE_PRESENT)) == (_PAGE_PSE|_PAGE_PRESENT))
+
 /* to find an entry in a page-table-directory. */
 #define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
 
diff -urNp linux-1141/include/asm-i386/tlb.h linux-1150/include/asm-i386/tlb.h
--- linux-1141/include/asm-i386/tlb.h
+++ linux-1150/include/asm-i386/tlb.h
@@ -1 +1,20 @@
+#ifndef _I386_TLB_H
+#define _I386_TLB_H
+
+/*
+ * x86 doesn't need any special per-pte or
+ * per-vma handling..
+ */
+#define tlb_start_vma(tlb, vma) do { } while (0)
+#define tlb_end_vma(tlb, vma) do { } while (0)
+#define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)
+
+/*
+ * .. because we flush the whole mm when it
+ * fills up.
+ */
+#define tlb_flush(tlb) flush_tlb_mm((tlb)->mm)
+
 #include <asm-generic/tlb.h>
+
+#endif
diff -urNp linux-1141/include/asm-s390/pgtable.h linux-1150/include/asm-s390/pgtable.h
--- linux-1141/include/asm-s390/pgtable.h
+++ linux-1150/include/asm-s390/pgtable.h
@@ -258,6 +258,8 @@ extern inline int pmd_bad(pmd_t pmd)
 	return (pmd_val(pmd) & (~PAGE_MASK & ~_PAGE_TABLE_INV)) != _PAGE_TABLE;
 }
 
+#define pmd_large(x)	0	/* P3: looks bogus, compare w/ pmd_huge */
+
 extern inline int pte_present(pte_t pte) { return pte_val(pte) & _PAGE_PRESENT; }
 extern inline int pte_none(pte_t pte)
 {
@@ -436,7 +438,7 @@ static inline pte_t mk_pte_phys(unsigned
 	                                                                  \
 	if (!(pgprot_val(__pgprot) & _PAGE_ISCLEAN)) {			  \
 		int __users = !!__page->buffers + !!__page->mapping;      \
-		if (__users + page_count(__page) == 1)                    \
+		if (__users + atomic_read(&__page->count) == 1)           \
 			pte_val(__pte) |= _PAGE_MKCLEAN;                  \
 	}								  \
 	__pte;                                                            \
diff -urNp linux-1141/include/asm-s390x/pgtable.h linux-1150/include/asm-s390x/pgtable.h
--- linux-1141/include/asm-s390x/pgtable.h
+++ linux-1150/include/asm-s390x/pgtable.h
@@ -272,6 +272,8 @@ extern inline int pmd_bad(pmd_t pmd)
 	return (pmd_val(pmd) & (~PAGE_MASK & ~_PMD_ENTRY_INV)) != _PMD_ENTRY;
 }
 
+#define pmd_large(x)	0	/* P3: looks bogus, compare w/ pmd_huge */
+
 extern inline int pte_present(pte_t pte)
 {
 	return pte_val(pte) & _PAGE_PRESENT;
@@ -455,7 +457,7 @@ extern inline pte_t mk_pte_phys(unsigned
  	                                                                  \
 	if (!(pgprot_val(__pgprot) & _PAGE_ISCLEAN)) {			  \
 		int __users = !!__page->buffers + !!__page->mapping;      \
-		if (__users + page_count(__page) == 1)                    \
+		if (__users + atomic_read(&__page->count) == 1)           \
 			pte_val(__pte) |= _PAGE_MKCLEAN;                  \
 	}								  \
 	__pte;                                                            \
diff -urNp linux-1141/include/asm-x86_64/page.h linux-1150/include/asm-x86_64/page.h
--- linux-1141/include/asm-x86_64/page.h
+++ linux-1150/include/asm-x86_64/page.h
@@ -51,6 +51,14 @@ typedef struct { unsigned long pgd; } pg
 typedef struct { unsigned long pml4; } pml4_t;
 #define PTE_MASK	PHYSICAL_PAGE_MASK
 
+#define HPAGE_SHIFT 21
+
+#ifdef CONFIG_HUGETLB_PAGE
+#define HPAGE_SIZE     ((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK     (~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER     (HPAGE_SHIFT - PAGE_SHIFT)
+#endif
+
 typedef struct { unsigned long pgprot; } pgprot_t;
 
 #define pte_val(x)	((x).pte)
diff -urNp linux-1141/include/asm-x86_64/pgtable.h linux-1150/include/asm-x86_64/pgtable.h
--- linux-1141/include/asm-x86_64/pgtable.h
+++ linux-1150/include/asm-x86_64/pgtable.h
@@ -359,6 +359,8 @@ static inline void ptep_mkdirty(pte_t *p
  	__pte;									 \
 }) 
 
+#define mk_pte_huge(entry) ((entry).pte |= _PAGE_PRESENT | _PAGE_PSE)
+
 /* This takes a physical page address that is used by the remapping functions */
 static inline pte_t mk_pte_phys(unsigned long physpage, pgprot_t pgprot)
 { 
@@ -378,6 +380,9 @@ extern inline pte_t pte_modify(pte_t pte
 #define page_pte(page) page_pte_prot(page, __pgprot(0))
 #define pmd_page_kernel(pmd) (__va(pmd_val(pmd) & PHYSICAL_PAGE_MASK))
 
+#define pmd_large(pmd) \
+	((pmd_val(pmd) & (_PAGE_PSE|_PAGE_PRESENT)) == (_PAGE_PSE|_PAGE_PRESENT))
+
 /* to find an entry in a page-table-directory. */
 #define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
 #define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
diff -urNp linux-1141/include/asm-x86_64/processor.h linux-1150/include/asm-x86_64/processor.h
--- linux-1141/include/asm-x86_64/processor.h
+++ linux-1150/include/asm-x86_64/processor.h
@@ -264,7 +264,8 @@ static inline void clear_in_cr4 (unsigne
 /*
  * User space process size: 512GB - 1GB (default).
  */
-#define TASK_SIZE	(0x0000007fc0000000)
+#define TASK_SIZE_64	(0x0000007fc0000000)
+#define TASK_SIZE	TASK_SIZE_64
 
 /*
  *  32 bit user space process size - determined by personality
@@ -272,6 +273,9 @@ static inline void clear_in_cr4 (unsigne
 #define IA32_PAGE_OFFSET ((current->personality & ADDR_LIMIT_3GB) ? 0xc0000000 : 0xFFFFe000)
 #define TASK_SIZE_32	IA32_PAGE_OFFSET
 
+#define HUGE_TASK_SIZE	\
+	((current->thread.flags & THREAD_IA32) ? TASK_SIZE_32 : TASK_SIZE_64)
+
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
  */
diff -urNp linux-1141/include/asm-x86_64/tlb.h linux-1150/include/asm-x86_64/tlb.h
--- linux-1141/include/asm-x86_64/tlb.h
+++ linux-1150/include/asm-x86_64/tlb.h
@@ -1 +1,20 @@
+#ifndef _X86_64_TLB_H
+#define _X86_64_TLB_H
+
+/*
+ * x86_64 doesn't need any special per-pte or
+ * per-vma handling..
+ */
+#define tlb_start_vma(tlb, vma) do { } while (0)
+#define tlb_end_vma(tlb, vma) do { } while (0)
+#define __tlb_remove_tlb_entry(tlb, ptep, address) do { } while (0)
+
+/*
+ * .. because we flush the whole mm when it
+ * fills up.
+ */
+#define tlb_flush(tlb) flush_tlb_mm((tlb)->mm)
+
 #include <asm-generic/tlb.h>
+
+#endif
diff -urNp linux-1141/include/linux/fs.h linux-1150/include/linux/fs.h
--- linux-1141/include/linux/fs.h
+++ linux-1150/include/linux/fs.h
@@ -1630,6 +1630,7 @@ extern int generic_osync_inode(struct in
 
 extern int inode_change_ok(struct inode *, struct iattr *);
 extern int inode_setattr(struct inode *, struct iattr *);
+extern int setattr_mask(unsigned int ia_valid);
 
 /*
  * Common dentry functions for inclusion in the VFS
diff -urNp linux-1141/include/linux/hugetlb.h linux-1150/include/linux/hugetlb.h
--- linux-1141/include/linux/hugetlb.h
+++ linux-1150/include/linux/hugetlb.h
@@ -0,0 +1,111 @@
+#ifndef _LINUX_HUGETLB_H
+#define _LINUX_HUGETLB_H
+
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+#include <linux/mm_inline.h>
+
+#ifdef CONFIG_HUGETLB_PAGE
+
+struct ctl_table;
+
+struct hugetlbfs_config {
+	uid_t	uid;
+	gid_t	gid;
+	umode_t	mode;
+};
+
+static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
+{
+	return vma->vm_flags & VM_HUGETLB;
+}
+
+int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void *, size_t *);
+int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
+void huge_page_release(struct page *);
+int hugetlb_report_meminfo(char *);
+int is_hugepage_mem_enough(size_t);
+struct page *follow_huge_addr(struct mm_struct *mm, struct vm_area_struct *vma,
+			unsigned long address, int write);
+struct vm_area_struct *hugepage_vma(struct mm_struct *mm,
+					unsigned long address);
+struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+				pmd_t *pmd, int write);
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
+int pmd_huge(pmd_t pmd);
+int zap_one_hugepage(struct vm_area_struct *vma, unsigned long address, unsigned long size);
+
+extern int htlbpage_max, htlbpool_max;
+
+static inline void
+mark_mm_hugetlb(struct mm_struct *mm, struct vm_area_struct *vma)
+{
+	if (is_vm_hugetlb_page(vma))
+		mm->used_hugetlb = 1;
+}
+
+#ifndef ARCH_HAS_VALID_HUGEPAGE_RANGE
+#define check_valid_hugepage_range(addr, len)	0
+#endif
+
+#else /* !CONFIG_HUGETLB_PAGE */
+
+static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
+{
+	return 0;
+}
+
+#define follow_hugetlb_page(m,v,p,vs,a,b,i)	({ BUG(); 0; })
+#define follow_pin_hugetlb_page(m,v,p,vs,a,b,i)	({ BUG(); 0; })
+#define follow_huge_addr(mm, vma, addr, write)	0
+#define follow_pin_huge_addr(mm, vma, addr, w)	0
+#define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
+#define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
+#define zap_hugepage_range(vma, start, len)	BUG()
+#define unmap_hugepage_range(vma, start, end)	BUG()
+#define huge_page_release(page)			BUG()
+#define is_hugepage_mem_enough(size)		0
+#define hugetlb_report_meminfo(buf)		0
+#define hugepage_vma(mm, addr)			0
+#define mark_mm_hugetlb(mm, vma)		do { } while (0)
+#define follow_huge_pmd(mm, addr, pmd, write)	0
+#define follow_pin_huge_pmd(mm, addr, pmd, w)	0
+#define is_aligned_hugepage_range(addr, len)	0
+#define pmd_huge(x)	0
+#define check_valid_hugepage_range(addr, len)	0
+#define zap_one_hugepage(vma, address, size)	({ BUG(); 0; })
+
+#ifndef HPAGE_MASK
+#define HPAGE_MASK	0		/* Keep the compiler happy */
+#define HPAGE_SIZE	0
+#endif
+
+#endif /* !CONFIG_HUGETLB_PAGE */
+
+#ifdef CONFIG_HUGETLBFS
+extern struct file_operations hugetlbfs_file_operations;
+extern struct vm_operations_struct hugetlb_vm_ops;
+struct file *hugetlb_zero_setup(size_t);
+
+static inline int is_file_hugepages(struct file *file)
+{
+	return file->f_op == &hugetlbfs_file_operations;
+}
+
+static inline void set_file_hugepages(struct file *file)
+{
+	file->f_op = &hugetlbfs_file_operations;
+}
+#else /* !CONFIG_HUGETLBFS */
+
+#define is_file_hugepages(file)		0
+#define set_file_hugepages(file)	BUG()
+#define hugetlb_zero_setup(size)	ERR_PTR(-ENOSYS)
+
+#endif /* !CONFIG_HUGETLBFS */
+
+#endif /* _LINUX_HUGETLB_H */
diff -urNp linux-1141/include/linux/kernel.h linux-1150/include/linux/kernel.h
--- linux-1141/include/linux/kernel.h
+++ linux-1150/include/linux/kernel.h
@@ -28,6 +28,7 @@
 #define STACK_MAGIC	0xdeadbeef
 
 #define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
+#define ALIGN(x,a) (((x)+(a)-1)&~((a)-1))
 
 #define	KERN_EMERG	"<0>"	/* system is unusable			*/
 #define	KERN_ALERT	"<1>"	/* action must be taken immediately	*/
diff -urNp linux-1141/include/linux/mm.h linux-1150/include/linux/mm.h
--- linux-1141/include/linux/mm.h
+++ linux-1150/include/linux/mm.h
@@ -120,6 +120,7 @@ struct vm_area_struct {
 #define VM_RESERVED	0x00080000	/* Don't unmap it from swap_out */
 
 #define VM_ACCOUNT	0x00100000	/* Memory is a vm accounted object */
+#define VM_HUGETLB	0x00400000	/* Huge TLB Page VM */
 #define VM_NO_UNLOCK	0x00800000	/* do not unlock */
 
 /* arches may define VM_STACK_FLAGS for their own purposes */
@@ -219,24 +220,6 @@ typedef struct page {
 } mem_map_t;
 
 /*
- * Methods to modify the page usage count.
- *
- * What counts for a page usage:
- * - cache mapping   (page->mapping)
- * - disk mapping    (page->buffers)
- * - page mapped in a task's page tables, each mapping
- *   is counted separately
- *
- * Also, many kernel routines increase the page count before a critical
- * routine so they can be sure the page doesn't go away from under them.
- */
-#define get_page(p)		atomic_inc(&(p)->count)
-#define put_page(p)		__free_page(p)
-#define put_page_testzero(p) 	atomic_dec_and_test(&(p)->count)
-#define page_count(p)		atomic_read(&(p)->count)
-#define set_page_count(p,v) 	atomic_set(&(p)->count, v)
-
-/*
  * Various page->flags bits:
  *
  * PG_reserved is set for special pages, which can never be swapped
@@ -343,6 +326,8 @@ typedef struct page {
 #define PG_active_cache		19
 #define PG_sync			20
 #define PG_fresh_page		21	/* Page freshly read from disk */
+#define PG_compound		22	/* Part of a compound page */
+
 /* note: don't make page flags of values 24 or higher! */
 
 /* Make it prettier to test the above... */
@@ -533,6 +518,78 @@ extern void FASTCALL(set_page_dirty(stru
 #define SetPageReserved(page)		set_bit(PG_reserved, &(page)->flags)
 #define ClearPageReserved(page)		clear_bit(PG_reserved, &(page)->flags)
 
+#define PageCompound(page)		test_bit(PG_compound, &(page)->flags)
+#define SetPageCompound(page)		set_bit(PG_compound, &(page)->flags)
+#define ClearPageCompound(page)		clear_bit(PG_compound, &(page)->flags)
+
+/*
+ * There is only one 'core' page-freeing function.
+ */
+extern void FASTCALL(__free_pages(struct page *page, unsigned int order));
+extern void FASTCALL(free_pages_ok(struct page *page, unsigned int order));
+extern void FASTCALL(free_pages(unsigned long addr, unsigned int order));
+
+#define __free_page(page) put_page(page)
+#define free_page(addr) free_pages((addr),0)
+
+/*
+ * Methods to modify the page usage count.
+ *
+ * What counts for a page usage:
+ * - cache mapping   (page->mapping)
+ * - disk mapping    (page->buffers)
+ * - page mapped in a task's page tables, each mapping
+ *   is counted separately
+ *
+ * Also, many kernel routines increase the page count before a critical
+ * routine so they can be sure the page doesn't go away from under them.
+ */
+
+extern void show_stack(unsigned long * esp);
+struct page;
+
+#ifdef CONFIG_HUGETLB_PAGE
+
+
+#define get_page(p)							\
+do {									\
+	struct page *___page = (struct page *)(p);			\
+ \
+ \
+ \
+ \
+	if (PageCompound(___page)) {					\
+		___page = (struct page *)___page->lru.next;		\
+		BUG_ON(!PageCompound(___page));				\
+	}								\
+	atomic_inc(&___page->count);					\
+} while (0)
+
+
+#else		/* CONFIG_HUGETLB_PAGE */
+
+#define get_page(p)							\
+do {									\
+	struct page *___page = (struct page *)(p);			\
+ \
+ \
+ \
+ \
+	atomic_inc(&___page->count);					\
+} while (0)
+
+#endif		/* CONFIG_HUGETLB_PAGE */
+
+#define put_page(p)							\
+do {									\
+	struct page *___page = (struct page *)(p);			\
+ \
+ \
+ \
+ \
+	__free_pages(___page, 0);						\
+} while (0)
+
 /*
  * Return true if this page is mapped into pagetables.  Subtle: test pte.direct
  * rather than pte.chain.  Because sometimes pte.direct is 64-bit, and .chain
@@ -588,15 +645,6 @@ extern unsigned long FASTCALL(get_zeroed
  */
 #define get_free_page get_zeroed_page
 
-/*
- * There is only one 'core' page-freeing function.
- */
-extern void FASTCALL(__free_pages(struct page *page, unsigned int order));
-extern void FASTCALL(free_pages(unsigned long addr, unsigned int order));
-
-#define __free_page(page) __free_pages((page), 0)
-#define free_page(addr) free_pages((addr),0)
-
 extern void FASTCALL(fixup_freespace(struct zone_struct *, int));
 extern void show_free_areas(void);
 extern void show_free_areas_node(pg_data_t *pgdat);
@@ -655,10 +703,7 @@ extern void swapin_readahead(swp_entry_t
 extern struct address_space swapper_space;
 #define PageSwapCache(page) ((page)->mapping == &swapper_space)
 
-static inline int is_page_cache_freeable(struct page * page)
-{
-	return page_count(page) - !!page->buffers == 1;
-}
+#define is_page_cache_freeable(page) (page_count(page) - !!page->buffers == 1)
 
 extern int can_share_swap_page(struct page *);
 extern int remove_exclusive_swap_page(struct page *);
diff -urNp linux-1141/include/linux/mm_inline.h linux-1150/include/linux/mm_inline.h
--- linux-1141/include/linux/mm_inline.h
+++ linux-1150/include/linux/mm_inline.h
@@ -28,6 +28,14 @@
 GPL_HEADER()
 
 /*
+ * internal page count APIs - use at your own risk!
+ * they are hugetlb-unaware.
+ */
+#define put_page_testzero(p) 	atomic_dec_and_test(&(p)->count)
+#define page_count(p)		atomic_read(&(p)->count)
+#define set_page_count(p,v) 	atomic_set(&(p)->count, v)
+
+/*
  * These inline functions tend to need bits and pieces of all the
  * other VM include files, meaning they cannot be defined inside
  * one of the other VM include files.
@@ -99,6 +107,7 @@ static inline void add_page_to_active_an
 	struct zone_struct * zone = page_zone(page);
 	DEBUG_LRU_PAGE(page);
 	SetPageActiveAnon(page);
+	BUG_ON(PageCompound(page));
 	list_add(&page->lru, &zone->active_anon_list[age]);
 	page->age = age + zone->anon_age_bias;
 	zone->active_anon_count[age]++;
@@ -110,6 +119,7 @@ static inline void add_page_to_active_ca
 	struct zone_struct * zone = page_zone(page);
 	DEBUG_LRU_PAGE(page);
 	SetPageActiveCache(page);
+	BUG_ON(PageCompound(page));
 	list_add(&page->lru, &zone->active_cache_list[age]);
 	page->age = age + zone->cache_age_bias;
 	zone->active_cache_count[age]++;
@@ -129,6 +139,7 @@ static inline void add_page_to_inactive_
 	struct zone_struct * zone = page_zone(page);
 	DEBUG_LRU_PAGE(page);
 	SetPageInactiveDirty(page);
+	BUG_ON(PageCompound(page));
 	list_add(&page->lru, &zone->inactive_dirty_list);
 	zone->inactive_dirty_pages++;
 }
@@ -138,6 +149,7 @@ static inline void add_page_to_inactive_
 	struct zone_struct * zone = page_zone(page);
 	DEBUG_LRU_PAGE(page);
 	SetPageInactiveLaundry(page);
+	BUG_ON(PageCompound(page));
 	list_add(&page->lru, &zone->inactive_laundry_list);
 	zone->inactive_laundry_pages++;
 }
@@ -147,6 +159,7 @@ static inline void add_page_to_inactive_
 	struct zone_struct * zone = page_zone(page);
 	DEBUG_LRU_PAGE(page);
 	SetPageInactiveClean(page);
+	BUG_ON(PageCompound(page));
 	list_add(&page->lru, &zone->inactive_clean_list);
 	zone->inactive_clean_pages++;
 }
@@ -155,6 +168,8 @@ static inline void del_page_from_active_
 {
 	struct zone_struct * zone = page_zone(page);
 	unsigned char age;
+
+	BUG_ON(PageCompound(page));
 	list_del(&page->lru);
 	ClearPageActiveAnon(page);
 	zone->active_anon_pages--;
@@ -168,6 +183,8 @@ static inline void del_page_from_active_
 {
 	struct zone_struct * zone = page_zone(page);
 	unsigned char age;
+
+	BUG_ON(PageCompound(page));
 	list_del(&page->lru);
 	ClearPageActiveCache(page);
 	zone->active_cache_pages--;
@@ -180,6 +197,8 @@ static inline void del_page_from_active_
 static inline void del_page_from_inactive_dirty_list(struct page * page)
 {
 	struct zone_struct * zone = page_zone(page);
+
+	BUG_ON(PageCompound(page));
 	list_del(&page->lru);
 	ClearPageInactiveDirty(page);
 	zone->inactive_dirty_pages--;
@@ -189,6 +208,8 @@ static inline void del_page_from_inactiv
 static inline void del_page_from_inactive_laundry_list(struct page * page)
 {
 	struct zone_struct * zone = page_zone(page);
+
+	BUG_ON(PageCompound(page));
 	list_del(&page->lru);
 	ClearPageInactiveLaundry(page);
 	zone->inactive_laundry_pages--;
@@ -198,6 +219,8 @@ static inline void del_page_from_inactiv
 static inline void del_page_from_inactive_clean_list(struct page * page)
 {
 	struct zone_struct * zone = page_zone(page);
+
+	BUG_ON(PageCompound(page));
 	list_del(&page->lru);
 	ClearPageInactiveClean(page);
 	zone->inactive_clean_pages--;
diff -urNp linux-1141/include/linux/mmzone.h linux-1150/include/linux/mmzone.h
--- linux-1141/include/linux/mmzone.h
+++ linux-1150/include/linux/mmzone.h
@@ -14,7 +14,7 @@
  * Free memory management - zoned buddy allocator.
  */
 
-#define MAX_ORDER 10
+#define MAX_ORDER 11
 
 typedef struct free_area_struct {
 	struct list_head	free_list;
diff -urNp linux-1141/include/linux/pagemap.h linux-1150/include/linux/pagemap.h
--- linux-1141/include/linux/pagemap.h
+++ linux-1150/include/linux/pagemap.h
@@ -88,6 +88,7 @@ extern struct page *find_trylock_page(st
 extern void add_to_page_cache(struct page * page, struct address_space *mapping, unsigned long index);
 extern void add_to_page_cache_locked(struct page * page, struct address_space *mapping, unsigned long index);
 extern int add_to_page_cache_unique(struct page * page, struct address_space *mapping, unsigned long index, struct page **hash);
+extern int add_to_page_cache_unique_nolru(struct page * page, struct address_space *mapping, unsigned long index, struct page **hash);
 extern wait_queue_head_t *FASTCALL(page_waitqueue(struct page *page));
 
 extern void ___wait_on_page(struct page *);
diff -urNp linux-1141/include/linux/sched.h linux-1150/include/linux/sched.h
--- linux-1141/include/linux/sched.h
+++ linux-1150/include/linux/sched.h
@@ -309,6 +309,9 @@ struct mm_struct {
 		unsigned long	writable;
 	}		mm_stat;
 
+#ifdef CONFIG_HUGETLB_PAGE
+	int used_hugetlb;
+#endif
 	/* Architecture-specific MM context */
 	mm_context_t context;
 
diff -urNp linux-1141/include/linux/shm.h linux-1150/include/linux/shm.h
--- linux-1141/include/linux/shm.h
+++ linux-1150/include/linux/shm.h
@@ -75,6 +75,7 @@ struct shm_info {
 /* shm_mode upper byte flags */
 #define	SHM_DEST	01000	/* segment will be destroyed on last detach */
 #define SHM_LOCKED      02000   /* segment will not be swapped */
+#define SHM_HUGETLB	04000	/* segment will use huge TLB pages */
 
 asmlinkage long sys_shmget (key_t key, size_t size, int flag);
 asmlinkage long sys_shmat (int shmid, char *shmaddr, int shmflg, unsigned long *addr);
diff -urNp linux-1141/include/linux/sysctl.h linux-1150/include/linux/sysctl.h
--- linux-1141/include/linux/sysctl.h
+++ linux-1150/include/linux/sysctl.h
@@ -149,6 +149,7 @@ enum
 	VM_MIN_READAHEAD=12,    /* Min file readahead */
 	VM_MAX_READAHEAD=13,    /* Max file readahead */
 	VM_PAGEBUF=22,		/* struct: Control pagebuf parameters */
+	VM_HUGETLB_POOL=23,	/* int: size of the hugetlb pool, in MB */
 };
 
 
diff -urNp linux-1141/include/net/irda/irda.h linux-1150/include/net/irda/irda.h
--- linux-1141/include/net/irda/irda.h
+++ linux-1150/include/net/irda/irda.h
@@ -54,8 +54,8 @@ typedef __u32 magic_t;
 #define IRDA_MIN(a, b) (((a) < (b)) ? (a) : (b))
 #endif
 
-#ifndef ALIGN
-#  define ALIGN __attribute__((aligned))
+#ifndef FIELD_ALIGN
+#  define FIELD_ALIGN __attribute__((aligned))
 #endif
 #ifndef PACK
 #  define PACK __attribute__((packed))
diff -urNp linux-1141/include/net/irda/irqueue.h linux-1150/include/net/irda/irqueue.h
--- linux-1141/include/net/irda/irqueue.h
+++ linux-1150/include/net/irda/irqueue.h
@@ -49,8 +49,8 @@
 #define HASHBIN_SIZE   8
 #define HASHBIN_MASK   0x7
 
-#ifndef ALIGN 
-#define ALIGN __attribute__((aligned))
+#ifndef FIELD_ALIGN 
+#define FIELD_ALIGN __attribute__((aligned))
 #endif
 
 #define Q_NULL { NULL, NULL, "", 0 }
@@ -75,8 +75,8 @@ typedef struct hashbin_t {
 	__u32      magic;
 	int        hb_type;
 	int        hb_size;
-	spinlock_t hb_mutex[HASHBIN_SIZE] ALIGN;
-	irda_queue_t   *hb_queue[HASHBIN_SIZE] ALIGN;
+	spinlock_t hb_mutex[HASHBIN_SIZE] FIELD_ALIGN;
+	irda_queue_t   *hb_queue[HASHBIN_SIZE] FIELD_ALIGN;
 
 	irda_queue_t* hb_current;
 } hashbin_t;
diff -urNp linux-1141/ipc/shm.c linux-1150/ipc/shm.c
--- linux-1141/ipc/shm.c
+++ linux-1150/ipc/shm.c
@@ -22,6 +22,7 @@
 #include <linux/file.h>
 #include <linux/mman.h>
 #include <linux/proc_fs.h>
+#include <linux/hugetlb.h>
 #include <asm/uaccess.h>
 
 #include "util.h"
@@ -126,9 +127,10 @@ static void shm_destroy (struct shmid_ke
 	shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	shm_rmid (shp->id);
 	shm_unlock_deleted(shp);
-	shmem_lock(shp->shm_file, 0,
-		&shp->shm_locker_mm,
-		&shp->shm_locker_pid);
+	if (!is_file_hugepages(shp->shm_file))
+		shmem_lock(shp->shm_file, 0,
+			&shp->shm_locker_mm,
+			&shp->shm_locker_pid);
 	fput (shp->shm_file);
 	kfree (shp);
 }
@@ -178,6 +180,13 @@ static struct vm_operations_struct shm_v
 	nopage:	shmem_nopage,
 };
 
+static int want_hugetlb(int shmflg, size_t size)
+{
+	if (shmflg & SHM_HUGETLB)
+		return 1;
+	return 0;
+}
+
 static int newseg (key_t key, int shmflg, size_t size)
 {
 	int error;
@@ -196,8 +205,12 @@ static int newseg (key_t key, int shmflg
 	shp = (struct shmid_kernel *) kmalloc (sizeof (*shp), GFP_USER);
 	if (!shp)
 		return -ENOMEM;
-	sprintf (name, "SYSV%08x", key);
-	file = shmem_file_setup(name, size);
+	if (want_hugetlb(shmflg, size))
+		file = hugetlb_zero_setup(size);
+	else {
+		sprintf (name, "SYSV%08x", key);
+		file = shmem_file_setup(name, size);
+	}
 	error = PTR_ERR(file);
 	if (IS_ERR(file))
 		goto no_file;
@@ -217,7 +230,10 @@ static int newseg (key_t key, int shmflg
 	shp->id = shm_buildid(id,shp->shm_perm.seq);
 	shp->shm_file = file;
 	file->f_dentry->d_inode->i_ino = shp->id;
-	file->f_op = &shm_file_operations;
+	if (want_hugetlb(shmflg, size))
+		set_file_hugepages(file);
+	else
+		file->f_op = &shm_file_operations;
 	shm_tot += numpages;
 	shm_unlock (id);
 	return shp->id;
@@ -351,27 +367,35 @@ static inline unsigned long copy_shminfo
 	}
 }
 
-static void shm_get_stat (unsigned long *rss, unsigned long *swp) 
+static void shm_get_stat(unsigned long *rss, unsigned long *swp) 
 {
-	struct shmem_inode_info *info;
 	int i;
 
 	*rss = 0;
 	*swp = 0;
 
-	for(i = 0; i <= shm_ids.max_id; i++) {
-		struct shmid_kernel* shp;
-		struct inode * inode;
+	for (i = 0; i <= shm_ids.max_id; i++) {
+		struct shmid_kernel *shp;
+		struct inode *inode;
 
 		shp = shm_get(i);
-		if(shp == NULL)
+		if(!shp)
 			continue;
+
 		inode = shp->shm_file->f_dentry->d_inode;
-		info = SHMEM_I(inode);
-		spin_lock (&info->lock);
-		*rss += inode->i_mapping->nrpages;
-		*swp += info->swapped;
-		spin_unlock (&info->lock);
+
+		if (is_file_hugepages(shp->shm_file)) {
+			struct address_space *mapping = inode->i_mapping;
+			spin_lock(&mapping->i_shared_lock);
+			*rss += (HPAGE_SIZE/PAGE_SIZE)*mapping->nrpages;
+			spin_unlock(&mapping->i_shared_lock);
+		} else {
+			struct shmem_inode_info *info = SHMEM_I(inode);
+			spin_lock(&info->lock);
+			*rss += inode->i_mapping->nrpages;
+			*swp += info->swapped;
+			spin_unlock(&info->lock);
+		}
 	}
 }
 
@@ -478,15 +502,18 @@ asmlinkage long sys_shmctl (int shmid, i
 			goto out_unlock;
 		}
 		if(cmd==SHM_LOCK) {
-			err = shmem_lock(shp->shm_file, 1,
-					&shp->shm_locker_mm,
-					&shp->shm_locker_pid);
+			err = 0;
+			if (!is_file_hugepages(shp->shm_file))
+				err = shmem_lock(shp->shm_file, 1,
+						&shp->shm_locker_mm,
+						&shp->shm_locker_pid);
 			if (!err)
 				shp->shm_flags |= SHM_LOCKED;
 		} else {
-			shmem_lock(shp->shm_file, 0,
-					&shp->shm_locker_mm,
-					&shp->shm_locker_pid);
+			if (!is_file_hugepages(shp->shm_file))
+				shmem_lock(shp->shm_file, 0,
+						&shp->shm_locker_mm,
+						&shp->shm_locker_pid);
 			shp->shm_flags &= ~SHM_LOCKED;
 		}
 		shm_unlock(shmid);
@@ -687,7 +714,7 @@ asmlinkage long sys_shmdt (char *shmaddr
 	down_write(&mm->mmap_sem);
 	for (shmd = mm->mmap; shmd; shmd = shmdnext) {
 		shmdnext = shmd->vm_next;
-		if (shmd->vm_ops == &shm_vm_ops
+		if ((shmd->vm_ops == &shm_vm_ops || is_vm_hugetlb_page(shmd))
 		    && shmd->vm_start - (shmd->vm_pgoff << PAGE_SHIFT) == (ulong) shmaddr) {
 			do_munmap(mm, shmd->vm_start, shmd->vm_end - shmd->vm_start, 1);
 			retval = 0;
diff -urNp linux-1141/kernel/sysctl.c linux-1150/kernel/sysctl.c
--- linux-1141/kernel/sysctl.c
+++ linux-1150/kernel/sysctl.c
@@ -31,6 +31,7 @@
 #include <linux/sysrq.h>
 #include <linux/highuid.h>
 #include <linux/aio.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 
@@ -84,6 +85,7 @@ extern int msg_ctlmax;
 extern int msg_ctlmnb;
 extern int msg_ctlmni;
 extern int sem_ctls[];
+extern long nr_hugepages;
 #endif
 
 #ifdef __sparc__
@@ -340,6 +342,10 @@ static ctl_table vm_table[] = {
 	&vm_max_readahead,sizeof(int), 0644, NULL, &proc_dointvec},
 	{VM_MAX_MAP_COUNT, "max_map_count",
 	 &max_map_count, sizeof(int), 0644, NULL, &proc_dointvec},
+#ifdef CONFIG_HUGETLB_PAGE
+	{VM_HUGETLB_POOL, "hugetlb_pool", &htlbpool_max, sizeof(int), 0644,
+			NULL, &hugetlb_sysctl_handler},
+#endif
 	{0}
 };
 
diff -urNp linux-1141/mm/bootmem.c linux-1150/mm/bootmem.c
--- linux-1141/mm/bootmem.c
+++ linux-1150/mm/bootmem.c
@@ -9,6 +9,7 @@
  *  system memory and memory holes as well.
  */
 
+#include <linux/mm_inline.h>
 #include <linux/mm.h>
 #include <linux/kernel_stat.h>
 #include <linux/swap.h>
diff -urNp linux-1141/mm/filemap.c linux-1150/mm/filemap.c
--- linux-1141/mm/filemap.c
+++ linux-1150/mm/filemap.c
@@ -706,6 +706,26 @@ int add_to_page_cache_unique(struct page
 	return err;
 }
 
+int add_to_page_cache_unique_nolru(struct page * page,
+	struct address_space *mapping, unsigned long offset,
+	struct page **hash)
+{
+	int err;
+	struct page *alias;
+
+	lock_pagecache();
+	alias = __find_page_nolock(mapping, offset, *hash);
+
+	err = 1;
+	if (!alias) {
+		__add_to_page_cache(page,mapping,offset,hash);
+		err = 0;
+	}
+	unlock_pagecache();
+
+	return err;
+}
+
 /*
  * This adds the requested page to the page cache if it isn't already there,
  * and schedules an I/O to read in its contents from disk.
diff -urNp linux-1141/mm/memory.c linux-1150/mm/memory.c
--- linux-1141/mm/memory.c
+++ linux-1150/mm/memory.c
@@ -48,6 +48,7 @@
 #include <linux/vcache.h>
 #include <linux/slab.h>
 #include <linux/mm_inline.h>
+#include <linux/hugetlb.h>
 
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -63,6 +64,19 @@ struct page *highmem_start_page;
 void vm_account(struct vm_area_struct *vma, pte_t pte, unsigned long address, long adj)
 {
 	struct mm_struct *mm = vma->vm_mm;
+
+	if (is_vm_hugetlb_page(vma)) {
+		int num_pages = (HPAGE_SIZE/PAGE_SIZE) * adj;
+		mm->mm_stat.present += num_pages;
+		mm->mm_stat.rss += num_pages;
+		mm->mm_stat.drs += num_pages;
+		if (vma->vm_flags & VM_WRITE)
+			mm->mm_stat.writable += num_pages;
+		if (!(vma->vm_flags & VM_DONTCOPY))
+			mm->mm_stat.sharable += num_pages;			
+		return;
+	}
+		
 	if (pte_present(pte)) {
 		struct page *page = pte_page(pte);
 		if (VALID_PAGE(page) && !PageReserved(page)) {
@@ -162,7 +176,7 @@ void __free_pte(pte_t pte)
  * Note: this doesn't free the actual pages themselves. That
  * has been handled earlier when unmapping all the memory regions.
  */
-static inline void free_one_pmd(pmd_t * dir)
+void free_one_pmd(pmd_t * dir)
 {
 	struct page *pte;
 
@@ -318,6 +332,9 @@ int copy_page_range(struct mm_struct *ds
 	unsigned long cow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
 	struct pte_chain * pte_chain = NULL;
 
+	if (is_vm_hugetlb_page(vma))
+		return copy_hugetlb_page_range(dst, src, vma);
+
 	pte_chain = pte_chain_alloc(GFP_ATOMIC);
 	if (!pte_chain) {
 		spin_unlock(&dst->page_table_lock);
@@ -475,6 +492,8 @@ static inline int zap_pte_range(mmu_gath
 	pte_t * ptep, *mapping;
 	int freed = 0;
 
+	if (unlikely(pmd_large(*pmd)))
+		return zap_one_hugepage(tlb_vma(tlb), address, size);
 	if (pmd_none(*pmd))
 		return 0;
 	if (pmd_bad(*pmd)) {
@@ -536,7 +555,14 @@ static inline int zap_pmd_range(mmu_gath
 	return freed;
 }
 
-#define ZAP_BLOCK_SIZE	(256 * PAGE_SIZE)
+#if !defined(CONFIG_HUGETLB_PAGE)
+# define ZAP_BLOCK_SIZE	(256 * PAGE_SIZE)
+#else
+/*
+ * In the hugetlb case we want the zapping to be hugetlb-size aligned:
+ */
+# define ZAP_BLOCK_SIZE	HPAGE_SIZE
+#endif
 
 /**
  * zap_page_range - remove user pages in a given range
@@ -552,7 +578,7 @@ void zap_page_range(struct vm_area_struc
 	pgd_t * dir;
 	unsigned long start, end, addr, block;
 	int freed;
- 
+
 	/*
 	 * Break the work up into blocks of ZAP_BLOCK_SIZE pages:
 	 * this decreases lock-hold time for the page_table_lock
@@ -605,20 +631,31 @@ void zap_page_range(struct vm_area_struc
 #endif
 
 /*
- * Do a quick page-table lookup for a single page. 
+ * Do a quick page-table lookup for a single page.
+ * mm->page_table_lock must be held.
  */
-struct page * follow_page(struct mm_struct *mm, unsigned long address, int write) 
+struct page *
+follow_page(struct mm_struct *mm, unsigned long address, int write) 
 {
 	pgd_t *pgd;
 	pmd_t *pmd;
 	pte_t *ptep, pte;
+	struct vm_area_struct *vma;
+
+	vma = hugepage_vma(mm, address);
+	if (vma)
+		return follow_huge_addr(mm, vma, address, write);
 
 	pgd = pgd_offset(mm, address);
 	if (pgd_none(*pgd) || pgd_bad(*pgd))
 		goto out;
 
 	pmd = pmd_offset(pgd, address);
-	if (pmd_none(*pmd) || pmd_bad(*pmd))
+	if (pmd_none(*pmd))
+		goto out;
+	if (pmd_huge(*pmd))
+		return follow_huge_pmd(mm, address, pmd, write);
+	if (pmd_bad(*pmd))
 		goto out;
 
 	ptep = pte_offset_map(pmd, address);
@@ -631,8 +668,10 @@ struct page * follow_page(struct mm_stru
 		struct page *page = pte_page(pte);
 		prefetch(page);
 		if (!write ||
-		    (pte_write(pte) && pte_dirty(pte)))
+		    (pte_write(pte) && pte_dirty(pte))) {
+			BUG_ON(!pfn_valid(pte_pfn(pte)));
 			return pte_page(pte);
+		}
 	}
 
 out:
@@ -707,7 +746,11 @@ int get_user_pages(struct task_struct *t
 
 		if ( !vma || (pages && vma->vm_flags & VM_IO) || !(flags & vma->vm_flags) )
 			return i ? : -EFAULT;
-
+		if (is_vm_hugetlb_page(vma)) {
+			i = follow_hugetlb_page(mm, vma, pages, vmas,
+						&start, &len, i);
+			continue;
+		}
 		spin_lock(&mm->page_table_lock);
 		do {
 			struct page *map;
diff -urNp linux-1141/mm/mmap.c linux-1150/mm/mmap.c
--- linux-1141/mm/mmap.c
+++ linux-1150/mm/mmap.c
@@ -33,6 +33,7 @@
 #include <linux/personality.h>
 #include <linux/compiler.h>
 #include <linux/profile.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgalloc.h>
@@ -1167,6 +1168,13 @@ int do_munmap(struct mm_struct *mm, unsi
 	    && mm->map_count >= max_map_count)
 		return -ENOMEM;
 
+	/* Check for alignment of hugetlbfs regions */
+	if (is_vm_hugetlb_page(mpnt)) {
+		int ret = is_aligned_hugepage_range(addr, len);
+		if (ret)
+			return ret;
+	}
+
 	/* Something will probably happen, so notify. */
 	if (mpnt->vm_file && (mpnt->vm_flags & VM_EXEC))
 		profile_exec_unmap(mm);
diff -urNp linux-1141/mm/mprotect.c linux-1150/mm/mprotect.c
--- linux-1141/mm/mprotect.c
+++ linux-1150/mm/mprotect.c
@@ -25,6 +25,7 @@
 #include <linux/shm.h>
 #include <linux/mman.h>
 #include <linux/highmem.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgalloc.h>
@@ -377,6 +378,11 @@ asmlinkage long sys_mprotect(unsigned lo
 		unsigned int newflags;
 		int last = 0;
 
+		if (is_vm_hugetlb_page(vma)) {
+			error = -EACCES;
+			goto out;
+		}
+
 		/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */
 
 		newflags = prot | (vma->vm_flags & ~(PROT_READ | PROT_WRITE | PROT_EXEC));
diff -urNp linux-1141/mm/mremap.c linux-1150/mm/mremap.c
--- linux-1141/mm/mremap.c
+++ linux-1150/mm/mremap.c
@@ -27,6 +27,7 @@
 #include <linux/mman.h>
 #include <linux/swap.h>
 #include <linux/highmem.h>
+#include <linux/hugetlb.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgalloc.h>
@@ -338,6 +339,11 @@ unsigned long do_mremap(unsigned long ad
 	vma = find_vma(current->mm, addr);
 	if (!vma || vma->vm_start > addr)
 		goto out;
+	if (is_vm_hugetlb_page(vma)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
 	/* We can't remap across vm area boundaries */
 	if (old_len > vma->vm_end - addr)
 		goto out;
diff -urNp linux-1141/mm/page_alloc.c linux-1150/mm/page_alloc.c
--- linux-1141/mm/page_alloc.c
+++ linux-1150/mm/page_alloc.c
@@ -62,6 +62,73 @@ static int zone_extrafree_max[MAX_NR_ZON
 	|| ((zone) != page_zone(page))					\
 )
 
+#ifndef CONFIG_HUGETLB_PAGE
+#define prep_compound_page(page, order) do { } while (0)
+#define destroy_compound_page(page, order) do { } while (0)
+#else
+/*
+ * Higher-order pages are called "compound pages".  They are structured thusly:
+ *
+ * The first PAGE_SIZE page is called the "head page".
+ *
+ * The remaining PAGE_SIZE pages are called "tail pages".
+ *
+ * All pages have PG_compound set.  All pages have their lru.next pointing at
+ * the head page (even the head page has this).
+ *
+ * The head page's lru.prev, if non-zero, holds the address of the compound
+ * page's put_page() function.
+ *
+ * The order of the allocation is stored in the first tail page's lru.prev.
+ * This is only for debug at present.  This usage means that zero-order pages
+ * may not be compound.
+ */
+static void prep_compound_page(struct page *page, unsigned long order)
+{
+	int i;
+	int nr_pages = 1 << order;
+
+	BUG_ON(page_count(page) != 1);
+	page->lru.prev = NULL;
+	page[1].lru.prev = (void *)order;
+	for (i = 0; i < nr_pages; i++) {
+		struct page *p = page + i;
+
+		SetPageCompound(p);
+		p->lru.next = (void *)page;
+		if (unlikely(i && page_count(p))) {
+			printk("prep_compound_page(): incorrect sub-page count %08x, of page %016Lx(%08lx).\n", page_count(p), (unsigned long long)(p-mem_map)*PAGE_SIZE, p->flags);
+			set_page_count(p, 0);
+		}
+		BUG_ON(p->mapping);
+	}
+}
+
+static void destroy_compound_page(struct page *page, unsigned long order)
+{
+	int i;
+	int nr_pages = 1 << order;
+
+	BUG_ON(page[1].lru.prev != (void *)order);
+
+	for (i = 0; i < nr_pages; i++) {
+		struct page *p = page + i;
+
+		BUG_ON(!PageCompound(p));
+		BUG_ON(page_count(p));
+		if (p->lru.next != (void *)page) {
+			printk("ugh - idx %d, page %p, order %ld.\n",
+					i, page, order);
+			printk("p->lru.next (%p) != page (%p)\n",
+					p->lru.next, page);
+			BUG();
+		}
+		BUG_ON(p->mapping);
+		ClearPageCompound(p);
+	}
+}
+#endif		/* CONFIG_HUGETLB_PAGE */
+
 /*
  * Freeing function for a buddy system allocator.
  * Contrary to prior comments, this is *NOT* hairy, and there
@@ -130,6 +197,15 @@ static void __free_pages_ok (struct page
 		BUG();
 	if (page->pte.direct)
 		BUG();
+	if (page_count(page)) {
+		static int once = 1;
+		printk("free_pages_ok(): incorrect sub-page count %08x, of page %016Lx(%08lx).\n", page_count(page), (unsigned long long)(page-mem_map)*PAGE_SIZE, page->flags);
+		if (once) {
+			once = 0;
+			show_stack(NULL);
+		}
+
+	}
 	ClearPageReferenced(page);
 	ClearPageDirty(page);
 	ClearPageFresh(page);
@@ -157,7 +233,8 @@ static void __free_pages_ok (struct page
 	}
 
 	spin_lock(&zone->lock);
-
+	if (order)
+		destroy_compound_page(page, order);
 	zone->free_pages -= mask;
 
 	while (mask + (1 << (MAX_ORDER-1))) {
@@ -269,6 +346,8 @@ static struct page * rmqueue(zone_t *zon
 			if (BAD_RANGE(zone,page))
 				BUG();
 			DEBUG_LRU_PAGE(page);
+			if (order)
+				prep_compound_page(page, order);
 			return page;	
 		}
 		curr_order++;
@@ -299,8 +378,12 @@ void fixup_freespace(zone_t * zone, int 
 		struct page * page;
 		int worktodo = max_t(int, 64, zone->pages_min-zone->free_pages);
 		do {
-			if ((page = reclaim_page(zone)))
-				__free_pages_ok(page, 0);
+			if ((page = reclaim_page(zone))) {
+				if (page_count(page) != 1)
+					printk("fixup_freespace(): incorrect sub-page count %08x, of page %016Lx(%08lx).\n", page_count(page), (unsigned long long)(page-mem_map)*PAGE_SIZE, page->flags);
+					set_page_count(page, 0);
+					__free_pages_ok(page, 0);
+			}
 		} while (page && worktodo-- > 0);
 	}
 }
@@ -667,6 +750,24 @@ unsigned long get_zeroed_page(unsigned i
 
 void __free_pages(struct page *page, unsigned int order)
 {
+	if (PageCompound(page)) {
+		page = (struct page *)page->lru.next;
+		BUG_ON(!PageCompound(page));
+
+		if (put_page_testzero(page)) {
+			if (page->lru.prev)	/* destructor? */
+				(*(void(*)(struct page *))page->lru.prev)(page);
+			else
+				__free_pages_ok(page, order);
+		}
+		return;
+	}
+	if (!PageReserved(page) && put_page_testzero(page))
+		__free_pages_ok(page, order);
+}
+
+void free_pages_ok(struct page *page, unsigned int order)
+{
 	if (!PageReserved(page) && put_page_testzero(page))
 		__free_pages_ok(page, order);
 }
diff -urNp linux-1141/mm/swapfile.c linux-1150/mm/swapfile.c
--- linux-1141/mm/swapfile.c
+++ linux-1150/mm/swapfile.c
@@ -5,6 +5,7 @@
  *  Swap reorganised 29.12.95, Stephen Tweedie
  */
 
+#include <linux/mm_inline.h>
 #include <linux/slab.h>
 #include <linux/smp_lock.h>
 #include <linux/kernel_stat.h>
